FN Clarivate Analytics Web of Science
VR 1.0
PT J
AU Charalampous, K
   Kostavelis, I
   Gasteratos, A
AF Charalampous, Konstantinos
   Kostavelis, Ioannis
   Gasteratos, Antonios
TI Robot navigation in large-scale social maps: An action recognition
   approach
SO EXPERT SYSTEMS WITH APPLICATIONS
LA English
DT Article
DE Robot navigation; Action recognition; Social mapping; Human
   environments; Proxemics
AB As robots tend to establish their presence in every day human environments the necessity for them to attain socially acceptable behavior is a condition sine qua non. Consequently, robots need to learn and react appropriately, should they be able to share the same space with people and to reconcile their operation to man's activity. This work proposes an integrated robot framework that allows navigation in a human populated environment. This is the first work that employs the performed actions of individuals so as to re-plan and design a collision-free and at the same time a socially acceptable trajectory. Expandability is another feature of the suggested mapping module since it is capable of incorporating an unconstrained number of actions and subsequently responses, according to the needs of the task in hand and the environment in which the robot operates. Moreover, the paper addresses the integration of the proposed mapping module with the rest of the robot framework in order to operate in a seamless fashion. The generic design of this architecture allows the replacement of modules with other similar ones, thus providing adaptability with respect to the environment and so on. The method utilizes off-line constructed 3D metric maps organized in terms of a topological graph. During its perambulation the robot is ample to detect humans while it exploits deep learning strategies to recognize their activities. The memorized actions are seamlessly associated with specific rules -deriving from the proxemics theory-and are organized in an efficient manner to be recalled during robot's navigation. Moreover, the paper exhibits the differences of the robot navigation in inhabited and uninhabited environments and demonstrates the alteration of the robot's trajectory with respect to the recognized actions and poses of the individuals. The system has been evaluated on a robot able to acquire RGB-D data in domestic environments. The human detection and the action recognition modules exhibited remarkable performance, the human detection one was flawless about its decision while the action recognition one confused actions regarding the number of individuals that participate in them. Last, the robot navigation component was proved capable of extracting safe trajectories in human populated environments. (C) 2016 Elsevier Ltd. All rights reserved.
C1 [Charalampous, Konstantinos; Kostavelis, Ioannis; Gasteratos, Antonios] Democritus Univ Thrace, Prod & Management Engn Dept, Lab Robot & Automat, Vas Sophias 12, GR-67100 Xanthi, Greece.
RP Charalampous, K (reprint author), Democritus Univ Thrace, Prod & Management Engn Dept, Lab Robot & Automat, Vas Sophias 12, GR-67100 Xanthi, Greece.
EM kchara@pme.duth.gr; gkostave@pme.duth.gr; agaster@pme.duth.gr
RI Gasteratos, Antonios/B-7796-2012
OI Gasteratos, Antonios/0000-0002-5421-0332; Kostavelis,
   Ioannis/0000-0003-2882-2914
CR ARGYLE M, 1965, SOCIOMETRY, V28, P289, DOI 10.2307/2786027
   Argyle M., 2013, BODILY COMMUNICATION
   BALDASSARE M, 1978, ANNU REV SOCIOL, V4, P29, DOI 10.1146/annurev.so.04.080178.000333
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Boukas E, 2015, IEEE T AUTOM SCI ENG, V12, P739, DOI 10.1109/TASE.2014.2323175
   BURGOON JK, 1978, UNSPOKEN DIALOGUE IN
   Charalampous K., 2015, ROBOTICS AUTONOMOUS
   Charrel RN, 2016, B WORLD HEALTH ORGAN, V94, P574, DOI 10.2471/BLT.16.171207
   Dayoub F, 2013, IEEE INT C INT ROBOT, P1923, DOI 10.1109/IROS.2013.6696611
   Dollar P., 2010, BMVC, V2, P7
   Ferrer G, 2014, IEEE INT C INT ROBOT, P1730, DOI 10.1109/IROS.2014.6942788
   Ferrer G, 2013, 2013 EUROPEAN CONFERENCE ON MOBILE ROBOTS (ECMR 2013), P331, DOI 10.1109/ECMR.2013.6698863
   Ferrer G, 2013, IEEE INT C INT ROBOT, P1688, DOI 10.1109/IROS.2013.6696576
   Fox D, 2001, ADV NEURAL INFORM PR, V14
   Hall E. T., 1969, HIDDEN DIMENSION 199
   HELBING D, 1995, PHYS REV E, V51, P4282, DOI 10.1103/PhysRevE.51.4282
   Hernandez E, 2014, EXPERT SYST APPL, V41, P2897, DOI 10.1016/j.eswa.2013.10.024
   Hidalgo-Paniagua A, 2016, EXPERT SYST APPL, V58, P20, DOI 10.1016/j.eswa.2016.03.035
   Kirby R., 2010, THESIS
   Koay K. L, 2014, INT J SOCIAL ROBOTIC
   Koenig S, 2005, IEEE T ROBOT, V21, P354, DOI 10.1109/TRO.2004.838026
   Koenig S., 2001, NIPS, P1539
   Kosmatopoulos EB, 2009, IEEE T NEURAL NETWOR, V20, P1009, DOI 10.1109/TNN.2009.2014061
   Kostavelis I, 2015, ROBOT AUTON SYST, V66, P86, DOI 10.1016/j.robot.2014.12.006
   Kostavelis I, 2013, ROBOT AUTON SYST, V61, P1460, DOI 10.1016/j.robot.2013.07.008
   Labayrade R, 2002, IV'2002: IEEE INTELLIGENT VEHICLE SYMPOSIUM, PROCEEDINGS, P646
   Luber  M., 2013, ROBOTICS SCI SYSTEMS
   Luber M, 2012, IEEE INT C INT ROBOT, P902, DOI 10.1109/IROS.2012.6385716
   Martinez J., 2013, THESIS
   Mead R., 2011, P 2011 ROB SCI SYST
   Mead R, 2012, ICMI '12: PROCEEDINGS OF THE ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P333
   Mitka E, 2012, SAFETY SCI, V50, P1888, DOI 10.1016/j.ssci.2012.05.009
   O'Callaghan ST, 2011, IEEE INT CONF ROBOT, DOI 10.1109/ICRA.2011.5980478
   Pacchierotti E, 2006, SPRINGER TRAC ADV RO, V25, P293
   Papadakis P, 2013, IEEE INT C INT ROBOT, P1701, DOI 10.1109/IROS.2013.6696578
   Poppe R, 2010, IMAGE VISION COMPUT, V28, P976, DOI 10.1016/j.imavis.2009.11.014
   Rios-Martinez J, 2012, IEEE INT CONF ROBOT, P2880, DOI 10.1109/ICRA.2012.6224934
   Sisbot EA, 2007, IEEE T ROBOT, V23, P874, DOI 10.1109/TRO.2007.904911
   Svenstrup M, 2010, IEEE INT C INT ROBOT, P4293, DOI 10.1109/IROS.2010.5651531
   Tadokoro S., 1995, P IEEE RSJ INT C INT, V2, P518
   Takayama L, 2009, 2009 IEEE-RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, P5495, DOI 10.1109/IROS.2009.5354145
   Torta Elena, 2011, Social Robotics. Proceedings Third International Conference (ICSR 2011), P21, DOI 10.1007/978-3-642-25504-5_3
   VIOLA P, 2003, P 9 IEEE INT C COMP, V2, P734, DOI DOI 10.1109/ICCV.2003.1238422
   Wang H, 2009, BMVC 2009 BRIT MACH
NR 44
TC 19
Z9 19
U1 8
U2 108
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0957-4174
EI 1873-6793
J9 EXPERT SYST APPL
JI Expert Syst. Appl.
PD DEC 30
PY 2016
VL 66
BP 261
EP 273
DI 10.1016/j.eswa.2016.09.026
PG 13
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic; Operations Research & Management Science
SC Computer Science; Engineering; Operations Research & Management Science
GA EA1AX
UT WOS:000386321800021
DA 2020-02-19
ER

PT J
AU Gando, G
   Yamada, T
   Sato, H
   Oyama, S
   Kurihara, M
AF Gando, Gota
   Yamada, Taiga
   Sato, Haruhiko
   Oyama, Satoshi
   Kurihara, Masahito
TI Fine-tuning deep convolutional neural networks for distinguishing
   illustrations from photographs
SO EXPERT SYSTEMS WITH APPLICATIONS
LA English
DT Article
DE Aggregation systems; Machine learning; Deep learning; Illustrations
AB Systems for aggregating illustrations require a function for automatically distinguishing illustrations from photographs as they crawl the network to collect images. A previous attempt to implement this functionality by designing basic features that were deemed useful for classification achieved an accuracy of only about 58%. On the other hand, deep neural networks had been successful in computer vision tasks, and convolutional neural networks (CNNs) had performed good at extracting such useful image features automatically. We evaluated alternative methods to implement this classification functionality with focus on deep neural networks. As the result of experiments, the method that fine-tuned deep convolutional neural network (DCNN) acquired 96.8% accuracy, outperforming the other models including the custom CNN models that were trained from scratch. We conclude that DCNN with fine-tuning is the best method for implementing a function for automatically distinguishing illustrations from photographs. (C) 2016 Elsevier Ltd. All rights reserved.
C1 [Gando, Gota] Alt Inc, Koto Ku, Aomi 2-7-4, Tokyo, Japan.
   [Yamada, Taiga] Panasonic Syst Design Co Ltd, Kohoku Ku, Shinyokohama 3-1-9, Yokohama, Kanagawa, Japan.
   [Sato, Haruhiko] Hokkai Gakuen Univ, Dept Elect & Informat Engn, Fac Engn, Toyohira Ku, Asahimachi 4-1-40, Sapporo, Hokkaido, Japan.
   [Oyama, Satoshi; Kurihara, Masahito] Hokkaido Univ, Grad Sch Informat Sci & Technol, Kita Ku, Kita 14 Nishi 9, Sapporo, Hokkaido, Japan.
RP Gando, G (reprint author), Alt Inc, Koto Ku, Aomi 2-7-4, Tokyo, Japan.
EM gando@complex.ist.hokudai.ac.jp; t-ymd@complex.ist.hokudai.ac.jp;
   haru@complex.ist.hokudai.ac.jp; oyama@complex.ist.hokudai.ac.jp;
   kurihara@complex.ist.hokudai.ac.jp
CR Athitsos V, 1997, IEEE WORKSHOP ON CONTENT-BASED ACCESS OF IMAGE AND VIDEO LIBRARIES, PROCEEDINGS, P10, DOI 10.1109/IVL.1997.629715
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Bishop CM, 1995, NEURAL NETWORKS PATT
   Chapelle O, 1999, IEEE T NEURAL NETWOR, V10, P1055, DOI 10.1109/72.788646
   Connolly C, 1997, IEEE T IMAGE PROCESS, V6, P1046, DOI 10.1109/83.597279
   Csurka G., 2004, WORKSH STAT LEARN CO, V1, P1, DOI DOI 10.1234/12345678
   Cutzu F., 2003, CYPR02, DOI [10.1109/CVPR.2003.1211484, DOI 10.1109/CVPR.2003.1211484]
   Deng J., 2009, CYPR09
   Donahue J., 2013, CORR
   Fergus Rob, 2013, CORR
   FUKUSHIMA K, 1980, BIOL CYBERN, V36, P193, DOI 10.1007/BF00344251
   Gatys Leon A., 2015, CORR
   HUBEL DH, 1968, J PHYSIOL-LONDON, V195, P215, DOI 10.1113/jphysiol.1968.sp008455
   Ioffe S., 2015, CORR
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Karayev Sergey, 2013, CORR
   Krizhevsky A., 2012, ADV NEURAL INFORM PR
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Nair V, 2010, ICML, V27, P807, DOI DOI 10.0RG/PAPERS/432.PDF
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Szegedy C., 2015, CVPR 2015
   Young P., 2014, P TACL, V2, P67, DOI DOI 10.1162/tacl_a_00166
   Zeiler M. D., 2012, CORR, Vabs/1212.5701
NR 25
TC 16
Z9 18
U1 6
U2 75
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0957-4174
EI 1873-6793
J9 EXPERT SYST APPL
JI Expert Syst. Appl.
PD DEC 30
PY 2016
VL 66
BP 295
EP 301
DI 10.1016/j.eswa.2016.08.057
PG 7
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic; Operations Research & Management Science
SC Computer Science; Engineering; Operations Research & Management Science
GA EA1AX
UT WOS:000386321800023
OA Green Published
DA 2020-02-19
ER

PT J
AU Abousaleh, FS
   Lim, T
   Cheng, WH
   Yu, NH
   Hossain, MA
   Alhamid, MF
AF Abousaleh, Fatma S.
   Lim, Tekoing
   Cheng, Wen-Huang
   Yu, Neng-Hao
   Hossain, M. Anwar
   Alhamid, Mohammed F.
TI A novel comparative deep learning framework for facial age estimation
SO EURASIP JOURNAL ON IMAGE AND VIDEO PROCESSING
LA English
DT Article
DE Deep learning; Facial age estimation; Region convolutional neural
   network; Comparative framework
ID GENDER
AB Developing automatic facial age estimation algorithms that are comparable or even superior to the human ability in age estimation becomes an attractive yet challenging topic emerging in recent years. The conventional methods estimate one person's age directly from the given facial image. In contrast, motivated by human cognitive processes, we proposed a comparative deep learning framework, called Comparative Region Convolutional Neural Network (CRCNN), by first comparing the input face with reference faces of known age to generate a set of hints (comparative relations, i.e., the input face is younger or older than each reference). Then, an estimation stage aggregates all the hints to estimate the person's age. Our approach has several advantages: first, the age estimation task is split into several comparative stages, which is simpler than directly computing the person's age; secondly, in addition to the input face itself, side information (comparative relations) can be explicitly involved to benefit the estimation task; finally, few incorrect comparisons will not influence much the accuracy of the result, making this approach more robust than the conventional approach. To the best of our knowledge, the proposed approach is the first comparative deep learning framework for facial age estimation. Furthermore, we proposed to incorporate the Method of Auxiliary Coordinates (MAC) for training, which reduces the ill-conditioning problem of the deep network and affords an efficient and distributed optimization. In comparison to the best results from the state-of-the-art methods, the CRCNN showed a significant outperformance on all the benchmarks, with a relative improvement of 13.24% (on FG-NET), 23.20% (on MORPH), and 4.74% (IoG).
C1 [Abousaleh, Fatma S.] Acad Sinica, IIS, Taiwan Int Grad Program, Social Networks & Human Centered Comp Program, Acad Rd, Taipei 11529, Taiwan.
   [Abousaleh, Fatma S.; Lim, Tekoing; Cheng, Wen-Huang] Acad Sinica, Res Ctr Informat Technol Innovat CITI, Acad Rd, Taipei 11529, Taiwan.
   [Yu, Neng-Hao] Natl Chengchi Univ, Dept Comp Sci, ZhiNan Rd, Taipei 11605, Taiwan.
   [Hossain, M. Anwar; Alhamid, Mohammed F.] King Saud Univ, Dept Software Engn, Coll Comp & Informat Sci, Riyadh 11362, Saudi Arabia.
RP Yu, NH (reprint author), Natl Chengchi Univ, Dept Comp Sci, ZhiNan Rd, Taipei 11605, Taiwan.
EM jonesyu@nccu.edu.tw
RI ; Hossain, M. Anwar/J-9601-2013
OI Yu, Neng-hao/0000-0003-0717-8269; Hossain, M. Anwar/0000-0002-7673-8410
FU Deanship of Scientific Research at King Saud UniversityDeanship of
   Scientific Research at King Saud University [RGP-049]
FX The authors extend their appreciation to the Deanship of Scientific
   Research at King Saud University for funding this work through the
   research group project No. RGP-049.
CR Bengio Yoshua, 2013, Statistical Language and Speech Processing. First International Conference, SLSP 2013. Proceedings: LNCS 7978, P1, DOI 10.1007/978-3-642-39593-2_1
   Burges C., 2005, P 22 INT C MACH LEAR, P89, DOI DOI 10.1145/1102351.1102363
   Carreira-Perpinan M., 2014, P 17 INT C ART INT S, P10
   Carroll J. B., 1993, HUMAN COGNITIVE ABIL
   Chang KY, 2011, PROC CVPR IEEE, P585, DOI 10.1109/CVPR.2011.5995437
   Cheng WH, 2007, IEEE T CIRC SYST VID, V17, P43, DOI 10.1109/TCSVT.2006.885717
   Chenjing Yan, 2014, Advances in Multimedia Information Processing - PCM 2014. 15th Pacific-Rim Conference on Multimedia. Proceedings: LNCS 8879, P211, DOI 10.1007/978-3-319-13168-9_22
   Choi SE, 2010, I C CONT AUTOMAT ROB, P1280, DOI 10.1109/ICARCV.2010.5707432
   Choi SE, 2011, PATTERN RECOGN, V44, P1262, DOI 10.1016/j.patcog.2010.12.005
   CHOPRA S, 2005, PROC CVPR IEEE, P539, DOI DOI 10.1109/CVPR.2005.202
   Freund Y., 2003, J MACHINE LEARNING R, V4, P933, DOI DOI 10.1162/JMLR.2003.4.6.933
   Fu Y, 2010, IEEE T PATTERN ANAL, V32, P1955, DOI 10.1109/TPAMI.2010.36
   GALLAGHER AC, 2007, [No title captured], P1
   Geng X, 2007, IEEE T PATTERN ANAL, V29, P2234, DOI 10.1109/TPAMI.2007.70733
   Geng X, 2013, IEEE T PATTERN ANAL, V35, P2401, DOI 10.1109/TPAMI.2013.51
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Guo GD, 2014, IMAGE VISION COMPUT, V32, P761, DOI 10.1016/j.imavis.2014.04.011
   Gurpinar F., 2016, P IEEE C COMP VIS PA, P80
   Huerta I, 2014, EUR C COMP VIS, P667
   JANG JSR, 1993, IEEE T SYST MAN CYB, V23, P665, DOI 10.1109/21.256541
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Joachims T., 2002, P 8 ACM SIGKDD INT C, P133, DOI DOI 10.1145/775047.775067
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   Lanitis A, 2004, IEEE T SYST MAN CY B, V34, P621, DOI 10.1109/TSMCB.2003.817091
   Lanitis A, 2002, IEEE T PATTERN ANAL, V24, P442, DOI 10.1109/34.993553
   Li M., 2014, P 20 ACM SIGKDD INT, P661, DOI [10. 1145/2623330.2623612., DOI 10.1145/2623330.2623612]
   Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5
   PATRICK EA, 1970, INFORM CONTROL, V16, P128, DOI 10.1016/S0019-9958(70)90081-1
   Pontes JK, 2016, PATTERN RECOGN, V54, P34, DOI 10.1016/j.patcog.2015.12.003
   Quinlan J. R., 1993, C4 5 PROGRAMS MACHIN
   Ricanek K, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P341
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Sanchez-Riera J, 2016, PATTERN RECOGN LETT, V73, P1, DOI 10.1016/j.patrec.2015.12.006
   Serre T, 2007, IEEE T PATTERN ANAL, V29, P411, DOI 10.1109/TPAMI.2007.56
   Takimoto H, 2008, ELECTR COMMUN JPN, V91, P32, DOI 10.1002/ecj.10125
   Tiong Hoo Lim, 2015, 2015 IEEE Tenth International Conference on Intelligent Sensors, Sensor Networks and Information Processing (ISSNIP), P1, DOI 10.1109/ISSNIP.2015.7106936
   Tsai TH, 2016, NEUROCOMPUTING, V177, P529, DOI 10.1016/j.neucom.2015.11.050
   Vapnik V. N., 1998, STAT LEARNING THEORY
   Wu B, 2016, THIRTIETH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, P272
   Wu B, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P1336, DOI 10.1145/2964284.2964335
   Ylioinas J, 2013, LECT NOTES COMPUT SC, V8156, P141
   You CW, 2016, PERVASIVE MOB COMPUT, V25, P67, DOI 10.1016/j.pmcj.2015.07.006
NR 42
TC 10
Z9 10
U1 0
U2 13
PU SPRINGEROPEN
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 1687-5176
EI 1687-5281
J9 EURASIP J IMAGE VIDE
JI EURASIP J. Image Video Process.
PD DEC 19
PY 2016
AR 47
DI 10.1186/s13640-016-0151-4
PG 13
WC Engineering, Electrical & Electronic; Imaging Science & Photographic
   Technology
SC Engineering; Imaging Science & Photographic Technology
GA EH2HK
UT WOS:000391588200001
OA DOAJ Gold
DA 2020-02-19
ER

PT J
AU Zhuang, BH
   Wang, LJ
   Lu, HC
AF Zhuang, Bohan
   Wang, Lijun
   Lu, Huchuan
TI Visual tracking via shallow and deep collaborative model
SO NEUROCOMPUTING
LA English
DT Article
DE Visual tracking; Deep learning; Shallow feature learning; Collaborative
   tracking
AB In this paper, we propose a robust tracking method based on the collaboration of a generative model and a discriminative classifier, where features are learned by shallow and deep architectures, respectively. For the generative model, we introduce a block-based incremental learning scheme, in which a local binary mask is constructed to deal with occlusion. The similarity degrees between the local patches and their corresponding subspace are integrated to formulate a more accurate global appearance model. In the discriminative model, we exploit the advances of deep learning architectures to learn generic features which are robust to both background clutters and foreground appearance variations. To this end, we first construct a discriminative training set from auxiliary video sequences. A deep classification neural network is then trained offline on this training set. Through online fine-tuning, both the hierarchical feature extractor and the classifier can be adapted to the appearance change of the target for effective online tracking. The collaboration of these two models achieves a good balance in handling occlusion and target appearance change, which are two contradictory challenging factors in visual tracking. Both quantitative and qualitative evaluations against several state-of-the-art algorithms on challenging image sequences demonstrate the accuracy and the robustness of the proposed tracker. (C) 2016 Elsevier B.V. All rights reserved.
C1 [Zhuang, Bohan; Wang, Lijun; Lu, Huchuan] Dalian Univ Technol, Sch Informat & Commun Engn, Dalian 116023, Peoples R China.
RP Zhuang, BH (reprint author), Dalian Univ Technol, Sch Informat & Commun Engn, Dalian 116023, Peoples R China.
EM bohan.zhuang@adelaide.edu.au
OI Zhuang, Bohan/0000-0002-0074-0303
CR Adam A., 2006, CVPR
   Ahonen T., IEEE T PATTERN ANAL, V28
   Avidan S, 2007, IEEE T PATTERN ANAL, V29, P261, DOI 10.1109/TPAMI.2007.35
   Babenko B, 2009, PROC CVPR IEEE, P983, DOI 10.1109/CVPRW.2009.5206737
   Bao CL, 2012, PROC CVPR IEEE, P1830, DOI 10.1109/CVPR.2012.6247881
   Barnard M, 2014, IEEE T MULTIMEDIA, V16, P864, DOI 10.1109/TMM.2014.2301977
   Chu CT, 2013, IEEE T MULTIMEDIA, V15, P1602, DOI 10.1109/TMM.2013.2266634
   Coates A., 2012, NIPS, V25, P2690
   Collins RT, 2005, IEEE T PATTERN ANAL, V27, P1631, DOI 10.1109/TPAMI.2005.205
   Dinh T. B., 2011 IEEE WORKSH APP, P642
   Dong Wang, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P1751, DOI 10.1109/ICPR.2010.433
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Grabner H, 2008, LECT NOTES COMPUT SC, V5302, P234, DOI 10.1007/978-3-540-88682-2_19
   Grabner Helmut, 2006, P BMVC, V1, P6, DOI DOI 10.5244/C.20.6
   Grabner M, 2007, PROC CVPR IEEE, P200
   Hare S, 2011, IEEE I CONF COMP VIS, P263, DOI 10.1109/ICCV.2011.6126251
   Henriques JF, 2012, LECT NOTES COMPUT SC, V7575, P702, DOI 10.1007/978-3-642-33765-9_50
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hu WM, 2011, INT J COMPUT VISION, V91, P303, DOI 10.1007/s11263-010-0399-6
   Jia X, 2012, PROC CVPR IEEE, P1822, DOI 10.1109/CVPR.2012.6247880
   Kalal Z, 2010, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2010.5540231
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, V1, P4
   Kusy B, 2009, 2009 INTERNATIONAL CONFERENCE ON INFORMATION PROCESSING IN SENSOR NETWORKS (IPSN 2009), P109
   Le QV, 2013, INT CONF ACOUST SPEE, P8595, DOI 10.1109/ICASSP.2013.6639343
   Lee H., 2007, ADV NEURAL INFORM PR, P873
   Lee H., 2009, P 26 ANN INT C MACH, P609, DOI DOI 10.1145/1553374.1553453
   Liu BY, 2011, PROC CVPR IEEE, P1313, DOI 10.1109/CVPR.2011.5995730
   Liu R, 2009, IEEE I CONF COMP VIS, P1459, DOI 10.1109/ICCV.2009.5459285
   Martinez AM, 2002, IEEE T PATTERN ANAL, V24, P748, DOI 10.1109/TPAMI.2002.1008382
   Mei X., 2009, ICCV
   Ranzato M. A., 2007, ADV NEURAL INFORM PR, p1185 
   Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7
   Sabirin H, 2012, IEEE T MULTIMEDIA, V14, P657, DOI 10.1109/TMM.2012.2187777
   Schulz H., 2012, 11 EUR S ART NEUR NE, V3
   Sevilla-Lara L, 2012, PROC CVPR IEEE, P1910, DOI 10.1109/CVPR.2012.6247891
   Suau X, 2012, IEEE T MULTIMEDIA, V14, P575, DOI 10.1109/TMM.2012.2189853
   Sun Yi, 2013, ICCV
   Wang D, 2013, IEEE T IMAGE PROCESS, V22, P314, DOI 10.1109/TIP.2012.2202677
   Wang N., 2013, ADV NEURAL INFORM PR, P809
   Wang T., 2007, ICASSP, V1, P1
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Xie L., 2013, IEEE T MULTIMED, V15
   Yu Q, 2008, LECT NOTES COMPUT SC, V5303, P678
   Zhang KH, 2012, LECT NOTES COMPUT SC, V7574, P864, DOI 10.1007/978-3-642-33712-3_62
   Zhang TZ, 2012, PROC CVPR IEEE, P2042, DOI 10.1109/CVPR.2012.6247908
   Zhang Z, 2013, IEEE T MULTIMEDIA, V15, P106, DOI 10.1109/TMM.2012.2225040
   Zhong W., 2012, CVPR
   Zhuang B., ARXIV160302844
   Zhuang BH, 2014, IEEE T IMAGE PROCESS, V23, P1872, DOI 10.1109/TIP.2014.2308414
NR 50
TC 12
Z9 12
U1 1
U2 37
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0925-2312
EI 1872-8286
J9 NEUROCOMPUTING
JI Neurocomputing
PD DEC 19
PY 2016
VL 218
BP 61
EP 71
DI 10.1016/j.neucom.2016.08.070
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA EC3VA
UT WOS:000388053700007
DA 2020-02-19
ER

PT J
AU Costilla-Reyes, O
   Scully, P
   Ozanyan, KB
AF Costilla-Reyes, Omar
   Scully, Patricia
   Ozanyan, Krikor B.
TI Temporal Pattern Recognition in Gait Activities Recorded With a
   Footprint Imaging Sensor System
SO IEEE SENSORS JOURNAL
LA English
DT Article
DE Floor sensor; sensor fusion; gait analysis; pattern recognition; machine
   learning
AB In this paper, we assess the capability of a unique unobtrusive footprint imaging sensor system, based on plastic optical fiber technology, to allow efficient gait analysis from time domain sensor data by pattern recognition techniques. Trial gait classification experiments are executed as ten manners of walking, affecting the amplitude and frequency characteristics of the temporal signals. The data analysis involves the design of five temporal features, subsequently analyzed in 14 different machine learning models, representing linear, non-linear, ensemble, and deep learning models. The model performance is presented as cross-validated accuracy scores for the best model-feature combinations, along with the optimal hyper-parameters for each of them. The best classification performance was observed for a random forest model with the adjacent mean feature, yielding a mean validation score of 90.84% +/- 2.46%. We conclude that the floor sensor system is capable of detecting changes in gait by means of pattern recognition techniques applied in the time domain. This suggests that the footprint imaging sensor system is suitable for gait analysis applications ranging from healthcare to security.
C1 [Costilla-Reyes, Omar; Ozanyan, Krikor B.] Univ Manchester, Sch Elect & Elect Engn, Manchester M13 9PL, Lancs, England.
   [Costilla-Reyes, Omar; Scully, Patricia; Ozanyan, Krikor B.] Univ Manchester, Photon Sci Inst, Manchester M13 9PL, Lancs, England.
   [Scully, Patricia] Univ Manchester, Sch Chem Engn, Manchester M13 9PL, Lancs, England.
RP Costilla-Reyes, O (reprint author), Univ Manchester, Sch Elect & Elect Engn, Manchester M13 9PL, Lancs, England.; Costilla-Reyes, O (reprint author), Univ Manchester, Photon Sci Inst, Manchester M13 9PL, Lancs, England.
EM omar.costilla.reyes@gmail.com
OI Scully, Patricia/0000-0003-4785-2019
FU CONACyT (Mexico)Consejo Nacional de Ciencia y Tecnologia (CONACyT)
FX The author O. Costilla-Reyes would like to acknowledge CONACyT (Mexico)
   for a studentship and Dr. Paul Wright for help with programming the
   CPLD.
CR Bergstra J., 2010, P 9 PYTH SCI C, P1
   Bilney B, 2003, GAIT POSTURE, V17, P68, DOI 10.1016/S0966-6362(02)00053-X
   Bishop C. M., 2006, PATTERN RECOGN, V16
   Cantoral-Ceballos JA, 2015, IEEE SENS J, V15, P279, DOI 10.1109/JSEN.2014.2341455
   Costilla-Reyes Omar, 2015, 2015 IEEE Sensors. Proceedings, P1, DOI 10.1109/ICSENS.2015.7370174
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Gafurov D., 2007, P ANN NORW COMP SCI, P19
   Glorot X., 2010, JLMR P TRACK, p[249, 2010], DOI DOI 10.1177/1753193409103364.
   Hausdorff JM, 2008, J GERONTOL A-BIOL, V63, P1335, DOI 10.1093/gerona/63.12.1335
   Headon R., 2001, P 2001 WORKSH PERC U, P1
   Kleinberger T, 2007, LECT NOTES COMPUT SC, V4555, P103
   Kohavi R., 1995, IJCAI, V14, P2
   Le Q. V, 2015, SIMPLE WAY INITIALIZ
   LeCun Y, 1998, LECT NOTES COMPUT SC, V1524, P9
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Leusmann P., 2011, 2011 12th IEEE International Conference on Mobile Data Management (MDM 2011), P61, DOI 10.1109/MDM.2011.29
   Nair V, 2010, ICML, V27, P807, DOI DOI 10.0RG/PAPERS/432.PDF
   Ozanyan KB, 2005, IEEE SENS J, V5, P167, DOI 10.1109/JSEN.2005.843895
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Qian G, 2010, IEEE SENS J, V10, P1447, DOI 10.1109/JSEN.2010.2045158
   Rashidi P, 2013, IEEE J BIOMED HEALTH, V17, P579, DOI 10.1109/JBHI.2012.2234129
   Saripalle SK, 2014, HUM MOVEMENT SCI, V33, P238, DOI 10.1016/j.humov.2013.09.004
   Trevor H, 2001, ELEMENTS STAT LEARNI, V1, P371
   Vera-Rodriguez R, 2013, IEEE T PATTERN ANAL, V35, P823, DOI 10.1109/TPAMI.2012.164
   Wilson C. Brown, 2014, GERONTECHNOLOGY, V13, P174
   Zammit GV, 2010, J FOOT ANKLE RES, V3, DOI 10.1186/1757-1146-3-11
   Ziefle Martina, 2011, Information Quality in e-Health. Proceedings 7th Conference of the Workgroup Human-Computer Interaction and Usability Engineering of the Austrian Computer Society, USAB 2011, P607, DOI 10.1007/978-3-642-25364-5_43
NR 27
TC 16
Z9 16
U1 0
U2 25
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1530-437X
EI 1558-1748
J9 IEEE SENS J
JI IEEE Sens. J.
PD DEC 15
PY 2016
VL 16
IS 24
BP 8815
EP 8822
DI 10.1109/JSEN.2016.2583260
PG 8
WC Engineering, Electrical & Electronic; Instruments & Instrumentation;
   Physics, Applied
SC Engineering; Instruments & Instrumentation; Physics
GA ED7NU
UT WOS:000389053900021
DA 2020-02-19
ER

PT J
AU Zeng, R
   Wu, JS
   Shao, ZH
   Chen, Y
   Chen, BJ
   Senhadji, L
   Shu, HZ
AF Zeng, Rui
   Wu, Jiasong
   Shao, Zhuhong
   Chen, Yang
   Chen, Beijing
   Senhadji, Lotfi
   Shu, Huazhong
TI Color image classification via quaternion principal component analysis
   network
SO NEUROCOMPUTING
LA English
DT Article
DE Deep learning; Convolutional neural network; Quaternion; QPCANet;
   PCANet; Color image classification
ID LEARNING ALGORITHM; SCALE; CODE
AB The principal component analysis network (PCANet), which is one of the recently proposed deep learning architectures, achieves the state-of-the-art classification accuracy in various datasets and reveals a simple baseline for deep learning networks. However, the performance of PCANet may be degraded when dealing with color images due to the fact that the architecture of PCANet cannot properly utilize the spatial relationship between each color channel in three dimensional color image. In this paper, a quaternion principal component analysis network (QPCANet), which extends PCANet by using quaternion theory, is proposed for color image classification. Compared to PCANet, the proposed QPCANet takes into account the spatial distribution information of RGB channels in color images and ensures larger amount of intra-class invariance by using quaternion domain representation for color images. Experiments conducted on different color image datasets such as UC Merced Land Use, Georgia Tech face, CURet and Caltech-101 have revealed that the proposed QPCANet generally achieves higher classification accuracy than PCANet in color image classification task. The experimental results also verify that QPCANet has much better rotation invariance than PCANet when color image dataset contains lots of rotation information and demonstrate even a simple one-layer QPCANet may obtain satisfactory accuracy when compared with two-layer PCANet. (C) 2016 Elsevier B.V. All rights reserved.
C1 [Zeng, Rui; Wu, Jiasong; Chen, Yang; Shu, Huazhong] Southeast Univ, Minist Educ, Key Lab Comp Network & Informat Integrat, LIST, Nanjing 210096, Jiangsu, Peoples R China.
   [Wu, Jiasong; Chen, Yang; Senhadji, Lotfi] INSERM, U1099, F-35000 Rennes, France.
   [Wu, Jiasong; Chen, Yang; Senhadji, Lotfi] Univ Rennes 1, LTSI, F-35000 Rennes, France.
   [Zeng, Rui; Wu, Jiasong; Chen, Yang; Shu, Huazhong] Ctr Rech Informat Biomed Sinofrancais, Nanjing 210096, Jiangsu, Peoples R China.
   [Shao, Zhuhong] Capital Normal Univ, Coll Informat Engn, Beijing 100048, Peoples R China.
   [Chen, Beijing] Nanjing Univ Informat Sci & Technol, China USA Comp Sci Res Ctr, Nanjing 210044, Jiangsu, Peoples R China.
RP Wu, JS (reprint author), Southeast Univ, Minist Educ, Key Lab Comp Network & Informat Integrat, LIST, Nanjing 210096, Jiangsu, Peoples R China.
RI Senhadji, Lotfi/E-5903-2013
OI Senhadji, Lotfi/0000-0001-9434-6341
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61201344, 61271312, 61401085, 61572258, 11301074];
   Scientific Research Foundation for the Returned Overseas Chinese
   ScholarsScientific Research Foundation for the Returned Overseas Chinese
   Scholars; Qing Lan ProjectJiangsu Polytech Institute; '333'
   projectNatural Science Foundation of Jiangsu Province [BRA2015288]; Open
   Fund of China USA Computer Science Research Center [KJR16026]; State
   Education Ministry
FX This work was supported by the National Natural Science Foundation of
   China (Nos. 61201344, 61271312, 61401085, 61572258, and 11301074), and
   by the Project Sponsored by, the Scientific Research Foundation for the
   Returned Overseas Chinese Scholars, State Education Ministry, by the
   Qing Lan Project and the '333' project (No. BRA2015288), and by the Open
   Fund of China USA Computer Science Research Center (KJR16026). The
   authors are also grateful to the anonymous reviewers for their
   constructive comments and suggestions to greatly improve the quality of
   this work and the clarity of the presentation.
CR Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Bihan N. L., 2003, P ICIP
   Bruna J, 2013, IEEE T PATTERN ANAL, V35, P1872, DOI 10.1109/TPAMI.2012.230
   Chan TH, 2015, IEEE T IMAGE PROCESS, V24, P5017, DOI 10.1109/TIP.2015.2475625
   Chan WL, 2008, IEEE T IMAGE PROCESS, V17, P1069, DOI 10.1109/TIP.2008.924282
   Chen B., 2014, J MATH IMAGING VIS, V51
   Dana KJ, 1999, ACM T GRAPHIC, V18, P1, DOI 10.1145/300776.300778
   Ell TA, 2007, IEEE T IMAGE PROCESS, V16, P22, DOI 10.1109/TIP.2006.884955
   Ertoz L, 2003, SIAM PROC S, P47
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Feng ZY, 2015, NEUROCOMPUTING, V157, P11, DOI 10.1016/j.neucom.2015.01.043
   Gai S, 2016, NEUROCOMPUTING, V196, P133, DOI 10.1016/j.neucom.2015.12.112
   Gan YF, 2014, INT CONF SIGN PROCES, P1268, DOI 10.1109/ICOSP.2014.7015203
   Gu B, 2017, IEEE T NEUR NET LEAR, V28, P1646, DOI 10.1109/TNNLS.2016.2544779
   Gu B, 2015, IEEE T NEUR NET LEAR, V26, P1403, DOI 10.1109/TNNLS.2014.2342533
   Guo LQ, 2011, PATTERN RECOGN, V44, P187, DOI 10.1016/j.patcog.2010.08.017
   Guo LQ, 2012, OPT EXPRESS, V20, P18846, DOI 10.1364/OE.20.018846
   Guo Q, 2016, NEUROCOMPUTING, V184, P78, DOI 10.1016/j.neucom.2015.07.135
   Guo YM, 2016, NEUROCOMPUTING, V187, P27, DOI 10.1016/j.neucom.2015.09.116
   Hamilton WR., 1847, P ROY IRISH ACAD, V4, P1
   He K., 2015, ARXIV151203385
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Jain AK, 1997, PATTERN RECOGN, V30, P295, DOI 10.1016/S0031-3203(96)00068-4
   Jia ZH, 2015, COMM COM INF SC, V547, P323, DOI 10.1007/978-3-662-48570-5_32
   Khemchandani R, 2015, NEUROCOMPUTING, V165, P444, DOI 10.1016/j.neucom.2015.03.074
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   Kuipers J. B., 1999, QUATERNIONS ROTATION
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lei Z., 2015, IEEE T CIRCUITS SYST
   Li FF, 2007, COMPUT VIS IMAGE UND, V106, P59, DOI 10.1016/j.cviu.2005.09.012
   Li SZ, 2015, NEUROCOMPUTING, V151, P565, DOI 10.1016/j.neucom.2014.06.086
   Lin Y., 2010, P NIPS
   Liwicki S, 2014, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2014.21
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mallat S, 2012, COMMUN PUR APPL MATH, V65, P1331, DOI 10.1002/cpa.21413
   Nefian A. V., 2013, GEORGIA TECH FACE DA
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0
   Pei S. C., 2003, P ICIP
   Qin HW, 2016, NEUROCOMPUTING, V187, P49, DOI 10.1016/j.neucom.2015.10.122
   Qin PD, 2016, NEUROCOMPUTING, V190, P1, DOI 10.1016/j.neucom.2015.12.091
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Salakhutdinov R, 2009, ARTIF INTELL, P448, DOI DOI 10.1109/CVPRW.2009.5206577
   Sermanet P., 2013, ARXIV13126229
   Shakhnarovich Gregory, 2011, HDB FACE RECOGNITION, P19
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Strehl A., 2000, WORKSH ART INT WEB S, P58
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   van de Sande KEA, 2010, IEEE T PATTERN ANAL, V32, P1582, DOI 10.1109/TPAMI.2009.154
   Wen XZ, 2015, INFORM SCIENCES, V295, P395, DOI 10.1016/j.ins.2014.10.040
   Xu J, 2016, NEUROCOMPUTING, V191, P214, DOI 10.1016/j.neucom.2016.01.034
   Yang JC, 2009, PROC CVPR IEEE, P1794, DOI 10.1109/CVPRW.2009.5206757
   Yang Y., 2010, P 18 SIGSPATIAL INT, P270, DOI DOI 10.1145/1869790.1869829
   Yu Kai, 2009, P NIPS
   Zeng R, 2015, INT CONF ACOUST SPEE, P1971, DOI 10.1109/ICASSP.2015.7178315
   Zhang FZ, 1997, LINEAR ALGEBRA APPL, V251, P21, DOI 10.1016/0024-3795(95)00543-9
   Zhang Z, 2015, NEUROCOMPUTING, V149, P1058, DOI 10.1016/j.neucom.2014.07.028
   Zhao Y., 2015, IEEE T CIRCUITS SYST
   Zheng YH, 2015, J INTELL FUZZY SYST, V28, P961, DOI 10.3233/IFS-141378
NR 63
TC 27
Z9 29
U1 2
U2 25
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0925-2312
EI 1872-8286
J9 NEUROCOMPUTING
JI Neurocomputing
PD DEC 5
PY 2016
VL 216
BP 416
EP 428
DI 10.1016/j.neucom.2016.08.006
PG 13
WC Computer Science, Artificial Intelligence
SC Computer Science
GA ED3VU
UT WOS:000388777400040
DA 2020-02-19
ER

PT J
AU Wagner, SA
AF Wagner, Simon A.
TI SAR ATR by a Combination of Convolutional Neural Network and Support
   Vector Machines
SO IEEE TRANSACTIONS ON AEROSPACE AND ELECTRONIC SYSTEMS
LA English
DT Article
ID AUTOMATIC TARGET RECOGNITION; CLASSIFICATION
AB A combination of a convolutional neural network, which belongs to the deep learning research field, and support vector machines is presented as an efficient automatic target recognition system. Additional training methods that incorporate prior knowledge to the classifier and further improve its robustness against imaging errors and target variations are also presented. These methods generate artificial training data by elastic distortion and affine transformations that represent typical examples of image errors, like a changing range scale dependent on the depression angle or an incorrectly estimated aspect angle. With these examples presented to the classifier during the training, the system should become invariant against these variations and thus more robust. For the classification, the spotlight synthetic aperture radar images of the moving and stationary target acquisition and recognition database are used. Results are shown for the ten class database with a forced decision classification as well as with rejection class.
C1 [Wagner, Simon A.] Fraunhofer Inst High Frequency Phys & Radar Tech, FHR, Cognit Radar, Fraunhoferstr 20, D-53343 Wachtberg, Germany.
RP Wagner, SA (reprint author), Fraunhofer Inst High Frequency Phys & Radar Tech, FHR, Cognit Radar, Fraunhoferstr 20, D-53343 Wachtberg, Germany.
EM simon.wagner@fhr.fraunhofer.de
RI Wagner, Simon/J-6416-2019
OI Wagner, Simon/0000-0001-9309-4111
CR Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Bieker T., 2008, P 2008 11 INT C INF, P1
   Chadwick J., 2007, P 2007 IET INT C RAD, P1
   Chen SZ, 2014, 2014 INTERNATIONAL CONFERENCE ON DATA SCIENCE AND ADVANCED ANALYTICS (DSAA), P541, DOI 10.1109/DSAA.2014.7058124
   Diemunsch J, 1998, P SOC PHOTO-OPT INS, V3370, P481, DOI 10.1117/12.321851
   Gil-Pita R, 2002, NEURAL NETWORKS FOR SIGNAL PROCESSING XII, PROCEEDINGS, P425, DOI 10.1109/NNSP.2002.1030054
   Haumtratz T., 2010, 2010 11 INT RAD S IR, P1
   Haykin S., 2009, NEURAL NETWORKS LEAR
   Jacobs SP, 2000, IEEE T AERO ELEC SYS, V36, P364, DOI 10.1109/7.845214
   Jain AK, 2000, IEEE T PATTERN ANAL, V22, P4, DOI 10.1109/34.824819
   Keydel ER, 1996, P SOC PHOTO-OPT INS, V2757, P228, DOI 10.1117/12.242059
   Lauer F, 2007, PATTERN RECOGN, V40, P1816, DOI 10.1016/j.patcog.2006.10.011
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lecun Y., 1989, CRGTR894 U TOR DEP C
   Meth R, 1998, INT CONF ACOUST SPEE, P2745, DOI 10.1109/ICASSP.1998.678091
   Molchanov P, 2014, IEEE T AERO ELEC SYS, V50, P1454, DOI 10.1109/TAES.2014.120266
   Morgan DAE, 2015, PROC SPIE, V9475, DOI 10.1117/12.2176558
   Niu XX, 2012, PATTERN RECOGN, V45, P1318, DOI 10.1016/j.patcog.2011.09.021
   Niyogi P, 1998, P IEEE, V86, P2196, DOI 10.1109/5.726787
   O'Sullivan JA, 2001, IEEE T AERO ELEC SYS, V37, P91, DOI 10.1109/7.913670
   Odegaard N., 2016, P 2016 IEEE RAD C RA
   Ozdemir C., 2012, INVERSE SYNTHETIC AP
   Papson S, 2012, IEEE T AERO ELEC SYS, V48, P969, DOI 10.1109/TAES.2012.6178042
   Park JI, 2014, IEEE T AERO ELEC SYS, V50, P1092, DOI 10.1109/TAES.2013.120378
   Pratt W.K., 2007, DIGITAL IMAGE PROCES
   Profeta A, 2016, PROC SPIE, V9843, DOI 10.1117/12.2225934
   Qun Zhao, 1999, IJCNN'99. International Joint Conference on Neural Networks. Proceedings (Cat. No.99CH36339), P3198, DOI 10.1109/IJCNN.1999.836166
   ROSENBACH K, 1995, RECORD OF THE IEEE 1995 INTERNATIONAL RADAR CONFERENCE, P405, DOI 10.1109/RADAR.1995.522581
   Schachter BJ, 2015, PROC SPIE, V9476, DOI 10.1117/12.2176193
   Schiller J., 2011, Proceedings 2011 Microwaves, Radar and Remote Sensing Symposium (MRRS 2011), P24, DOI 10.1109/MRRS.2011.6053594
   Scholkopf B., 2002, LEARNING KERNELS
   Schumacher R, 2005, RADAR CONF, P167
   Simard PY, 2003, PROC INT CONF DOC, P958
   Tait P., 2013, RADAR SONAR NAVIGATI, V33, P37
   Vapnik V.N., 2000, NATURE STAT LEARNING
   Wagner S., 2014, P 2014 17 INT C INF
   Wagner S., 2015, P NATO SPEC M SET 22
   Wehner D. R., 1995, HIGH RESOLUTION RADA
   Wilmanski M, 2016, PROC SPIE, V9843, DOI 10.1117/12.2220290
   Wit J., 2014, INT RAD C LILL, P1, DOI DOI 10.1109/RADAR.2014.7060268
   Zhang HC, 2012, IEEE T AERO ELEC SYS, V48, P2481, DOI 10.1109/TAES.2012.6237604
   Zhao Q, 2000, OPT ENG, V39, P1230, DOI 10.1117/1.602495
NR 43
TC 47
Z9 52
U1 8
U2 41
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 0018-9251
EI 1557-9603
J9 IEEE T AERO ELEC SYS
JI IEEE Trans. Aerosp. Electron. Syst.
PD DEC
PY 2016
VL 52
IS 6
BP 2861
EP 2872
DI 10.1109/TAES.2016.160061
PG 12
WC Engineering, Aerospace; Engineering, Electrical & Electronic;
   Telecommunications
SC Engineering; Telecommunications
GA EN1XV
UT WOS:000395804700019
DA 2020-02-19
ER

PT J
AU Wang, XY
   Gao, LJ
   Mao, SW
AF Wang, Xuyu
   Gao, Lingjun
   Mao, Shiwen
TI CSI Phase Fingerprinting for Indoor Localization With a Deep Learning
   Approach
SO IEEE INTERNET OF THINGS JOURNAL
LA English
DT Article
DE Channel state information (CSI); deep learning; fingerprinting; indoor
   localization; phase calibration
AB With the increasing demand of location-based services, indoor localization based on fingerprinting has become an increasingly important technique due to its high accuracy and low hardware requirement. In this paper, we propose PhaseFi, a fingerprinting system for indoor localization with calibrated channel state information (CSI) phase information. In PhaseFi, the raw phase information is first extracted from the multiple antennas and multiple subcarriers of the IEEE 802.11n network interface card by accessing the modified device driver. Then a linear transformation is applied to extract the calibrated phase information, which we prove to have a bounded variance. For the offline stage, we design a deep network with three hidden layers to train the calibrated phase data, and employ the weights of the deep network to represent fingerprints. A greedy learning algorithm is incorporated to train the weights layer-by-layer to reduce computational complexity, where a subnetwork between two consecutive layers forms a restricted Boltzmann machine. In the online stage, we use a probabilistic method based on the radial basis function for online location estimation. The proposed PhaseFi scheme is implemented and validated with extensive experiments in two representation indoor environments. It is shown to outperform three benchmark schemes based on CSI or received signal strength in both scenarios.
C1 [Wang, Xuyu; Gao, Lingjun; Mao, Shiwen] Auburn Univ, Dept Elect & Comp Engn, Auburn, AL 36849 USA.
   [Gao, Lingjun] DataYes Inc, Shanghai 200122, Peoples R China.
RP Wang, XY (reprint author), Auburn Univ, Dept Elect & Comp Engn, Auburn, AL 36849 USA.
EM xzw0029@tigermail.auburn.edu; lzg0014@tigermail.auburn.edu;
   smao@ieee.org
OI Mao, Shiwen/0000-0002-7052-0007
FU U.S. National Science FoundationNational Science Foundation (NSF)
   [CNS-1247955]; Wireless Engineering Research and Education Center at
   Auburn University
FX This work was supported in part by the U.S. National Science Foundation
   under Grant CNS-1247955 and in part by the Wireless Engineering Research
   and Education Center at Auburn University. This work was presented in
   part at the IEEE GLOBECOM 2015, San Diego, CA, USA.
CR Bahl P., 2000, Proceedings IEEE INFOCOM 2000. Conference on Computer Communications. Nineteenth Annual Joint Conference of the IEEE Computer and Communications Societies (Cat. No.00CH37064), P775, DOI 10.1109/INFCOM.2000.832252
   Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153
   Brunato M, 2005, COMPUT NETW, V47, P825, DOI 10.1016/j.comnet.2004.09.004
   Chapre Y, 2014, C LOCAL COMPUT NETW, P202, DOI 10.1109/LCN.2014.6925773
   Chenshu Wu, 2015, 2015 IEEE Conference on Computer Communications (INFOCOM). Proceedings, P2497, DOI 10.1109/INFOCOM.2015.7218639
   Dayekh S., 2010, P IEEE WCNC, P1
   Gu YY, 2009, IEEE COMMUN SURV TUT, V11, P13, DOI 10.1109/SURV.2009.090103
   Halperin D, 2010, ACM SIGCOMM COMP COM, V40, P159, DOI 10.1145/1851275.1851203
   He SN, 2016, IEEE COMMUN SURV TUT, V18, P466, DOI 10.1109/COMST.2015.2464084
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Jamieson K., 2013, NSDI, P71
   Liang D, 2015, IEEE INTERNET THINGS, V2, P573, DOI 10.1109/JIOT.2015.2453419
   Liu H, 2007, IEEE T SYST MAN CY C, V37, P1067, DOI 10.1109/TSMCC.2007.905750
   Qian K, 2014, INT C PAR DISTRIB SY, P1, DOI 10.1109/PADSW.2014.7097784
   Rai A, 2012, MOBICOM 12: PROCEEDINGS OF THE 18TH ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING, P293
   Sen S., 2013, P 11 ANN INT C MOB S, p249~262, DOI DOI 10.1145/2462456.2464463
   Sen S., 2012, P 10 INT C MOB SYST, P183, DOI DOI 10.1145/2307636.2307654
   Suining He, 2015, 2015 IEEE Conference on Computer Communications (INFOCOM). Proceedings, P2506, DOI 10.1109/INFOCOM.2015.7218640
   Wang Xiaoyi, 2015, MATH PROBL ENG, V2015, P1, DOI DOI 10.1016/J.CMET.2015.09.010
   Wang XY, 2017, IEEE T VEH TECHNOL, V66, P763, DOI 10.1109/TVT.2016.2545523
   Wang XY, 2015, IEEE WCNC, P2215, DOI 10.1109/WCNC.2015.7127811
   Wang XY, 2015, IEEE WCNC, P1666, DOI 10.1109/WCNC.2015.7127718
   Wang XY, 2014, PROCEDIA COMPUT SCI, V34, P392, DOI 10.1016/j.procs.2014.07.044
   Wu C, 2015, P IEEE INFOCOM, P2038, DOI DOI 10.1109/INFOCOM.2015.7218588
   Wu KS, 2013, IEEE T PARALL DISTR, V24, P1300, DOI 10.1109/TPDS.2012.214
   Xiao J, 2012, IEEE IC COMP COM NET, DOI 10.1109/ICCCN.2012.6289200
   Xiong J, 2015, MOBICOM '15: PROCEEDINGS OF THE 21ST ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING, P537, DOI 10.1145/2789168.2790125
   Yang Z, 2012, MOBICOM 12: PROCEEDINGS OF THE 18TH ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING, P269
   Yang Z, 2013, ACM COMPUT SURV, V46, DOI 10.1145/2543581.2543592
   Youssef M, 2005, Proceedings of the Third International Conference on Mobile Systems, Applications, and Services (MobiSys 2005), P205, DOI 10.1145/1067170.1067193
   Yutian Wen, 2015, 2015 IEEE Conference on Computer Communications (INFOCOM). Proceedings, P2479, DOI 10.1109/INFOCOM.2015.7218637
   Zhang XL, 2014, IEEE T PARALL DISTR, V25, P1876, DOI 10.1109/TPDS.2013.250
NR 32
TC 101
Z9 104
U1 4
U2 53
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 2327-4662
J9 IEEE INTERNET THINGS
JI IEEE Internet Things J.
PD DEC
PY 2016
VL 3
IS 6
BP 1113
EP 1123
DI 10.1109/JIOT.2016.2558659
PG 11
WC Computer Science, Information Systems; Engineering, Electrical &
   Electronic; Telecommunications
SC Computer Science; Engineering; Telecommunications
GA EJ2OA
UT WOS:000393048500022
OA Bronze
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Jiang, XH
   Xue, HR
   Zhang, LN
   Zhou, YQ
AF Jiang Xinhua
   Xue Heru
   Zhang Lina
   Zhou Yanqing
TI HYPERSPECTRAL DATA FEATURE EXTRACTION USING DEEP BELIEF NETWORK
SO INTERNATIONAL JOURNAL ON SMART SENSING AND INTELLIGENT SYSTEMS
LA English
DT Article
DE Hyperspectral; Feature extraction; Deep learning; Deep belief network;
   Restricted boltzman machines
ID RESTRICTED BOLTZMANN MACHINES; ALGORITHM; DIMENSIONALITY; CLASSIFICATION
AB Hyperspectral data has rich spectrum information, strong correlation between bands and high data redundancy. Feature band extraction of hyperspectral data is a prerequisite and an important basis for the subsequent study of classification and target recognition. Deep belief network is a kind of deep learning model, the paper proposed a deep belief network to realize the characteristics band extraction of hyperspectral data, and use the advantages of unsupervised and supervised learning of deep belief network, and to extract feature bands of spectral data from low level to high-level gradually. The extracted feature band has a stronger discriminant performance, so that it can better to classify hyperspectral data. Finally, the AVIRIS data is used to extract the feature band, and the SVM classifier is used to classify the data, which verifies the effectiveness of the method.
C1 [Jiang Xinhua; Xue Heru; Zhou Yanqing] Inner Mongolia Agr Univ, Coll Comp & Informat Engn, Hohhot 010018, Inner Mongolia, Peoples R China.
   [Zhang Lina] Inner Mongolia Normal Univ, Coll Phys & Elect Informat, Hohhot 010022, Inner Mongolia, Peoples R China.
RP Xue, HR (reprint author), Inner Mongolia Agr Univ, Coll Comp & Informat Engn, Hohhot 010018, Inner Mongolia, Peoples R China.
EM jiang-xh@163.com
FU national and international scientific and technological cooperation
   special projects [2015DFA00530]; national natural science foundation of
   ChinaNational Natural Science Foundation of China [61461042, 61461041]
FX This work was supported by national and international scientific and
   technological cooperation special projects (No. 2015DFA00530), and
   supported by national natural science foundation of China (No. 61461042,
   No. 61461041).
CR Ba J., 2015, C NEUR INF PROC SYST, P1
   Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153
   Cheng SX, 2014, SPECTROSC SPECT ANAL, V34, P1362, DOI 10.3964/j.issn.1000-0593(2014)05-1362-05
   Cheriyadat A, 2003, INT GEOSCI REMOTE SE, P3420
   Dahl GE, 2012, IEEE T AUDIO SPEECH, V20, P30, DOI 10.1109/TASL.2011.2134090
   FAN LIHENG, 2014, ACTA OPT SINICA, V34, P1
   Feng Dingcheng, 2014, [自动化学报, Acta Automatica Sinica], V40, P2253
   Fisher M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818057
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton G. E., 1983, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, P448
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hinton GE., 2010, PRACTICAL GUIDE TRAI
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hjelm RD, 2014, NEUROIMAGE, V96, P245, DOI 10.1016/j.neuroimage.2014.03.048
   [霍雷刚 Huo Leigang], 2014, [电子与信息学报, Journal of Electronics & Information Technology], V36, P2723
   Jia XP, 2013, P IEEE, V101, P676, DOI 10.1109/JPROC.2012.2229082
   Larochelle H, 2009, J MACH LEARN RES, V10, P1
   Le Roux N, 2008, NEURAL COMPUT, V20, P1631, DOI 10.1162/neco.2008.04-07-510
   [刘敬 Liu Jing], 2012, [计算机科学, Computer Science], V39, P274
   Mnih V, 2010, LECT NOTES COMPUT SC, V6316, P210, DOI 10.1007/978-3-642-15567-3_16
   [孙康 Sun Kang], 2015, [测绘通报, Bulletin of Surveying and Mapping], P105
   Sun W., 2014, J TONGJI U, V42
   Wang LP, 2015, INT J SMART SENS INT, V8, P316
   Wang YQ, 2015, INT J SMART SENS INT, V8, P1203
   Wang YQ, 2014, INT J SMART SENS INT, V7, P1807
   Xie J, 2015, PATTERN RECOGN, V48, P447, DOI 10.1016/j.patcog.2014.08.014
   Yan L, 2015, REMOTE SENS ENVIRON, V158, P478, DOI 10.1016/j.rse.2014.11.024
   [张春霞 Zhang Chunxia], 2015, [工程数学学报, Chinese Journal of Engineering Mathematics], V32, P159
   [周颂洋 Zhou Songyang], 2014, [遥感技术与应用, Remote Sensing Technology and Application], V29, P695
NR 30
TC 0
Z9 0
U1 1
U2 6
PU INT JOURNAL SMART SENSING & INTELLIGENT SYSTEMS
PI PALMERSTON
PA INT JOURNAL SMART SENSING & INTELLIGENT SYSTEMS, PALMERSTON, 00000, NEW
   ZEALAND
SN 1178-5608
J9 INT J SMART SENS INT
JI Int. J. Smart Sens. Intell. Syst.
PD DEC
PY 2016
VL 9
IS 4
BP 1991
EP 2009
PG 19
WC Engineering, Electrical & Electronic
SC Engineering
GA EJ0QT
UT WOS:000392915900017
OA DOAJ Gold
DA 2020-02-19
ER

PT J
AU Chen, Y
   Zhao, DJ
   Wang, ZY
   Wang, ZY
   Tang, GL
   Huang, L
AF Chen, Yang
   Zhao, Dong-Jie
   Wang, Zi-Yang
   Wang, Zhong-Yi
   Tang, Guiliang
   Huang, Lan
TI Plant Electrical Signal Classification Based on Waveform Similarity
SO ALGORITHMS
LA English
DT Article
DE plant action potential (AP); AP recognition; template matching;
   difference threshold; nonlinear features; AP classification
ID VECTOR MACHINE CLASSIFICATION; INDUCED ACTION-POTENTIALS;
   NEURAL-NETWORK; REAL-TIME; LIGHT; TRANSMISSION; SIMULATION; MECHANISM;
   ROOT
AB (1) Background: Plant electrical signals are important physiological traits which reflect plant physiological state. As a kind of phenotypic data, plant action potential (AP) evoked by external stimuli-e.g., electrical stimulation, environmental stress-may be associated with inhibition of gene expression related to stress tolerance. However, plant AP is a response to environment changes and full of variability. It is an aperiodic signal with refractory period, discontinuity, noise, and artifacts. In consequence, there are still challenges to automatically recognize and classify plant AP; (2) Methods: Therefore, we proposed an AP recognition algorithm based on dynamic difference threshold to extract all waveforms similar to AP. Next, an incremental template matching algorithm was used to classify the AP and non-AP waveforms; (3) Results: Experiment results indicated that the template matching algorithm achieved a classification rate of 96.0%, and it was superior to backpropagation artificial neural networks (BP-ANNs), supported vector machine (SVM) and deep learning method; (4) Conclusion: These findings imply that the proposed methods are likely to expand possibilities for rapidly recognizing and classifying plant action potentials in the database in the future.
C1 [Chen, Yang; Wang, Zi-Yang; Wang, Zhong-Yi; Huang, Lan] China Agr Univ, Coll Informat & Elect Engn, Beijing 100083, Peoples R China.
   [Chen, Yang; Wang, Zi-Yang; Wang, Zhong-Yi; Huang, Lan] Minist Agr, Key Lab Agr Informat Acquisit Technol Beijing, Beijing 100083, Peoples R China.
   [Zhao, Dong-Jie; Tang, Guiliang] Michigan Technol Univ, Dept Biol Sci, Houghton, MI 49931 USA.
   [Wang, Zhong-Yi] Minist Educ, Modern Precis Agr Syst Integrat Res Key Lab, Beijing 100083, Peoples R China.
RP Huang, L (reprint author), China Agr Univ, Coll Informat & Elect Engn, Beijing 100083, Peoples R China.; Huang, L (reprint author), Minist Agr, Key Lab Agr Informat Acquisit Technol Beijing, Beijing 100083, Peoples R China.
EM yancychy@cau.edu.cn; dongjiez@mtu.edu; s13111175@cau.edu.cn;
   wzyhl@cau.edu.cn; gtang1@mtu.edu; hlan@cau.edu.cn
OI Huang, Lan/0000-0002-0233-3269; Chen, Yang/0000-0003-0147-3443
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61571443]; Specialized Research Fund for the
   Doctoral Program of Higher EducationSpecialized Research Fund for the
   Doctoral Program of Higher Education (SRFDP) [20130008110035]; National
   Key Scientific Instrument and Equipment Development Projects
   [2011YQ080052]; Key Laboratory of Agricultural Information Acquisition
   Technology of the Chinese Ministry of Agriculture
FX This research was supported by the National Natural Science Foundation
   of China (61571443), the Specialized Research Fund for the Doctoral
   Program of Higher Education (20130008110035), and the National Key
   Scientific Instrument and Equipment Development Projects (2011YQ080052).
   The authors would like to thank the Key Laboratory of Agricultural
   Information Acquisition Technology of the Chinese Ministry of
   Agriculture for their support.
CR Aditya K., 2011, 2011 IEEE International Conference on Robotics and Biomimetics (ROBIO), P1218, DOI 10.1109/ROBIO.2011.6181454
   AGOSTI RD, 2014, ARCH SCI, V67, P125
   Arbateni K, 2014, NEUROCOMPUTING, V145, P438, DOI 10.1016/j.neucom.2014.05.009
   Arzeno NM, 2008, IEEE T BIO-MED ENG, V55, P478, DOI 10.1109/TBME.2007.912658
   Baumert M, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0041920
   Bhateja Vikrant, 2013, International Journal of Measurement Technologies and Instrumentation Engineering, V3, P46, DOI 10.4018/ijmtie.2013070104
   Chatterjee SK, 2015, J R SOC INTERFACE, V12, DOI 10.1098/rsif.2014.1225
   Chatterjee SK, 2014, MEASUREMENT, V53, P101, DOI 10.1016/j.measurement.2014.03.040
   Chen K., 2012, P 2012 8 IEEE INT C
   Cilimkovic M, 2015, NEURAL NETWORKS BACK, V15
   Das S, 2015, COMPUT ELECTRON AGR, V118, P15, DOI 10.1016/j.compag.2015.08.013
   Davies E, 2004, NEW PHYTOL, V161, P607, DOI 10.1111/j.1469-8137.2003.01018.x
   Dziubinska H, 2001, J PLANT PHYSIOL, V158, P1167, DOI 10.1078/S0176-1617(04)70143-1
   Favre P, 2007, PHYSIOL PLANTARUM, V131, P263, DOI 10.1111/j.1399-3054.2007.00954.x
   Favre P, 2011, J PLANT PHYSIOL, V168, P653, DOI 10.1016/j.jplph.2010.09.014
   Felle HH, 2007, PLANTA, V226, P203, DOI 10.1007/s00425-006-0458-y
   Fiorani F, 2013, ANNU REV PLANT BIOL, V64, P267, DOI 10.1146/annurev-arplant-050312-120137
   Fromm J., 2012, PLANT ELECTROPHYSIOL, P207, DOI DOI 10.1007/978-3-642-29110-4_8
   Fromm J, 2007, PLANT CELL ENVIRON, V30, P249, DOI 10.1111/j.1365-3040.2006.01614.x
   Galle A, 2015, ENVIRON EXP BOT, V114, P15, DOI 10.1016/j.envexpbot.2014.06.013
   Gil PM, 2008, J PLANT PHYSIOL, V165, P1070, DOI 10.1016/j.jplph.2007.07.014
   Grams TEE, 2007, PLANT CELL ENVIRON, V30, P79, DOI 10.1111/j.1365-3040.2006.01607.x
   Grosskinsky DK, 2015, J EXP BOT, V66, P5429, DOI 10.1093/jxb/erv345
   Hedrich R, 2016, TRENDS PLANT SCI, V21, P376, DOI 10.1016/j.tplants.2016.01.016
   Hinton G., 2012, NEURAL NETWORKS TRIC, V9, P599, DOI DOI 10.1007/978-3-642-35289-8_32
   HORNIK K, 1989, NEURAL NETWORKS, V2, P359, DOI 10.1016/0893-6080(89)90020-8
   Huang L, 2006, MOD SCI INSTRUM, V4, P45
   Huang L, 2010, COMPUT ELECTRON AGR, V71, pS54, DOI 10.1016/j.compag.2009.07.014
   Kabir MA, 2012, BIOMED SIGNAL PROCES, V7, P481, DOI 10.1016/j.bspc.2011.11.003
   Kalovrektis K., 2013, AM J BIOINFORM RES, V3, P21
   Khatun S, 2016, IEEE J TRANSL ENG HE, V4, DOI 10.1109/JTEHM.2016.2544298
   Kim J.A., 2016, PLOS ONE, V11
   Kocarev L, 2002, PHYS REV E, V65, DOI 10.1103/PhysRevE.65.046215
   Krol E, 2010, CELL BIO RES PROG, P1
   Krupenina NA, 2007, BBA-BIOENERGETICS, V1767, P781, DOI 10.1016/j.bbabio.2007.01.004
   Lanata A, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0140783
   Langkvist M, 2014, PATTERN RECOGN LETT, V42, P11, DOI 10.1016/j.patrec.2014.01.008
   Lautner S, 2014, PLANT CELL ENVIRON, V37, P254, DOI 10.1111/pce.12150
   Lauzon Francis Quintal, 2012, 2012 11th International Conference on Information Sciences, Signal Processing and their Applications (ISSPA), P1438, DOI 10.1109/ISSPA.2012.6310529
   Lee LH, 2012, APPL INTELL, V37, P80, DOI 10.1007/s10489-011-0314-z
   Liu SH, 2011, J MED BIOL ENG, V31, P67, DOI 10.5405/jmbe.676
   Chau AL, 2014, FUTURE GENER COMP SY, V36, P57, DOI 10.1016/j.future.2013.06.021
   Luz EJD, 2016, COMPUT METH PROG BIO, V127, P144, DOI 10.1016/j.cmpb.2015.12.008
   Macedo F.C.O., 2015, ACTA PHYSIOL PLANT, V37, P1
   Maffei ME, 2007, TRENDS PLANT SCI, V12, P310, DOI 10.1016/j.tplants.2007.06.001
   Mancuso S, 1999, AUST J PLANT PHYSIOL, V26, P55, DOI 10.1071/PP98098
   Manikandan MS, 2012, BIOMED SIGNAL PROCES, V7, P118, DOI 10.1016/j.bspc.2011.03.004
   Martinis M., 2002, PHYS REV E, V70, P127
   Martis RJ, 2013, BIOMED SIGNAL PROCES, V8, P437, DOI 10.1016/j.bspc.2013.01.005
   Masi E, 2009, P NATL ACAD SCI USA, V106, P4048, DOI 10.1073/pnas.0804640106
   Mousavi SAR, 2013, NATURE, V500, P422, DOI 10.1038/nature12478
   Nakai Y., 2014, P 36 IEEE ANN INT C
   Noble WS, 2006, NAT BIOTECHNOL, V24, P1565, DOI 10.1038/nbt1206-1565
   Ozbay Y, 2010, DIGIT SIGNAL PROCESS, V20, P1040, DOI 10.1016/j.dsp.2009.10.016
   PAN J, 1985, IEEE T BIO-MED ENG, V32, P230, DOI 10.1109/TBME.1985.325532
   PARISOT C, 2014, ARCH SCI, V67, P139
   Pavlovic A, 2011, J EXP BOT, V62, P1991, DOI 10.1093/jxb/erq404
   Poungponsri S, 2013, NEUROCOMPUTING, V117, P206, DOI 10.1016/j.neucom.2013.02.010
   Richman JS, 2004, METHOD ENZYMOL, V384, P172
   Riedl M, 2013, EUR PHYS J-SPEC TOP, V222, P249, DOI 10.1140/epjst/e2013-01862-7
   Saini I, 2013, J ADV RES, V4, P331, DOI 10.1016/j.jare.2012.05.007
   Salvador-Recatala V, 2014, NEW PHYTOL, V203, P674, DOI 10.1111/nph.12807
   Salvadora S, 2007, INTELL DATA ANAL, V11, P561, DOI 10.3233/IDA-2007-11508
   Sansone M, 2013, J HEALTHC ENG, V4, P465, DOI 10.1260/2040-2295.4.4.465
   Satija U., 2015, P 2 IEEE INT C SIGN
   Sedgwick P, 2012, BRIT MED J, V344, DOI 10.1136/bmj.e4483
   Shadmand S, 2016, BIOMED SIGNAL PROCES, V25, P12, DOI 10.1016/j.bspc.2015.10.008
   Silva C, 1999, BRAIN TOPOGR, V11, P201, DOI 10.1023/A:1022281712161
   Stahlberg R., 2008, PLANT CELL ENVIRON, V20, P101
   Stahlberg R, 2006, PLANT SIGNAL BEHAV, V1, P15, DOI 10.4161/psb.1.1.2275
   Stankovic B, 1998, PHYSIOL PLANTARUM, V103, P51, DOI 10.1034/j.1399-3054.1998.1030107.x
   Stefanski A, 2009, WORLD SCI SER NONLIN, V67, P3
   Sukhov V, 2016, FRONT PLANT SCI, V7, DOI 10.3389/fpls.2016.01092
   Sukhov V, 2015, FUNCT PLANT BIOL, V42, P727, DOI 10.1071/FP15052
   Sukhov V, 2015, FRONT PLANT SCI, V5, DOI 10.3389/fpls.2014.00766
   Sukhov V, 2014, PLANT CELL ENVIRON, V37, P2532, DOI 10.1111/pce.12321
   Sukhov V, 2013, J MEMBRANE BIOL, V246, P287, DOI 10.1007/s00232-013-9529-8
   Sukhov V, 2011, J THEOR BIOL, V291, P47, DOI 10.1016/j.jtbi.2011.09.019
   Surova L, 2016, J PLANT PHYSIOL, V202, P57, DOI 10.1016/j.jplph.2016.05.024
   Trebacz K, 2006, COMMUNICATION IN PLANTS: NEURONAL ASPECTS OF PLANT LIFE, P277
   van Bel AJE, 2005, ANNU PLANT REV, V18, P263, DOI 10.1002/9780470988572.ch12
   Vodeneev V. A., 2016, Biophysics, V61, P505, DOI 10.1134/S0006350916030209
   Volkov AG, 2007, PLANT SIGNAL BEHAV, V2, P139, DOI 10.4161/psb.2.3.4217
   Wessel JR, 2016, BRAIN TOPOGR
   WOLF A, 1985, PHYSICA D, V16, P285, DOI 10.1016/0167-2789(85)90011-9
   Yan XF, 2009, PROG NAT SCI-MATER, V19, P531, DOI 10.1016/j.pnsc.2008.08.009
   Yang R, 2012, PLANT ELECTROPHYSIOL, P63
   ZAWADZKI T, 1991, PHYSIOL PLANTARUM, V83, P601, DOI 10.1111/j.1399-3054.1991.tb02475.x
   Zhang F, 2009, IEEE T BIOMED CIRC S, V3, P220, DOI 10.1109/TBCAS.2009.2020093
   Zhang XH, 2012, CHINESE SCI BULL, V57, P413, DOI 10.1007/s11434-011-4820-5
   Zhao D.J., 2015, SCI REP, V5, P1
   Zhao DJ, 2013, MATH COMPUT MODEL, V58, P556, DOI 10.1016/j.mcm.2011.10.065
   Zimmermann MR, 2009, PLANT PHYSIOL, V149, P1593, DOI 10.1104/pp.108.133884
NR 93
TC 3
Z9 3
U1 0
U2 19
PU MDPI
PI BASEL
PA ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
SN 1999-4893
J9 ALGORITHMS
JI Algorithms
PD DEC
PY 2016
VL 9
IS 4
AR 70
DI 10.3390/a9040070
PG 23
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods
SC Computer Science
GA EH2US
UT WOS:000391624500009
OA DOAJ Gold
DA 2020-02-19
ER

PT J
AU Ripoll, VJR
   Wojdel, A
   Romero, E
   Ramos, P
   Brugada, J
AF Ribas Ripoll, Vicent J.
   Wojdel, Anna
   Romero, Enrique
   Ramos, Pablo
   Brugada, Josep
TI ECG assessment based on neural networks with pretraining
SO APPLIED SOFT COMPUTING
LA English
DT Article
DE Neural networks; Pretraining; Restricted Boltzmann machines; Deep
   learning; Electrocardiography; Cardiology
ID CLASSIFICATION
AB In this paper, we present a new automatic screening method to assess whether a patient from ambulatory care or emergency should be referred to a cardiology service. This method is based on deep neural networks with pretraining and takes as an input a raw ECG signal without annotation.
   This work is based on a prospective clinical study that took place at Hospital Clinic in Barcelona between 2011-2012 and recruited 1390 patients. For each patient, we recorded a 12-lead ECG and the diagnosis was conducted by the cardiology service at the same hospital. Normal, borderline normal and normal variant ECGs were labelled as normal and the rest as abnormal.
   Our deep neural networks with pretraining were tested through cross-validation with a cohort of 416 test patients. The performance of our model was compared against other standard classification methods such as neural networks without pretraining, Support Vector Machines, Extreme Learning Machines, k-Nearest Neighbours and a professional classification algorithm certified for medical use that annotates the raw ECG signals prior to classification.
   The resulting best classifier was a pretrained neural network with three hidden layers and 700 units in every layer. This network yielded an accuracy of 0.8552, a sensitivity of 0.9176 and a specificity of 0.7827. The best alternative classification method was a Support Vector Machine with a Gaussian kernel, which yielded an accuracy of 0.8476, a sensitivity of 0.9446 and a specificity of 0.7346. The professional classification algorithm yielded an accuracy of 0.8407, a sensitivity of 0.8558 and a specificity of 0.8214.
   Neural networks with pretraining automatically obtain a representation of the input data without resorting to any annotation and, thus, simplify the process of assessing normality of ECG signals. The results that we have obtained are slightly better than those obtained with the professional classification system and, for some network configurations, they can be considered as exchangeable.
   Neural networks with pretraining open up a promising line of research for the automatic assessment of ECG signals that may be used in the future in clinical practice. (C) 2016 Elsevier B.V. All rights reserved.
C1 [Ribas Ripoll, Vicent J.; Wojdel, Anna] Custom Software & Elect, Marie Curie 8, Barcelona 08042, Spain.
   [Romero, Enrique] Univ Politecn Cataluna, Soft Comp SOCO Res Grp Llenguatges & Sistemes Inf, Edifici Omega,Campus Nord, ES-08034 Barcelona, Spain.
   [Ramos, Pablo; Brugada, Josep] Univ Barcelona, Hosp Clin, Villarroel 170, E-08036 Barcelona, Spain.
RP Ripoll, VJR (reprint author), Custom Software & Elect, Marie Curie 8, Barcelona 08042, Spain.
EM vribas@tecnocse.com
OI , Vicent/0000-0002-7266-6106
FU Shockomics project, under the EU FP7 framework
FX This project has been partially funded by the Shockomics project, under
   the EU FP7 framework.
CR Andreao RV, 2008, COMPUT BIOL MED, V38, P659, DOI 10.1016/j.compbiomed.2008.03.006
   Bengio Yoshua, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P437, DOI 10.1007/978-3-642-35289-8_26
   Bengio Yoshua, 2013, Statistical Language and Speech Processing. First International Conference, SLSP 2013. Proceedings: LNCS 7978, P1, DOI 10.1007/978-3-642-39593-2_1
   Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Christov I, 2006, MED ENG PHYS, V28, P876, DOI 10.1016/j.medengphy.2005.12.010
   Ciresan D, 2012, PROC CVPR IEEE, P3642, DOI 10.1109/CVPR.2012.6248110
   Cristianini N, INTRO SUPPORT VECTOR
   Dumont J, 2008, IEEE ENG MED BIO, P165, DOI 10.1109/IEMBS.2008.4649116
   Erhan D, 2010, J MACH LEARN RES, V11, P625
   Fayn J, 2011, IEEE T BIO-MED ENG, V58, P95, DOI 10.1109/TBME.2010.2071872
   GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596
   Giri D, 2013, KNOWL-BASED SYST, V37, P274, DOI 10.1016/j.knosys.2012.08.011
   Haykin S., 1999, NEURAL NETWORKS COMP
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Huang G, 2015, NEURAL NETWORKS, V61, P32, DOI 10.1016/j.neunet.2014.10.001
   Huang GB, 2006, IEEE T NEURAL NETWOR, V17, P879, DOI 10.1109/TNN.2006.875977
   Karpagachelvi K., 2011, COMPUTER INFORM SCI, V4, P42
   Kiranyaz S, 2011, EXPERT SYST APPL, V38, P3220, DOI 10.1016/j.eswa.2010.09.010
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lanata A, 2011, EXPERT SYST APPL, V38, P6798, DOI 10.1016/j.eswa.2010.12.066
   Le Q., 2012, INT C MACH LEARN ICM
   Martis RJ, 2013, BIOMED SIGNAL PROCES, V8, P888, DOI 10.1016/j.bspc.2013.08.008
   Mitra M, 2013, PROC TECH, V10, P76, DOI 10.1016/j.protcy.2013.12.339
   Moavenian M, 2010, EXPERT SYST APPL, V37, P3088, DOI 10.1016/j.eswa.2009.09.021
   MURPHY JG, 2006, MAYO CLIN CARDIOLOGY
   Murthy N., 2015, ADV INTELLIGENT SYST, V308, P693
   Patra D., 2005, IAENG INT J COMPUTER, V36, P1
   Pecchia L, 2011, IEEE T BIO-MED ENG, V58, P800, DOI 10.1109/TBME.2010.2092776
   Ripoll VJR, 2014, COMPUT CARDIOL, V41, P1061
   Ruiz-Rodriguez JC, 2013, INTENS CARE MED, V39, P1618, DOI 10.1007/s00134-013-2964-2
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Salakhutdinov R., 2008, ADV NEURAL INFORM PR, V20, P1249
   Smolensky P, 1986, PARALLEL DISTRIBUTED, P194
   Vincent P, 2008, P 25 INT C MACH LEAR, P1096, DOI DOI 10.1145/1390156.1390294
   Yildiz A, 2011, EXPERT SYST APPL, V38, P12880, DOI 10.1016/j.eswa.2011.04.080
NR 38
TC 9
Z9 9
U1 0
U2 26
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 1568-4946
EI 1872-9681
J9 APPL SOFT COMPUT
JI Appl. Soft. Comput.
PD DEC
PY 2016
VL 49
BP 399
EP 406
DI 10.1016/j.asoc.2016.08.013
PG 8
WC Computer Science, Artificial Intelligence; Computer Science,
   Interdisciplinary Applications
SC Computer Science
GA EI1ZL
UT WOS:000392285600028
DA 2020-02-19
ER

PT J
AU Menchon-Lara, RM
   Sancho-Gomez, JL
   Bueno-Crespo, A
AF Menchon-Lara, Rosa-Maria
   Sancho-Gomez, Jose-Luis
   Bueno-Crespo, Andres
TI Early-stage atherosclerosis detection using deep learning over carotid
   ultrasound images
SO APPLIED SOFT COMPUTING
LA English
DT Article
DE Deep learning; Auto-encoders; Extreme learning machine; Intima-media
   thickness; Image segmentation
ID INTIMA-MEDIA THICKNESS; MEASUREMENT SYSTEM; HOUGH TRANSFORM; ARTERY
   WALL; SEGMENTATION; MACHINE; VARIABILITY; ALGORITHM; SNAKES
AB This paper proposes a computer-aided diagnosis tool for the early detection of atherosclerosis. This pathology is responsible for major cardiovascular diseases, which are the main cause of death worldwide. Among preventive measures, the intima-media thickness (IMT) of the common carotid artery stands out as early indicator of atherosclerosis and cardiovascular risk. In particular, IMT is evaluated by means of ultrasound scans. Usually, during the radiological examination, the specialist detects the optimal measurement area, identifies the layers of the arterial wall and manually marks pairs of points on the image to estimate the thickness of the artery. Therefore, this manual procedure entails subjectivity and variability in the IMT evaluation. Instead, this article suggests a fully automatic segmentation technique for ultrasound images of the common carotid artery. The proposed methodology is based on machine learning and artificial neural networks for the recognition of IMT intensity patterns in the images. For this purpose, a deep learning strategy has been developed to obtain abstract and efficient data representations by means of auto-encoders with multiple hidden layers. In particular, the considered deep architecture has been designed under the concept of extreme learning machine (ELM). The correct identification of the arterial layers is achieved in a totally user-independent and repeatable manner, which not only improves the IMT measurement in daily clinical practice but also facilitates the clinical research. A database consisting of 67 ultrasound images has been used in the validation of the suggested system, in which the resulting automatic contours for each image have been compared with the average of four manual segmentations performed by two different observers (ground-truth). Specifically, the IMT measured by the proposed algorithm is 0.625 +/- 0.167 mm (mean +/- standard deviation), whereas the corresponding ground-truth value is 0.619 +/- 0.176 mm. Thus, our method shows a difference between automatic and manual measures of only 5.79 +/- 34.42 mu m. Furthermore, different quantitative evaluations reported in this paper indicate that this procedure outperforms other methods presented in the literature. (C) 2016 Elsevier B.V. All rights reserved.
C1 [Menchon-Lara, Rosa-Maria; Sancho-Gomez, Jose-Luis] Univ Politecn Cartagena, Dept Tecnol Informac & Comunicac, Murcia, Spain.
   [Bueno-Crespo, Andres] Univ Catolica San Antonio, Dept Informat Sistemas, Murcia, Spain.
RP Menchon-Lara, RM (reprint author), Univ Politecn Cartagena, Dept Tecnol Informac & Comunicac, Murcia, Spain.
EM rmml@alu.upct.es
RI Bueno-Crespo, Andres/O-5423-2016; Bueno-Crespo, Andres/AAB-2204-2019;
   Sancho-Gomez, Jose Luis/A-2293-2015
OI Bueno-Crespo, Andres/0000-0003-1734-6852; Bueno-Crespo,
   Andres/0000-0003-1734-6852; Sancho-Gomez, Jose Luis/0000-0001-8009-1616;
   Menchon-Lara, Rosa Maria/0000-0002-1543-6670
CR Bastida-Jumilla MC, 2015, BIOMED SIGNAL PROCES, V16, P68, DOI 10.1016/j.bspc.2014.08.012
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Cheng DC, 2008, IEEE T INF TECHNOL B, V12, P792, DOI 10.1109/TITB.2008.926413
   Cheng DC, 2002, COMPUT METH PROG BIO, V67, P27, DOI 10.1016/S0169-2607(00)00149-8
   Delsanto S, 2007, IEEE T INSTRUM MEAS, V56, P1265, DOI 10.1109/TIM.2007.900433
   Deng L., 2014, MSRTR201421
   Destrempes F, 2009, IEEE T MED IMAGING, V28, P215, DOI 10.1109/TMI.2008.929098
   Faita F, 2008, J ULTRAS MED, V27, P1353, DOI 10.7863/jum.2008.27.9.1353
   Golemati S, 2007, ULTRASOUND MED BIOL, V33, P1918, DOI 10.1016/j.ultrasmedbio.2007.05.021
   Gustavsson T., 1994, COMPUT CARDIOL, V1994, P297
   Huang GB, 2006, NEUROCOMPUTING, V70, P489, DOI 10.1016/j.neucom.2005.12.126
   Huang GB, 2012, IEEE T SYST MAN CY B, V42, P513, DOI 10.1109/TSMCB.2011.2168604
   Ilea DE, 2013, IEEE T ULTRASON FERR, V60, P158, DOI 10.1109/TUFFC.2013.2547
   Kasun LLC, 2013, IEEE INTELL SYST, V28, P31
   Lee YB, 2010, COMPUT BIOL MED, V40, P687, DOI 10.1016/j.compbiomed.2010.03.010
   Liang Q, 2000, IEEE T MED IMAGING, V19, P127, DOI 10.1109/42.836372
   Liguori C, 2001, IEEE T INSTRUM MEAS, V50, P1684, DOI 10.1109/19.982968
   Loizou CP, 2007, MED BIOL ENG COMPUT, V45, P35, DOI 10.1007/s11517-006-0140-3
   Loizou CP, 2014, MED BIOL ENG COMPUT, V52, P1073, DOI 10.1007/s11517-014-1203-5
   Menchon-Lara RM, 2015, NEUROCOMPUTING, V151, P161, DOI 10.1016/j.neucom.2014.09.066
   Menchon-Lara RM, 2014, MED BIOL ENG COMPUT, V52, P169, DOI 10.1007/s11517-013-1128-4
   Molinari F, 2012, ULTRASONICS, V52, P949, DOI 10.1016/j.ultras.2012.03.005
   Molinari F, 2010, COMPUT METH PROG BIO, V100, P201, DOI 10.1016/j.cmpb.2010.04.007
   Molinari F, 2010, IEEE T ULTRASON FERR, V57, P1112, DOI 10.1109/TUFFC.2010.1522
   Nikita KS, 2013, COMPUT MED IMAG GRAP, V37, P1, DOI 10.1016/j.compmedimag.2012.12.001
   Petroudi S, 2012, IEEE T BIO-MED ENG, V59, P3060, DOI 10.1109/TBME.2012.2214387
   Rocha R, 2010, IMAGE VISION COMPUT, V28, P614, DOI 10.1016/j.imavis.2009.09.017
   Santhiyakumari N, 2008, SIGNAL IMAGE VIDEO P, V2, P183, DOI 10.1007/s11760-007-0048-x
   Selzer RH, 2001, ATHEROSCLEROSIS, V154, P185, DOI 10.1016/S0021-9150(00)00461-5
   SHEEHAN FH, 1983, CIRCULATION, V68, P550, DOI 10.1161/01.CIR.68.3.550
   Stein JH, 2005, J AM SOC ECHOCARDIOG, V18, P244, DOI 10.1016/j.echo.2004.12.002
   Suri JS, 2000, PATTERN ANAL APPL, V3, P39, DOI 10.1007/s100440050005
   Touboul PJ, 2012, CEREBROVASC DIS, V34, P290, DOI 10.1159/000343145
   Wendelhag I, 1997, STROKE, V28, P2195, DOI 10.1161/01.STR.28.11.2195
   WHO, GLOBAL ATLAS CARDIOV
   Xu XY, 2012, COMPUT MED IMAG GRAP, V36, P248, DOI 10.1016/j.compmedimag.2011.06.007
   Zong WW, 2013, NEUROCOMPUTING, V101, P229, DOI 10.1016/j.neucom.2012.08.010
NR 37
TC 10
Z9 10
U1 6
U2 35
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 1568-4946
EI 1872-9681
J9 APPL SOFT COMPUT
JI Appl. Soft. Comput.
PD DEC
PY 2016
VL 49
BP 616
EP 628
DI 10.1016/j.asoc.2016.08.055
PG 13
WC Computer Science, Artificial Intelligence; Computer Science,
   Interdisciplinary Applications
SC Computer Science
GA EI1ZL
UT WOS:000392285600045
DA 2020-02-19
ER

PT J
AU Park, C
   Choi, KH
   Lee, C
   Lim, S
AF Park, Cheoneum
   Choi, Kyoung-Ho
   Lee, Changki
   Lim, Soojong
TI Korean Coreference Resolution with Guided Mention Pair Model Using Deep
   Learning
SO ETRI JOURNAL
LA English
DT Article
DE Coreference resolution; Guide mention pair; Multi-pass sieve; Mention
   pair; Deep learning
AB The general method of machine learning has encountered disadvantages in terms of the significant amount of time and effort required for feature extraction and engineering in natural language processing. However, in recent years, these disadvantages have been solved using deep learning. In this paper, we propose a mention pair (MP) model using deep learning, and a system that combines both rule-based and deep learning-based systems using a guided MP as a coreference resolution, which is an information extraction technique. Our experiment results confirm that the proposed deep-learning based coreference resolution system achieves a better level of performance than rule- and statistics-based systems applied separately.
C1 [Park, Cheoneum; Choi, Kyoung-Ho; Lee, Changki] Kangwon Natl Univ, Dept Comp Sci, Chunchon, South Korea.
   [Lim, Soojong] ETRI, SW & Content Res Lab, Daejeon, South Korea.
RP Lee, C (reprint author), Kangwon Natl Univ, Dept Comp Sci, Chunchon, South Korea.
EM parkce3@gmail.com; gtraccoon@gmail.com; leeck@kangwon.ac.kr;
   isj@etri.re.kr
FU Institute for Information & communications Technology Promotion (IITP)
   grant - Korea government (MSIP) [R0101-16-0062]
FX This work was supported by Institute for Information & communications
   Technology Promotion (IITP) grant funded by the Korea government (MSIP)
   (no. R0101-16-0062, Development of Knowledge Evolutionary WiseQA
   Platform Technology for Human Knowledge Augmented Services).
CR Bagga A, 1998, 1 INT C LANG RES EV, P563
   Chen C., 2012, JOINT C EMNLP CONLL, P56
   Chen Z.-Q., 2009, IEEE C SUST POW GEN, P1
   Clark K, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P1405
   Collobert R, 2011, J MACH LEARN RES, V12, P2493
   Fernandes E., 2012, JOINT C EMNLP CONLL, P41
   Glorot X., 2011, P 14 INT C ART INT S, V15, P315
   GROSZ BJ, 1995, COMPUT LINGUIST, V21, P203
   Haghighi A., 2010, HUMAN LANGUAGE TECHN, P385
   Hinton G. E., CORR
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Lappin S., 1994, Computational Linguistics, V20, P535
   Lee C., 2014, ANN C HUM COGN LANG, P87
   Lee H, 2013, COMPUT LINGUIST, V39, P885, DOI 10.1162/COLI_a_00152
   Luo Xiaoqiang, 2005, P C HUM LANG TECHN E, P25, DOI DOI 10.3115/1220575.1220579
   Noam Chomsky, 1981, LECT GOVT BINDING
   Pradhan S, 2011, P 15 C COMP NAT LANG, P1
   Rahimian F, 2014, 2014 IEEE/WIC/ACM INTERNATIONAL JOINT CONFERENCES ON WEB INTELLIGENCE (WI) AND INTELLIGENT AGENT TECHNOLOGIES (IAT), VOL 2, P46, DOI 10.1109/WI-IAT.2014.79
   Rahman A, 2009, P 2009 C EMP METH NA, V2, P968
   Vilain Marc, 1995, P 6 MESS UND C MUC 6, V6, P45, DOI DOI 10.3115/1072399.1072405
   Yoon Y.-C., 2006, P KOR SOC COGN SCI S, P81
   최경호, 2015, [KIISE Transactions on Computing Practices, 정보과학회 컴퓨팅의 실제 논문지], V21, P333
   이창기, 2014, [Journal of KIISE, 정보과학회논문지], V41, P992
NR 23
TC 6
Z9 7
U1 1
U2 20
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 1225-6463
EI 2233-7326
J9 ETRI J
JI ETRI J.
PD DEC
PY 2016
VL 38
IS 6
BP 1207
EP 1217
DI 10.4218/etrij.16.0115.0896
PG 11
WC Engineering, Electrical & Electronic; Telecommunications
SC Engineering; Telecommunications
GA EH5ZK
UT WOS:000391851700019
DA 2020-02-19
ER

PT J
AU Manic, M
   Amarasinghe, K
   Rodriguez-Andina, JJ
   Rieger, C
AF Manic, Milos
   Amarasinghe, Kasun
   Rodriguez-Andina, Juan J.
   Rieger, Craig
TI Intelligent Buildings of the Future Cyberaware, Deep Learning Powered,
   and Human Interacting
SO IEEE INDUSTRIAL ELECTRONICS MAGAZINE
LA English
DT Article
ID SMART GRIDS; ENERGY; SECURITY
C1 [Manic, Milos] Virginia Commonwealth Univ, Dept Comp Sci, Richmond, VA 23284 USA.
   [Manic, Milos] Virginia Commonwealth Univ, Modern Heurist Res Grp, Richmond, VA 23284 USA.
   [Amarasinghe, Kasun] Virginia Commonwealth Univ, Comp Sci, Richmond, VA 23284 USA.
   [Amarasinghe, Kasun] Univ Vigo, Dept Elect Technol, Vigo, Spain.
   [Rieger, Craig] Idaho Natl Lab, Idaho Falls, ID USA.
RP Manic, M (reprint author), Virginia Commonwealth Univ, Dept Comp Sci, Richmond, VA 23284 USA.; Manic, M (reprint author), Virginia Commonwealth Univ, Modern Heurist Res Grp, Richmond, VA 23284 USA.
EM misko@ieee.org; amarasinghek@vcu.edu; jjrdguez@uvigo.es;
   craig.rieger@inl.gov
RI rieger, craig/D-1808-2017
OI rieger, craig/0000-0002-3198-8838; Rodriguez-Andina, Juan
   J./0000-0002-0919-1793; Manic, Milos/0000-0003-1484-7678
CR Alderton D., 2015, P DOE OE EN STOR PRO
   Amarasinghe K., 2016, P IEEE S SER COMP IN
   Amarasinghe K, 2015, IEEE IND ELEC, P5421
   [Anonymous], 2016, SEC INT CONN BUILD A
   [Anonymous], 2016, INT BUILD AUT TECHN
   [Anonymous], 2013, DOEEIA0484
   [Anonymous], 2010, CONVOLUTIONAL NEURAL
   Bache K., 2013, UCI MACHINE LEARNING
   Bigler T., 2011, EMERGING TECHNOLOGIE, P1
   Boyes Hugh, 2013, RESILIENCE CYBER SEC
   Bruckner D, 2012, IEEE IND ELEC, P6285, DOI 10.1109/IECON.2012.6389021
   Bruckner D, 2012, IEEE T IND INFORM, V8, P206, DOI 10.1109/TII.2011.2176741
   Cass S, 2013, IEEE SPECTRUM, V50, P83
   Clastres C, 2011, ENERG POLICY, V39, P5399, DOI 10.1016/j.enpol.2011.05.024
   Cupelli D, 2009, SOL ENERG MAT SOL C, V93, P2008, DOI 10.1016/j.solmat.2009.08.002
   Dedinec A., ENERGY IN PRESS
   Energy Efficiency and Renewable Energy Clearinghouse, 1993, DOECH10093290 EN EFF
   Federal Bureau of Investigation Cyber Alert Newark Division, 2012, VULN TRID NIAG FRAM
   Fernandes E, 2016, P IEEE S SECUR PRIV, P636, DOI 10.1109/SP.2016.44
   Fisk D, 2012, INTELL BUILD INT, V4, P169, DOI 10.1080/17508975.2012.695277
   Halfawy M, 2005, J COMPUT CIVIL ENG, V19, P172, DOI 10.1061/(ASCE)0887-3801(2005)19:2(172)
   Hernandez G., 2014, P BLACKH 2014 C LAS
   Hewlett J. D., 2012, Proceedings of the 2012 5th International Symposium on Resilient Control Systems (ISRCS), P31, DOI 10.1109/ISRCS.2012.6309289
   Hinton G. E., 2009, SCHOLARPEDIA, V4, P5947, DOI DOI 10.4249/scholarpedia.5947
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Jazizadeha F., 2012, P CONSTR RES C, P1830
   Jetcheva JG, 2014, ENERG BUILDINGS, V84, P214, DOI 10.1016/j.enbuild.2014.08.004
   Johnson Controls, 2016, DISTR EN STOR SYST L
   Johnson Emily, 2016, INFORMATIONWEEK  APR
   Kupzog F, 2011, EURASIP J EMBED SYST, DOI 10.1155/2011/737543
   Kwak J. Y., 2011, AAMAS WORKSH AG TECH
   LeCun Y., 1998, HDB BRAIN THEORY NEU, V3361, P255
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lutron Electronics Co. Inc, COMM SOL WHOL BUILD
   Manic M., 2014, IEEE SMART GRID NEWS
   Manic M, 2016, IEEE IND ELECTRON M, V10, P25, DOI 10.1109/MIE.2015.2513749
   Marino D.L., 2016, P 42 ANN C IEEE IND
   Mocanu E, 2016, SUSTAIN ENERGY GRIDS, V6, P91, DOI 10.1016/j.segan.2016.02.005
   Salakhutdinov R., 2007, P INT C MACH LEARN, V24, P791, DOI DOI 10.1145/1273496.1273596
   Sauter T, 2011, IEEE IND ELECTRON M, V5, P35, DOI 10.1109/MIE.2011.942175
   Schiff J. L., 2015, 3 REASONS WARY INTER
   Siano P, 2014, RENEW SUST ENERG REV, V30, P461, DOI 10.1016/j.rser.2013.10.022
   SPD Control Systems Corporation, SPD SMART CLASS AUT
   Sutskever I., 2014, ADV NEURAL INFORM PR, P3104, DOI DOI 10.1007/S10107-014-0839-0
   U.S. Energy Information Administration, 2010, MAN EN CONS SURV
   U.S. Green Building Council, 2016, SMART GLASS APPL POL
   United States Energy Information Administration, 2015, ANN EN OUTL 2015
   US Dept. of Energy, 2015, QUADR TECHN REV 2015
   Van Der Meulen Rob, 2015, GARTNER SAYS 6 4 BIL
   Veracode, 2015, INT THINGS SEC RES S
   Wagner A, 2007, ENERG BUILDINGS, V39, P758, DOI 10.1016/j.enbuild.2007.02.013
   WERBOS PJ, 1990, P IEEE, V78, P1550, DOI 10.1109/5.58337
   Wijayasekara D, 2014, IEEE T IND INFORM, V10, P1829, DOI 10.1109/TII.2014.2328291
   Wijayasekara D, 2011, NUCL ENG DES, V241, P2549, DOI 10.1016/j.nucengdes.2011.04.045
   Yu C. N., IEEE T SMART GRID, P1
   Yuan YL, 2014, INT GEOSCI REMOTE SE, DOI 10.1109/IGARSS.2014.6946435
NR 56
TC 30
Z9 30
U1 2
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1932-4529
EI 1941-0115
J9 IEEE IND ELECTRON M
JI IEEE Ind. Electron. Mag.
PD DEC
PY 2016
VL 10
IS 4
BP 32
EP 49
DI 10.1109/MIE.2016.2615575
PG 18
WC Engineering, Electrical & Electronic
SC Engineering
GA EH3ZU
UT WOS:000391711500005
DA 2020-02-19
ER

PT J
AU Rebai, I
   BenAyed, Y
   Mahdi, W
AF Rebai, Ilyes
   BenAyed, Yassine
   Mahdi, Walid
TI Deep multilayer multiple kernel learning
SO NEURAL COMPUTING & APPLICATIONS
LA English
DT Article
DE Deep learning; Support vector machine; Multilayer multiple kernel
   learning; Optimization methods; Gradient ascent
AB Multiple kernel learning (MKL) approach has been proposed for kernel methods and has shown high performance for solving some real-world applications. It consists on learning the optimal kernel from one layer of multiple predefined kernels. Unfortunately, this approach is not rich enough to solve relatively complex problems. With the emergence and the success of the deep learning concept, multilayer of multiple kernel learning (MLMKL) methods were inspired by the idea of deep architecture. They are introduced in order to improve the conventional MKL methods. Such architectures tend to learn deep kernel machines by exploring the combinations of multiple kernels in a multilayer structure. However, existing MLMKL methods often have trouble with the optimization of the network for two or more layers. Additionally, they do not always outperform the simplest method of combining multiple kernels (i.e., MKL). In order to improve the effectiveness of MKL approaches, we introduce, in this paper, a novel backpropagation MLMKL framework. Specifically, we propose to optimize the network over an adaptive backpropagation algorithm. We use the gradient ascent method instead of dual objective function, or the estimation of the leave-one-out error. We test our proposed method through a large set of experiments on a variety of benchmark data sets. We have successfully optimized the system over many layers. Empirical results over an extensive set of experiments show that our algorithm achieves high performance compared to the traditional MKL approach and existing MLMKL methods.
C1 [Rebai, Ilyes; BenAyed, Yassine] Univ Sfax, MIRACL Multimedia InfoRmat Syst & Adv Comp Lab, Sfax, Tunisia.
   [Mahdi, Walid] Taif Univ, Coll Comp & Informat Technol, At Taif, Saudi Arabia.
RP Rebai, I (reprint author), Univ Sfax, MIRACL Multimedia InfoRmat Syst & Adv Comp Lab, Sfax, Tunisia.
EM rebai_ilyes@hotmail.fr; yassine.benayed@gmail.com;
   walid.mahdi@isimsf.rnu.tn
OI rebai, ilyes/0000-0003-4504-3146
CR Bach F, 2004, P 21 INT C MACH LEAR, P1
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Cho Y., 2009, ADV NEURAL INFORM PR, V22, P342, DOI DOI 10.1021/ed028p10
   Cho Y. Y., 2012, THESIS
   Ciresan DC, 2010, NEURAL COMPUT, V22, P3207, DOI 10.1162/NECO_a_00052
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411
   Cortes C., 2009, P ADV NEUR INF PROC, P396
   Cortes C, 2009, P 25 C UNC ART INT, P109
   Cristianini N., 2000, SUPPORT VECTOR MACHI
   Guoguo Chen, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P4087, DOI 10.1109/ICASSP.2014.6854370
   Hu MQ, 2009, IEEE T NEURAL NETWOR, V20, P827, DOI 10.1109/TNN.2009.2014229
   Kloft M., 2009, ADV NEURAL INFORM PR, P997
   Kloft M, 2011, J MACH LEARN RES, V12, P953
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   Lanckriet GRG, 2004, J MACH LEARN RES, V5, P27
   Martin K, 2013, P INT 2013, P2589
   Mika S, 1999, ADV NEUR IN, V11, P536
   Rakotomamonjy A, 2008, J MACH LEARN RES, V9, P2491
   Rojas R., 1996, NEURAL NETWORKS SYST
   Rumelhart D.E., 1986, PARALLEL DISTRIBUTED, V1, P318, DOI DOI 10.1016/B978-1-4832-1446-7.50035-2
   Shawe-Taylor J, 2004, KERNEL METHODS PATTE
   Sonnenburg S, 2006, J MACH LEARN RES, V7, P1531
   Strobl EV, 2013, 2013 12TH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA 2013), VOL 1, P414, DOI 10.1109/ICMLA.2013.84
   Varma M., 2009, P 26 ANN INT C MACH, P1065, DOI DOI 10.1145/1553374.1553510
   Vishwanathan S. V. N., 2010, ADV NEURAL INFORM PR, P2361
   Wiering M. e. a., 2013, 25 BEN ART INT C, P1
   Xu XX, 2013, IEEE T NEUR NET LEAR, V24, P749, DOI 10.1109/TNNLS.2012.2237183
   Xu Z, 2009, ADV NEURAL INFORM PR, P1825
   Zhuang J., 2011, P 14 INT C ART INT S, P909
   Zien A, 2007, P 24 INT C MACH LEAR, P1191, DOI DOI 10.1145/1273496.1273646
NR 30
TC 9
Z9 9
U1 0
U2 9
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 0941-0643
EI 1433-3058
J9 NEURAL COMPUT APPL
JI Neural Comput. Appl.
PD DEC
PY 2016
VL 27
IS 8
SI SI
BP 2305
EP 2314
DI 10.1007/s00521-015-2066-x
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA EI3UT
UT WOS:000392418300010
DA 2020-02-19
ER

PT J
AU Gong, WJ
   Zhang, XN
   Gonzalez, J
   Sobral, A
   Bouwmans, T
   Tu, CH
   Zahzah, E
AF Gong, Wenjuan
   Zhang, Xuena
   Gonzalez, Jordi
   Sobral, Andrews
   Bouwmans, Thierry
   Tu, Changhe
   Zahzah, El-hadi
TI Human Pose Estimation from Monocular Images: A Comprehensive Survey
SO SENSORS
LA English
DT Article
DE human pose estimation; human body models; generative methods;
   discriminative methods; top-down methods; bottom-up methods
ID 3D HUMAN POSE; POINT SET REGISTRATION; HUMAN MOTION ANALYSIS; ACTIVE
   SHAPE MODELS; OBJECT RECOGNITION; BODY POSE; PICTORIAL STRUCTURES;
   FLEXIBLE MIXTURES; TRACKING; CAPTURE
AB Human pose estimation refers to the estimation of the location of body parts and how they are connected in an image. Human pose estimation from monocular images has wide applications (e.g., image indexing). Several surveys on human pose estimation can be found in the literature, but they focus on a certain category; for example, model-based approaches or human motion analysis, etc. As far as we know, an overall review of this problem domain has yet to be provided. Furthermore, recent advancements based on deep learning have brought novel algorithms for this problem. In this paper, a comprehensive survey of human pose estimation from monocular images is carried out including milestone works and recent advancements. Based on one standard pipeline for the solution of computer vision problems, this survey splits the problem into several modules: feature extraction and description, human body models, and modeling methods. Problem modeling methods are approached based on two means of categorization in this survey. One way to categorize includes top-down and bottom-up methods, and another way includes generative and discriminative methods. Considering the fact that one direct application of human pose estimation is to provide initialization for automatic video surveillance, there are additional sections for motion-related methods in all modules: motion features, motion models, and motion-based methods. Finally, the paper also collects 26 publicly available data sets for validation and provides error measurement methods that are frequently used.
C1 [Gong, Wenjuan; Zhang, Xuena] China Univ Petr, Dept Comp Sci & Technol, Qingdao 266580, Peoples R China.
   [Gonzalez, Jordi] Univ Autonoma Barcelona, Comp Vis Ctr, Catalonia 08193, Spain.
   [Sobral, Andrews; Bouwmans, Thierry] Univ La Rochelle, Lab MIA, F-17042 La Rochelle, France.
   [Sobral, Andrews; Zahzah, El-hadi] Univ La Rochelle, Lab L3i, F-17042 La Rochelle, France.
   [Tu, Changhe] Shandong Univ, Sch Comp Sci & Technol, Jinan 250100, Peoples R China.
RP Gong, WJ (reprint author), China Univ Petr, Dept Comp Sci & Technol, Qingdao 266580, Peoples R China.
EM wenjuangong@upc.edu.cn; xuena_zhanghh@163.com; poal@cvc.uab.es;
   andrews.sobral@univ-lr.fr; thierry.bouwmans@univ-lr.fr; chtu@sdu.edu.cn;
   ezahzah@univ-lr.fr
RI BOUWMANS, Thierry/H-7041-2017; Gonzalez, Jordi/I-1812-2015
OI BOUWMANS, Thierry/0000-0003-4018-8856; Gonzalez,
   Jordi/0000-0001-8033-0306; Sobral, Andrews/0000-0002-1047-3755; Gong,
   Wenjuan/0000-0001-7805-3629
FU Natural Science Foundation of ShandongNatural Science Foundation of
   Shandong Province [ZR2015FL015]; Qingdao Technology Plan
   [15-9-1-69-jch]; Ministry of Science and Technology of ChinaMinistry of
   Science and Technology, China [2015IM010300]; MINECO/FEDER
   [TIN2015-65464-R]; COST Action iV&L Net (European Network on Integrating
   Vision and Language) [IC1307]; COST (European Cooperation in Science and
   Technology)European Cooperation in Science and Technology (COST)
FX his work has been supported by the Natural Science Foundation of
   Shandong with project number ZR2015FL015, the Qingdao Technology Plan
   with project number 15-9-1-69-jch, the Ministry of Science and
   Technology of China with project number 2015IM010300, the Spanish
   project TIN2015-65464-R (MINECO/FEDER) and the COST Action IC1307 iV&L
   Net (European Network on Integrating Vision and Language), supported by
   COST (European Cooperation in Science and Technology).
CR Agarwal A, 2006, LECT NOTES COMPUT SC, V3951, P30
   Agarwal A, 2006, LECT NOTES COMPUT SC, V3851, P50
   Agarwal A, 2006, IEEE T PATTERN ANAL, V28, P44, DOI 10.1109/TPAMI.2006.21
   Agarwal A, 2004, LECT NOTES COMPUT SC, V3023, P54
   Agarwal A, 2004, PROC CVPR IEEE, P882
   Agarwal A., 2004, P INT C MACH LEARN, P9
   Aggarwal JK, 2011, ACM COMPUT SURV, V43, DOI 10.1145/1922649.1922653
   Akhter I, 2015, PROC CVPR IEEE, P1446, DOI 10.1109/CVPR.2015.7298751
   Allen B, 2002, ACM T GRAPHIC, V21, P612
   Amit Y, 1997, NEURAL COMPUT, V9, P1545, DOI 10.1162/neco.1997.9.7.1545
   Andriluka M, 2008, PROC CVPR IEEE, P1873, DOI 10.1109/CVPR.2008.4587583
   Andriluka M, 2010, PROC CVPR IEEE, P623, DOI 10.1109/CVPR.2010.5540156
   Andriluka M, 2009, PROC CVPR IEEE, P1014, DOI 10.1109/CVPRW.2009.5206754
   Anguelov D, 2005, ACM T GRAPHIC, V24, P408, DOI 10.1145/1073204.1073207
   [Anonymous], 2012, P 29 INT C MACH LEAR, P297
   ARKIN EM, 1991, IEEE T PATTERN ANAL, V13, P209, DOI 10.1109/34.75509
   Baak A., 2013, CONSUMER DEPTH CAMER, P71, DOI DOI 10.1007/978-1-4471-4640-7_5
   Babagholami-Mohamadabadi B, 2014, IEEE SIGNAL PROC LET, V21, P297, DOI 10.1109/LSP.2014.2301726
   Balan A.O., 2006, CVPR, V1, P758
   Balan AO, 2007, IEEE I CONF COMP VIS, P1379
   Balan Alexandru O., 2007, P IEEE C COMP VIS PA, P1
   Barbulescu A, 2012, INT C PATT RECOG, P2484
   Baumberg A., 1994, P EUR C COMP VIS STO, P297
   Baumberg A. M., 1995, THESIS
   Belagiannis V, 2014, PROC CVPR IEEE, P1669, DOI 10.1109/CVPR.2014.216
   Belagiannis V, 2014, LECT NOTES COMPUT SC, V8563, P20, DOI 10.1007/978-3-319-08849-5_3
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   BISSACCO A, 2006, P NIPS, P169
   Blinn J. F., 1977, ACM SIGGRAPH COMPUTE, V11, P192, DOI [DOI 10.1145/965141.563893, 10.1145/563858.563893]
   Bo LF, 2010, INT J COMPUT VISION, V87, P28, DOI 10.1007/s11263-008-0204-y
   Bo YH, 2013, IEEE COMPUT SOC CONF, P1041, DOI 10.1109/CVPRW.2013.151
   Bourdev L, 2010, LECT NOTES COMPUT SC, V6316, P168, DOI 10.1007/978-3-642-15567-3_13
   Bowden R, 2000, IMAGE VISION COMPUT, V18, P729, DOI 10.1016/S0262-8856(99)00076-1
   Bray M, 2006, LECT NOTES COMPUT SC, V3952, P642
   Bregler C, 2004, INT J COMPUT VISION, V56, P179, DOI 10.1023/B:VISI.0000011203.00237.9b
   Bregler C, 1998, PROC CVPR IEEE, P8, DOI 10.1109/CVPR.1998.698581
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Brox T, 2005, LECT NOTES COMPUT SC, V3663, P109
   BRUBAKER MA, 2007, [No title captured], P1
   Brubaker MA, 2010, INT J COMPUT VISION, V87, P140, DOI 10.1007/s11263-009-0274-5
   BUNTINE W, 1992, MACH LEARN, V8, P75, DOI 10.1023/A:1022686419106
   Buys K, 2014, J VIS COMMUN IMAGE R, V25, P39, DOI 10.1016/j.jvcir.2013.03.011
   Carreira J, 2016, PROC CVPR IEEE, P4733, DOI 10.1109/CVPR.2016.512
   CEDRAS C, 1995, IMAGE VISION COMPUT, V13, P129, DOI 10.1016/0262-8856(95)93154-K
   Chang JY, 2013, ETRI J, V35, P949, DOI 10.4218/etrij.13.2013.0063
   Chen C, 2011, COMPUT VIS IMAGE UND, V115, P290, DOI 10.1016/j.cviu.2010.11.007
   Chen LL, 2013, PATTERN RECOGN LETT, V34, P1995, DOI 10.1016/j.patrec.2013.02.006
   Chen XJ, 2015, PROC CVPR IEEE, P3945, DOI 10.1109/CVPR.2015.7299020
   Chen Xianjie, 2014, ADV NEURAL INFORM PR, P1736
   Cheng SY, 2004, 2004 IEEE INTELLIGENT VEHICLES SYMPOSIUM, P84
   Cherian A, 2014, PROC CVPR IEEE, P2361, DOI 10.1109/CVPR.2014.302
   Cheung GKM, 2004, 2ND INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P373, DOI 10.1109/TDPVT.2004.1335262
   Cheung GKM, 2003, PROC CVPR IEEE, P77
   Christoudias CM, 2005, PROC CVPR IEEE, P1067
   COOTES TF, 1995, COMPUT VIS IMAGE UND, V61, P38, DOI 10.1006/cviu.1995.1004
   Cour T, 2008, LECT NOTES COMPUT SC, V5305, P158, DOI 10.1007/978-3-540-88693-8_12
   Cour T, 2009, PROC CVPR IEEE, P919, DOI 10.1109/CVPRW.2009.5206667
   Dalal N, 2005, PROC CVPR IEEE, P886
   Dalal N, 2006, LECT NOTES COMPUT SC, V3952, P428
   Dantone M, 2013, PROC CVPR IEEE, P3041, DOI 10.1109/CVPR.2013.391
   Datta A., 2008, P IEEE C COMP VIS PA, P1
   Daubney B., 2011, P BRIT MACH VIS C DU, P1
   de Aguiar E., 2007, IEEE INT C COMP VIS, P1
   Demirdjian D, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1071
   Deutscher J, 2005, INT J COMPUT VISION, V61, P185, DOI 10.1023/B:VISI.0000043757.18370.9c
   Dimitrijevic M, 2006, COMPUT VIS IMAGE UND, V104, P127, DOI 10.1016/j.cviu.2006.07.007
   Dinh DL, 2014, APPL INTELL, V41, P473, DOI 10.1007/s10489-014-0535-z
   Duan K, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.116
   Eichner M, 2012, INT J COMPUT VISION, V99, P190, DOI 10.1007/s11263-012-0524-9
   Eichner M., 2009, P BRIT MACH VIS C, P1
   Eichner M, 2012, IEEE T PATTERN ANAL, V34, P2282, DOI 10.1109/TPAMI.2012.85
   Eichner M, 2010, LECT NOTES COMPUT SC, V6311, P228, DOI 10.1007/978-3-642-15549-9_17
   Ek C. H., 2007, MACHINE LEARNING MUL, P132, DOI DOI 10.1007/978-3-540-78155-4
   Elgammal A, 2004, PROC CVPR IEEE, P681
   Fan XC, 2015, PROC CVPR IEEE, P1347, DOI 10.1109/CVPR.2015.7298740
   Felzenszwalb Pedro, 2008, P IEEE C COMP VIS PA, V08, P1, DOI DOI 10.1109/CVPR.2008.4587597
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Felzenszwalb PF, 2005, INT J COMPUT VISION, V61, P55, DOI 10.1023/B:VISI.0000042934.15159.49
   Ferrari V, 2009, PROC CVPR IEEE, P1, DOI 10.1109/CVPRW.2009.5206495
   Ferrari V, 2009, LECT NOTES COMPUT SC, V5604, P128, DOI 10.1007/978-3-642-03061-1_7
   Flitti F, 2010, IEEE IMAGE PROC, P1517, DOI 10.1109/ICIP.2010.5652502
   Fragkiadaki K, 2013, PROC CVPR IEEE, P2059, DOI 10.1109/CVPR.2013.268
   Freifeld O, 2012, LECT NOTES COMPUT SC, V7572, P1, DOI 10.1007/978-3-642-33718-5_1
   Freifeld O, 2010, PROC CVPR IEEE, P639, DOI 10.1109/CVPR.2010.5540154
   Gall J, 2010, LECT NOTES COMPUT SC, V6313, P425
   Gall J, 2009, PROC CVPR IEEE, P1746, DOI 10.1109/CVPRW.2009.5206755
   Gall J, 2010, INT J COMPUT VISION, V87, P75, DOI 10.1007/s11263-008-0173-1
   Ganapathi V, 2012, LECT NOTES COMPUT SC, V7577, P738, DOI 10.1007/978-3-642-33783-3_53
   Ganapathi V, 2010, PROC CVPR IEEE, P755, DOI 10.1109/CVPR.2010.5540141
   Gavrila DM, 2007, IEEE T PATTERN ANAL, V29, P1408, DOI 10.1109/TPAMI.2007.1062
   Ge S, 2015, IEEE WINT CONF APPL, P94, DOI 10.1109/WACV.2015.20
   Ge S, 2015, SENSORS-BASEL, V15, P15218, DOI 10.3390/s150715218
   Ghosh S., 2015, P ADV NEUR INF PROC, P1997
   Girshick R, 2011, IEEE I CONF COMP VIS, P415, DOI 10.1109/ICCV.2011.6126270
   Gkioxari G., 2014, ARXIV14065212
   Gkioxari G, 2015, IEEE I CONF COMP VIS, P1080, DOI 10.1109/ICCV.2015.129
   Gkioxari G, 2014, PROC CVPR IEEE, P3582, DOI 10.1109/CVPR.2014.458
   Gong S., 2010, P 1 ACM MULT FIR IT, P47
   Gong W., 2012, EURASIP J ADV SIG PR, V2012, P1
   Gorelick L, 2006, IEEE T PATTERN ANAL, V28, P1991, DOI 10.1109/TPAMI.2006.253
   Grauman K, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P641
   Guan P, 2009, IEEE I CONF COMP VIS, P1381
   Gudukbay U, 2013, DIGIT SIGNAL PROCESS, V23, P1441, DOI 10.1016/j.dsp.2013.06.008
   Guo F., 2008, J IMAGE VIDEO PROCES, V2008, P4
   Gupta A, 2008, P IEEE C COMP VIS PA, V2008, P1, DOI DOI 10.1109/CVPR.2008.4587511
   Hahn M, 2007, 3DIM 2007: SIXTH INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P257
   Hara Kota, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P687, DOI 10.1109/FG.2011.5771331
   Hara K, 2013, PROC CVPR IEEE, P3390, DOI 10.1109/CVPR.2013.435
   Hernandez N, 2008, J CHEMOMETR, V22, P686, DOI 10.1002/cem.1168
   Hernandez-Vela A, 2012, PROC CVPR IEEE, P726, DOI 10.1109/CVPR.2012.6247742
   Hirota M., 2003, P INT TECHN C ENH SA, P1
   Hou S., 2007, P 11 IEEE INT C COMP, P1
   Hu WM, 2004, IEEE T SYST MAN CY C, V34, P334, DOI 10.1109/TSMCC.2004.829274
   Huang JB, 2010, PROC CVPR IEEE, P3618, DOI 10.1109/CVPR.2010.5539919
   Huang JB, 2010, LECT NOTES COMPUT SC, V5994, P48
   Huayan Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2433, DOI 10.1109/CVPR.2011.5995722
   Ionescu C, 2011, IEEE I CONF COMP VIS, P2220, DOI 10.1109/ICCV.2011.6126500
   Ionescu C, 2009, IEEE I CONF COMP VIS, P1157, DOI 10.1109/ICCV.2009.5459346
   Jaeggli T, 2007, LECT NOTES COMPUT SC, V4843, P608
   Jain A., 2014, P AS C COMP VIS ACCV, p302~315
   Jain A., 2014, ARXIV13127302
   Jiang H., 2010, ACCV, P228
   Jiang H, 2011, IEEE T PATTERN ANAL, V33, P1911, DOI 10.1109/TPAMI.2011.92
   Johnson S., 2010, P BRIT MACH VIS C, DOI [DOI 10.5244/C.24.12, 10.5244/C.24.12]
   Johnson S, 2011, PROC CVPR IEEE, P1465, DOI 10.1109/CVPR.2011.5995318
   JORDAN MI, 1994, NEURAL COMPUT, V6, P181, DOI 10.1162/neco.1994.6.2.181
   Ju SX, 1996, PROCEEDINGS OF THE SECOND INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, P38, DOI 10.1109/AFGR.1996.557241
   KAKADIARIS IA, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P618, DOI 10.1109/ICCV.1995.466881
   Kanaujia A., 2014, ARXIV14100117
   Kanaujia A., 2007, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2007.383341
   Kehl R, 2005, PROC CVPR IEEE, P129
   Kiefel M, 2014, LECT NOTES COMPUT SC, V8693, P331, DOI 10.1007/978-3-319-10602-1_22
   Kohli P, 2005, IEEE I CONF COMP VIS, P922
   Kohli P, 2008, INT J COMPUT VISION, V79, P285, DOI 10.1007/s11263-007-0120-6
   Kolmogorov V, 2004, IEEE T PATTERN ANAL, V26, P147, DOI 10.1109/TPAMI.2004.1262177
   Komodakis N, 2011, IEEE T PATTERN ANAL, V33, P531, DOI 10.1109/TPAMI.2010.108
   Koschan A, 2003, PATTERN RECOGN LETT, V24, P1751, DOI 10.1016/S0167-8655(02)00330-6
   Krotosky SJ, 2004, 2004 IEEE INTELLIGENT VEHICLES SYMPOSIUM, P698
   Kuo P, 2011, COMPUT VIS IMAGE UND, V115, P242, DOI 10.1016/j.cviu.2010.09.001
   Ladicky L, 2013, PROC CVPR IEEE, P3578, DOI 10.1109/CVPR.2013.459
   Lallemand J, 2014, LECT NOTES COMPUT SC, V8563, P10, DOI 10.1007/978-3-319-08849-5_2
   Lazebnik S., 2006, P IEEE INT C COMP VI, V2, P2169, DOI DOI 10.1109/CVPR.2006.68
   Lee CS, 2010, INT J COMPUT VISION, V87, P118, DOI 10.1007/s11263-009-0266-5
   Lee CS, 2007, IEEE I CONF COMP VIS, P1570
   Lee MW, 2006, LECT NOTES COMPUT SC, V3953, P368
   Lee MW, 2004, LECT NOTES COMPUT SC, V3022, P126
   Lehrmann AM, 2013, IEEE I CONF COMP VIS, P1281, DOI 10.1109/ICCV.2013.162
   Lepetit Vincent, 2005, Foundations and Trends in Computer Graphics and Vision, V1, P1, DOI 10.1561/0600000001
   Lepetit V, 2006, IEEE T PATTERN ANAL, V28, P1465, DOI 10.1109/TPAMI.2006.188
   Li SJ, 2015, LECT NOTES COMPUT SC, V9004, P332, DOI 10.1007/978-3-319-16808-1_23
   Li SJ, 2014, IEEE COMPUT SOC CONF, P488, DOI 10.1109/CVPRW.2014.78
   Li Y, 2009, PROCEEDINGS OF 2009 CONFERENCE ON SYSTEMS SCIENCE, MANAGEMENT SCIENCE & SYSTEM DYNAMICS, VOL 4, P49, DOI 10.1145/1631005.1631018
   Liefeng Bo, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2403, DOI 10.1109/CVPRW.2009.5206699
   Liu YB, 2011, PROC CVPR IEEE, P1249, DOI 10.1109/CVPR.2011.5995424
   Liu Z, 2015, J VIS COMMUN IMAGE R, V32, P10, DOI 10.1016/j.jvcir.2015.06.013
   LOWE D. G, 1999, P IEEE INT C COMP VI, V1999, P1150, DOI DOI 10.1109/ICCV.1999.790410
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu HC, 2013, IEEE T IMAGE PROCESS, V22, P4040, DOI 10.1109/TIP.2013.2268975
   Lu Y., 2013, P 47 ANN C INF SCI S, P1
   Memisevic R, 2012, IEEE T PATTERN ANAL, V34, P778, DOI 10.1109/TPAMI.2011.154
   METAXAS D, 1993, IEEE T PATTERN ANAL, V15, P580, DOI 10.1109/34.216727
   Moeslund TB, 2001, COMPUT VIS IMAGE UND, V81, P231, DOI 10.1006/cviu.2000.0897
   Moeslund TB, 2006, COMPUT VIS IMAGE UND, V104, P90, DOI 10.1016/j.cviu.2006.08.002
   Morariu V. I., 2006, P IEEE INT C COMP VI, p545~552
   Mori G, 2005, IEEE I CONF COMP VIS, P1417
   Mori G, 2006, IEEE T PATTERN ANAL, V28, P1052, DOI 10.1109/TPAMI.2006.149
   Mori G, 2005, IEEE T PATTERN ANAL, V27, P1832, DOI 10.1109/TPAMI.2005.220
   Mori G., 2015, ARXIV150700302
   Nakariyakul S, 2014, INFORM SCIENCES, V278, P545, DOI 10.1016/j.ins.2014.03.072
   Nayak S, 2009, IEEE T PATTERN ANAL, V31, P795, DOI 10.1109/TPAMI.2008.80
   Nie BX, 2015, PROC CVPR IEEE, P1293, DOI 10.1109/CVPR.2015.7298734
   Ning H., 2008, P IEEE C COMP VIS PA, P1, DOI [DOI 10.1109/CVPR.2008.4587534, DOI 10.1109/ICPR.2008.4761237]
   Okada R, 2008, LECT NOTES COMPUT SC, V5303, P434, DOI 10.1007/978-3-540-88688-4_32
   Oleinikov G, 2014, IEEE WINT CONF APPL, P682, DOI 10.1109/WACV.2014.6836036
   Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7
   Orrite-Urunuela C, 2004, INT C PATT RECOG, P244, DOI 10.1109/ICPR.2004.1333749
   Ouyang W., 2014, P IEEE C COMP VIS PA, P2329
   Parameswaran V, 2004, PROC CVPR IEEE, P16
   Park SI, 2006, ACM T GRAPHIC, V25, P881, DOI 10.1145/1141911.1141970
   Penmetsa Surya, 2014, ELECT LETT COMPUT VI, V13, P18
   Perez-Sala X, 2014, SENSORS-BASEL, V14, P4189, DOI 10.3390/s140304189
   Pfister T, 2015, IEEE I CONF COMP VIS, P1913, DOI 10.1109/ICCV.2015.222
   Pfister T, 2015, LECT NOTES COMPUT SC, V9003, P538, DOI 10.1007/978-3-319-16865-4_35
   Pinto N, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.0040027
   Pishchulin L, 2013, PROC CVPR IEEE, P588, DOI 10.1109/CVPR.2013.82
   Pons-Moll Gerard, 2011, Pattern Recognition. Proceedings 33rd DAGM Symposium, P416, DOI 10.1007/978-3-642-23123-0_42
   Pons-Moll G., 2011, VISUAL ANAL HUMANS, P139, DOI DOI 10.1007/978-0-85729-997-0_9
   Pons-Moll G, 2011, IEEE I CONF COMP VIS, P1243, DOI 10.1109/ICCV.2011.6126375
   Poppe R, 2007, COMPUT VIS IMAGE UND, V108, P4, DOI 10.1016/j.cviu.2006.10.016
   Poppe R, 2010, IMAGE VISION COMPUT, V28, P976, DOI 10.1016/j.imavis.2009.11.014
   Radwan I, 2013, IEEE I CONF COMP VIS, P1888, DOI 10.1109/ICCV.2013.237
   Ramakrishna V, 2013, PROC CVPR IEEE, P3728, DOI 10.1109/CVPR.2013.478
   Ramanan D., 2006, NIPS, P1129
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   REHG JM, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P612, DOI 10.1109/ICCV.1995.466882
   Rius I, 2009, PATTERN RECOGN, V42, P2907, DOI 10.1016/j.patcog.2009.02.012
   Roberts TJ, 2004, LECT NOTES COMPUT SC, V2034, P291
   Rogez G., 2008, IEEE C COMP VIS PATT, P1
   Rogez G, 2008, PATTERN RECOGN, V41, P2926, DOI 10.1016/j.patcog.2008.02.012
   Ronfard R, 2002, LECT NOTES COMPUT SC, V2353, P700
   Rosales R, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P378, DOI 10.1109/ICCV.2001.937543
   Rosales R, 2006, INT J COMPUT VISION, V67, P251, DOI 10.1007/s11263-006-5165-4
   Roth S, 2004, PROC CVPR IEEE, P886
   Rothrock B, 2013, PROC CVPR IEEE, P3214, DOI 10.1109/CVPR.2013.413
   Sabzmeydani P., 2007, IEEE C COMP VIS PATT, P1, DOI DOI 10.1109/CVPR.2007.383134
   Sand P, 2003, ACM T GRAPHIC, V22, P578, DOI 10.1145/882262.882310
   Sapp B, 2013, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR.2013.471
   Sapp B, 2011, PROC CVPR IEEE, P1281, DOI 10.1109/CVPR.2011.5995607
   Sapp B, 2010, PROC CVPR IEEE, P422, DOI 10.1109/CVPR.2010.5540182
   Sapp B, 2010, LECT NOTES COMPUT SC, V6312, P406, DOI 10.1007/978-3-642-15552-9_30
   Scovanner P., 2007, P 15 INT C MULT, P357, DOI DOI 10.1145/1291233.1291311
   Sedai S, 2013, PATTERN RECOGN, V46, P3223, DOI 10.1016/j.patcog.2013.05.019
   Sedai S, 2011, C IND ELECT APPL, P293, DOI 10.1109/ICIEA.2011.5975597
   Sedai S, 2009, 2009 DIGITAL IMAGE COMPUTING: TECHNIQUES AND APPLICATIONS (DICTA 2009), P484, DOI 10.1109/DICTA.2009.81
   Sedai S., 2010, P BMVC, P1
   Serre T, 2005, PROC CVPR IEEE, P994
   Shakhnarovich G, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P750
   Shotton J, 2013, COMMUN ACM, V56, P116, DOI 10.1145/2398356.2398381
   Sidenbladh H., 2000, Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), P368, DOI 10.1109/AFGR.2000.840661
   Sidenbladh H, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P709, DOI 10.1109/ICCV.2001.937696
   Sidenbladh H., 2010, P EUR C COMP VIS HER, P702
   Sigal L., 2007, ADV NEURAL INFORM PR, P1337
   Sigal L, 2006, LECT NOTES COMPUT SC, V4069, P185
   Sigal L, 2012, INT J COMPUT VISION, V98, P15, DOI 10.1007/s11263-011-0493-4
   Sigal Leonid, 2006, P IEEE C COMP VIS PA, P2041
   Simo-Serra E, 2013, PROC CVPR IEEE, P3634, DOI 10.1109/CVPR.2013.466
   Slama R., 2013, P IEEE INT C WORKSH, P1
   Sminchisescu C, 2005, PROC CVPR IEEE, P390
   Sminchisescu C, 2003, INT J ROBOT RES, V22, P371, DOI 10.1177/0278364903022006003
   Sminchisescu C, 2001, PROC CVPR IEEE, P447
   SMINCHISESCU C, 2002, P INT C CENTR EUR CO
   Sminchisescu C., 2014, P INT C MACH LEARN B, P759
   Sminchisescu C., 2011, VISUAL ANAL HUMANS, P225
   Souvenir R., 2012, P C COMP VIS PATT RE, P1
   Stoll C, 2011, IEEE I CONF COMP VIS, P951, DOI 10.1109/ICCV.2011.6126338
   Sun L, 2014, MULTIMED TOOLS APPL, V73, P327, DOI 10.1007/s11042-013-1617-3
   Sun M, 2012, PROC CVPR IEEE, P1616, DOI 10.1109/CVPR.2012.6247854
   Sun M, 2011, IEEE I CONF COMP VIS, P723, DOI 10.1109/ICCV.2011.6126309
   Tashiro K, 2014, IEEE INT C SEMANT CO, P60, DOI 10.1109/ICSC.2014.20
   TAYLOR J, 2012, PROC CVPR IEEE, P103
   Tian J., 2014, J COMPUTER COMMUNICA, V2, P78
   Tian T. P., 2005, P IEEE COMP SOC C CO
   Tian TP, 2010, PROC CVPR IEEE, P81, DOI 10.1109/CVPR.2010.5540227
   Tian Y., 2010, P AS C COMP VIS, V3, P679
   Tian YD, 2012, LECT NOTES COMPUT SC, V7576, P256, DOI 10.1007/978-3-642-33715-4_19
   Tompson J., 2014, ADV NEURAL INFORM PR, V27, P1799
   Torres F., 2013, P COMP VIS WINT WORK
   Toshev A, 2014, PROC CVPR IEEE, P1653, DOI 10.1109/CVPR.2014.214
   Toyama K, 2002, INT J COMPUT VISION, V48, P9, DOI 10.1023/A:1014899027014
   Urtasun R, 2005, IEEE I CONF COMP VIS, P403
   Urtasun R, 2005, PROC CVPR IEEE, P932
   Urtasun R, 2004, LECT NOTES COMPUT SC, V3023, P92
   URTASUN R, 2006, CVPR, P238, DOI DOI 10.1109/CVPR.2006.15
   Urtasun R, 2008, PROC CVPR IEEE, P149
   Urtasun R, 2007, LECT NOTES COMPUT SC, V4814, P104
   Viola P, 2005, INT J COMPUT VISION, V63, P153, DOI 10.1007/s11263-005-6644-8
   Wang AH, 2014, APPL MATH INFORM SCI, V8, P2471, DOI 10.12785/amis/080543
   Wang CY, 2013, PROC CVPR IEEE, P915, DOI 10.1109/CVPR.2013.123
   Wang F, 2013, PROC CVPR IEEE, P596, DOI 10.1109/CVPR.2013.83
   Wang Jack M., 2007, P 24 INT C MACH LEAR, V227, P975, DOI DOI 10.1145/1273496.1273619
   Wang JJ, 2010, PROC CVPR IEEE, P3360, DOI 10.1109/CVPR.2010.5540018
   Wang L, 2003, IEEE T PATTERN ANAL, V25, P1505, DOI 10.1109/TPAMI.2003.1251144
   Wang Y, 2008, LECT NOTES COMPUT SC, V5304, P710, DOI 10.1007/978-3-540-88690-7_53
   Wang Y, 2012, J MACH LEARN RES, V13, P3075
   Wang Y, 2011, PROC CVPR IEEE, P1705, DOI 10.1109/CVPR.2011.5995519
   Wei XL, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778779
   Wei XLK, 2009, IEEE I CONF COMP VIS, P1873, DOI 10.1109/ICCV.2009.5459415
   Weinland D, 2011, COMPUT VIS IMAGE UND, V115, P224, DOI 10.1016/j.cviu.2010.10.002
   Weinrich C, 2013, IEEE SYS MAN CYBERN, P4384, DOI 10.1109/SMC.2013.748
   Weiss D.J., 2010, ADV NEURAL INFORM PR, P2415
   Wenjuan Gong, 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P1287, DOI 10.1109/ICCVW.2011.6130400
   Wren CR, 1997, IEEE T PATTERN ANAL, V19, P780, DOI 10.1109/34.598236
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Wu B, 2005, IEEE I CONF COMP VIS, P90
   Wu JX, 2011, IEEE INT CONF ROBOT, P860
   Wu Y, 2005, IEEE T PATTERN ANAL, V27, P1910, DOI 10.1109/TPAMI.2005.233
   Xiao Y, 2012, IEEE IMAGE PROC, P5, DOI 10.1109/ICIP.2012.6466781
   Yacoob Y, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P120, DOI 10.1109/ICCV.1998.710709
   Yang HY, 2005, IEEE COMP SOC ANN, P71
   YANG Mingjin, 2010, P 2010 IEEE AS PAC P, P1
   Yang Y, 2013, IEEE T PATTERN ANAL, V35, P1635, DOI 10.1109/TPAMI.2012.253
   Yang Y, 2013, IEEE T PATTERN ANAL, V35, P2878, DOI 10.1109/TPAMI.2012.261
   Yang Y, 2011, PROC CVPR IEEE, P1385, DOI 10.1109/CVPR.2011.5995741
   Yao A, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.67
   Yao A, 2012, INT J COMPUT VISION, V100, P16, DOI 10.1007/s11263-012-0532-9
   Youding Zhu, 2008, 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P1, DOI 10.1109/CVPRW.2008.4563163
   Yu TH, 2013, PROC CVPR IEEE, P3642, DOI 10.1109/CVPR.2013.467
   Yuan-Kai Wang, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P535, DOI 10.1109/ICPR.2010.136
   ZAHN CT, 1972, IEEE T COMPUT, VC 21, P269, DOI 10.1109/TC.1972.5008949
   ZHANG W., 2014, MATH PROBL ENG, V2014, P1, DOI DOI 10.1371/J0URNAL.P0NE.0099693
   Zhang WC, 2014, IEEE T IMAGE PROCESS, V23, P5374, DOI 10.1109/TIP.2014.2364113
   ZHAO X, 2008, P INT C PATT REC, P1, DOI DOI 10.1109/ICPR.2008.4761707
   Zhu Q., 2006, IEEE COMP SOC C COMP, P1491, DOI DOI 10.1109/CVPR.2006.119
   Zolfaghari M, 2014, MACH VISION APPL, V25, P1489, DOI 10.1007/s00138-014-0613-6
   Zuffi S, 2015, PROC CVPR IEEE, P3537, DOI 10.1109/CVPR.2015.7298976
   Zuffi S, 2012, PROC CVPR IEEE, P3546, DOI 10.1109/CVPR.2012.6248098
NR 296
TC 15
Z9 15
U1 0
U2 27
PU MDPI
PI BASEL
PA ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
EI 1424-8220
J9 SENSORS-BASEL
JI Sensors
PD DEC
PY 2016
VL 16
IS 12
AR 1966
DI 10.3390/s16121966
PG 39
WC Chemistry, Analytical; Engineering, Electrical & Electronic; Instruments
   & Instrumentation
SC Chemistry; Engineering; Instruments & Instrumentation
GA EG8JM
UT WOS:000391303000001
OA DOAJ Gold, Green Published
DA 2020-02-19
ER

PT J
AU Wu, ZY
   Ding, XQ
   Zhang, GR
AF Wu, Zhiyong
   Ding, Xiangqian
   Zhang, Guangrui
TI A Novel Method for Classification of ECG Arrhythmias Using Deep Belief
   Networks
SO INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE AND APPLICATIONS
LA English
DT Article
DE ECG arrhythmias classification; restricted Boltzmann machine; deep
   belief networks; deep learning
ID NEURAL-NETWORKS; LEARNING ALGORITHM; COMPONENT ANALYSIS; MORPHOLOGY; PCA
AB In this paper, a novel approach based on deep belief networks (DBN) for electrocardiograph (ECG) arrhythmias classification is proposed. The construction process of ECG classification model consists of two steps: features learning for ECG signals and supervised fine-tuning. In order to deeply extract features from continuous ECG signals, two types of restricted Boltzmann machine (RBM) including Gaussian-Bernoulli and Bernoulli-Bernoulli are stacked to form DBN. The parameters of RBM can be learned by two training algorithms such as contrastive divergence and persistent contrastive divergence. A suitable feature representation from the raw ECG data can therefore be extracted in an unsupervised way. In order to enhance the performance of DBN, a fine-tuning process is carried out, which uses backpropagation by adding a softmax regression layer on the top of the resulting hidden representation layer to perform multiclass classification. The method is then validated by experiments on the well-known MIT-BIH arrhythmia database. Considering the real clinical application, the interpatient heartbeat dataset is divided into two sets and grouped into four classes (N, S, V, F) following the recommendations of AAMI. The experiment results show our approach achieves better performance with less feature learning time than traditional hand-designed methods on the classification of ECG arrhythmias.
C1 [Wu, Zhiyong; Ding, Xiangqian; Zhang, Guangrui] Ocean Univ China, Coll Informat Sci & Engn, 23 Hongkong West Rd, Qingdao 266073, Shandong, Peoples R China.
   [Wu, Zhiyong] Shandong Univ Technol, Sch Comp Sci & Technol, 266 New Village West Rd, Zibo 255000, Shandong, Peoples R China.
RP Wu, ZY (reprint author), Ocean Univ China, Coll Informat Sci & Engn, 23 Hongkong West Rd, Qingdao 266073, Shandong, Peoples R China.; Wu, ZY (reprint author), Shandong Univ Technol, Sch Comp Sci & Technol, 266 New Village West Rd, Zibo 255000, Shandong, Peoples R China.
EM wuzhiyong_sdut@sina.com; dingxq1995@vip.sina.com;
   zhang.guangrui@outlook.com
FU National Key Technology R&D Program of ChinaNational Key Technology R&D
   Program [2015BAF04B02]; Funds for strategic development planning project
   of Qingdao [14-8-1-7-gx]; health services industry clusters oriented
   technology service innovation pilot project
FX This study is partially supported by The National Key Technology R&D
   Program of China (Grant No. 2015BAF04B02), the Funds for strategic
   development planning project of Qingdao (Grant No. 14-8-1-7-gx) and
   health services industry clusters oriented technology service innovation
   pilot project in 2015.
CR ACKLEY DH, 1985, COGNITIVE SCI, V9, P147
   Al Rahhal MM, 2016, INFORM SCIENCES, V345, P340, DOI 10.1016/j.ins.2016.01.082
   [Anonymous], 1999, EC571998R2008 ANSIAA
   Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Cecotti H., 2008, P 19 INT C PATT REC, P1, DOI DOI 10.1109/ICPR.2008.4761638
   Ceylan R, 2007, EXPERT SYST APPL, V33, P286, DOI 10.1016/j.eswa.2006.05.014
   Cho K, 2011, LECT NOTES COMPUT SC, V6791, P10, DOI 10.1007/978-3-642-21735-7_2
   de Chazal P, 2004, IEEE T BIO-MED ENG, V51, P1196, DOI 10.1109/TBME.2004.827359
   de Chazal P, 2006, IEEE T BIO-MED ENG, V53, P2535, DOI 10.1109/TBME.2006.883802
   Dilmac S, 2015, APPL SOFT COMPUT, V36, P641, DOI 10.1016/j.asoc.2015.07.010
   Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947
   Hayat M, 2015, IEEE T PATTERN ANAL, V37, P713, DOI 10.1109/TPAMI.2014.2353635
   Hinton G., 2012, NEURAL NETWORKS TRIC, V9, P599, DOI DOI 10.1007/978-3-642-35289-8_32
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton G. E., 1983, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, P448
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hinton GE, 1984, BOLTZMANN MACHINES C
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Inan OT, 2006, IEEE T BIO-MED ENG, V53, P2507, DOI 10.1109/TBME.2006.880879
   Ince T, 2009, IEEE T BIO-MED ENG, V56, P1415, DOI 10.1109/TBME.2009.2013934
   Jiang W, 2007, IEEE T NEURAL NETWOR, V18, P1750, DOI 10.1109/TNN.2007.900239
   Kampouraki A, 2009, IEEE T INF TECHNOL B, V13, P512, DOI 10.1109/TITB.2008.2003323
   Khalaf AF, 2015, EXPERT SYST APPL, V42, P8361, DOI 10.1016/j.eswa.2015.06.046
   Kiranyaz S, 2016, IEEE T BIO-MED ENG, V63, P664, DOI 10.1109/TBME.2015.2468589
   Korurek M, 2008, J BIOMED INFORM, V41, P874, DOI 10.1016/j.jbi.2008.01.014
   Krizhevsky A, 2009, LEARNING MULTIPLE LA
   Langkvist M., 2012, ADV ARTIFICIAL NEURA, V2012, P5, DOI DOI 10.1155/2012/107046
   Langkvist M, 2014, PATTERN RECOGN LETT, V42, P11, DOI 10.1016/j.patrec.2014.01.008
   Luz EJD, 2013, EXPERT SYST APPL, V40, P3561, DOI 10.1016/j.eswa.2012.12.063
   Martis RJ, 2013, BIOMED SIGNAL PROCES, V8, P437, DOI 10.1016/j.bspc.2013.01.005
   Martis RJ, 2012, EXPERT SYST APPL, V39, P11792, DOI 10.1016/j.eswa.2012.04.072
   Moody GA, 2001, IEEE ENG MED BIOL, V20, P45, DOI 10.1109/51.932724
   Polat K, 2007, APPL MATH COMPUT, V186, P898, DOI 10.1016/j.amc.2006.08.020
   Rodríguez R., 2015, J. appl. res. technol, V13, P261
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Shadmand S, 2016, BIOMED SIGNAL PROCES, V25, P12, DOI 10.1016/j.bspc.2015.10.008
   Shen CP, 2012, EXPERT SYST APPL, V39, P7845, DOI 10.1016/j.eswa.2012.01.093
   Shin HC, 2013, IEEE T PATTERN ANAL, V35, P1930, DOI 10.1109/TPAMI.2012.277
   Smolensky P, 1986, PARALLEL DISTRIBUTED, P194
   Soria ML, 2009, COMPUT CARDIOL, V36, P561
   Tieleman T., 2008, P 25 INT C MACH LEAR, P1064, DOI DOI 10.1145/1390156.1390290
   Tieleman T., 2009, P 26 ANN INT C MACH, P1033, DOI DOI 10.1145/1553374.1553506
   Ubeyli ED, 2007, DIGIT SIGNAL PROCESS, V17, P675, DOI 10.1016/j.dsp.2006.11.009
   Waktare JEP, 1997, PACE, V20, P2658, DOI 10.1111/j.1540-8159.1997.tb06114.x
   Wang D., 2012, INT J INF ED TECHNOL, V3, P505
   Wang YS, 2016, NEUROCOMPUTING, V184, P232, DOI 10.1016/j.neucom.2015.08.104
   Yu SN, 2008, EXPERT SYST APPL, V34, P2841, DOI 10.1016/j.eswa.2007.05.006
   Yu SN, 2009, EXPERT SYST APPL, V36, P2088, DOI 10.1016/j.eswa.2007.12.016
NR 49
TC 11
Z9 12
U1 2
U2 21
PU WORLD SCIENTIFIC PUBL CO PTE LTD
PI SINGAPORE
PA 5 TOH TUCK LINK, SINGAPORE 596224, SINGAPORE
SN 1469-0268
EI 1757-5885
J9 INT J COMPUT INTELL
JI Int. J. Comput. Intell. Appl.
PD DEC
PY 2016
VL 15
IS 4
AR 1650021
DI 10.1142/S1469026816500218
PG 14
WC Computer Science, Artificial Intelligence
SC Computer Science
GA EG3JB
UT WOS:000390938800003
DA 2020-02-19
ER

PT J
AU Eryilmaz, SB
   Neftci, E
   Joshi, S
   Kim, S
   BrightSky, M
   Lung, HL
   Lam, C
   Cauwenberghs, G
   Wong, HSP
AF Eryilmaz, Sukru Burc
   Neftci, Emre
   Joshi, Siddharth
   Kim, SangBum
   BrightSky, Matthew
   Lung, Hsiang-Lan
   Lam, Chung
   Cauwenberghs, Gert
   Wong, Hon-Sum Philip
TI Training a Probabilistic Graphical Model With Resistive Switching
   Electronic Synapses
SO IEEE TRANSACTIONS ON ELECTRON DEVICES
LA English
DT Article
DE Brain-inspired hardware; cognitive computing; neuromorphic computing;
   phase change memory(PCM); resistive memory
ID MEMORY; NETWORK; SYSTEM; POWER
AB Current large-scale implementations of deep learning and data mining require thousands of processors, massive amounts of off-chip memory, and consume giga-joules of energy. New memory technologies, such as nanoscale two-terminal resistive switching memory devices, offer a compact, scalable, and low-power alternative that permits on-chip colocated processing and memory in fine-grain distributed parallel architecture. Here, we report the first use of resistive memory devices for implementing and training a restricted Boltzmann machine (RBM), a generative probabilistic graphical model as a key component for unsupervised learning in deep networks. We experimentally demonstrate a 45-synapse RBM realized with 90 resistive phase change memory (PCM) elements trained with a bioinspired variant of the contrastive divergence algorithm, implementing Hebbian and anti-Hebbian weight updates. The resistive PCM devices show a twofold to tenfold reduction in error rate in a missing pixel pattern completion task trained over 30 epochs, compared with untrained case. Measured programming energy consumption is 6.1 nJ per epoch with the PCM devices, a factor of similar to 150 times lower than the conventional processor-memory systems. We analyze and discuss the dependence of learning performance on cycle-to-cycle variations and number of gradual levels in the PCM analog memory devices.
C1 [Eryilmaz, Sukru Burc; Wong, Hon-Sum Philip] Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA.
   [Neftci, Emre] Univ Calif Irvine, Dept Cognit Sci, Irvine, CA 92697 USA.
   [Joshi, Siddharth] Univ Calif San Diego, Dept Elect & Comp Engn, San Diego, CA 92093 USA.
   [Kim, SangBum; BrightSky, Matthew; Lam, Chung] IBM Res, Yorktown Hts, NY 10598 USA.
   [Lung, Hsiang-Lan] Macronix Int Co Ltd, Emerging Cent Lab, Hsinchu Sci Pk, Hsinchu 300, Taiwan.
   [Cauwenberghs, Gert] Univ Calif San Diego, Dept Bioengn, San Diego, CA 92093 USA.
RP Eryilmaz, SB (reprint author), Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA.
EM eryilmaz@stanford.edu; eneftci@uci.edu; sijoshi@eng.ucsd.edu;
   sangbum.kim@us.ibm.com; breitm@us.ibm.com; sllung@mxic.com.tw;
   clam@us.ibm.com; gert@ucsd.edu; hspwong@stanford.edu
RI Kim, SangBum/B-7069-2016
OI Kim, SangBum/0000-0001-7460-3750
FU SONIC, one of six centers of STARnet, a Semiconductor Research
   Corporation Program - MARCO; DARPAUnited States Department of
   DefenseDefense Advanced Research Projects Agency (DARPA); NSF Expedition
   on Computing under Visual Cortex on Silicon [1317470]; Member Companies
   of the Stanford Non-Volatile Memory Technology Research Initiative;
   Stanford SystemX Alliance
FX This work was supported in part by SONIC, one of six centers of STARnet,
   a Semiconductor Research Corporation Program sponsored by MARCO and
   DARPA, in part by NSF Expedition on Computing under Visual Cortex on
   Silicon under Award 1317470, and in part by the Member Companies of the
   Stanford Non-Volatile Memory Technology Research Initiative and the
   Stanford SystemX Alliance. The review of this paper was arranged by
   Editor G.-H. Koh. (Corresponding author: S. Burc Eryilmaz.)
CR Aly MMS, 2015, COMPUTER, V48, P24, DOI 10.1109/MC.2015.376
   [Anonymous], 2016, DUAL CURR INP AN TO
   [Anonymous], 2016, 16 CHANN CURR INP AN
   Atzori L, 2010, COMPUT NETW, V54, P2787, DOI 10.1016/j.comnet.2010.05.010
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   Bichler O, 2012, IEEE T ELECTRON DEV, V59, P2206, DOI 10.1109/TED.2012.2197951
   Burr G. W., 2014, IEDM, DOI [10.1109/IEDM.2014.7047135, DOI 10.1109/IEDM.2014.7047135]
   Chanthbouala A, 2012, NAT MATER, V11, P860, DOI [10.1038/nmat3415, 10.1038/NMAT3415]
   Chen PY, 2015, DES AUT TEST EUROPE, P854
   Erhan D, 2010, J MACH LEARN RES, V11, P625
   Eryilmaz S. B., 2015, P IEEE INT EL DEV M
   Eryilmaz SB, 2014, FRONT NEUROSCI-SWITZ, V8, DOI 10.3389/fnins.2014.00205
   Han S., 2016, EIE EFFICIENT INFERE
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hong S, 2010, CONF PROC INT SYMP C, P280
   Jeddeloh J., 2012, 2012 IEEE Symposium on VLSI Technology, P87, DOI 10.1109/VLSIT.2012.6242474
   Kang S, 2007, IEEE J SOLID-ST CIRC, V42, P210, DOI 10.1109/JSSC.2006.888349
   Kang U, 2010, IEEE J SOLID-ST CIRC, V45, P111, DOI 10.1109/JSSC.2009.2034408
   Kim JS, 2012, IEEE J SOLID-ST CIRC, V47, P107, DOI 10.1109/JSSC.2011.2164731
   Kim S., 2015, P IEEE IEDM
   Kuzum D., 2011, P IEDM
   Kuzum D, 2013, NANOTECHNOLOGY, V24, DOI 10.1088/0957-4484/24/38/382001
   Kuzum D, 2012, NANO LETT, V12, P2179, DOI 10.1021/nl201040y
   Lai YF, 2006, APPL PHYS A-MATER, V84, P21, DOI 10.1007/s00339-006-3571-7
   Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050
   Le Q. V., 2012, ICML, P8595
   Le Roux N, 2008, NEURAL COMPUT, V20, P1631, DOI 10.1162/neco.2008.04-07-510
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lee DH, 2015, LECT NOTES ARTIF INT, V9284, P498, DOI 10.1007/978-3-319-23528-8_31
   Lefurgy C, 2003, COMPUTER, V36, P39, DOI 10.1109/MC.2003.1250880
   Liu B, 2004, SEMICOND SCI TECH, V19, pL61, DOI 10.1088/0268-1242/19/6/L01
   MacKay D. J. C., 2003, INFORM THEORY INFERE
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Natarajan S., 2014, P IEEE INT EL DEV M
   Neftci E, 2014, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00272
   Nirschl T, 2007, INT EL DEVICES MEET, P461, DOI 10.1109/IEDM.2007.4418973
   Ohno T, 2011, NAT MATER, V10, P591, DOI [10.1038/nmat3054, 10.1038/NMAT3054]
   Park S, 2015, SCI REP-UK, V5, DOI 10.1038/srep10123
   Prezioso M, 2015, NATURE, V521, P61, DOI 10.1038/nature14441
   Rajendran B, 2011, P IEEE INT MEM WORKS, P1
   Salakhutdinov R., 2008, P 25 INT C MACH LEAR, P872, DOI [10.1145/1390156.1390266, DOI 10.1145/1390156.1390266]
   Servalli G., 2009, P IEDM
   Shao YS, 2013, 2013 IEEE INTERNATIONAL SYMPOSIUM ON LOW POWER ELECTRONICS AND DESIGN (ISLPED), P389, DOI 10.1109/ISLPED.2013.6629328
   Shulaker M. M., 2014, P IEEE IEDM
   Wong H. -S. P., 2016, STANFORD MEMORY TREN
   Wong HSP, 2015, NAT NANOTECHNOL, V10, P191, DOI 10.1038/nnano.2015.29
   Young Yang Liauw, 2012, 2012 IEEE International Solid-State Circuits Conference (ISSCC), P406, DOI 10.1109/ISSCC.2012.6177067
   Yu SM, 2011, IEEE T ELECTRON DEV, V58, P2729, DOI 10.1109/TED.2011.2147791
   Yu T, 2012, BIOMED CIRC SYST C, P21, DOI 10.1109/BioCAS.2012.6418479
NR 51
TC 16
Z9 16
U1 2
U2 32
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 0018-9383
EI 1557-9646
J9 IEEE T ELECTRON DEV
JI IEEE Trans. Electron Devices
PD DEC
PY 2016
VL 63
IS 12
BP 5004
EP 5011
DI 10.1109/TED.2016.2616483
PG 8
WC Engineering, Electrical & Electronic; Physics, Applied
SC Engineering; Physics
GA EE1LE
UT WOS:000389342200060
OA Green Published
DA 2020-02-19
ER

PT J
AU Zhang, J
   Ding, SF
   Zhang, N
   Xue, Y
AF Zhang, Jian
   Ding, Shifei
   Zhang, Nan
   Xue, Yu
TI Weight Uncertainty in Boltzmann Machine
SO COGNITIVE COMPUTATION
LA English
DT Article
DE RBM; DBM; DBN; Weight uncertainty
ID SEGMENTATION
AB Based on restricted Boltzmann machine (RBM), the deep learning models can be roughly divided into deep belief networks (DBNs) and deep Boltzmann machine (DBM). However, the overfitting problems commonly exist in neural networks and RBM models. In order to alleviate the overfitting problem, lots of research has been done. This paper alleviated the overfitting problem in RBM and proposed the weight uncertainty semi-restricted Boltzmann machine (WSRBM) to improve the ability of image recognition and image reconstruction.
   First, this paper built weight uncertainty RBM model based on maximum likelihood estimation. And in the experimental section, this paper verified the effectiveness of the weight uncertainty deep belief network and the weight uncertainty deep Boltzmann machine. Second, in order to obtain better reconstructed images, this paper used the semi-restricted Boltzmann machine (SRBM) as the feature extractor and built the WSRBM. Lastly, this paper used hybrid Monte Carlo sampling and cRBM to improve the classification ability of WSDBM.
   The experiments showed that the weight uncertainty RBM, weight uncertainty DBN and weight uncertainty DBM were effective compared with the dropout method. And the WSDBM model performed well in image recognition and image reconstruction as well.
   This paper introduced the weight uncertainty method to RBM, and proposed a WSDBM model, which was effective in image recognition and image reconstruction.
C1 [Zhang, Jian; Ding, Shifei; Zhang, Nan] China Univ Min & Technol, Sch Comp Sci & Technol, Xuzhou 221116, Peoples R China.
   [Zhang, Jian; Ding, Shifei; Zhang, Nan] Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
   [Xue, Yu] Nanjing Univ Informat Sci & Technol, Coll Comp & Software, Nanjing 210044, Jiangsu, Peoples R China.
RP Ding, SF (reprint author), China Univ Min & Technol, Sch Comp Sci & Technol, Xuzhou 221116, Peoples R China.; Ding, SF (reprint author), Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
EM dingsf@cumt.edu.cn
OI Zhang, Nan/0000-0001-9620-5665
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61379101, 61672522]; National Key Basic Research
   Program of ChinaNational Basic Research Program of China [2013CB329502];
   Priority Academic Program Development of Jiangsu Higer Education
   Institutions; Jiangsu Collaborative Innovation Center on Atmospheric
   Environment and Equipment Technology
FX This work is supported by the National Natural Science Foundation of
   China (No. 61379101), the National Natural Science Foundation of China
   (No. 61672522), the National Key Basic Research Program of China (No.
   2013CB329502), the Priority Academic Program Development of Jiangsu
   Higer Education Institutions and the Jiangsu Collaborative Innovation
   Center on Atmospheric Environment and Equipment Technology.
CR Ackley H, 1987, COMPLEX SYST, V1, P995
   Bengio Y, 2013, INT CONF ACOUST SPEE, P8624, DOI 10.1109/ICASSP.2013.6639349
   Blundell Charles, 2015, INT C MACH LEARN
   Desjardins G., 2010, P 13 INT C ART INT S, V9, P145
   Ding SF, 2015, MATH PROBL ENG, DOI 10.1155/2015/129021
   Erhan D, 2010, J MACH LEARN RES, V11, P625
   Gu B, 2015, IEEE T NEUR NET LEAR, V26, P1403, DOI 10.1109/TNNLS.2014.2342533
   Hinton G., 2010, MOMENTUM, V9, P926
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton G. E., 2012, ADV NEURAL INFORM PR, P2447
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178
   KIRKPATRICK S, 1983, SCIENCE, V220, P671, DOI 10.1126/science.220.4598.671
   Krizhevsky A, 2009, LEARNING MULTIPLE LA
   Krizhevsky Alex, 2010, INT C ART INT STAT, P621
   Kusy B, 2009, 2009 INTERNATIONAL CONFERENCE ON INFORMATION PROCESSING IN SENSOR NETWORKS (IPSN 2009), P109
   Le Roux N, 2008, NEURAL COMPUT, V20, P1631, DOI 10.1162/neco.2008.04-07-510
   Lee  T., 2015, INT C MACH LEARN
   Li J, 2015, IEEE T INF FOREN SEC, V10, P507, DOI 10.1109/TIFS.2014.2381872
   NEAL RM, 1992, ARTIF INTELL, V56, P71, DOI 10.1016/0004-3702(92)90065-6
   Norouzi Mohammad, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2735, DOI 10.1109/CVPRW.2009.5206577
   OSINDERO S, 2008, [No title captured], V20, P1121
   Salakhutdinov R, 2008, LEARNING EVALUATING
   Salakhutdinov R., 2009, ADV NEURAL INFORM PR, P1598
   Salakhutdinov R, 2012, NEURAL COMPUT, V24, P1967, DOI 10.1162/NECO_a_00311
   Salakhutdinov Ruslan, 2010, INT C ART INT STAT, P693
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Tieleman T., 2008, P 25 INT C MACH LEAR, P1064, DOI DOI 10.1145/1390156.1390290
   Tieleman T., 2009, P 26 ANN INT C MACH, P1033, DOI DOI 10.1145/1553374.1553506
   Zhang J, 2016, INT J MACH LEARN CYB, V7, P111, DOI 10.1007/s13042-015-0419-5
   Zhang N, 2016, NEUROCOMPUTING, V171, P1066, DOI 10.1016/j.neucom.2015.07.058
   Zheng YH, 2015, J INTELL FUZZY SYST, V28, P961, DOI 10.3233/IFS-141378
NR 33
TC 6
Z9 8
U1 1
U2 10
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 1866-9956
EI 1866-9964
J9 COGN COMPUT
JI Cogn. Comput.
PD DEC
PY 2016
VL 8
IS 6
BP 1064
EP 1073
DI 10.1007/s12559-016-9429-1
PG 10
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA EE0XW
UT WOS:000389304300005
DA 2020-02-19
ER

PT J
AU Hosseini-Asl, E
   Zurada, JM
   Nasraoui, O
AF Hosseini-Asl, Ehsan
   Zurada, Jacek M.
   Nasraoui, Olfa
TI Deep Learning of Part-Based Representation of Data Using Sparse
   Autoencoders With Nonnegativity Constraints
SO IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
LA English
DT Article
DE Autoencoder; deep architectures; feature learning; nonnegativity
   constraints; part-based representation
ID ARCHITECTURES; RECOGNITION; ALGORITHM; OBJECTS
AB We demonstrate a new deep learning autoencoder network, trained by a nonnegativity constraint algorithm (nonnegativity-constrained autoencoder), that learns features that show part-based representation of data. The learning algorithm is based on constraining negative weights. The performance of the algorithm is assessed based on decomposing data into parts and its prediction performance is tested on three standard image data sets and one text data set. The results indicate that the nonnegativity constraint forces the autoencoder to learn features that amount to a part-based representation of data, while improving sparsity and reconstruction quality in comparison with the traditional sparse autoencoder and nonnegative matrix factorization. It is also shown that this newly acquired representation improves the prediction performance of a deep neural network.
C1 [Hosseini-Asl, Ehsan; Zurada, Jacek M.] Univ Louisville, Dept Elect & Comp Engn, Louisville, KY 40292 USA.
   [Zurada, Jacek M.] Univ Social Sci, Inst Informat Technol, PL-90113 Lodz, Poland.
   [Nasraoui, Olfa] Univ Louisville, Dept Comp Sci, Louisville, KY 40292 USA.
RP Hosseini-Asl, E (reprint author), Univ Louisville, Dept Elect & Comp Engn, Louisville, KY 40292 USA.
EM ehsan.hosseiniasl@louisville.edu; jacek.zurada@louisville.edu;
   olfa.nasraoui@louisvile.edu
RI Nasraoui, Olfa/V-8973-2019
OI Zurada, Jacek/0000-0001-6622-534X
FU Kentucky Science and Engineering Foundation [KSEF-3113-RDE-017]
FX This work was supported by the Kentucky Science and Engineering
   Foundation under Grant KSEF-3113-RDE-017. (Corresponding author: Ehsan
   Hosseini-Asl.)
CR Andrew Ng, 2011, CS294A LECT NOTES, P72
   Bengio S, 2013, IEEE T PATTERN ANAL, V35, P1795, DOI 10.1109/TPAMI.2013.118
   Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153
   Bengio Y., 2007, LARGE SCALE KERNEL M, V34, P1
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   BYRD RH, 1995, SIAM J SCI COMPUT, V16, P1190, DOI 10.1137/0916069
   Chorowski J, 2015, IEEE T NEUR NET LEAR, V26, P62, DOI 10.1109/TNNLS.2014.2310059
   Deng L, 2014, APSIPA TRANS SIGNAL, V3, DOI 10.1017/atsip.2013.9
   Erhan D, 2010, J MACH LEARN RES, V11, P625
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton G. E., 1994, ADV NEURAL INFORMATI, P3
   Hinton G. E., 2012, IMPROVING NEURAL NET
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hoyer PO, 2004, J MACH LEARN RES, V5, P1457
   Hutchinson B, 2013, IEEE T PATTERN ANAL, V35, P1944, DOI 10.1109/TPAMI.2012.268
   LeCun Y, 2004, PROC CVPR IEEE, P97
   LeCun Y, 1998, LECT NOTES COMPUT SC, V1524, P9
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565
   Lee DD, 2001, ADV NEUR IN, V13, P556
   Lee H., 2007, ADV NEURAL INFORM PR, P873
   Lemme A, 2012, NEURAL NETWORKS, V33, P194, DOI 10.1016/j.neunet.2012.05.003
   Makhzani A., 2013, K SPARSE AUTOENCODER
   Moody J., 1995, ADV NEURAL INFORM PR, V4, P950
   Nair V, 2009, ADV NEURAL INF PROCE, V22, P1339
   Nguyen T. D., 2013, P AS C MACH LEARN, P133
   Nocedal Jorge, 1999, NUMERICAL OPTIMIZATI, V2
   Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7
   Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0
   Raina R., 2007, LEARNING, P759, DOI DOI 10.1145/1273496.1273592
   Ranzato M., 2006, ADV NEURAL INFORM PR, V19, P1137
   Ranzato M. A., 2007, ADV NEURAL INFORM PR, p1185 
   Samaria F. S., 1994, Proceedings of the Second IEEE Workshop on Applications of Computer Vision (Cat. No.94TH06742), P138, DOI 10.1109/ACV.1994.341300
   Schmidt M., 2008, MATLAB SOFTWARE
   SPARCKJONES K, 1972, J DOC, V28, P11, DOI 10.1108/eb026526
   Tan P.-N., 2005, INTRO DATA MINING
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   VANESSEN DC, 1994, NEURON, V13, P1, DOI 10.1016/0896-6273(94)90455-3
   Vincent P, 2008, P 25 INT C MACH LEAR, P1096, DOI DOI 10.1145/1390156.1390294
   WACHSMUTH E, 1994, CEREB CORTEX, V4, P509, DOI 10.1093/cercor/4.5.509
   ZURADA JM, 1992, INTRO ARTIFICIAL NEU
NR 43
TC 79
Z9 80
U1 5
U2 31
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 2162-237X
EI 2162-2388
J9 IEEE T NEUR NET LEAR
JI IEEE Trans. Neural Netw. Learn. Syst.
PD DEC
PY 2016
VL 27
IS 12
BP 2486
EP 2498
DI 10.1109/TNNLS.2015.2479223
PG 13
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Computer Science, Theory & Methods; Engineering,
   Electrical & Electronic
SC Computer Science; Engineering
GA ED5VE
UT WOS:000388919600003
PM 26529786
OA Green Published
DA 2020-02-19
ER

PT J
AU Simon, MO
   Corneanu, C
   Nasrollahi, K
   Nikisins, O
   Escalera, S
   Sun, YL
   Li, HQ
   Sun, ZA
   Moeslund, TB
   Greitans, M
AF Oliu Simon, Marc
   Corneanu, Ciprian
   Nasrollahi, Kamal
   Nikisins, Olegs
   Escalera, Sergio
   Sun, Yunlian
   Li, Haiqing
   Sun, Zhenan
   Moeslund, Thomas B.
   Greitans, Modris
TI Improved RGB-D-T based face recognition
SO IET BIOMETRICS
LA English
DT Article
DE face recognition; learning (artificial intelligence); neural nets;
   visual databases; image colour analysis; RGB-D-T database; histogram of
   Gabor ordinal measures; Haar-like rectangular features; histogram of
   oriented gradients; local binary patterns; handcrafted features;
   CNN-based recognition block; multimodal RGB-depth-thermal based facial
   recognition; deep learning convolutional neural networks; multimodal
   facial recognition; unimodal facial recognition systems; improved
   RGB-D-T based face recognition
AB Reliable facial recognition systems are of crucial importance in various applications from entertainment to security. Thanks to the deep-learning concepts introduced in the field, a significant improvement in the performance of the unimodal facial recognition systems has been observed in the recent years. At the same time a multimodal facial recognition is a promising approach. This study combines the latest successes in both directions by applying deep learning convolutional neural networks (CNN) to the multimodal RGB, depth, and thermal (RGB-D-T) based facial recognition problem outperforming previously published results. Furthermore, a late fusion of the CNN-based recognition block with various hand-crafted features (local binary patterns, histograms of oriented gradients, Haar-like rectangular features, histograms of Gabor ordinal measures) is introduced, demonstrating even better recognition performance on a benchmark RGB-D-T database. The obtained results in this study show that the classical engineered features and CNN-based features can complement each other for recognition purposes.
C1 [Oliu Simon, Marc; Corneanu, Ciprian; Escalera, Sergio] Univ Barcelona, Comp Vis Ctr, Human Pose Recovery & Behav Anal HuPBA Grp, Barcelona 08193, Spain.
   [Nasrollahi, Kamal; Moeslund, Thomas B.] Aalborg Univ, Visual Anal People VAP Lab, Rendsburggade 14, DK-9000 Aalborg, Denmark.
   [Nikisins, Olegs; Greitans, Modris] Inst Elect & Comp Sci, Dzerbenes 14, LV-1006 Riga, Latvia.
   [Sun, Yunlian; Li, Haiqing; Sun, Zhenan] Chinese Acad Sci CASIA, Inst Automat, Natl Lab Pattern Recognit NLPR, 95 Zhongguancun East Rd, Beijing 100190, Peoples R China.
RP Nasrollahi, K (reprint author), Aalborg Univ, Visual Anal People VAP Lab, Rendsburggade 14, DK-9000 Aalborg, Denmark.
EM kn@create.aau.dk
RI Greitans, Modris/E-5947-2018
CR Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   Bebis G, 2006, IMAGE VISION COMPUT, V24, P727, DOI 10.1016/j.imavis.2006.01.017
   Chai ZH, 2014, IEEE T INF FOREN SEC, V9, P14, DOI 10.1109/TIFS.2013.2290064
   Goswami G, 2014, IEEE T INF FOREN SEC, V9, P1629, DOI 10.1109/TIFS.2014.2343913
   Henry P, 2012, INT J ROBOT RES, V31, P647, DOI 10.1177/0278364911434148
   Hsu GS, 2014, IEEE T INF FOREN SEC, V9, P2110, DOI 10.1109/TIFS.2014.2361028
   Huang Gary B., 2007, 0749 U MASS
   Mansanet J, 2016, PATTERN RECOGN LETT, V70, P80, DOI 10.1016/j.patrec.2015.11.015
   Martinez A, 1998, 24 CVC, P24
   Nasrollahi K., 2013, PROGR PATTERN RECOGN, P334
   Phillips PJ, 2005, PROC CVPR IEEE, P947
   Phillips PJ, 2000, IEEE T PATTERN ANAL, V22, P1090, DOI 10.1109/34.879790
   Sun N, 2008, NEURAL COMPUT APPL, V17, P59, DOI 10.1007/s00521-007-0111-0
   Zhang DQ, 2006, PATTERN RECOGN, V39, P140, DOI 10.1016/j.patcog.2005.08.002
   Zhao HT, 2008, IEEE T SYST MAN CY B, V38, P210, DOI 10.1109/TSMCB.2007.908870
NR 15
TC 9
Z9 9
U1 1
U2 22
PU INST ENGINEERING TECHNOLOGY-IET
PI HERTFORD
PA MICHAEL FARADAY HOUSE SIX HILLS WAY STEVENAGE, HERTFORD SG1 2AY, ENGLAND
SN 2047-4938
EI 2047-4946
J9 IET BIOMETRICS
JI IET Biom.
PD DEC
PY 2016
VL 5
IS 4
BP 297
EP 304
DI 10.1049/iet-bmt.2015.0057
PG 8
WC Computer Science, Artificial Intelligence
SC Computer Science
GA ED3CO
UT WOS:000388727100004
DA 2020-02-19
ER

PT J
AU Huang, SY
   Li, X
   Zhang, ZF
   He, ZZ
   Wu, F
   Liu, W
   Tang, JH
   Zhuang, YT
AF Huang, Siyu
   Li, Xi
   Zhang, Zhongfei
   He, Zhouzhou
   Wu, Fei
   Liu, Wei
   Tang, Jinhui
   Zhuang, Yueting
TI Deep Learning Driven Visual Path Prediction From a Single Image
SO IEEE TRANSACTIONS ON IMAGE PROCESSING
LA English
DT Article
DE Visual path prediction; visual context model; convolutional neural
   networks; deep learning
ID SCENE; MODEL; REPRESENTATION; RECOGNITION; FEATURES
AB Capabilities of inference and prediction are the significant components of visual systems. Visual path prediction is an important and challenging task among them, with the goal to infer the future path of a visual object in a static scene. This task is complicated as it needs high-level semantic understandings of both the scenes and underlying motion patterns in video sequences. In practice, cluttered situations have also raised higher demands on the effectiveness and robustness of models. Motivated by these observations, we propose a deep learning framework, which simultaneously performs deep feature learning for visual representation in conjunction with spatiotemporal context modeling. After that, a unified path-planning scheme is proposed to make accurate path prediction based on the analytic results returned by the deep context models. The highly effective visual representation and deep context models ensure that our framework makes a deep semantic understanding of the scenes and motion patterns, consequently improving the performance on visual path prediction task. In experiments, we extensively evaluate the model's performance by constructing two large benchmark datasets from the adaptation of video tracking datasets. The qualitative and quantitative experimental results show that our approach outperforms the state-of-the-art approaches and owns a better generalization capability.
C1 [Huang, Siyu; He, Zhouzhou] Zhejiang Univ, Coll Informat Sci & Elect Engn, Hangzhou 310027, Zhejiang, Peoples R China.
   [Li, Xi; Wu, Fei; Zhuang, Yueting] Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310027, Zhejiang, Peoples R China.
   [Zhang, Zhongfei] Zhejiang Univ, Coll Informat Sci & Elect Engn, Hangzhou 310027, Zhejiang, Peoples R China.
   [Zhang, Zhongfei] SUNY Binghamton, Watson Sch, Dept Comp Sci, Binghamton, NY 13902 USA.
   [Liu, Wei] Tencent AI Lab, Shenzhen 518057, Peoples R China.
   [Tang, Jinhui] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Jiangsu, Peoples R China.
RP Li, X (reprint author), Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310027, Zhejiang, Peoples R China.
EM siyuhuang@zju.edu.cn; xilizju@zju.edu.cn; zhongfei@zju.edu.cn;
   zhouzhouhe@zju.edu.cn; wufei@cs.zju.edu.cn; wliu@ee.columbia.edu;
   jinhuitang@njust.edu.cn; yzhuang@cs.zju.edu.cn
RI Li, Xi/L-1234-2013
OI He, Zhouzhou/0000-0003-3947-4011
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61672456, 61472353, U1509206]; Zhejiang Provincial
   Engineering Center on Media Data Cloud Processing and Analysis
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61672456, Grant 61472353, and Grant
   U1509206 and in part by the Zhejiang Provincial Engineering Center on
   Media Data Cloud Processing and Analysis. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Dacheng Tao. (Corresponding author: Xi Li.)
CR Byeon W, 2015, PROC CVPR IEEE, P3547, DOI 10.1109/CVPR.2015.7298977
   Ciresan D, 2012, PROC CVPR IEEE, P3642, DOI 10.1109/CVPR.2012.6248110
   Ding CX, 2015, IEEE T IMAGE PROCESS, V24, P980, DOI 10.1109/TIP.2015.2390959
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Duan LX, 2012, IEEE T PATTERN ANAL, V34, P465, DOI 10.1109/TPAMI.2011.114
   DUBUISSON MP, 1994, INT C PATT RECOG, P566, DOI 10.1109/ICPR.1994.576361
   Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231
   Fouhey DF, 2014, PROC CVPR IEEE, P2027, DOI 10.1109/CVPR.2014.260
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Hawkins J., 2007, ON INTELLLIGENCE
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Jiang YG, 2013, INT J MULTIMED INF R, V2, P73, DOI 10.1007/s13735-012-0024-2
   Juneja M, 2013, PROC CVPR IEEE, P923, DOI 10.1109/CVPR.2013.124
   Jutzi B., 2016, KIT IPF SOFTWARE DAT
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Keller Christoph G., 2011, Pattern Recognition. Proceedings 33rd DAGM Symposium, P386, DOI 10.1007/978-3-642-23123-0_39
   Kitani KM, 2012, LECT NOTES COMPUT SC, V7575, P201, DOI 10.1007/978-3-642-33765-9_15
   Koppula HS, 2016, IEEE T PATTERN ANAL, V38, P14, DOI 10.1109/TPAMI.2015.2430335
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lampert CH, 2015, PROC CVPR IEEE, P942, DOI 10.1109/CVPR.2015.7298696
   Lan T, 2014, LECT NOTES COMPUT SC, V8691, P689, DOI 10.1007/978-3-319-10578-9_45
   Li J, 2017, IEEE T NEUR NET LEAR, V28, P690, DOI 10.1109/TNNLS.2016.2522428
   Li L. -J., 2010, TRENDS TOPICS COMPUT, P57, DOI DOI 10.1007/978-3-642-35749-7
   Li Q, 2014, IEEE T IMAGE PROCESS, V23, P4812, DOI 10.1109/TIP.2014.2358193
   Li XY, 2016, ACM T INTEL SYST TEC, V7, DOI 10.1145/2819000
   Lin JY, 2013, IEEE T IMAGE PROCESS, V22, P4545, DOI 10.1109/TIP.2013.2274389
   Liu BY, 2010, PROC CVPR IEEE, P1253, DOI 10.1109/CVPR.2010.5539823
   Liu C, 2008, LECT NOTES COMPUT SC, V5304, P28, DOI 10.1007/978-3-540-88690-7_3
   Liu TL, 2016, IEEE T PATTERN ANAL, V38, P447, DOI 10.1109/TPAMI.2015.2456899
   Merler M, 2012, IEEE T MULTIMEDIA, V14, P88, DOI 10.1109/TMM.2011.2168948
   Munoz D, 2012, LECT NOTES COMPUT SC, V7577, P668, DOI 10.1007/978-3-642-33783-3_48
   Munoz D, 2010, LECT NOTES COMPUT SC, V6316, P57, DOI 10.1007/978-3-642-15567-3_5
   Nascimento JC, 2013, IEEE T IMAGE PROCESS, V22, P1712, DOI 10.1109/TIP.2012.2226899
   Oh S, 2011, P IEEE C COMP VIS PA, P3153, DOI DOI 10.1109/CVPR.2011.5995586
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Saxena A, 2008, INT J COMPUT VISION, V76, P53, DOI 10.1007/s11263-007-0071-y
   Silberman N., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P601, DOI 10.1109/ICCVW.2011.6130298
   Singh S, 2012, LECT NOTES COMPUT SC, V7573, P73, DOI 10.1007/978-3-642-33709-3_6
   Tran SD, 2008, LECT NOTES COMPUT SC, V5303, P610, DOI 10.1007/978-3-540-88688-4_45
   Walker J, 2015, IEEE I CONF COMP VIS, P2443, DOI 10.1109/ICCV.2015.281
   Walker J, 2014, PROC CVPR IEEE, P3302, DOI 10.1109/CVPR.2014.416
   Wang HR, 2014, IEEE T IMAGE PROCESS, V23, P570, DOI 10.1109/TIP.2013.2292550
   Wang LM, 2014, IEEE T IMAGE PROCESS, V23, P810, DOI 10.1109/TIP.2013.2295753
   Xu C, 2016, IEEE T IMAGE PROCESS, V25, P1495, DOI 10.1109/TIP.2016.2524207
   Xu C, 2015, IEEE T PATTERN ANAL, V37, P2531, DOI 10.1109/TPAMI.2015.2417578
   Yao J, 2012, PROC CVPR IEEE, P702, DOI 10.1109/CVPR.2012.6247739
   Yu J, 2013, PATTERN RECOGN, V46, P483, DOI 10.1016/j.patcog.2012.08.006
   Yuen J, 2010, LECT NOTES COMPUT SC, V6312, P707, DOI 10.1007/978-3-642-15552-9_51
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang LM, 2014, IEEE T IMAGE PROCESS, V23, P4150, DOI 10.1109/TIP.2014.2344433
NR 51
TC 12
Z9 12
U1 1
U2 36
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1057-7149
EI 1941-0042
J9 IEEE T IMAGE PROCESS
JI IEEE Trans. Image Process.
PD DEC
PY 2016
VL 25
IS 12
BP 5892
EP 5904
DI 10.1109/TIP.2016.2613686
PG 13
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
SC Computer Science; Engineering
GA EC5VK
UT WOS:000388205200014
PM 28114063
DA 2020-02-19
ER

PT J
AU Sasaki, K
   Noda, K
   Ogata, T
AF Sasaki, Kazuma
   Noda, Kuniaki
   Ogata, Tetsuya
TI Visual motor integration of robot's drawing behavior using recurrent
   neural network
SO ROBOTICS AND AUTONOMOUS SYSTEMS
LA English
DT Article
DE Drawing ability; Drawing robot; Deep learning
ID PERCEPTION; DYNAMICS
AB Drawing is a way of visually expressing our feelings, knowledge, and situation. People draw pictures to share information with other human beings. This study investigates visuomotor memory (VM), which is a reusable memory storing drawing behavioral data. We propose a neural network-based model for acquiring a computational memory that can replicate VM through self-organized learning of a robot's actual drawing experiences. To design the model, we assume that VM has the following two characteristics: (1) it is formed by bottom-up learning and integration of temporal drawn pictures and motion data, and (2) it allows the observers to associate drawing motions from pictures. The proposed model comprises a deep neural network for dimensionally compressing temporal drawn images and a continuous-time recurrent neural network for integration learning of drawing motions and temporal drawn images. Two experiments are conducted on unicursal shape learning to investigate whether the proposed model can learn the function without any shape information for visual processing. Based on the first experiment, the model can learn 15 drawing sequences for three types of pictures, acquiring associative memory for drawing motions through the bottom-up learning process. Thus, it can associate drawing motions from untrained drawn images. In the second experiment, four types of pictures are trained, with four distorted variations per type. In this case, the model can organize the different shapes based on their distortions by utilizing both the image information and the drawing motions, even if visual characteristics are not shared. (C) 2016 The Authors. Published by Elsevier B.V.
C1 [Sasaki, Kazuma; Noda, Kuniaki; Ogata, Tetsuya] Waseda Univ, Grad Sch Fundamental Sci & Engn, Dept Intermedia Art & Sci, Shinjuku Ku, 3-4-1 Okubo, Tokyo 1698555, Japan.
RP Sasaki, K (reprint author), Waseda Univ, Grad Sch Fundamental Sci & Engn, Dept Intermedia Art & Sci, Shinjuku Ku, 3-4-1 Okubo, Tokyo 1698555, Japan.
EM ssk.sasaki@suou.waseda.jp; kuniaki.noda@akane.waseda.jp; ogata@waseda.jp
OI Sasaki, Kazuma/0000-0002-4586-801X; Ogata, Tetsuya/0000-0001-7015-0379
FU MEXTMinistry of Education, Culture, Sports, Science and Technology,
   Japan (MEXT) [15H01710, 24119003]
FX The work has been supported by MEXT Grant-in-Aid for Scientific Research
   (A) 15H01710, and Scientific Research on Innovative Areas "Constructive
   Developmental Science" 24119003.
CR Arie H, 2012, ROBOT AUTON SYST, V60, P729, DOI 10.1016/j.robot.2011.11.005
   Asada M, 2009, IEEE T AUTON MENT DE, V1, P12, DOI 10.1109/TAMD.2009.2021702
   BABCOCK MK, 1988, AM J PSYCHOL, V101, P111, DOI 10.2307/1422797
   BEER RD, 1995, ADAPT BEHAV, V3, P469, DOI 10.1177/105971239500300405
   Calinon S, 2005, IEEE-RAS INT C HUMAN, P161
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Deussen Oliver, 2012, P 8 ANN S COMP AESTH, P25
   Droniou A, 2015, ROBOT AUTON SYST, V71, P83, DOI 10.1016/j.robot.2014.11.005
   Eitz M., 2012, ACM T GRAPHIC, V31, P1, DOI DOI 10.HTTP://DX.D0L0RG/10.1145/2185520.2335395
   FREYD JJ, 1983, MEM COGNITION, V11, P342, DOI 10.3758/BF03202447
   Graves A, 2012, STUDIES COMPUTATIONA
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hommel B, 2001, BEHAV BRAIN SCI, V24, P849, DOI 10.1017/S0140525X01000103
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kudoh S, 2009, ROBOT AUTON SYST, V57, P279, DOI 10.1016/j.robot.2008.10.007
   Lam JHM, 2009, 2009 IEEE-RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, P2315, DOI 10.1109/IROS.2009.5354709
   Martens J., 2010, P 27 INT C MACH LEAR, V951, P2010
   McCrea S., 2014, AM J PSYCHIAT NEUROS, V2, P60
   MOCHIZUKI K, 2013, P IEEE INT C SYST MA, P2336, DOI DOI 10.1109/SMC.2013.399
   Mohan V, 2011, AUTON ROBOT, V31, P21, DOI 10.1007/s10514-011-9229-0
   Mueller S, 2013, IEEE INT C INT ROBOT, P1734, DOI 10.1109/IROS.2013.6696583
   Nishide S, 2014, IEEE INT CONF ROBOT, P4785, DOI 10.1109/ICRA.2014.6907559
   Noda K, 2014, ROBOT AUTON SYST, V62, P721, DOI 10.1016/j.robot.2014.03.003
   Parkinson J, 2007, Q J EXP PSYCHOL, V60, P1265, DOI 10.1080/17470210600937460
   Pignocchi A, 2010, CONSCIOUS COGN, V19, P887, DOI 10.1016/j.concog.2010.04.009
   Robotics A., 2015, NAO HUMANOID
   Rumelhart D.E., 1986, PARALLEL DISTRIBUTED, V1
   Schraudolph NN, 2002, NEURAL COMPUT, V14, P1723, DOI 10.1162/08997660260028683
   Tresset P, 2013, COMPUT GRAPH-UK, V37, P348, DOI 10.1016/j.cag.2013.01.012
   Wacom, 2015, INT PEN TOUCH SMALL
   Waterman A. H., 1798, P R SOC B, V282
   WERBOS PJ, 1990, P IEEE, V78, P1550, DOI 10.1109/5.58337
   Yamashita Y, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.1000220
   Yu Q., 2016, INT J COMPUT VISION, P1, DOI DOI 10.1007/S11263-016-0932-3
   Yu Q., 2016, P IEEE C COMP VIS PA
NR 35
TC 7
Z9 7
U1 0
U2 10
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0921-8890
EI 1872-793X
J9 ROBOT AUTON SYST
JI Robot. Auton. Syst.
PD DEC
PY 2016
VL 86
BP 184
EP 195
DI 10.1016/j.robot.2016.08.022
PG 12
WC Automation & Control Systems; Computer Science, Artificial Intelligence;
   Robotics
SC Automation & Control Systems; Computer Science; Robotics
GA EC3UH
UT WOS:000388051800016
OA Other Gold
DA 2020-02-19
ER

PT J
AU Li, HX
   Li, Y
   Porikli, F
AF Li, Hanxi
   Li, Yi
   Porikli, Fatih
TI Convolutional neural net bagging for online visual tracking
SO COMPUTER VISION AND IMAGE UNDERSTANDING
LA English
DT Article
DE Visual tracking; Deep learning; Ensemble learning
ID ROBUST OBJECT TRACKING
AB Recently, Convolutional Neural Nets (CNNs) have been successfully applied to online visual tracking. However, a major problem is that such models may be inevitably over-fitted due to two main factors. The first one is the label noise because the online training of any models relies solely on the detection of the previous frames. The second one is the model uncertainty due to the randomized training strategy. In this work, we cope with noisy labels and the model uncertainty within the framework of bagging (bootstrap aggregating), resulting in efficient and effective visual tracking. Instead of using multiple models in a bag, we design a single multitask CNN for learning effective feature representations of the target object. In our model, each task has the same structure and shares the same set of convolutional features, but is trained using different random samples generated for different tasks. A significant advantage is that the bagging overhead for our model is minimal, and no extra efforts are needed to handle the outputs of different tasks as done in those multi-lifespan models. Experiments demonstrate that our CNN tracker outperforms the state-of-the-art methods on three recent benchmarks (over 80 video sequences), which illustrates the superiority of the feature representations learned by our purely online bagging framework. (C) 2016 Published by Elsevier Inc.
C1 [Li, Hanxi] Jiangxi Normal Univ, Sch Comp & Informat Engn, 99 Ziyang Rd, Nanchang, Jiangxi, Peoples R China.
   [Li, Hanxi; Li, Yi; Porikli, Fatih] Data61, Comp Vis Res Grp, Alexandria, NSW, Australia.
   [Porikli, Fatih] Australian Natl Univ, Res Sch Engn, Canberra, ACT, Australia.
   [Li, Yi] Toyota Res Inst, Ann Arbor, MI 48105 USA.
RP Li, Y (reprint author), Data61, Comp Vis Res Grp, Alexandria, NSW, Australia.; Li, Y (reprint author), Toyota Res Inst, Ann Arbor, MI 48105 USA.
EM lihanxi2001@gmail.com; yi.li@tri.global; fatih.porikli@anu.edu.au
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61462042, 61672079]; Australian Research Council's
   Discovery Projects Funding SchemeAustralian Research Council
   [DP150104645]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61462042 and Grant 61672079, and in part
   by the Australian Research Council's Discovery Projects Funding Scheme
   under Project DP150104645.
CR Adam A., 2006, ROBUST FRAGMENTS BAS, V1
   Babenko B, 2009, PROC CVPR IEEE, P983, DOI 10.1109/CVPRW.2009.5206737
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734
   Ciresan Dan, 2012, MULTICOLUMN DEEP NEU
   Collins RT, 2005, IEEE T PATTERN ANAL, V27, P1631, DOI 10.1109/TPAMI.2005.205
   Dalal N, 2005, PROC CVPR IEEE, P886
   Dinh TB, 2011, PROC CVPR IEEE, P1177, DOI 10.1109/CVPR.2011.5995733
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Jingjing Xiao, 2013, 2013 IEEE International Conference on Computer Vision Workshops (ICCVW), P137, DOI 10.1109/ICCVW.2013.24
   Kristan M, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P98, DOI 10.1109/ICCVW.2013.20
   Wu Y, 2013, ONLINE OBJECT TRACKI
   Wu Yaowu, 2015, OBJECT TRACKING BENC
   Xing JL, 2013, IEEE I CONF COMP VIS, P665, DOI 10.1109/ICCV.2013.88
   Zhang B., 2016, INT J COMPUT VISION, V118, P1
   Zhang K., ARXIV150104505
   Zhong W, 2012, PROC CVPR IEEE, P1838, DOI 10.1109/CVPR.2012.6247882
NR 17
TC 10
Z9 10
U1 0
U2 22
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1077-3142
EI 1090-235X
J9 COMPUT VIS IMAGE UND
JI Comput. Vis. Image Underst.
PD DEC
PY 2016
VL 153
BP 120
EP 129
DI 10.1016/j.cviu.2016.07.002
PG 10
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
SC Computer Science; Engineering
GA EE4IW
UT WOS:000389566500012
DA 2020-02-19
ER

PT J
AU Pan, B
   Shi, ZW
   Zhang, N
   Xie, SB
AF Pan, Bin
   Shi, Zhenwei
   Zhang, Ning
   Xie, Shaobiao
TI Hyperspectral Image Classification Based on Nonlinear Spectral-Spatial
   Network
SO IEEE GEOSCIENCE AND REMOTE SENSING LETTERS
LA English
DT Article
DE Deep learning; hyperspectral image classification; nonlinear spectral-
   spatial network (NSSNet)
ID FEATURES
AB Recently, for the task of hyperspectral image classification, deep-learning-based methods have revealed promising performance. However, the complex network structure and the time-consuming training process have restricted their applications. In this letter, we construct a much simpler network, i.e., the nonlinear spectral-spatial network (NSSNet), for hyperspectral image classification. NSSNet is developed from the basic structure of a principal component analysis network. Nonlinear information is included in NSSNet, to generate a more discriminative feature expression. Moreover, spectral and spatial features are combined to further improve the classification accuracy. Experimental results indicate that our method achieves better performance than state-of-the-art deep-learning-based methods.
C1 [Pan, Bin; Shi, Zhenwei] Beihang Univ, Sch Astronaut, Image Proc Ctr, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
   [Pan, Bin; Shi, Zhenwei] Beihang Univ, Beijing Key Lab Digital Media, Beijing 100191, Peoples R China.
   [Zhang, Ning] Shanghai Aerosp Elect Technol Inst, Shanghai 201109, Peoples R China.
   [Xie, Shaobiao] Shanghai Acad Spaceflight Technol, Shanghai 201109, Peoples R China.
RP Shi, ZW (reprint author), Beihang Univ, Sch Astronaut, Image Proc Ctr, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.; Shi, ZW (reprint author), Beihang Univ, Beijing Key Lab Digital Media, Beijing 100191, Peoples R China.
EM panbin@buaa.edu.cn; shizhenwei@buaa.edu.cn; dzs_zhangning@163.com;
   boyish9747@outlook.com
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61671037, 61273245]; Beijing Natural Science
   FoundationBeijing Natural Science Foundation [4152031]; State Key
   Laboratory of Virtual Reality Technology and Systems, Beihang University
   [BUAA-VR-16ZZ-03]; Shanghai Association for Science and Technology
   [SAST2016090]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61671037 and Grant 61273245; by the
   Beijing Natural Science Foundation under Grant 4152031; by the funding
   project of the State Key Laboratory of Virtual Reality Technology and
   Systems, Beihang University under Grant BUAA-VR-16ZZ-03; and by the
   Shanghai Association for Science and Technology under Grant SAST2016090.
CR Camps-Valls G, 2006, IEEE GEOSCI REMOTE S, V3, P93, DOI 10.1109/LGRS.2005.857031
   Chan TH, 2015, IEEE T IMAGE PROCESS, V24, P5017, DOI 10.1109/TIP.2015.2475625
   Chen YS, 2015, IEEE J-STARS, V8, P2381, DOI 10.1109/JSTARS.2015.2388577
   Chen YS, 2014, IEEE J-STARS, V7, P2094, DOI 10.1109/JSTARS.2014.2329330
   Chi MM, 2009, IEEE GEOSCI REMOTE S, V6, P762, DOI 10.1109/LGRS.2009.2024624
   Gu YF, 2012, IEEE T GEOSCI REMOTE, V50, P2852, DOI 10.1109/TGRS.2011.2176341
   Gualtieri J. A., 1999, P SOC PHOTO-OPT INS, P221
   Hu W H, 2015, J SENSORS, V2015, P1
   Kang XD, 2014, IEEE T GEOSCI REMOTE, V52, P2666, DOI 10.1109/TGRS.2013.2264508
   Li J, 2015, IEEE T GEOSCI REMOTE, V53, P1592, DOI 10.1109/TGRS.2014.2345739
   Liang HM, 2016, REMOTE SENS-BASEL, V8, DOI 10.3390/rs8020099
   Liu YZ, 2015, INT J REMOTE SENS, V36, P3459, DOI 10.1080/01431161.2015.1055607
   Ma XR, 2015, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-015-0071-8
   Makantasis K, 2015, INT GEOSCI REMOTE SE, P4959, DOI 10.1109/IGARSS.2015.7326945
   Zhang LF, 2012, IEEE T GEOSCI REMOTE, V50, P879, DOI 10.1109/TGRS.2011.2162339
   Zhao WZ, 2015, INT J REMOTE SENS, V36, P3368, DOI 10.1080/2150704X.2015.1062157
NR 16
TC 29
Z9 30
U1 4
U2 28
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1545-598X
EI 1558-0571
J9 IEEE GEOSCI REMOTE S
JI IEEE Geosci. Remote Sens. Lett.
PD DEC
PY 2016
VL 13
IS 12
BP 1782
EP 1786
DI 10.1109/LGRS.2016.2608963
PG 5
WC Geochemistry & Geophysics; Engineering, Electrical & Electronic; Remote
   Sensing; Imaging Science & Photographic Technology
SC Geochemistry & Geophysics; Engineering; Remote Sensing; Imaging Science
   & Photographic Technology
GA EG8HZ
UT WOS:000391298500007
DA 2020-02-19
ER

PT J
AU Aptoula, E
   Ozdemir, MC
   Yanikoglu, B
AF Aptoula, Erchan
   Ozdemir, Murat Can
   Yanikoglu, Berrin
TI Deep Learning With Attribute Profiles for Hyperspectral Image
   Classification
SO IEEE GEOSCIENCE AND REMOTE SENSING LETTERS
LA English
DT Article
DE Attribute profiles (APs); deep learning; hyperspectral images;
   mathematical morphology; pixel classification
ID SPECTRAL-SPATIAL CLASSIFICATION; FEATURE-EXTRACTION
AB Effective spatial-spectral pixel description is of crucial significance for the classification of hyperspectral remote sensing images. Attribute profiles are considered as one of the most prominent approaches in this regard, since they can capture efficiently arbitrary geometric and spectral properties. Lately though, the advent of deep learning in its various forms has also led to remarkable classification performances by operating directly on hyperspectral input. In this letter, we explore the collaboration potential of these two powerful feature extraction approaches. Specifically, we propose a new strategy for hyperspectral image classification, where attribute filtered images are stacked and provided as input to convolutional neural networks. Our experiments with two real hyperspectral remote sensing data sets show that the proposed strategy leads to a performance improvement, as opposed to using each of the involved approaches individually.
C1 [Aptoula, Erchan] Gebze Tech Univ, TR-41400 Gebze, Turkey.
   [Ozdemir, Murat Can; Yanikoglu, Berrin] Sabanci Univ, TR-34956 Istanbul, Turkey.
RP Aptoula, E (reprint author), Gebze Tech Univ, TR-41400 Gebze, Turkey.
EM eaptoula@gtu.edu.tr
FU BAGEP Award of the Science Academy
FX The authors would like to thank Prof. P. Gamba for making available to
   the community the Pavia data sets. This work was supported by the BAGEP
   Award of the Science Academy.
CR Aptoula E, 2016, IEEE T GEOSCI REMOTE, V54, P3208, DOI 10.1109/TGRS.2015.2513424
   Aptoula E, 2015, IEEE GEOSCI REMOTE S, V12, P2031, DOI 10.1109/LGRS.2015.2443860
   Cavallaro G, 2016, IEEE T GEOSCI REMOTE, V54, P3899, DOI 10.1109/TGRS.2016.2530690
   Chen YS, 2015, IEEE J-STARS, V8, P2381, DOI 10.1109/JSTARS.2015.2388577
   Chen YS, 2014, IEEE J-STARS, V7, P2094, DOI 10.1109/JSTARS.2014.2329330
   Dalla Mura M, 2010, IEEE T GEOSCI REMOTE, V48, P3747, DOI 10.1109/TGRS.2010.2048116
   Demir B, 2016, IEEE T GEOSCI REMOTE, V54, P2096, DOI 10.1109/TGRS.2015.2496167
   Ghamisi P, 2015, IEEE T GEOSCI REMOTE, V53, P2335, DOI 10.1109/TGRS.2014.2358934
   Ghamisi P, 2014, IEEE T GEOSCI REMOTE, V52, P5771, DOI 10.1109/TGRS.2013.2292544
   Ghazi M. M., 2015, P CLEF WORKSH NOT TO
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Makantasis K, 2015, INT GEOSCI REMOTE SE, P4959, DOI 10.1109/IGARSS.2015.7326945
   Mura M. D., 2011, IEEE GEOSCI REMOTE S, V8, P542, DOI DOI 10.1109/LGRS.2010.2091253
   Romero A, 2016, IEEE T GEOSCI REMOTE, V54, P1349, DOI 10.1109/TGRS.2015.2478379
   Tao C, 2015, IEEE GEOSCI REMOTE S, V12, P2438, DOI 10.1109/LGRS.2015.2482520
   Yue J, 2015, REMOTE SENS LETT, V6, P468, DOI 10.1080/2150704X.2015.1047045
   Zhang LP, 2016, IEEE GEOSC REM SEN M, V4, P22, DOI 10.1109/MGRS.2016.2540798
   Zhao WZ, 2016, IEEE T GEOSCI REMOTE, V54, P4544, DOI 10.1109/TGRS.2016.2543748
   Zhao WZ, 2015, INT J REMOTE SENS, V36, P3368, DOI 10.1080/2150704X.2015.1062157
   Zhong Q., 2015, J APPL STAT, V1, P1, DOI DOI 10.1007/S00500-015-1789-Z
NR 20
TC 46
Z9 50
U1 3
U2 48
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1545-598X
EI 1558-0571
J9 IEEE GEOSCI REMOTE S
JI IEEE Geosci. Remote Sens. Lett.
PD DEC
PY 2016
VL 13
IS 12
BP 1970
EP 1974
DI 10.1109/LGRS.2016.2619354
PG 5
WC Geochemistry & Geophysics; Engineering, Electrical & Electronic; Remote
   Sensing; Imaging Science & Photographic Technology
SC Geochemistry & Geophysics; Engineering; Remote Sensing; Imaging Science
   & Photographic Technology
GA EG8HZ
UT WOS:000391298500044
DA 2020-02-19
ER

PT J
AU Das, M
   Ghosh, SK
AF Das, Monidipa
   Ghosh, Soumya K.
TI Deep-STEP: A Deep Learning Approach for Spatiotemporal Prediction of
   Remote Sensing Data
SO IEEE GEOSCIENCE AND REMOTE SENSING LETTERS
LA English
DT Article
DE Deep learning; deep stacking network (DSN); satellite remote sensing
   imagery; spatiotemporal prediction
AB With the advent of advanced remote sensing technologies in past few decades, acquiring higher resolution satellite images has become easier and cheaper in recent days. However, on the other hand, it has offered a big challenge to the remote sensing community in smart image interpretation from such huge volume of data. Deep learning, which offers efficient algorithms for extracting multiple levels of feature abstractions, may be suitable to serve the purpose. This letter presents a deep learning approach (Deep-STEP) for spatiotemporal prediction of satellite remote sensing data. The proposed learning architecture is derived from a deep stacking network, consisting of a stack of multilayer perceptron, each of which models the spatial feature of the associated region at a particular time instant. The proposed method has been demonstrated on normalized difference vegetation index (NDVI) data sets, derived from satellite remote sensing imagery, containing several thousands to millions of pixels/records. The experimental results (related to NDVI prediction) reveal that the proposed architecture exhibits fairly satisfactory performance with promising learning capabilities.
C1 [Das, Monidipa; Ghosh, Soumya K.] IIT Kharagpur, Dept Comp Sci & Engn, Kharagpur 721302, W Bengal, India.
RP Das, M (reprint author), IIT Kharagpur, Dept Comp Sci & Engn, Kharagpur 721302, W Bengal, India.
EM monidipadas@hotmail.com; skg@iitkgp.ac.in
CR Collobert R., 2011, P 14 INT C ART INT S, V15, P224
   Crespo JL, 2007, VISUAL COMPUT, V23, P419, DOI 10.1007/s00371-007-0114-y
   Das M., 2014, P ANN IEEE IND C IND, P1
   Das M, 2016, INT GEOSCI REMOTE SE, P5913, DOI 10.1109/IGARSS.2016.7730545
   Deng L, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 3 AND 4, P1692
   Deng L, 2013, FOUND TRENDS SIGNAL, V7, pI, DOI 10.1561/2000000039
   Heeger D. J., 1986, Proceedings of the Workshop on Motion: Representation and Analysis (Cat. No.86CH2322-6), P131
   Shekhar S, 2015, ISPRS INT J GEO-INF, V4, P2306, DOI 10.3390/ijgi4042306
   Zhang F, 2016, IEEE T GEOSCI REMOTE, V54, P1793, DOI 10.1109/TGRS.2015.2488681
   Zhang LP, 2016, IEEE GEOSC REM SEN M, V4, P22, DOI 10.1109/MGRS.2016.2540798
   Zou Q, 2015, IEEE GEOSCI REMOTE S, V12, P2321, DOI 10.1109/LGRS.2015.2475299
NR 11
TC 19
Z9 19
U1 4
U2 43
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1545-598X
EI 1558-0571
J9 IEEE GEOSCI REMOTE S
JI IEEE Geosci. Remote Sens. Lett.
PD DEC
PY 2016
VL 13
IS 12
BP 1984
EP 1988
DI 10.1109/LGRS.2016.2619984
PG 5
WC Geochemistry & Geophysics; Engineering, Electrical & Electronic; Remote
   Sensing; Imaging Science & Photographic Technology
SC Geochemistry & Geophysics; Engineering; Remote Sensing; Imaging Science
   & Photographic Technology
GA EG8HZ
UT WOS:000391298500047
DA 2020-02-19
ER

PT J
AU Cheng, G
   Zhou, PC
   Han, JW
AF Cheng, Gong
   Zhou, Peicheng
   Han, Junwei
TI Learning Rotation-Invariant Convolutional Neural Networks for Object
   Detection in VHR Optical Remote Sensing Images
SO IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING
LA English
DT Article
DE Convolutional neural networks (CNNs); machine learning; object
   detection; remote sensing images; rotation-invariant CNN (RICNN)
ID BINARY HYPOTHESIS MODEL; SPARSE REPRESENTATION; SCENE CLASSIFICATION;
   TARGET DETECTION; VEHICLE DETECTION; VISUAL SALIENCY; SHIP DETECTION;
   FEATURES
AB Object detection in very high resolution optical remote sensing images is a fundamental problem faced for remote sensing image analysis. Due to the advances of powerful feature representations, machine-learning-based object detection is receiving increasing attention. Although numerous feature representations exist, most of them are handcrafted or shallow-learning-based features. As the object detection task becomes more challenging, their description capability becomes limited or even impoverished. More recently, deep learning algorithms, especially convolutional neural networks (CNNs), have shown their much stronger feature representation power in computer vision. Despite the progress made in nature scene images, it is problematic to directly use the CNN feature for object detection in optical remote sensing images because it is difficult to effectively deal with the problem of object rotation variations. To address this problem, this paper proposes a novel and effective approach to learn a rotation-invariant CNN (RICNN) model for advancing the performance of object detection, which is achieved by introducing and learning a new rotation-invariant layer on the basis of the existing CNN architectures. However, different from the training of traditional CNN models that only optimizes the multinomial logistic regression objective, our RICNN model is trained by optimizing a new objective function via imposing a regularization constraint, which explicitly enforces the feature representations of the training samples before and after rotating to be mapped close to each other, hence achieving rotation invariance. To facilitate training, we first train the rotation-invariant layer and then domain-specifically fine-tune the whole RICNN network to further boost the performance. Comprehensive evaluations on a publicly available ten-class object detection data set demonstrate the effectiveness of the proposed method.
C1 [Cheng, Gong; Zhou, Peicheng; Han, Junwei] Northwestern Polytech Univ, Sch Automat, Xian 710072, Peoples R China.
RP Han, JW (reprint author), Northwestern Polytech Univ, Sch Automat, Xian 710072, Peoples R China.
EM junweihan2010@gmail.com
RI ben, chris/V-4839-2019; Cheng, Gong/I-9551-2019
OI Cheng, Gong/0000-0001-5030-0683
FU National Science Foundation of ChinaNational Natural Science Foundation
   of China [61401357, 61473231]; Fundamental Research Funds for the
   Central UniversitiesFundamental Research Funds for the Central
   Universities [3102016ZY023]; Innovation Foundation for Doctor
   Dissertation of NPU [CX201622]; Aerospace Science Foundation of China
   [20140153003]
FX This work was supported in part by the National Science Foundation of
   China under Grant 61401357 and Grant 61473231, by the Fundamental
   Research Funds for the Central Universities under Grant 3102016ZY023, by
   the Innovation Foundation for Doctor Dissertation of NPU under Grant
   CX201622, and by the Aerospace Science Foundation of China under Grant
   20140153003. (Corresponding author: Junwei Han.)
CR Aytekin O, 2013, IEEE GEOSCI REMOTE S, V10, P471, DOI 10.1109/LGRS.2012.2210189
   Bi FK, 2012, IEEE GEOSCI REMOTE S, V9, P749, DOI 10.1109/LGRS.2011.2180695
   Chen Y, 2011, IEEE J-STSP, V5, P629, DOI 10.1109/JSTSP.2011.2113170
   Cheng G, 2016, ISPRS J PHOTOGRAMM, V117, P11, DOI 10.1016/j.isprsjprs.2016.03.014
   Cheng G, 2015, PROC CVPR IEEE, P1173, DOI 10.1109/CVPR.2015.7298721
   Cheng G, 2015, IET COMPUT VIS, V9, P639, DOI 10.1049/iet-cvi.2014.0270
   Cheng G, 2015, IEEE T GEOSCI REMOTE, V53, P4238, DOI 10.1109/TGRS.2015.2393857
   Cheng G, 2014, ISPRS J PHOTOGRAMM, V98, P119, DOI 10.1016/j.isprsjprs.2014.10.002
   Cheng G, 2013, ISPRS J PHOTOGRAMM, V85, P32, DOI 10.1016/j.isprsjprs.2013.08.001
   Cheng G, 2013, INT J REMOTE SENS, V34, P45, DOI 10.1080/01431161.2012.705443
   Dalal N, 2005, PROC CVPR IEEE, P886
   Das S, 2011, IEEE T GEOSCI REMOTE, V49, P3906, DOI 10.1109/TGRS.2011.2136381
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Eikvil L, 2009, ISPRS J PHOTOGRAMM, V64, P65, DOI 10.1016/j.isprsjprs.2008.09.005
   Fei-Fei L, 2005, PROC CVPR IEEE, P524
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Girshick R, 2014, P IEEE C COMP VIS PA, P1
   Girshick R, 2016, IEEE T PATTERN ANAL, V38, P142, DOI 10.1109/TPAMI.2015.2437384
   Grabner H, 2008, ISPRS J PHOTOGRAMM, V63, P382, DOI 10.1016/j.isprsjprs.2007.10.005
   Han JW, 2016, IEEE T CYBERNETICS, V46, P487, DOI 10.1109/TCYB.2015.2404432
   Han JW, 2015, IEEE T CIRC SYST VID, V25, P1309, DOI 10.1109/TCSVT.2014.2381471
   Han JW, 2015, IEEE T GEOSCI REMOTE, V53, P3325, DOI 10.1109/TGRS.2014.2374218
   Han JW, 2014, ISPRS J PHOTOGRAMM, V89, P37, DOI 10.1016/j.isprsjprs.2013.12.011
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hu F, 2015, REMOTE SENS-BASEL, V7, P14680, DOI 10.3390/rs71114680
   Hu F, 2015, IEEE J-STARS, V8, P2015, DOI 10.1109/JSTARS.2015.2444405
   Huang X, 2009, INT J REMOTE SENS, V30, P1977, DOI 10.1080/01431160802546837
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Leitloff J, 2010, IEEE T GEOSCI REMOTE, V48, P2795, DOI 10.1109/TGRS.2010.2043109
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Song BQ, 2016, IEEE J-STARS, V9, P1613, DOI 10.1109/JSTARS.2015.2508285
   Sun H, 2012, IEEE GEOSCI REMOTE S, V9, P109, DOI 10.1109/LGRS.2011.2161569
   Tuermer S, 2013, IEEE J-STARS, V6, P2327, DOI 10.1109/JSTARS.2013.2242846
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Xia GS, 2015, REMOTE SENS-BASEL, V7, P15014, DOI 10.3390/rs71115014
   Xia GS, 2014, INT J COMPUT VISION, V106, P31, DOI 10.1007/s11263-013-0640-1
   Xia GS, 2010, INT J COMPUT VISION, V88, P382, DOI 10.1007/s11263-009-0312-3
   Xu S, 2010, IEEE GEOSCI REMOTE S, V7, P366, DOI 10.1109/LGRS.2009.2035644
   Yao C, 2016, NEUROCOMPUTING, V207, P346, DOI 10.1016/j.neucom.2016.05.017
   Yao XW, 2016, IEEE T GEOSCI REMOTE, V54, P3660, DOI 10.1109/TGRS.2016.2523563
   Yao XW, 2015, NEUROCOMPUTING, V164, P162, DOI 10.1016/j.neucom.2015.02.073
   Yokoya N, 2015, IEEE J-STARS, V8, P2053, DOI 10.1109/JSTARS.2015.2404578
   Zhang DW, 2015, IEEE GEOSCI REMOTE S, V12, P701, DOI 10.1109/LGRS.2014.2358994
   Zhang LF, 2014, IEEE T GEOSCI REMOTE, V52, P1030, DOI 10.1109/TGRS.2013.2246837
   Zhang YX, 2015, IEEE J-STARS, V8, P2513, DOI 10.1109/JSTARS.2014.2368173
   Zhang YX, 2015, IEEE T GEOSCI REMOTE, V53, P1346, DOI 10.1109/TGRS.2014.2337883
   Zhong P, 2007, IEEE T GEOSCI REMOTE, V45, P3978, DOI 10.1109/TGRS.2007.907109
   Zhong YF, 2014, IEEE J-STARS, V7, P1889, DOI 10.1109/JSTARS.2013.2280063
   Zhou PC, 2016, MULTIDIM SYST SIGN P, V27, P925, DOI 10.1007/s11045-015-0370-3
   Zhu CR, 2010, IEEE T GEOSCI REMOTE, V48, P3446, DOI 10.1109/TGRS.2010.2046330
NR 51
TC 406
Z9 427
U1 72
U2 495
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 0196-2892
EI 1558-0644
J9 IEEE T GEOSCI REMOTE
JI IEEE Trans. Geosci. Remote Sensing
PD DEC
PY 2016
VL 54
IS 12
BP 7405
EP 7415
DI 10.1109/TGRS.2016.2601622
PG 11
WC Geochemistry & Geophysics; Engineering, Electrical & Electronic; Remote
   Sensing; Imaging Science & Photographic Technology
SC Geochemistry & Geophysics; Engineering; Remote Sensing; Imaging Science
   & Photographic Technology
GA DZ2ZW
UT WOS:000385713500051
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Wang, H
   Yeung, DY
AF Wang, Hao
   Yeung, Dit-Yan
TI Towards Bayesian Deep Learning: A Framework and Some Existing Methods
SO IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING
LA English
DT Article
DE Artificial intelligence; data mining; Bayesian networks; neural
   networks; deep learning; machine learning
AB While perception tasks such as visual object recognition and text understanding play an important role in human intelligence, subsequent tasks that involve inference, reasoning, and planning require an even higher level of intelligence. The past few years have seen major advances in many perception tasks using deep learning models. For higher-level inference, however, probabilistic graphical models with their Bayesian nature are still more powerful and flexible. To achieve integrated intelligence that involves both perception and inference, it is naturally desirable to tightly integrate deep learning and Bayesian models within a principled probabilistic framework, which we call Bayesian deep learning. In this unified framework, the perception of text or images using deep learning can boost the performance of higher-level inference and in return, the feedback from the inference process is able to enhance the perception of text or images. This paper proposes a general framework for Bayesian deep learning and reviews its recent applications on recommender systems, topic models, and control. In this paper, we also discuss the relationship and differences between Bayesian deep learning and other related topics such as the Bayesian treatment of neural networks.
C1 [Wang, Hao; Yeung, Dit-Yan] Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Kowloon, Hong Kong, Peoples R China.
RP Wang, H (reprint author), Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Kowloon, Hong Kong, Peoples R China.
EM hwangaz@cse.ust.hk; dyyeung@cse.ust.hk
CR Adomavicius G, 2012, IEEE T KNOWL DATA EN, V24, P896, DOI 10.1109/TKDE.2011.15
   Balan A. K., 2015, ADV NEURAL INFORM PR, P3420
   Bartolini I, 2011, IEEE T KNOWL DATA EN, V23, P190, DOI 10.1109/TKDE.2010.86
   Bengio Y., 2013, ADV NEURAL INFORM PR, V26, P899
   Bishop CM, 2006, PATTERN RECOGNITION
   Blei D., 2006, ICML, V23, P113, DOI DOI 10.1145/1143844.1143859
   Blei D., 2006, ADV NEURAL INFORM PR, V18, P147
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Blundell C., 2015, P 32 INT C MACH LEAR, P1613
   BOURLARD H, 1988, BIOL CYBERN, V59, P291, DOI 10.1007/BF00332918
   Cai Y, 2014, IEEE T KNOWL DATA EN, V26, P766, DOI 10.1109/TKDE.2013.7
   Chen M, 2014, P 31 INT C MACH LEAR, P1476
   Chen M., 2012, P 29 INT C MACH LEAR, P767, DOI DOI 10.1007/S11222-007-9033-Z
   Chen TQ, 2012, J MACH LEARN RES, V13, P3619
   Gal Y., 2015, P INT C MACH LEARN D
   Gales MJF, 2006, COMPUT SPEECH LANG, V20, P22, DOI 10.1016/j.csl.2004.12.002
   Gan Z., 2015, P 32 INT C MACH LEAR, P1823
   Gan Z., 2015, P 18 INT C ART INT S, P267
   Georgiev K., 2013, P 30 INT C MACH LEAR, P1148
   Goodfellow I, 2016, DEEP LEARNING
   Gupta AK, 2000, MATRIX VARIATE DISTR
   Harrison J, 1999, BAYESIAN FORECASTING
   Hernandez-Lobato J. M., 2015, INT C MACH LEARN, V37, P1861
   Hinton G.E., 1993, P 6 ANN C COMP LEARN, P5, DOI DOI 10.1145/168304.168306
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Hoffman M., 2010, ADV NEURAL INFORM PR, P856
   Hoffman MD, 2013, J MACH LEARN RES, V14, P1303
   Hornick MF, 2012, IEEE T KNOWL DATA EN, V24, P1478, DOI 10.1109/TKDE.2011.90
   Hu YF, 2008, IEEE DATA MINING, P263, DOI 10.1109/ICDM.2008.22
   HUBEL DH, 1968, J PHYSIOL-LONDON, V195, P215, DOI 10.1113/jphysiol.1968.sp008455
   Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178
   Kalchbrenner N, 2014, PROCEEDINGS OF THE 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P655
   Kingma D. P., 2013, CORR
   Le Cun Y., 1987, THESIS
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li S., 2015, P 24 ACM INT C INF K, V15, P811, DOI DOI 10.1145/2806416.2806527
   Li WJ, 2009, 21ST INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-09), PROCEEDINGS, P1126
   Liu NN, 2011, P 5 ACM C REC SYST, P37, DOI DOI 10.1145/2043932.2043943
   Lu ZQ, 2015, PROCEEDINGS OF THE TWENTY-NINTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, P217
   MACKAY DJC, 1992, NEURAL COMPUT, V4, P448, DOI 10.1162/neco.1992.4.3.448
   Matsubara T, 2014, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P583
   Neal R, 1995, THESIS
   NEAL RM, 1992, ARTIF INTELL, V56, P71, DOI 10.1016/0004-3702(92)90065-6
   Park YJ, 2013, IEEE T KNOWL DATA EN, V25, P1904, DOI 10.1109/TKDE.2012.119
   Porteous Ian, 2008, P 14 ACM SIGKDD INT, P569, DOI [DOI 10.1145/1401890.1401960, 10. 1145/1401890. 1401960]
   Purushotham S., 2012, P 29 INT C MACH LEAR, P759
   Ranzato M., 2006, ADV NEURAL INFORM PR, V19, P1137
   Ricci F., 2011, INTRO RECOMMENDER SY
   Sainath TN, 2013, INT CONF ACOUST SPEE, P6655, DOI 10.1109/ICASSP.2013.6638949
   Salah R, 2011, P 28 INT C MACH LEAR, P833, DOI 10.1016/j.wasman.2010.10.006
   Salakhutdinov R., 2007, P INT C MACH LEARN, V24, P791, DOI DOI 10.1145/1273496.1273596
   Salakhutdinov R. R., 2008, ADV NEURAL INFORM PR, V20, P1257, DOI DOI 10.1145/1390156.1390267
   Shewchuk J. R., 1994, CMUCS94125
   Singh A. P., 2008, P 14 ACM SIGKDD INT, P650, DOI DOI 10.1145/1401890.1401969
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   STRICHARTZ RS, 2003, GUIDE DISTRIBUTION T
   van den Oord Aaron, 2013, ADV NEURAL INFORM PR, P2643
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Wang C, 2011, P 17 ACM SIGKDD INT, P448, DOI DOI 10.1145/2020408.2020480
   Wang C., 2008, P 24 C UNC ART INT, P579
   Wang H., 2016, P 13 ANN C IN PRESS
   Wang H., 2016, P 13 ANN C NEUR INF
   Wang H, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1235, DOI 10.1145/2783258.2783273
   Wang H, 2015, PROCEEDINGS OF THE TWENTY-NINTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3052
   Wang H, 2015, IEEE T KNOWL DATA EN, V27, P1343, DOI 10.1109/TKDE.2014.2365789
   Wang Hao, 2013, P 23 INT JOINT C ART, P2719
   Wang XX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P627, DOI 10.1145/2647868.2654940
   Watter M., 2015, ADV NEURAL INFORM PR, P2728
   Wei YZ, 2005, IEEE T KNOWL DATA EN, V17, P1678, DOI 10.1109/TKDE.2005.200
   Ying H, 2016, LECT NOTES ARTIF INT, V9652, P555, DOI 10.1007/978-3-319-31750-2_44
   Zhang FZ, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P353, DOI 10.1145/2939672.2939673
   Zheng VW, 2010, PROCEEDINGS OF THE TWENTY-FOURTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-10), P236
   Zhou M., 2012, J MACHINE LEARNING R, P1462
   Zhou MY, 2015, IEEE T PATTERN ANAL, V37, P307, DOI 10.1109/TPAMI.2013.211
NR 75
TC 27
Z9 29
U1 11
U2 88
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1041-4347
EI 1558-2191
J9 IEEE T KNOWL DATA EN
JI IEEE Trans. Knowl. Data Eng.
PD DEC 1
PY 2016
VL 28
IS 12
BP 3395
EP 3408
DI 10.1109/TKDE.2016.2606428
PG 14
WC Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Engineering, Electrical & Electronic
SC Computer Science; Engineering
GA EC5ZB
UT WOS:000388214700019
DA 2020-02-19
ER

PT J
AU Koesdwiady, A
   Soua, R
   Karray, F
AF Koesdwiady, Arief
   Soua, Ridha
   Karray, Fakhreddine
TI Improving Traffic Flow Prediction With Weather Information in Connected
   Cars: A Deep Learning Approach
SO IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY
LA English
DT Article
DE Data fusion; deep learning; intelligent transportation systems (ITS);
   traffic prediction; weather information
ID MODEL; ALGORITHM
AB Transportation systems might be heavily affected by factors such as accidents and weather. Specifically, inclement weather conditions may have a drastic impact on travel time and traffic flow. This study has two objectives: first, to investigate a correlation between weather parameters and traffic flow and, second, to improve traffic flow prediction by proposing a novel holistic architecture. It incorporates deep belief networks for traffic and weather prediction and decision-level data fusion scheme to enhance prediction accuracy using weather conditions. The experimental results, using traffic and weather data originated from the San Francisco Bay Area of California, corroborate the effectiveness of the proposed approach compared with the state of the art.
C1 [Koesdwiady, Arief; Karray, Fakhreddine] Univ Waterloo, Ctr Pattern Anal & Machine Intelligence, Waterloo, ON N2L 3G1, Canada.
   [Soua, Ridha] Univ Luxembourg, Interdisciplinary Ctr Secur Reliabil & Trust SnT, L-1359 Luxembourg, Luxembourg.
RP Koesdwiady, A (reprint author), Univ Waterloo, Ctr Pattern Anal & Machine Intelligence, Waterloo, ON N2L 3G1, Canada.
EM abkoesdw@uwaterloo.ca
RI Koesdwiady, Arief/W-5274-2019; Soua, Ridha/H-2501-2018
OI Koesdwiady, Arief/0000-0002-5219-4789; Soua, Ridha/0000-0001-5662-4654
CR Abdelmoula R., 2016, THESIS
   [Anonymous], 2015, UN HAB UN HUM SETTL, DOI Unhabitat
   [Anonymous], 2012, IBM TRAFF MAN SOL
   [Anonymous], 2015, WEATH EV IMP ROADS
   [Anonymous], 2015, CALTR PERF MEAS SYST
   [Anonymous], 2012, TRANSP OUTL 2012 SEA
   [Anonymous], 2015, MESOWEST STAT UPD LO
   Barimani N, 2014, IET INTELL TRANSP SY, V8, P308, DOI 10.1049/iet-its.2013.0053
   Castanedo F, 2013, SCI WORLD J, DOI 10.1155/2013/704504
   Chandra SR, 2009, J INTELL TRANSPORT S, V13, P53, DOI 10.1080/15472450902858368
   Chang H, 2012, IET INTELL TRANSP SY, V6, P292, DOI 10.1049/iet-its.2011.0123
   Ciresan D, 2012, PROC CVPR IEEE, P3642, DOI 10.1109/CVPR.2012.6248110
   Cools M, 2009, TRANSPORT RES REC, P57, DOI 10.3141/2136-07
   Dunne S, 2013, IEEE T INTELL TRANSP, V14, P370, DOI 10.1109/TITS.2012.2225049
   Guo JH, 2010, TRANSPORT RES REC, P28, DOI 10.3141/2175-04
   Hall D.L., 2008, INF FUS 2008 11 INT, P1
   He Jingrui, 2013, P 23 INT JOINT C ART, P1387
   Highway Capacity Manual, 2010, HIGHWAY CAPACITY MAN, V1-4
   Hinton G, 2005, AISTATS, V10, P33
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hu JM, 2014, 2014 IEEE 17TH INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS (ITSC), P2267, DOI 10.1109/ITSC.2014.6958049
   Huang WH, 2014, IEEE T INTELL TRANSP, V15, P2191, DOI 10.1109/TITS.2014.2311123
   Jiang Z, 2014, PROCD SOC BEHV, V138, P811, DOI 10.1016/j.sbspro.2014.07.230
   Jin S, 2013, J ZHEJIANG UNIV-SC A, V14, P231, DOI 10.1631/jzus.A1200218
   Khaleghi B, 2013, INFORM FUSION, V14, P28, DOI 10.1016/j.inffus.2011.08.001
   Khosravi A, 2011, TRANSPORT RES C-EMER, V19, P1364, DOI 10.1016/j.trc.2011.04.002
   Kumar K, 2013, PROCD SOC BEHV, V104, P755, DOI 10.1016/j.sbspro.2013.11.170
   Li MW, 2013, NEUROCOMPUTING, V99, P230, DOI 10.1016/j.neucom.2012.08.002
   Lin L., 2015, TRANSP RES BOARD ANN
   Liu D, 2010, TENCON IEEE REGION, P736, DOI 10.1109/TENCON.2010.5686608
   Lv YS, 2015, IEEE T INTELL TRANSP, V16, P865, DOI 10.1109/TITS.2014.2345663
   McCrea J, 2010, EUR J OPER RES, V207, P676, DOI 10.1016/j.ejor.2010.05.018
   Min WL, 2011, TRANSPORT RES C-EMER, V19, P606, DOI 10.1016/j.trc.2010.10.002
   Nie LQ, 2015, IEEE T KNOWL DATA EN, V27, P2107, DOI 10.1109/TKDE.2015.2399298
   Tianshu Wu, 2012, 2012 9th International Conference on Fuzzy Systems and Knowledge Discovery, P2555, DOI 10.1109/FSKD.2012.6234335
   Tsapakis I, 2013, J TRANSP GEOGR, V28, P204, DOI 10.1016/j.jtrangeo.2012.11.003
   Van Lint J. W. C., 2012, ARTIF INTELL, V22, P22
   Vlahogianni E, 2012, NONLINEAR DYNAM, V69, P1949, DOI 10.1007/s11071-012-0399-x
   Williams BM, 2003, J TRANSP ENG-ASCE, V129, P664, DOI 10.1061/(ASCE)0733-947X(2003)129:6(664)
   Zargari SA, 2012, EXPERT SYST, V29, P124, DOI 10.1111/j.1468-0394.2010.00567.x
   Zhang YR, 2014, TRANSPORT RES C-EMER, V43, P65, DOI 10.1016/j.trc.2013.11.011
   Zhao YJ, 2012, TRANSPORT RES REC, P173, DOI 10.3141/2272-20
NR 42
TC 68
Z9 71
U1 15
U2 90
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 0018-9545
EI 1939-9359
J9 IEEE T VEH TECHNOL
JI IEEE Trans. Veh. Technol.
PD DEC
PY 2016
VL 65
IS 12
BP 9508
EP 9517
DI 10.1109/TVT.2016.2585575
PG 10
WC Engineering, Electrical & Electronic; Telecommunications; Transportation
   Science & Technology
SC Engineering; Telecommunications; Transportation
GA EF9RY
UT WOS:000390668900006
DA 2020-02-19
ER

PT J
AU Tan, WX
   Zhao, CJ
   Wu, HR
AF Tan, Wenxue
   Zhao, Chunjiang
   Wu, Huarui
TI Intelligent alerting for fruit-melon lesion image based on momentum deep
   learning
SO MULTIMEDIA TOOLS AND APPLICATIONS
LA English
DT Article
DE Lesion image; CNN; Deep network; Momentum learning; Intelligent alerting
ID EXPERT-SYSTEM; RECOGNITION; NETWORKS; DISEASE
AB Sensors and Internet of things (IoT) have been widely used in the digitalized orchards. Traditional disease-pest recognition and early warning systems, which are based on knowledge rule, expose many defects, discommodities, and it is difficult to meet current production management requirements of the fresh planting environment. On purpose to realize an intelligent unattended alerting for disease-pest of fruit-melon, this paper presents the convolutional neural network (CNN) for recognition of fruit-melon skin lesion image which is real-timely acquired by an infrared video sensor, which network is grounded upon so-called momentum deep learning rule. More specifically, (1) a suite of transformation methods of apple skin lesion image is devised to simulate orientation and light disturbance which always occurs in orchards, then to output a self-contained set of almost all lesion images which might appear in various dynamic sensing environment; and (2) the rule of variable momentum learning is formulated to update the free parameters of CNN. Experimental results demonstrate that the proposed presents a satisfying accuracy and recall rate which are up to 97.5 %, 98.5 % respectively. As compared with some shallow learning algorithms and generally accepted deep learning ones, it also offers a gratifying smoothness, stableness after convergence and a quick converging speed. In addition, the statistics from experiments of different benchmark data-sets suggests it is very effective to recognize image pattern.
C1 [Tan, Wenxue] Beijing Univ Technol, Coll Comp Sci, Beijing 100022, Peoples R China.
   [Tan, Wenxue] Hunan Univ Arts & Sci, Changde 415000, Peoples R China.
   [Zhao, Chunjiang; Wu, Huarui] Beijing Acad Agr & Forestry Sci, Beijing Res Ctr Informat Technol Agr, Beijing 100097, Peoples R China.
RP Tan, WX (reprint author), Beijing Univ Technol, Coll Comp Sci, Beijing 100022, Peoples R China.; Tan, WX (reprint author), Hunan Univ Arts & Sci, Changde 415000, Peoples R China.
EM twxpaper@163.com
FU Beijing Natural Science FoundationBeijing Natural Science Foundation
   [4151001]; Hunan Education Department Project [16A151]
FX This paper was funded by a grant from Beijing Natural Science Foundation
   (No. 4151001); Hunan Education Department Project (16A151); The authors
   also gratefully acknowledge the helpful comments and suggestions from
   reviewers, which contribute to a refined paper presentation.
CR Al-Jawfi R, 2009, INT ARAB J INF TECHN, V6, P304
   Chang FC, 2007, 2007 THIRD INTERNATIONAL CONFERENCE ON INTELLIGENT INFORMATION HIDING AND MULTIMEDIA SIGNAL PROCESSING, VOL 1, PROCEEDINGS, P210
   Chen B, 2013, IEEE T PATTERN ANAL, V35, P1887, DOI 10.1109/TPAMI.2013.19
   Chuang SC, 2006, INT C INN COMP INF C
   CLANCEY WJ, 1983, ARTIF INTELL, V20, P215, DOI 10.1016/0004-3702(83)90008-5
   DAHANAYAKE BW, 1993, 1993 IEEE INTERNATIONAL CONFERENCE ON NEURAL NETWORKS, VOLS 1-3, P831, DOI 10.1109/ICNN.1993.298666
   Dutot M, 2013, POSTHARVEST BIOL TEC, V85, P45, DOI 10.1016/j.postharvbio.2013.04.003
   Ghazikhani A, 2013, NEUROCOMPUTING, V122, P535, DOI 10.1016/j.neucom.2013.05.003
   Ghazikhani A, 2013, NEURAL COMPUT APPL, V23, P1283, DOI 10.1007/s00521-012-1071-6
   GROSSBERG S, 1994, PERCEPT PSYCHOPHYS, V55, P48, DOI 10.3758/BF03206880
   Harteveld DOC, 2014, PLANT DIS, V98, P401, DOI 10.1094/PDIS-06-13-0676-RE
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Juang CF, 2014, IEEE T NEUR NET LEAR, V25, P216, DOI 10.1109/TNNLS.2013.2253799
   Kadappa V, 2013, PATTERN RECOGN, V46, P2169, DOI 10.1016/j.patcog.2013.01.018
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee H, 2011, COMMUN ACM, V54, P95, DOI 10.1145/2001269.2001295
   Lemmetty A, 2013, PLANT DIS, V97, P1376, DOI 10.1094/PDIS-04-13-0397-PDN
   Liu Y, 2013, MATH PROBLEMS ENG
   Mutch J, 2008, INT J COMPUT VISION, V80, P45, DOI 10.1007/s11263-007-0118-0
   Pan Rong-ying, 2009, Journal of Shanghai Jiaotong University (English Edition), V14, P632, DOI 10.1007/s12204-009-0632-z
   Pandey S, 2012, APPL SOFT COMPUT, V12, P1214, DOI 10.1016/j.asoc.2011.10.011
   Qiao JF, 2014, NEUROCOMPUTING, V125, P7, DOI 10.1016/j.neucom.2012.09.038
   Qiao Y., 2015, J INFORM HIDING MULT, V6, P534
   Romeo J, 2013, EXPERT SYST APPL, V40, P2275, DOI 10.1016/j.eswa.2012.10.033
   Sarikaya R, 2014, IEEE-ACM T AUDIO SPE, V22, P778, DOI 10.1109/TASLP.2014.2303296
   ShekharYadav C., 2014, INT J COMPUTER APPL, V90, P37
   [司永胜 Si Yongsheng], 2009, [农业机械学报, Transactions of the Chinese Society of Agricultural Machinery], V40, P161
   Taylor GW, 2011, J MACH LEARN RES, V12, P1025
   Wang J, 2010, SENSOR LETT, V8, P178, DOI 10.1166/sl.2010.1223
   Wester R, 2015, IEEE SOFTWARE, V32, P37, DOI 10.1109/MS.2015.53
   Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120
   Xie Y, 2014, PATTERN RECOGN, V47, P1383, DOI 10.1016/j.patcog.2013.07.010
   Xing Y, 2015, J INF HIDING MULTIME, V6, P622
NR 34
TC 11
Z9 12
U1 4
U2 58
PU SPRINGER
PI DORDRECHT
PA VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN 1380-7501
EI 1573-7721
J9 MULTIMED TOOLS APPL
JI Multimed. Tools Appl.
PD DEC
PY 2016
VL 75
IS 24
BP 16741
EP 16761
DI 10.1007/s11042-015-2940-7
PG 21
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic
SC Computer Science; Engineering
GA EE4VV
UT WOS:000389604600002
DA 2020-02-19
ER

PT J
AU Elyan, E
   Gaber, MM
AF Elyan, Eyad
   Gaber, Mohamed Medhat
TI A fine-grained Random Forests using class decomposition: an application
   to medical diagnosis
SO NEURAL COMPUTING & APPLICATIONS
LA English
DT Article
DE Machine learning; Random Forests; Clustering; Ensemble learning
ID RECOGNITION
AB Class decomposition describes the process of segmenting each class into a number of homogeneous subclasses. This can be naturally achieved through clustering. Utilising class decomposition can provide a number of benefits to supervised learning, especially ensembles. It can be a computationally efficient way to provide a linearly separable data set without the need for feature engineering required by techniques like support vector machines and deep learning. For ensembles, the decomposition is a natural way to increase diversity, a key factor for the success of ensemble classifiers. In this paper, we propose to adopt class decomposition to the state-of-the-art ensemble learning Random Forests. Medical data for patient diagnosis may greatly benefit from this technique, as the same disease can have a diverse of symptoms. We have experimentally validated our proposed method on a number of data sets that are mainly related to the medical domain. Results reported in this paper show clearly that our method has significantly improved the accuracy of Random Forests.
C1 [Elyan, Eyad; Gaber, Mohamed Medhat] Robert Gordon Univ, Sch Comp Sci & Digital Media, Garthdee Rd, Aberdeen AB10 7GJ, Scotland.
RP Gaber, MM (reprint author), Robert Gordon Univ, Sch Comp Sci & Digital Media, Garthdee Rd, Aberdeen AB10 7GJ, Scotland.
EM e.elyan@rgu.ac.uk; m.gaber1@rgu.ac.uk
OI Elyan, Eyad/0000-0002-8342-9026; Gaber, Mohamed/0000-0003-0339-4474
CR Abdallah ZS, 2015, NEUROCOMPUTING, V150, P304, DOI 10.1016/j.neucom.2014.09.074
   Abdallah ZS, 2011, COMP INT DAT MIN CID, P283
   Amaratunga D, 2008, BIOINFORMATICS, V24, P2010, DOI 10.1093/bioinformatics/btn356
   Amit Y, 1997, NEURAL COMPUT, V9, P1545, DOI 10.1162/neco.1997.9.7.1545
   Bader-El-Den M, 2012, LECT NOTES COMPUT SC, V7664, P506, DOI 10.1007/978-3-642-34481-7_62
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.1007/BF00058655
   Dietterich T. G., 1991, AAAI-91. Proceedings Ninth National Conference on Artificial Intelligence, P572
   DRUCKER H, 1994, NEURAL COMPUT, V6, P1289, DOI 10.1162/neco.1994.6.6.1289
   Elter M, 2007, MED PHYS, V34, P4164, DOI 10.1118/1.2786864
   Fawagreh K, 2014, SYST SCI CONTROL ENG, V2, P602, DOI 10.1080/21642583.2014.956265
   Fawagreh K, 2014, LECT NOTES COMPUT SC, V8669, P85, DOI 10.1007/978-3-319-10840-7_11
   Fernandez-Delgado M, 2014, J MACH LEARN RES, V15, P3133
   FREUND Y, 1995, INFORM COMPUT, V121, P256, DOI 10.1006/inco.1995.1136
   Friedman JH, 2002, COMPUT STAT DATA AN, V38, P367, DOI 10.1016/S0167-9473(01)00065-2
   Ho T. K., 1995, INT C, V1, P278, DOI DOI 10.1109/ICDAR.1995.598994
   Ho TK, 1998, IEEE T PATTERN ANAL, V20, P832, DOI 10.1109/34.709601
   HONG ZQ, 1991, PATTERN RECOGN, V24, P317, DOI 10.1016/0031-3203(91)90074-F
   Jain A. K., 1988, ALGORITHMS CLUSTERIN, V6
   Latinne P., 2001, Multiple Classifier Systems. Second International Workshop, MCS 2001. Proceedings (Lecture Notes in Computer Science Vol.2096), P178
   Liaw A., 2002, R NEWS, V23, P18, DOI DOI 10.1177/154405910408300516
   Lichman M., 2013, UCI MACHINE LEARNING
   Little MA, 2007, BIOMED ENG ONLINE, V6, DOI 10.1186/1475-925X-6-23
   MacQueen J, 1967, P 5 BERK S MATH STAT, V1, P281, DOI DOI 10.1234/12345678
   MANGASARIAN OL, 1995, OPER RES, V43, P570, DOI 10.1287/opre.43.4.570
   Polaka I, 2013, INT C APPL INF COMM
   Polikar R., 2006, IEEE Circuits and Systems Magazine, V6, P21, DOI 10.1109/MCAS.2006.1688199
   Repository U, 1996, HEART DIS DATASET
   Robnik-Sikonja M, 2004, LECT NOTES COMPUT SC, V3201, P359
   Tsymbal A, 2006, LECT NOTES COMPUT SC, V4212, P801
   Vilalta R, 2003, DAT MIN 2003 ICDM 20, P673
   WOLPERT DH, 1992, NEURAL NETWORKS, V5, P241, DOI 10.1016/S0893-6080(05)80023-1
   Woolson R.F., 2008, WILEY ENCY CLIN TRIA, P1, DOI DOI 10.1002/9780471462422.EOCT979
NR 33
TC 5
Z9 5
U1 4
U2 16
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 0941-0643
EI 1433-3058
J9 NEURAL COMPUT APPL
JI Neural Comput. Appl.
PD DEC
PY 2016
VL 27
IS 8
SI SI
BP 2279
EP 2288
DI 10.1007/s00521-015-2064-z
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA EI3UT
UT WOS:000392418300008
OA Green Accepted
DA 2020-02-19
ER

PT J
AU Andrearczyk, V
   Whelan, P
AF Andrearczyk, Vincent
   Whelan, Paulf.
TI Using filter banks in Convolutional Neural Networks for texture
   classification
SO PATTERN RECOGNITION LETTERS
LA English
DT Article
DE Texture classification; Convolutional Neural Network; Dense orderless
   pooling; Filter banks; Energy layer
AB Deep learning has established many new state of the art solutions in the last decade in areas such as object, scene and speech recognition. In particular Convolutional Neural Network (CNN) is a category of deep learning which obtains excellent results in object detection and recognition tasks. Its architecture is indeed well suited to object analysis by learning and classifying complex (deep) features that represent parts of an object or the object itself. However, some of its features are very similar to texture analysis methods. CNN layers can be thought of as filter banks of complexity increasing with the depth. Filter banks are powerful tools to extract texture features and have been widely used in texture analysis. In this paper we develop a simple network architecture named Texture CNN (T-CNN) which explores this observation. It is built on the idea that the overall shape information extracted by the fully connected layers of a classic CNN is of minor importance in texture analysis. Therefore, we pool an energy measure from the last convolution layer which we connect to a fully connected layer. We show that our approach can improve the performance of a network while greatly reducing the memory usage and computation. (C) 2016 Elsevier B.V. All rights reserved.
C1 [Andrearczyk, Vincent; Whelan, Paulf.] Dublin City Univ, Sch Elect Engn, Vis Syst Grp, Dublin 9, Ireland.
RP Andrearczyk, V (reprint author), Dublin City Univ, Sch Elect Engn, Vis Syst Grp, Dublin 9, Ireland.
EM vincent.andrearczyk3@mail.dcu.ie
RI Whelan, Paul/C-7962-2011
OI Whelan, Paul/0000-0001-9230-7656
CR Bharati MH, 2004, CHEMOMETR INTELL LAB, V72, P57, DOI 10.1016/j.chemolab.2004.02.005
   Caenen G., 2004, COMP VIS PATT REC WO, P58
   Cimpoi M, 2015, PROC CVPR IEEE, P3828, DOI 10.1109/CVPR.2015.7299007
   Cimpoi M, 2014, PROC CVPR IEEE, P3606, DOI 10.1109/CVPR.2014.461
   Dana KJ, 1999, ACM T GRAPHIC, V18, P1, DOI 10.1145/300776.300778
   FOGEL I, 1989, BIOL CYBERN, V61, P103
   FUKUSHIMA K, 1980, BIOL CYBERN, V36, P193, DOI 10.1007/BF00344251
   Hafemann LG, 2014, INT C PATT RECOG, P1103, DOI 10.1109/ICPR.2014.199
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314
   Hayman E, 2004, LECT NOTES COMPUT SC, V2034, P253
   Jia Y., 2014, 14085093 ARXIV
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kylberg G., 2011, EXTERNAL REPORT BLUE, V35
   Kylberg G, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-17
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Malik J, 2001, INT J COMPUT VISION, V43, P7, DOI 10.1023/A:1011174803800
   Martins J, 2013, MACH VISION APPL, V24, P567, DOI 10.1007/s00138-012-0417-5
   Paula PL, 2014, MACH VISION APPL, V25, P1019, DOI 10.1007/s00138-014-0592-7
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sermanet P, 2012, INT C PATT RECOG, P3288
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tivive F. H. C., 2006, TENCON 2006, P1, DOI DOI 10.1109/TENCON.2006.343944
   Varma M, 2005, INT J COMPUT VISION, V62, P61, DOI 10.1023/B:VISI.0000046589.39864.ee
NR 23
TC 45
Z9 46
U1 3
U2 23
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0167-8655
EI 1872-7344
J9 PATTERN RECOGN LETT
JI Pattern Recognit. Lett.
PD DEC 1
PY 2016
VL 84
BP 63
EP 69
DI 10.1016/j.patrec.2016.08.016
PG 7
WC Computer Science, Artificial Intelligence
SC Computer Science
GA EF9PA
UT WOS:000390660900010
OA Green Accepted
DA 2020-02-19
ER

PT J
AU Tomczak, JM
AF Tomczak, Jakub M.
TI Learning Informative Features from Restricted Boltzmann Machines
SO NEURAL PROCESSING LETTERS
LA English
DT Article
DE Unsupervised learning; Entropy-based regularization; Orthonormality
   regularization; Restricted Boltzmann machine
AB In recent years deep learning paradigm achieved important empirical success in a number of practical applications such as object recognition, speech recognition and natural language processing. A lot of effort has been put on understanding theoretical aspects of this success, however, still there is no common view on how deep architectures should be trained and thus many open questions remain. One hypothesis focuses on formulating good criterion (prior) that may help to learn a set of features capable of disentangling hidden factors. Following this line of thinking, in this paper, we propose to add a penalty (regularization) term to the log-likelihood function that enforces hidden units to maximize entropy and to be pairwise uncorrelated, for given observables. We hypothesize that the proposed framework for learning informative features results in more discriminative data representation that maintains its generative capabilities. In order to verify our hypothesis we apply the regularization term to the Restricted Boltzmann Machine (RBM) and carry out empirical study on three classification problems: character recognition, object recognition, and document classification. The experiments confirm that the proposed approach indeed increases discriminative and generative performance in comparison to RBM trained without any regularization and with the weight-decay, the sparse regularization, the max-norm regularization, Dropout and Dropconnect.
C1 [Tomczak, Jakub M.] Wroclaw Univ Technol, Wybrzeze Wyspianskiego 27, PL-50370 Wroclaw, Poland.
RP Tomczak, JM (reprint author), Wroclaw Univ Technol, Wybrzeze Wyspianskiego 27, PL-50370 Wroclaw, Poland.
EM jakub.tomczak@pwr.edu.pl
OI Tomczak, Jakub/0000-0001-8634-694X
FU Ministry of Science and Higher Education, Republic of PolandMinistry of
   Science and Higher Education, Poland [B40020/I32]
FX The author is grateful to Adam Gonczarek and Maciej Zieba for fruitful
   discussions and anonymous reviews for helpful comments. The research
   conducted by the author has been partially co-financed by the Ministry
   of Science and Higher Education, Republic of Poland, Grant No.
   B40020/I32.
CR AKAIKE H, 1974, IEEE T AUTOMAT CONTR, VAC19, P716, DOI 10.1109/TAC.1974.1100705
   Arribas JI, 2005, IEEE T NEURAL NETWOR, V16, P799, DOI 10.1109/TNN.2005.849826
   Bengio Y., 2013, P 30 INT C MACH LEAR, P552
   Bengio Y., 2007, LARGE SCALE KERNEL M, V34, P1
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Cho K., 2013, P 30 INT C MACH LEAR, V28, P432
   Coates A., 2011, INT C ART INT STAT, V15, P215, DOI 10.1177/1753193410390845
   Collobert R, 2011, J MACH LEARN RES, V12, P2493
   Corduneanu A, 2002, P 19 C UNC ART INT M, P151
   Erhan D, 2010, J MACH LEARN RES, V11, P625
   Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193
   Grandvalet Y., 2005, ADV NEURAL INFORM PR, P529
   Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947
   Hanlin Goh, 2013, ADV NEURAL INFORM PR, P1878
   Hinton G., 2012, NEURAL NETWORKS TRIC, V9, P599, DOI DOI 10.1007/978-3-642-35289-8_32
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hjelm RD, 2014, NEUROIMAGE, V96, P245, DOI 10.1016/j.neuroimage.2014.03.048
   Lang K., 1995, Machine Learning. Proceedings of the Twelfth International Conference on Machine Learning, P331
   Larochelle H., 2008, P 25 INT C MACH LEAR, P536, DOI [10.1145/1390156.1390224, DOI 10.1145/1390156.1390224]
   Le Q. V., 2011, ADV NEURAL INFORM PR, P1017
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee H., 2008, ADV NEURAL INFORM PR, V20, P873
   Marlin B. M., 2010, P 13 INT C ART INT S, P509
   Nair V, 2009, ADV NEURAL INF PROCE, V22, P1339
   Niu G, 2012, INT C ART INT STAT, P509
   RISSANEN J, 1983, ANN STAT, V11, P416, DOI 10.1214/aos/1176346150
   Salah R, 2011, P 28 INT C MACH LEAR, P833, DOI 10.1016/j.wasman.2010.10.006
   Salakhutdinov R., 2008, P INT C MACH LEARN, V25
   Salakhutdinov R, 2013, IEEE T PATTERN ANAL, V35, P1958, DOI 10.1109/TPAMI.2012.269
   Seghouane AK, 2007, IEEE T NEURAL NETWOR, V18, P97, DOI 10.1109/TNN.2006.882813
   Shao J, 2012, PATTERN RECOGN LETT, V33, P271, DOI 10.1016/j.patrec.2011.10.018
   Smolensky P, 1986, PARALLEL DISTRIBUTED, P194
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Wager S., 2013, ADV NEURAL INFORM PR, V26, P351
   Wan L., 2013, P 30 INT C MACH LEAR, P1058
   Wang J, 2012, IEEE T PATTERN ANAL, V34, P2393, DOI 10.1109/TPAMI.2012.48
   Weiss Y., 2008, NIPS, V9, P6
   Xavier Glorot, 2011, P 14 INT C ART INT S, P315, DOI DOI 10.1177/1753193410395357
NR 39
TC 4
Z9 4
U1 2
U2 15
PU SPRINGER
PI DORDRECHT
PA VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN 1370-4621
EI 1573-773X
J9 NEURAL PROCESS LETT
JI Neural Process. Lett.
PD DEC
PY 2016
VL 44
IS 3
BP 735
EP 750
DI 10.1007/s11063-015-9491-9
PG 16
WC Computer Science, Artificial Intelligence
SC Computer Science
GA EB4RZ
UT WOS:000387362200010
OA Other Gold
DA 2020-02-19
ER

PT J
AU Chandra, B
   Sharma, RK
AF Chandra, B.
   Sharma, Rajesh K.
TI Deep learning with adaptive learning rate using laplacian score
SO EXPERT SYSTEMS WITH APPLICATIONS
LA English
DT Article
DE Adaptive learning rate; Deep learning; Gradient descent; Laplacian score
ID ARTIFICIAL NEURAL-NETWORKS; STOCK-PRICE; PREDICTION
AB An attempt has been made to improve the performance of Deep Learning with Multilayer Perceptron (MLP). Tuning the learning rate or finding an optimum learning rate in MLP is a major challenge. Depending on the value of the learning rate, classification accuracy can vary drastically. This issue has been taken as a challenge in this paper. In this paper, a new approach has been proposed to combine adaptive learning rate in conjunction with the concept of Laplacian score for varying the weights. Learning rate is taken as a function of parameter which itself is updated on the basis of error gradient by forming mini batches. Laplacian score of the neuron is further used for updating the incoming weights. This removes the bottleneck involved in finding the optimum value for the learning rate in Deep Learning by using MLP. It is observed on benchmark datasets that this approach leads to increase in classification accuracy as compared to the existing benchmark levels achieved by the well known methods of deep learning. (C) 2016 Published by Elsevier Ltd.
C1 [Chandra, B.; Sharma, Rajesh K.] Indian Inst Technol Delhi, Room MZ 149, New Delhi, India.
RP Chandra, B (reprint author), Indian Inst Technol Delhi, Room MZ 149, New Delhi, India.
EM bchandra104@yahoo.co.in; justrks@gmail.com
FU Council of Scientific and Industrial research (CSIR), IndiaCouncil of
   Scientific & Industrial Research (CSIR) - India
FX The Council of Scientific and Industrial research (CSIR), India, is
   thankfully acknowledged for providing Research Fellowship to Rajesh
   Kumar Sharma.
CR [Anonymous], 2011, INT J SOFT COMPUTING, V1, P32
   Benson Edwin Raj S, 2011, 2011 Proceedings of International Conference on Computer, Communication and Electrical Technology (ICCCET 2011), P152, DOI 10.1109/ICCCET.2011.5762457
   Cheng Z., 2015, ARXIV150303562
   Dahl GE, 2013, INT CONF ACOUST SPEE, P8609, DOI 10.1109/ICASSP.2013.6639346
   Dase R. K., 2010, INT J MACHINE INTELL, V2, P14, DOI 10.9735/0975-2927.2.2.14-17
   de Oliveira FA, 2013, EXPERT SYST APPL, V40, P7596, DOI 10.1016/j.eswa.2013.06.071
   Ghiassi M, 2015, EXPERT SYST APPL, V42, P3176, DOI 10.1016/j.eswa.2014.11.022
   Goodfellow I. J., 2013, JMLR W CP, P1319
   Hadavandi E, 2010, KNOWL-BASED SYST, V23, P800, DOI 10.1016/j.knosys.2010.05.004
   He X., 2005, P ADV NEUR INF PROC, V18, P507
   Hinton G. E, 2012, ARXIV12070580
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hochreiter S., 2001, GRADIENT FLOW RECURR
   Kara Y, 2011, EXPERT SYST APPL, V38, P5311, DOI 10.1016/j.eswa.2010.10.027
   Larochelle H., 2007, P 24 INT C MACH LEAR, P473, DOI DOI 10.1145/1273496.1273556
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee S, 2014, EXPERT SYST APPL, V41, P3041, DOI 10.1016/j.eswa.2013.10.034
   Iturriaga FJL, 2015, EXPERT SYST APPL, V42, P2857, DOI 10.1016/j.eswa.2014.11.025
   Maclaurin Dougal, 2015, ARXIV150203492
   Moosmayer DC, 2013, EXPERT SYST APPL, V40, P3028, DOI 10.1016/j.eswa.2012.12.018
   Pan CZ, 2013, EXPERT SYST APPL, V40, P1629, DOI 10.1016/j.eswa.2012.09.008
   Salah R, 2011, P 28 INT C MACH LEAR, P833, DOI 10.1016/j.wasman.2010.10.006
   Soudry D., 2014, ADV NEURAL INFORM PR, P963
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Xavier Glorot, 2011, P 14 INT C ART INT S, P315, DOI DOI 10.1177/1753193410395357
   Zeiler Matthew D, 2012, ARXIV12125701
NR 26
TC 15
Z9 16
U1 2
U2 126
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0957-4174
EI 1873-6793
J9 EXPERT SYST APPL
JI Expert Syst. Appl.
PD NOV 30
PY 2016
VL 63
BP 1
EP 7
DI 10.1016/j.eswa.2016.05.022
PG 7
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic; Operations Research & Management Science
SC Computer Science; Engineering; Operations Research & Management Science
GA DU5TH
UT WOS:000382273700001
DA 2020-02-19
ER

PT J
AU Cao, LJ
   Jiang, QL
   Cheng, M
   Wang, C
AF Cao, Liujuan
   Jiang, Qiling
   Cheng, Ming
   Wang, Cheng
TI Robust vehicle detection by combining deep features with exemplar
   classification
SO NEUROCOMPUTING
LA English
DT Article
DE Superpixel segmentation; SLIC; Deep Neural Network; Exemplar SVMs;
   Vehicle detection; Robust classification
AB Very recently, vehicle detection in satellite images has become an emerging research topic with various applications ranging from military to commercial systems. However, it retains as an open problem, mainly due to the complex variations in imaging conditions, object intra-class changes, as well as due to its low-resolution. Coming with the rapid advances in deep learning for feature-representation, in this paper we investigate the possibility to exploit deep neural featutes towards robust vehicle detection. In addition, along with the rapid growth in the data volume, new classification methodology is also demanded to explicitly handle the intra-class variations. In this paper, we propose a vehicle detection framework, which combines Deep Convolutional Neural Network (DNN) based feature learning with Exemplar-SVMs (E-SVMS) based, robust instance classifier to achieve robust vehicle detection in satellite images. In particular, we adopt DNN to learn discriminative image features, which has a high learning capacity. In our practice, the leverage of DNN has achieve significant performance boost by comparing to a serial of handcraft designed features. In addition, we adopt E-SVMs based robust classifier to further improve the classification robustness, which can be considered as an instance-specific metric learning scheme. By conducting extensive experiments with comparisons to a serial of state-of-the-art and alternative works, we further show that the combination of both schemes can benefit from each other to jointly improve the detection accuracy and effectiveness. (C) 2016 Elsevier B.V. All rights reserved.
C1 [Cao, Liujuan; Jiang, Qiling; Cheng, Ming; Wang, Cheng] Fujian Key Lab Sensing & Comp Smart City, Fujian, Peoples R China.
   [Cao, Liujuan; Jiang, Qiling; Cheng, Ming; Wang, Cheng] Xiamen Univ, Sch Informat Sci & Engn, Xiamen, Peoples R China.
RP Cheng, M (reprint author), Fujian Key Lab Sensing & Comp Smart City, Fujian, Peoples R China.
EM chm99@xmu.edu.cn
RI Wang, Cheng/A-9472-2012
OI Wang, Cheng/0000-0001-6075-796X
FU Special Fund for Earthquake Research in the Public Interest [201508025];
   Nature Science Foundation of ChinaNational Natural Science Foundation of
   China [61402388, 61422210, 61373076]; Fundamental Research Funds for the
   Central UniversitiesFundamental Research Funds for the Central
   Universities [20720150080, 2013121026]; CCF-Tencent Open Research Fund;
   Open Projects Program of National Laboratory of Pattern Recognition
FX This work is supported by the Special Fund for Earthquake Research in
   the Public Interest No. 201508025, the Nature Science Foundation of
   China (Nos. 61402388, 61422210 and 61373076), the Fundamental Research
   Funds for the Central Universities (Nos. 20720150080 and 2013121026),
   the CCF-Tencent Open Research Fund, and the Open Projects Program of
   National Laboratory of Pattern Recognition.
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Chen XY, 2013, 2013 SECOND IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION (ACPR 2013), P181, DOI 10.1109/ACPR.2013.33
   Chen XY, 2014, IEEE GEOSCI REMOTE S, V11, P1797, DOI 10.1109/LGRS.2014.2309695
   Cheng HY, 2012, IEEE T IMAGE PROCESS, V21, P2152, DOI 10.1109/TIP.2011.2172798
   Deng J., 2012, ILSVRC 2012
   Eikvil L, 2009, ISPRS J PHOTOGRAMM, V64, P65, DOI 10.1016/j.isprsjprs.2008.09.005
   Jarrett K, 2009, IEEE I CONF COMP VIS, P2146, DOI 10.1109/ICCV.2009.5459469
   Ji R., 2014, IEEE T IMAGE PROCESS, V23
   Ji RR, 2015, ACM T INTEL SYST TEC, V6, DOI 10.1145/2597181
   Ji RR, 2014, IEEE T GEOSCI REMOTE, V52, P1811, DOI 10.1109/TGRS.2013.2255297
   Ji RR, 2013, IEEE T MULTIMEDIA, V15, P153, DOI 10.1109/TMM.2012.2225035
   Jia Y., ARXIV14085093
   Kembhavi A, 2011, IEEE T PATTERN ANAL, V33, P1250, DOI 10.1109/TPAMI.2010.182
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   LeCun Y, 2004, PROC CVPR IEEE, P97
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Leitloff J, 2010, IEEE T GEOSCI REMOTE, V48, P2795, DOI 10.1109/TGRS.2010.2043109
   Malisiewicz T, 2011, IEEE I CONF COMP VIS, P89, DOI 10.1109/ICCV.2011.6126229
   Pengpeng Liang, 2012, 2012 15th International Conference on Information Fusion (FUSION 2012), P1629
   Platt JC, 2000, ADV NEUR IN, P61
   Tan Q., 2009, JOINT URBAN REMOTE S, P1
   Tuermer S, 2013, IEEE J-STARS, V6, P2327, DOI 10.1109/JSTARS.2013.2242846
NR 22
TC 6
Z9 6
U1 1
U2 32
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0925-2312
EI 1872-8286
J9 NEUROCOMPUTING
JI Neurocomputing
PD NOV 26
PY 2016
VL 215
SI SI
BP 225
EP 231
DI 10.1016/j.neucom.2016.03.094
PG 7
WC Computer Science, Artificial Intelligence
SC Computer Science
GA EB3WW
UT WOS:000387300700024
DA 2020-02-19
ER

PT J
AU Zhu, YY
   Zhang, CQ
   Zhou, DY
   Wang, XG
   Bai, X
   Liu, WY
AF Zhu, Yingying
   Zhang, Chengquan
   Zhou, Duoyou
   Wang, Xinggang
   Bai, Xiang
   Liu, Wenyu
TI Traffic sign detection and recognition using fully convolutional network
   guided proposals
SO NEUROCOMPUTING
LA English
DT Article
DE Traffic sign recognition; Fully convolutional network; Object proposal;
   Object detection
ID FEATURES
AB Detecting and recognizing traffic signs is a hot topic in the field of computer vision with lots of applications, e.g., safe driving, path planning, robot navigation etc. We propose a novel framework with two deep learning components including fully convolutional network (FCN) guided traffic sign proposals and deep convolutional neural network (CNN) for object classification. Our core idea is to use CNN to classify traffic sign proposals to perform fast and accurate traffic sign detection and recognition. Due to the complexity of the traffic scene, we improve the state-of-the-art object proposal method, EdgeBox, by incorporating with a trained FCN. The FCN guided object proposals can produce more discriminative candidates, which help to make the whole detection system fast and accurate. In the experiments, we have evaluated the proposed method on publicly available traffic sign benchmark, Swedish Traffic Signs Dataset (STSD), and achieved the state-of-the-art results. (C) 2016 Elsevier B.V. All rights reserved.
C1 [Zhu, Yingying; Zhang, Chengquan; Zhou, Duoyou; Wang, Xinggang; Bai, Xiang; Liu, Wenyu] Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan 430074, Peoples R China.
RP Liu, WY (reprint author), Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan 430074, Peoples R China.
EM liuwy@mail.hust.edu.cn
RI Liu, Wenyu/AAG-1426-2019; Wang, Xinggang/W-4374-2019
OI Liu, Wenyu/0000-0002-4582-7488
CR Bahlmann C, 2005, 2005 IEEE Intelligent Vehicles Symposium Proceedings, P255
   Bai X, 2015, IEEE T PATTERN ANAL, V37, P2361, DOI 10.1109/TPAMI.2015.2424863
   Bai  Xiang, 2016, IEEE T IMAGE PROCESS, V25
   Baro X, 2009, IEEE T INTELL TRANSP, V10, P113, DOI 10.1109/TITS.2008.2011702
   Benenson R, 2012, PROC CVPR IEEE, P2903, DOI 10.1109/CVPR.2012.6248017
   Chen T, 2016, IEEE T VEH TECHNOL, V65, P4006, DOI 10.1109/TVT.2015.2500275
   Cheng MM, 2014, PROC CVPR IEEE, P3286, DOI 10.1109/CVPR.2014.414
   Ciresan D, 2012, NEURAL NETWORKS, V32, P333, DOI 10.1016/j.neunet.2012.02.023
   Ciresan D, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P1918, DOI 10.1109/IJCNN.2011.6033458
   Creusen IM, 2010, IEEE IMAGE PROC, P2669, DOI 10.1109/ICIP.2010.5651637
   de la Escalera A, 2003, IMAGE VISION COMPUT, V21, P247, DOI 10.1016/S0262-8856(02)00156-7
   delaEscalera A, 1997, IEEE T IND ELECTRON, V44, P848, DOI 10.1109/41.649946
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Gao XW, 2006, J VIS COMMUN IMAGE R, V17, P675, DOI 10.1016/j.jvcir.2005.10.003
   Garcia-Garrido M. A., 2006, IEEE INT TRANSP SYST, P811
   Garcia-Garrido MA, 2005, LECT NOTES COMPUT SC, V3643, P543
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Hu CP, 2015, IEEE T INTELL TRANSP, V16, DOI 10.1109/TITS.2015.2441051
   Hu F, 2015, IEEE J-STARS, V8, P2015, DOI 10.1109/JSTARS.2015.2444405
   Krahenbuhl P, 2014, LECT NOTES COMPUT SC, V8693, P725, DOI 10.1007/978-3-319-10602-1_47
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Larsson F, 2011, LECT NOTES COMPUT SC, V6688, P238, DOI 10.1007/978-3-642-21227-7_23
   Li HJ, 2015, NEUROCOMPUTING, V169, P77, DOI 10.1016/j.neucom.2014.12.111
   Lillo-Castellano JM, 2015, NEUROCOMPUTING, V153, P286, DOI 10.1016/j.neucom.2014.11.026
   Ling Q, 2014, NEUROCOMPUTING, V133, P32, DOI 10.1016/j.neucom.2013.11.034
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Loy G., 2004, 2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566), P70
   Maldonado-Bascon S, 2007, IEEE T INTELL TRANSP, V8, P264, DOI 10.1109/TITS.2007.895311
   Mathias M., 2013, NEUR NETW IJCNN 2013, P1, DOI DOI 10.1109/IJCNN.2013.6707049
   Miura J, 2000, 2000 IEEE INTELLIGENT TRANSPORTATION SYSTEMS PROCEEDINGS, P52, DOI 10.1109/ITSC.2000.881017
   Ouyang WL, 2015, PROC CVPR IEEE, P2403, DOI 10.1109/CVPR.2015.7298854
   Pedersoli M, 2011, PROC CVPR IEEE, P1353, DOI 10.1109/CVPR.2011.5995668
   Piccioli G, 1996, IMAGE VISION COMPUT, V14, P209, DOI 10.1016/0262-8856(95)01057-2
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Ruta A, 2010, PATTERN RECOGN, V43, P416, DOI 10.1016/j.patcog.2009.05.018
   Sermanet P, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P2809, DOI 10.1109/IJCNN.2011.6033589
   Shen W., 2016, IEEE C CVPR IEEE LAS
   Shi BG, 2016, PATTERN RECOGN, V52, P448, DOI 10.1016/j.patcog.2015.11.005
   Sun ZL, 2014, NEUROCOMPUTING, V128, P153, DOI 10.1016/j.neucom.2012.11.057
   Timofte R, 2014, MACH VISION APPL, V25, P633, DOI 10.1007/s00138-011-0391-3
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Wang X., NEUROCOMPUTING
   Xia GS, 2014, INT J COMPUT VISION, V106, P31, DOI 10.1007/s11263-013-0640-1
   Xia YJ, 2015, NEUROCOMPUTING, V151, P700, DOI 10.1016/j.neucom.2014.05.091
   Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI 10.1109/ICCV.2015.164
   Yang F, 2016, NEUROCOMPUTING, V173, P1310, DOI 10.1016/j.neucom.2015.09.004
   Yang W, 2015, IEEE T GEOSCI REMOTE, V53, P4472, DOI 10.1109/TGRS.2015.2400449
   Yao C., 2014, P ECCV
   Yao C, 2014, IEEE T IMAGE PROCESS, V23, P4737, DOI 10.1109/TIP.2014.2353813
   Zaklouta F, 2014, ROBOT AUTON SYST, V62, P16, DOI 10.1016/j.robot.2012.07.019
   Zhang ZX, 2013, IEEE T CIRC SYST VID, V23, P518, DOI 10.1109/TCSVT.2012.2210670
   Zhang ZX, 2013, NEUROCOMPUTING, V99, P250, DOI 10.1016/j.neucom.2012.07.008
   Zhou Y, 2016, INT J COMPUT VISION, V118, P337, DOI 10.1007/s11263-015-0879-9
   Zhu YY, 2016, FRONT COMPUT SCI-CHI, V10, P19, DOI 10.1007/s11704-015-4488-0
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
NR 56
TC 45
Z9 48
U1 7
U2 86
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0925-2312
EI 1872-8286
J9 NEUROCOMPUTING
JI Neurocomputing
PD NOV 19
PY 2016
VL 214
BP 758
EP 766
DI 10.1016/j.neucom.2016.07.009
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA EA6LS
UT WOS:000386741300071
DA 2020-02-19
ER

PT J
AU Liu, Y
   Pan, Y
   Lai, HJ
   Liu, C
   Yin, J
AF Liu, Ye
   Pan, Yan
   Lai, Hanjiang
   Liu, Cong
   Yin, Jian
TI Margin-based two-stage supervised hashing for image retrieval
SO NEUROCOMPUTING
LA English
DT Article
DE Deep learning; Image retrieval; Image hashing; Neural network;
   Optimization algorithm
AB Similarity-preserving hashing is a widely used method for nearest neighbor search in large-scale image retrieval. Recently, supervised hashing methods are appealing in that they learn compact hash codes with fewer bits by incorporating supervised information. In this paper, we propose a new two-stage supervised hashing methods which decomposes the hash learning process into a stage of learning approximate hash codes followed by a stage of learning hash functions. In the first stage, we propose a margin-based objective to find approximate hash codes such that a pair of hash codes associating to a pair of similar (dissimilar) images has sufficiently small (large) Hamming distance. This objective results in a challenging optimization problem. We develop a coordinate descent algorithm to efficiently solve this optimization problem. In the second stage, we use convolutional neural networks to learn hash functions. We conduct extensive evaluations on several benchmark datasets with different kinds of images. The results show that the proposed margin-based hashing method has substantial improvement upon the state-of-the-art supervised or unsupervised hashing methods. (C) 2016 Elsevier B.V. All rights reserved.
C1 [Liu, Ye; Pan, Yan; Lai, Hanjiang; Liu, Cong; Yin, Jian] Sun Yat Sen Univ, Sch Data & Comp Sci, Guangzhou, Guangdong, Peoples R China.
   [Liu, Ye; Lai, Hanjiang; Yin, Jian] Sun Yat Sen Univ, Sch Informat Sci & Technol, Guangzhou, Guangdong, Peoples R China.
   [Pan, Yan] Sun Yat Sen Univ, Sch Software, Guangzhou, Guangdong, Peoples R China.
   [Yin, Jian] SYSU CMU Shunde Int Joint Res Inst, Foshan, Peoples R China.
   [Liu, Cong] Sun Yat Sen Univ, Sch Adv Comp, Guangzhou, Guangdong, Peoples R China.
   [Lai, Hanjiang] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore, Singapore.
RP Pan, Y (reprint author), Sun Yat Sen Univ, Sch Data & Comp Sci, Guangzhou, Guangdong, Peoples R China.
EM panyan5@mail.sysu.edu.cn; issjyin@mail.sysu.edu.cn
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61033010, 61272065, 61370021, 61472453, U1401256,
   U1501252]; Natural Science Foundation of Guangdong ProvinceNational
   Natural Science Foundation of Guangdong Province [S2011020001182,
   S2012010009311, S2013010011905]; Research Foundation of Science and
   Technology Plan Project in Guangdong Province [2011B031700004,
   2011B040200007, 2012A010701013]; Flamingo(Huolieniao) Network
   (Guangzhou) Co., Ltd.; Guangdong Provincial Key Laboratory of Big Data
   Analysis and Processing in China
FX This work is supported by the National Natural Science Foundation of
   China (61033010, 61272065, 61370021, 61472453, U1401256, U1501252),
   Natural Science Foundation of Guangdong Province (S2011020001182,
   S2012010009311, S2013010011905), Research Foundation of Science and
   Technology Plan Project in Guangdong Province (2011B031700004,
   2011B040200007, 2012A010701013). This work is also supported by the
   Industry University Cooperation and Joint Research Program between
   Flamingo(Huolieniao) Network (Guangzhou) Co., Ltd. and Guangdong
   Provincial Key Laboratory of Big Data Analysis and Processing in China.
CR Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518
   Gong YC, 2011, PROC CVPR IEEE, P817, DOI 10.1109/CVPR.2011.5995432
   Hinton GE, ARXIV12070580
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kulis B., 2009, P ADV NEUR INF PROC, P1042
   Kulis B, 2009, IEEE I CONF COMP VIS, P2130, DOI 10.1109/ICCV.2009.5459466
   Li X., 2013, P INT C MACH LEARN
   Lin G., 2013, P IEEE C COMP VIS SY
   Liu W. E., 2011, P INT C MACH LEARN, P1
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Norouzi M., 2011, INT C MACH LEARN, P353
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Salakhutdinov R., 2007, P INT C ART INT STAT, V11
   Sutskever I., 2013, P 30 INT C MACH LEAR, P1139
   Wang J, 2010, PROC CVPR IEEE, P3424, DOI 10.1109/CVPR.2010.5539994
   Weiss P, 2008, INT CONF ACOUST SPEE, P1173, DOI 10.1109/ICASSP.2008.4517824
   Xia RK, 2014, PROCEEDINGS OF THE TWENTY-EIGHTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2156
NR 19
TC 5
Z9 5
U1 0
U2 5
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0925-2312
EI 1872-8286
J9 NEUROCOMPUTING
JI Neurocomputing
PD NOV 19
PY 2016
VL 214
BP 894
EP 901
DI 10.1016/j.neucom.2016.07.024
PG 8
WC Computer Science, Artificial Intelligence
SC Computer Science
GA EA6LS
UT WOS:000386741300084
DA 2020-02-19
ER

PT J
AU Yu, LT
   Sun, XS
   Huang, Z
AF Yu, Litao
   Sun, Xiaoshuai
   Huang, Zi
TI Robust spatial-temporal deep model for multimedia event detection
SO NEUROCOMPUTING
LA English
DT Article
DE Multimedia event detection; Deep learning; GRU
ID NEURAL-NETWORKS
AB The task of multimedia event detection (MED) aims at training a set of models that can automatically detect the most event-relevant videos from large datasets. In this paper, we attempt to build a robust spatial-temporal deep neural network for large-scale video event detection. In our setting, each video follows a multiple instance assumption, where its visual segments contain both spatial and temporal properties of events. Regarding these properties, we try to implement the MED system by a two-step training phase: unsupervised recurrent video reconstruction and supervised fine-tuning. We conduct extensive experiments on the challenging TRECVID MED14 dataset, which indicate that with the consideration of both spatial and temporal information, the detection performance can be further boosted compared with the state-of-the-art MED models. (C) 2016 Elsevier B.V. All rights reserved.
C1 [Yu, Litao; Sun, Xiaoshuai; Huang, Zi] Univ Queensland, Brisbane, Qld 4072, Australia.
RP Yu, LT (reprint author), Univ Queensland, Brisbane, Qld 4072, Australia.
OI HUANG, ZI/0000-0002-9738-4949
CR BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181
   Chang X, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P581, DOI 10.1145/2733373.2806218
   Cho K., ARXIV 1409 1259
   Chung J., ARXIV14123555
   Doran G, 2014, MACH LEARN, V97, P79, DOI 10.1007/s10994-013-5429-5
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Gkalelis N., 2014, P INT C MULT RETR, P25
   Graves A., 2006, P 23 INT C MACH LEAR, P369, DOI DOI 10.1145/1143844.1143891
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Jegou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039
   Jozefowicz R., 2015, P 32 INT C MACH LEAR, V37, P2342, DOI DOI 10.1109/CVPR.2015.7298761
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Lai KT, 2014, PROC CVPR IEEE, P2251, DOI 10.1109/CVPR.2014.288
   Marchi E, 2015, INT CONF ACOUST SPEE, P1996, DOI 10.1109/ICASSP.2015.7178320
   Potapov D, 2014, LECT NOTES COMPUT SC, V8694, P540, DOI 10.1007/978-3-319-10599-4_35
   Ramanathan V., ARXIV150500315
   Shen F., 2015, IEEE ICCV
   Shen F, 2015, IEEE CVPR
   Simonyan K., ARXIV14091556
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Trichet R., 2015, IEEE AVSS, P1
   Wang F, 2014, IEEE T MULTIMEDIA, V16, P1303, DOI 10.1109/TMM.2014.2315780
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Xu ZW, 2015, PROC CVPR IEEE, P1798, DOI 10.1109/CVPR.2015.7298789
   Yang H., ARXIV 1510 01442
   Yang Y, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P59, DOI 10.1145/2733373.2806244
   Yu S.I., 2014, NIST TRECVID VID RET
   Zeiler MD., 2012, ARXIV12125701
NR 29
TC 3
Z9 3
U1 0
U2 8
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0925-2312
EI 1872-8286
J9 NEUROCOMPUTING
JI Neurocomputing
PD NOV 12
PY 2016
VL 213
SI SI
BP 48
EP 53
DI 10.1016/j.neucom.2016.03.102
PG 6
WC Computer Science, Artificial Intelligence
SC Computer Science
GA EA2EX
UT WOS:000386406700006
DA 2020-02-19
ER

PT J
AU Bianco, S
   Celona, L
   Schettini, R
AF Bianco, Simone
   Celona, Luigi
   Schettini, Raimondo
TI Robust smile detection using convolutional neural networks
SO JOURNAL OF ELECTRONIC IMAGING
LA English
DT Article
DE smile detection; deep learning; convolutional neural networks; face
   detection; face alignment
AB We present a fully automated approach for smile detection. Faces are detected using a multiview face detector and aligned and scaled using automatically detected eye locations. Then, we use a convolutional neural network (CNN) to determine whether it is a smiling face or not. To this end, we investigate different shallow CNN architectures that can be trained even when the amount of learning data is limited. We evaluate our complete processing pipeline on the largest publicly available image database for smile detection in an uncontrolled scenario. We investigate the robustness of the method to different kinds of geometric transformations (rotation, translation, and scaling) due to imprecise face localization, and to several kinds of distortions (compression, noise, and blur). To the best of our knowledge, this is the first time that this type of investigation has been performed for smile detection. Experimental results show that our proposal outperforms state- of- the- art methods on both high- and low- quality images. (C) 2016 SPIE and IS&T
C1 [Bianco, Simone; Celona, Luigi; Schettini, Raimondo] Univ Milano Bicocca, Dipartimento Informat Sistemist & Comunicaz, Viale Sarca 336, I-20126 Milan, Italy.
RP Bianco, S (reprint author), Univ Milano Bicocca, Dipartimento Informat Sistemist & Comunicaz, Viale Sarca 336, I-20126 Milan, Italy.
EM simone.bianco@disco.unimib.it
RI Bianco, Simone/T-1224-2019
OI Bianco, Simone/0000-0002-7070-1545; SCHETTINI,
   RAIMONDO/0000-0001-7461-1451; Celona, Luigi/0000-0002-5925-2646
CR Ahonen T, 2004, LECT NOTES COMPUT SC, V3021, P469
   An L, 2015, NEUROCOMPUTING, V149, P354, DOI 10.1016/j.neucom.2014.04.072
   [Anonymous], 2009, MPLAB GENKI DATABASE
   Asthana A, 2014, PROC CVPR IEEE, P1859, DOI 10.1109/CVPR.2014.240
   Bai Y, 2009, 2009 16TH IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOLS 1-6, P3305, DOI 10.1109/ICIP.2009.5413938
   Bianco S., 2016, ROBUST SMILE DETECTI
   Bianco S, 2014, IEEE T PATTERN ANAL, V36, P1505, DOI 10.1109/TPAMI.2013.2297710
   Bianco S, 2013, J ELECTRON IMAGING, V22, DOI 10.1117/1.JEI.22.2.023014
   Caifeng Shan, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P759, DOI 10.1109/FG.2011.5771343
   Cirean D. C., 2011, ADV NEURAL INFORM PR, V12
   Deniz O, 2011, PATTERN RECOGN LETT, V32, P1598, DOI 10.1016/j.patrec.2011.01.004
   EKMAN P, 1988, J PERS SOC PSYCHOL, V54, P414, DOI 10.1037/0022-3514.54.3.414
   Farfade SS, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P643, DOI 10.1145/2671188.2749408
   Gao Y, 2016, NEUROCOMPUTING, V174, P1077, DOI 10.1016/j.neucom.2015.10.022
   Hassner T., 2015, IEEE C COMP VIS PATT
   Jayaraman D, 2012, CONF REC ASILOMAR C, P1693, DOI 10.1109/ACSSC.2012.6489321
   Nair V, 2010, ICML, V27, P807, DOI DOI 10.0RG/PAPERS/432.PDF
   Ojansivu V, 2008, LECT NOTES COMPUT SC, V5099, P236, DOI 10.1007/978-3-540-69905-7_27
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Shan CF, 2012, IEEE T IMAGE PROCESS, V21, P431, DOI 10.1109/TIP.2011.2161587
   Shinohara Y, 2004, SIXTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P499, DOI 10.1109/AFGR.2004.1301582
   Whitehill J, 2009, IEEE T PATTERN ANAL, V31, P2106, DOI 10.1109/TPAMI.2009.42
   Zhang K., 2016, P IEEE C COMP VIS PA, P34
   Zhang YZ, 2012, 2012 11TH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA 2012), VOL 1, P482, DOI 10.1109/ICMLA.2012.88
NR 24
TC 9
Z9 9
U1 2
U2 6
PU IS&T & SPIE
PI BELLINGHAM
PA 1000 20TH ST, BELLINGHAM, WA 98225 USA
SN 1017-9909
EI 1560-229X
J9 J ELECTRON IMAGING
JI J. Electron. Imaging
PD NOV
PY 2016
VL 25
IS 6
DI 10.1117/1.JEI.25.6.063002
PG 4
WC Engineering, Electrical & Electronic; Optics; Imaging Science &
   Photographic Technology
SC Engineering; Optics; Imaging Science & Photographic Technology
GA EP0EI
UT WOS:000397059200042
DA 2020-02-19
ER

PT J
AU Dun, JY
   Zhang, SY
   Ye, XZ
   Zhang, Y
AF Dun, Jingyu
   Zhang, Sanyuan
   Ye, Xiuzi
   Zhang, Yin
TI System recognizing Bahamian license plate with touching characters
SO JOURNAL OF ELECTRONIC IMAGING
LA English
DT Article
DE license plate recognition; modified sliding window technique; license
   plate rectification; touching character segmentation
ID RECOGNITION; ALGORITHM; SEGMENTATION; LOCALIZATION; BINARIZATION
AB Various methods are proposed for license plate recognition, but none of them are universal. Some common methods for license plate localization, character extraction, and recognition are analyzed. Then a system is proposed to recognize the Bahamian license plate with touching characters. A vertical edge-based method with a modified sliding window technique is used to locate the license plate, and a machine learning process is used to trim the region. The located license plate is rectified by using the minimum enclosing box and the stroke width value. Then the vertical projection and pairs of extreme points are combined to segment the characters. Finally, a deep learning method is used to recognize the characters. 2996 images are experimented on and the total recognition accuracy achieves 83.29%. Typical methods of each stage are implemented to compare with the proposed methods. In addition, the proposed system is experimented on a public dataset to show the generalization ability of the system. The experimental results show that the proposed system performs better than the other methods and is able to be used in a real-time application. (C) 2016 SPIE and IS&T.
C1 [Dun, Jingyu; Zhang, Sanyuan; Zhang, Yin] Zhejiang Univ, Coll Comp Sci & Technol, Zheda Rd 38, Hangzhou 310027, Zhejiang, Peoples R China.
   [Ye, Xiuzi] Wenzhou Univ, Coll Math & Informat Sci, Chashan Higher Educ Dist, Wenzhou 325035, Peoples R China.
RP Dun, JY (reprint author), Zhejiang Univ, Coll Comp Sci & Technol, Zheda Rd 38, Hangzhou 310027, Zhejiang, Peoples R China.
EM dunjingyu@zju.edu.cn
FU China Natural Science FoundationNational Natural Science Foundation of
   China [61272304]; Zhejiang Provincial Natural Science Foundation of
   ChinaNatural Science Foundation of Zhejiang Province [LY15F020024]
FX This research work was supported by China Natural Science Foundation
   (No. 61272304) and Zhejiang Provincial Natural Science Foundation of
   China (No. LY15F020024). Thanks to the PlateSmart company for providing
   the images.
CR Anagnostopoulos CNE, 2006, IEEE T INTELL TRANSP, V7, P377, DOI 10.1109/TITS.2006.880641
   Anagnostopoulos CNE, 2008, IEEE T INTELL TRANSP, V9, P377, DOI 10.1109/TITS.2008.922938
   Ashtari AH, 2014, IEEE T INTELL TRANSP, V15, P1690, DOI 10.1109/TITS.2014.2304515
   Chen H., 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P2609, DOI 10.1109/ICIP.2011.6116200
   Chen ZX, 2013, INT J PATTERN RECOGN, V27, DOI 10.1142/S0218001413500249
   Cheng R, 2014, INT J SIGNAL PROCESS, V7, P998
   Dashtban M H, 2011, INT J COMPUT APPL, V26, P22
   De-Hui W. U., 2006, J HIGHWAY TRANSPORTA, V23, P143
   Deb K, 2012, PROC TECH, V4, P812, DOI 10.1016/j.protcy.2012.05.133
   Deb K, 2010, INT J CONTROL AUTOM, V8, P975, DOI 10.1007/s12555-010-0506-z
   Dehshibi MM, 2012, INT J COMPUTER ELECT, V4, P355
   Du S, 2013, IEEE T CIRC SYST VID, V23, P322, DOI 10.1109/TCSVT.2012.2203741
   Dun JY, 2015, IEEE INTEL TRANSP SY, V7, P51, DOI 10.1109/MITS.2015.2412146
   He HS, 2015, IEEE T INTELL TRANSP, V16, DOI 10.1109/TITS.2015.2437998
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Huang YP, 2009, EXPERT SYST APPL, V36, P9260, DOI 10.1016/j.eswa.2008.12.006
   Jiao JB, 2009, PATTERN RECOGN, V42, P358, DOI 10.1016/j.patcog.2008.08.016
   Kim MK, 2010, J OPT SOC KOREA, V14, P368, DOI 10.3807/JOSK.2010.14.4.368
   Lalimi MA, 2013, COMPUT ELECTR ENG, V39, P834, DOI 10.1016/j.compeleceng.2012.09.015
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li B, 2013, IEEE T INTELL TRANSP, V14, P1690, DOI 10.1109/TITS.2013.2267054
   Liang Z. Y., 2013, APPL MECH MAT, V385-386, P1429
   Miao L, 2012, INT CONF SIGN PROCES, P947, DOI 10.1109/ICoSP.2012.6491736
   Multimedia.Technology.Laboratory, 2009, MED DAT
   OpenAlpr.contributors, 2016, AUT LIC PLAT REC LIB
   Pal U, 2003, PATTERN RECOGN LETT, V24, P261, DOI 10.1016/S0167-8655(02)00240-4
   Pan MS, 2009, INT J AUTOM COMPUT, V6, P210, DOI 10.1007/s11633-009-0210-8
   Roy PP, 2012, PATTERN RECOGN, V45, P1972, DOI 10.1016/j.patcog.2011.09.026
   Rasooli M., 2011, INT J SIGNAL PROCESS, V4, P697
   Sedighi A, 2011, EXPERT SYST APPL, V38, P13497, DOI 10.1016/j.eswa.2011.02.030
   Shan BM, 2011, J COMPUT, V6, P246, DOI 10.4304/jcp.6.2.246-253
   Sheng J., 2015, 2 INT C INF SCI CONT
   Sheng-Feng Yu, 2012, 2012 5th International Conference on BioMedical Engineering and Informatics (BMEI), P156, DOI 10.1109/BMEI.2012.6513059
   Tarng W, 2013, INT J PATTERN RECOGN, V27, DOI 10.1142/S0218001413500250
   Vincent P, 2008, P 25 INT C MACH LEAR, P1096, DOI DOI 10.1145/1390156.1390294
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Wang F, 2008, PATTERN RECOGN LETT, V29, P1007, DOI 10.1016/j.patrec.2008.01.026
   Wang Mei, 2008, Computer Engineering, V34, P216
   Wazalwar D., 2012, J TRANSPORTATION TEC, V21, P13
   Wei Y, 2011, TRANSPORT RES REC, P10, DOI 10.3141/2256-02
   Wen Y, 2011, IEEE T INTELL TRANSP, V12, P830, DOI 10.1109/TITS.2011.2114346
   Wu BF, 2007, IET COMPUT VIS, V1, P2, DOI 10.1049/iet-cvi:20050132
   Yang X, 2014, INT CONF SIGN PROCES, P1195, DOI 10.1109/ICOSP.2014.7015189
   Yu Peng, 2011, Proceedings of the Sixth International Conference on Image and Graphics (ICIG 2011), P1023, DOI 10.1109/ICIG.2011.154
   Zheng LH, 2013, J COMPUT SYST SCI, V79, P245, DOI 10.1016/j.jcss.2012.05.006
   Zhu SY, 2015, J ELECTRON IMAGING, V24, DOI 10.1117/1.JEI.24.2.023020
NR 46
TC 0
Z9 0
U1 0
U2 10
PU IS&T & SPIE
PI BELLINGHAM
PA 1000 20TH ST, BELLINGHAM, WA 98225 USA
SN 1017-9909
EI 1560-229X
J9 J ELECTRON IMAGING
JI J. Electron. Imaging
PD NOV
PY 2016
VL 25
IS 6
AR 063009
DI 10.1117/1.JEI.25.6.063009
PG 11
WC Engineering, Electrical & Electronic; Optics; Imaging Science &
   Photographic Technology
SC Engineering; Optics; Imaging Science & Photographic Technology
GA EP0EI
UT WOS:000397059200049
DA 2020-02-19
ER

PT J
AU Sun, B
   Li, LD
   Zhou, GY
   He, J
AF Sun, Bo
   Li, Liandong
   Zhou, Guoyan
   He, Jun
TI Facial expression recognition in the wild based on multimodal texture
   features
SO JOURNAL OF ELECTRONIC IMAGING
LA English
DT Article
DE facial expression recognition; texture features; in the wild; deep
   learning; feature fusion
AB Facial expression recognition in the wild is a very challenging task. We describe our work in static and continuous facial expression recognition in the wild. We evaluate the recognition results of gray deep features and color deep features, and explore the fusion of multimodal texture features. For the continuous facial expression recognition, we design two temporal-spatial dense scale-invariant feature transform (SIFT) features and combine multimodal features to recognize expression from image sequences. For the static facial expression recognition based on video frames, we extract dense SIFT and some deep convolutional neural network (CNN) features, including our proposed CNN architecture. We train linear support vector machine and partial least squares classifiers for those kinds of features on the static facial expression in the wild (SFEW) and acted facial expression in the wild (AFEW) dataset, and we propose a fusion network to combine all the extracted features at decision level. The final achievement we gained is 56.32% on the SFEW testing set and 50.67% on the AFEW validation set, which are much better than the baseline recognition rates of 35.96% and 36.08%. (C) The Authors. Published by SPIE under a Creative Commons Attribution 3.0 Unported License.
C1 [Sun, Bo; Li, Liandong; Zhou, Guoyan; He, Jun] Beijing Normal Univ, Coll Informat Sci & Technol, 19 XinJieKouWai St, Beijing 100875, Peoples R China.
RP He, J (reprint author), Beijing Normal Univ, Coll Informat Sci & Technol, 19 XinJieKouWai St, Beijing 100875, Peoples R China.
EM hejun@bnu.edu.cn
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61501035, KJZXCJ2016042]; Fundamental Research
   Funds for the Central Universities of ChinaFundamental Research Funds
   for the Central Universities [2014KJJCA15]; National Education Science
   Twelfth Five-Year Plan Key Issues of the Ministry of Education
   [DCA140229]
FX This work was supported by the National Natural Science Foundation of
   China (Grant Nos. 61501035 and KJZXCJ2016042), the Fundamental Research
   Funds for the Central Universities of China (2014KJJCA15), and the
   National Education Science Twelfth Five-Year Plan Key Issues of the
   Ministry of Education (DCA140229).
CR Bradski G, 2000, DR DOBBS J, V25, P120
   Bucak SS, 2014, IEEE T PATTERN ANAL, V36, P1354, DOI 10.1109/TPAMI.2013.212
   Chen J., 2014, P 16 INT C MULT INT, P508, DOI [10.1145/2663204.2666277, DOI 10.1145/2663204.2666277]
   Dalal N, 2005, PROC CVPR IEEE, P886
   Dhall A., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P2106, DOI 10.1109/ICCVW.2011.6130508
   Dhall A, 2015, ICMI'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P423, DOI 10.1145/2818346.2829994
   Dhall A, 2012, IEEE MULTIMEDIA, V19, P34, DOI 10.1109/MMUL.2012.26
   Ekman P., 1997, WHAT FACE REVEALS BA
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Gonen M, 2011, J MACH LEARN RES, V12, P2211
   Goodfellow Ian J., 2013, Neural Information Processing. 20th International Conference, ICONIP 2013. Proceedings: LNCS 8228, P117, DOI 10.1007/978-3-642-42051-1_16
   Ng HW, 2015, ICMI'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P443, DOI 10.1145/2818346.2830593
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Kahou SE, 2015, ICMI'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P467, DOI 10.1145/2818346.2830596
   Kahou SE, 2013, ICMI'13: PROCEEDINGS OF THE 2013 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P543, DOI 10.1145/2522848.2531745
   Kim BG, 2017, J SUPERCOMPUT, V73, P1063, DOI [10.1007/s11227-016-1730-y, 10.1007/s12193-015-0209-0]
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lazebnik S., 2006, P IEEE INT C COMP VI, V2, P2169, DOI DOI 10.1109/CVPR.2006.68
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Liu M., 2014, P 16 INT C MULT INT, P494, DOI DOI 10.1145/2663204.2666274
   Liu MY, 2013, ICMI'13: PROCEEDINGS OF THE 2013 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P525, DOI 10.1145/2522848.2531738
   Liu P, 2014, PROC CVPR IEEE, P1805, DOI 10.1109/CVPR.2014.233
   Paivarinta J, 2011, LECT NOTES COMPUT SC, V6688, P360, DOI 10.1007/978-3-642-21227-7_34
   Rakotomamonjy A, 2008, J MACH LEARN RES, V9, P2491
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sikka K, 2013, ICMI'13: PROCEEDINGS OF THE 2013 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P517, DOI 10.1145/2522848.2531741
   Sikka K, 2012, LECT NOTES COMPUT SC, V7584, P250, DOI 10.1007/978-3-642-33868-7_25
   Sun B., 2014, P 16 INT C MULT INT, P481
   Valstar Michel F, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P921, DOI 10.1109/FG.2011.5771374
   Varma M., 2009, P 26 ANN INT C MACH, P1065, DOI DOI 10.1145/1553374.1553510
   Vedaldi A, 2010, P 18 ACM INT C MULT, P1469, DOI DOI 10.1145/1873951.1874249
   Wang JJ, 2010, PROC CVPR IEEE, P3360, DOI 10.1109/CVPR.2010.5540018
   Xiong XH, 2013, PROC CVPR IEEE, P532, DOI 10.1109/CVPR.2013.75
   Yao AB, 2015, ICMI'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P451, DOI 10.1145/2818346.2830585
   Yu ZD, 2015, ICMI'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P435
   Zhao GY, 2007, IEEE T PATTERN ANAL, V29, P915, DOI 10.1109/TPAMI.2007.1110
   Zhu XX, 2012, PROC CVPR IEEE, P2879, DOI 10.1109/CVPR.2012.6248014
NR 38
TC 20
Z9 20
U1 1
U2 19
PU IS&T & SPIE
PI BELLINGHAM
PA 1000 20TH ST, BELLINGHAM, WA 98225 USA
SN 1017-9909
EI 1560-229X
J9 J ELECTRON IMAGING
JI J. Electron. Imaging
PD NOV
PY 2016
VL 25
IS 6
AR 061407
DI 10.1117/1.JEI.25.6.061407
PG 8
WC Engineering, Electrical & Electronic; Optics; Imaging Science &
   Photographic Technology
SC Engineering; Optics; Imaging Science & Photographic Technology
GA EP0EI
UT WOS:000397059200008
OA Other Gold
DA 2020-02-19
ER

PT J
AU Christiansen, P
   Nielsen, LN
   Steen, KA
   Jorgensen, RN
   Karstoft, H
AF Christiansen, Peter
   Nielsen, Lars N.
   Steen, Kim A.
   Jorgensen, Rasmus N.
   Karstoft, Henrik
TI DeepAnomaly: Combining Background Subtraction and Deep Learning for
   Detecting Obstacles and Anomalies in an Agricultural Field
SO SENSORS
LA English
DT Article
DE anomaly detection; obstacle detection; autonomous farming; precision
   agriculture; camera; background subtraction; change detection;
   DeepAnomaly
ID CHALLENGE
AB Convolutional neural network (CNN)-based systems are increasingly used in autonomous vehicles for detecting obstacles. CNN-based object detection and per-pixel classification (semantic segmentation) algorithms are trained for detecting and classifying a predefined set of object types. These algorithms have difficulties in detecting distant and heavily occluded objects and are, by definition, not capable of detecting unknown object types or unusual scenarios. The visual characteristics of an agriculture field is homogeneous, and obstacles, like people, animals and other obstacles, occur rarely and are of distinct appearance compared to the field. This paper introduces DeepAnomaly, an algorithm combining deep learning and anomaly detection to exploit the homogenous characteristics of a field to perform anomaly detection. We demonstrate DeepAnomaly as a fast state-of-the-art detector for obstacles that are distant, heavily occluded and unknown. DeepAnomaly is compared to state-of-the-art obstacle detectors including "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks" (RCNN). In a human detector test case, we demonstrate that DeepAnomaly detects humans at longer ranges (45-90 m) than RCNN. RCNN has a similar performance at a short range (0-30 m). However, DeepAnomaly has much fewer model parameters and (182 ms/25 ms =) a 7.28-times faster processing time per image. Unlike most CNN-based methods, the high accuracy, the low computation time and the low memory footprint make it suitable for a real-time system running on a embedded GPU (Graphics Processing Unit).
C1 [Christiansen, Peter; Jorgensen, Rasmus N.; Karstoft, Henrik] Aarhus Univ, Dept Engn, DK-8200 Aarhus, Denmark.
   [Nielsen, Lars N.] Danske Commod, DK-8000 Aarhus, Denmark.
   [Steen, Kim A.] Agrolntelli, DK-8200 Aarhus, Denmark.
RP Christiansen, P (reprint author), Aarhus Univ, Dept Engn, DK-8200 Aarhus, Denmark.
EM pech@eng.au.dk; larsnn@gmail.com; kas@agrointelli.com; rnj@eng.au.dk;
   hka@eng.au.dk
RI Jorgensen, Rasmus Nyholm/M-7133-2018
OI Jorgensen, Rasmus Nyholm/0000-0002-1329-1674; Christiansen,
   Peter/0000-0003-1854-587X
FU Innovation Fund Denmark as part of the project "SAFE-Safer Autonomous
   Farming Equipment" [16-2014-0]
FX This research is sponsored by the Innovation Fund Denmark as part of the
   project "SAFE-Safer Autonomous Farming Equipment" (Project No.
   16-2014-0).
CR Benezeth Y, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3456695
   Bouwmans T., 2014, BACKGROUND MODELING
   Braham M, 2016, INT CONF SYST SIGNAL, P113
   Campos Y, 2016, APPL SOFT COMPUT, V45, P86, DOI 10.1016/j.asoc.2016.03.016
   Chandola V, 2009, ACM COMPUT SURV, V41, DOI 10.1145/1541880.1541882
   Chen L. C., 2014, 14127062 ARXIV
   Chen L.C., 2016, ARXIVCSCV160600915
   Christiansen P., 2016, P INT C AGR ENG AARH
   Christiansen P., 2015, PRECIS AGRIC, V15, P1330
   Christiansen P., 2015, PRECIS AGR UNPUB
   Davis J., 2006, P 23 INT C MACH LEAR, V23, P233, DOI DOI 10.1145/1143844.1143874
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1
   Dollar P, 2010, P BRIT MACH VIS C AB
   Dollar P, 2014, IEEE T PATTERN ANAL, V36, P1532, DOI 10.1109/TPAMI.2014.2300479
   Donahue J, 2014, P 31 INT C MACH LEAR
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Fawcett T, 2006, PATTERN RECOGN LETT, V27, P861, DOI 10.1016/j.patrec.2005.10.010
   Girshick R., 2014, CVPR, V10, P2
   Girshick R., 2015, P IEEE INT C COMP VI
   He K., 2015, ARXIVCSCV151203385
   He K., 2014, P EUR C COMP VIS ZUR
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Kragh M., 2016, P INT C AGR ENG AARH
   Kragh M, 2015, LECT NOTES COMPUT SC, V9163, P188, DOI 10.1007/978-3-319-20904-3_18
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li G., 2016, ARXIVCSCV160301976
   Lin T. -Y., 2014, P EUR C COMP VIS ZUR, P1
   Long Jonathan, 2015, P IEEE C COMP VIS PA
   McLachlan G., 1988, STAT TXB MONOGRAPHS
   Mottaghi R, 2014, PROC CVPR IEEE, P891, DOI 10.1109/CVPR.2014.119
   Nam W., 2014, ARXIV14061134
   Oksanen T., 2015, Agronomy Research, V13, P167
   Redmon J., 2015, ARXIV150602640
   Ren S., ARXIVCSCV150601497
   Ross P, 2015, IEEE INT CONF ROBOT, P3935, DOI 10.1109/ICRA.2015.7139748
   Ross P, 2014, IEEE INT CONF ROBOT, P1699, DOI 10.1109/ICRA.2014.6907080
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Stauffer C., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P246, DOI 10.1109/CVPR.1999.784637
   Steen KA, 2016, J IMAGING, V2, DOI 10.3390/jimaging2010006
   Steen KA, 2015, SENSORS-BASEL, V15, P5096, DOI 10.3390/s150305096
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Torr P.H.S., 2014, P IEEE INT C COMP VI
   Van Rijsbergen C. J., 1979, INFORM RETRIEVAL
   Xu P, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P107, DOI 10.1145/2647868.2654914
   Yosinski J, 2014, ADV NEUR IN, V27
   Yu F., 2015, ARXIVCSCV151107122
   Zeiler M. D., 2013, ARXIV13112901
   Zivkovic Z, 2006, PATTERN RECOGN LETT, V27, P773, DOI 10.1016/j.patrec.2005.11.005
NR 50
TC 16
Z9 17
U1 5
U2 68
PU MDPI
PI BASEL
PA ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
EI 1424-8220
J9 SENSORS-BASEL
JI Sensors
PD NOV
PY 2016
VL 16
IS 11
AR 1904
DI 10.3390/s16111904
PG 21
WC Chemistry, Analytical; Engineering, Electrical & Electronic; Instruments
   & Instrumentation
SC Chemistry; Engineering; Instruments & Instrumentation
GA EE5JB
UT WOS:000389641700137
OA DOAJ Gold, Green Published
DA 2020-02-19
ER

PT J
AU Donne, S
   De Vylder, J
   Goossens, B
   Philips, W
AF Donne, Simon
   De Vylder, Jonas
   Goossens, Bart
   Philips, Wilfried
TI MATE: Machine Learning for Adaptive Calibration Template Detection
SO SENSORS
LA English
DT Article
DE computer vision; camera calibration; checkerboard detection; deep
   learning
ID CAMERA CALIBRATION; CORNERS
AB The problem of camera calibration is two-fold. On the one hand, the parameters are estimated from known correspondences between the captured image and the real world. On the other, these correspondences themselves-typically in the form of chessboard corners-need to be found. Many distinct approaches for this feature template extraction are available, often of large computational and/or implementational complexity. We exploit the generalized nature of deep learning networks to detect checkerboard corners: our proposed method is a convolutional neural network (CNN) trained on a large set of example chessboard images, which generalizes several existing solutions. The network is trained explicitly against noisy inputs, as well as inputs with large degrees of lens distortion. The trained network that we evaluate is as accurate as existing techniques while offering improved execution time and increased adaptability to specific situations with little effort. The proposed method is not only robust against the types of degradation present in the training set (lens distortions, and large amounts of sensor noise), but also to perspective deformations, e.g., resulting from multi-camera set-ups.
C1 [Donne, Simon; De Vylder, Jonas; Goossens, Bart; Philips, Wilfried] Univ Ghent, IMinds IPI, B-9000 Ghent, Belgium.
RP Donne, S (reprint author), Univ Ghent, IMinds IPI, B-9000 Ghent, Belgium.
EM Simon.Donne@ugent.be; Jonas.DeVylder@telin.ugent.be;
   Bart.Goossens@telin.ugent.be; philips@telin.ugent.be
RI Goossens, Bart/H-4772-2018
OI Goossens, Bart/0000-0002-1666-5483; Donne, Simon/0000-0002-7461-8589
FU Ghent University: Belgisch Onderzoeksfonds (BOF) [01D21213]
FX The research in this work was mostly performed under a PhD grant from
   Ghent University: Belgisch Onderzoeksfonds (BOF) grant number 01D21213.
   Part of the research was performed in the scope of the iMinds BAHAMAS
   research project.
CR Ahmed M. T., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P463, DOI 10.1109/ICCV.1999.791257
   Arca Stefano, 2005, P INT C MULT IM PROC
   Bennett S, 2014, COMPUT VIS IMAGE UND, V118, P197, DOI 10.1016/j.cviu.2013.10.008
   Bok Y, 2016, PATTERN RECOGN LETT, V71, P66, DOI 10.1016/j.patrec.2015.12.008
   Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16
   Bouguet JY., CAMERA CALIBRATION T
   Brown D., 1966, PHOTOGRAMMETRIC ENG, V32, P444, DOI DOI 10.1234/12345678
   de la Escalera A, 2010, SENSORS-BASEL, V10, P2027, DOI 10.3390/s100302027
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Donne S., 2016, P 25 BELG DUTCH C MA
   Goossens B, 2014, IEEE IMAGE PROC, P2183, DOI 10.1109/ICIP.2014.7025441
   Ha JE, 2009, OPT ENG, V48, DOI 10.1117/1.3156053
   Harris C., 1988, ALV VIS C MANCH UK, V15, P50, DOI DOI 10.5244/C
   Heikkila J, 1997, PROC CVPR IEEE, P1106, DOI 10.1109/CVPR.1997.609468
   Junghee Jun, 1999, Proceedings of IEEE. IEEE Region 10 Conference. TENCON 99. `Multimedia Technology for Asia-Pacific Information Infrastructure' (Cat. No.99CH37030), P694, DOI 10.1109/TENCON.1999.818509
   Kannala J, 2006, IEEE T PATTERN ANAL, V28, P1335, DOI 10.1109/TPAMI.2006.153
   Le Q.V., 2011, P 28 INT C MACH LEAR, P265
   LeCun Y., 1998, MNIST DATABASE HANDW
   Lucchese L, 2002, APCCAS 2002: ASIA-PACIFIC CONFERENCE ON CIRCUITS AND SYSTEMS, VOL 2, PROCEEDINGS, P191, DOI 10.1109/APCCAS.2002.1115151
   Memon Q, 2001, INT J SYST SCI, V32, P1155, DOI 10.1080/00207720010024276
   Moravec H.P., 1977, P 5 INT JOINT C ART
   Nair V, 2010, ICML, V27, P807, DOI DOI 10.0RG/PAPERS/432.PDF
   Placht S, 2014, LECT NOTES COMPUT SC, V8692, P766, DOI 10.1007/978-3-319-10593-2_50
   Powell S, 2008, NEUROIMAGE, V39, P238, DOI 10.1016/j.neuroimage.2007.05.063
   Roels J., 2016, P 25 BELG DUTCH C MA
   Rosten E, 2006, LECT NOTES COMPUT SC, V3951, P430
   Rosten E, 2010, IEEE T PATTERN ANAL, V32, P105, DOI 10.1109/TPAMI.2008.275
   Rufli M, 2008, 2008 IEEE/RSJ INTERNATIONAL CONFERENCE ON ROBOTS AND INTELLIGENT SYSTEMS, VOLS 1-3, CONFERENCE PROCEEDINGS, P3121, DOI 10.1109/IROS.2008.4650703
   Scaramuzza D., 2013, OCAMCALIB OMNIDIRECT
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Smith SM, 1997, INT J COMPUT VISION, V23, P45, DOI 10.1023/A:1007963824710
   Sturm P. F., 1999, P IEEE COMP SOC C CO
   Sturm P, 2010, FOUND TRENDS COMPUT, V6, P1, DOI 10.1561/0600000023
   Su JD, 2013, INT CONF INFO SCI, P858, DOI 10.1109/ICIST.2013.6747676
   Sutskever I., 2013, P 30 INT C MACH LEAR, P1139
   TSAI RY, 1987, IEEE T ROBOTIC AUTOM, V3, P323, DOI 10.1109/JRA.1987.1087109
   Vezhnevets V., OPENCV CALIBRATION O
   Wang ZS, 2007, APPL MATH COMPUT, V185, P894, DOI 10.1016/j.amc.2006.05.210
   Xavier Glorot, 2011, P 14 INT C ART INT S, P315, DOI DOI 10.1177/1753193410395357
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
   Zhengyou Zhang, 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P666, DOI 10.1109/ICCV.1999.791289
   Zhu W., 2009, CISP 2 INT C IM SIGN, P1
NR 43
TC 8
Z9 8
U1 1
U2 15
PU MDPI
PI BASEL
PA ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
EI 1424-8220
J9 SENSORS-BASEL
JI Sensors
PD NOV
PY 2016
VL 16
IS 11
AR 1858
DI 10.3390/s16111858
PG 16
WC Chemistry, Analytical; Engineering, Electrical & Electronic; Instruments
   & Instrumentation
SC Chemistry; Engineering; Instruments & Instrumentation
GA EE5JB
UT WOS:000389641700091
OA DOAJ Gold, Green Published
DA 2020-02-19
ER

PT J
AU Miyajima, C
   Takeda, K
AF Miyajima, Chiyomi
   Takeda, Kazuya
TI Driver-Behavior Modeling Using On-Road Driving Data A new application
   for behavior signal processing
SO IEEE SIGNAL PROCESSING MAGAZINE
LA English
DT Article
ID GAZE
AB This article reviews data-centric approaches for statistical modeling of driver behavior. Modeling driver behavior is challenging due to its stochastic nature and the high degree of inter-and intradriver variability. One way to deal with the highly variable nature of driving behavior is to employ a data-centric approach that models driver behavior using large amounts of driving data collected from numerous drivers in a variety of traffic conditions. To obtain large amounts of realistic driving data, several projects have collected real-world driving data. Statistical machine-learning techniques, such as hidden Markov models (HMMs) and deep learning, have been successfully applied to model driver behavior using large amounts of driving data. We have also collected on-road data recording hundreds of drivers over more than 15 years. We have applied statistical signal processing and machine-learning techniques to this data to model various aspects of driver behavior, e.g., driver pedal-operation, car-following, and lane-change behaviors for predicting driver behavior and detecting risky driver behavior and driver frustration. By reviewing related studies and providing concrete examples of our own research, this article is intended to illustrate the usefulness of such data-centric approaches for statistical driver-behavior modeling.
C1 [Miyajima, Chiyomi] Nagoya Inst Technol, Dept Comp Sci, Nagoya, Aichi, Japan.
   [Miyajima, Chiyomi] Nagoya Univ, Grad Sch Informat Sci, Nagoya, Aichi, Japan.
   [Miyajima, Chiyomi; Takeda, Kazuya] Nagoya Univ, Inst Innovat Future Soc, Nagoya, Aichi, Japan.
   [Takeda, Kazuya] KDD R&D Labs, Adv Telecommun Res Labs, Tokyo, Japan.
   [Takeda, Kazuya] Nagoya Univ, Res Grp Signal Proc Applicat, Nagoya, Aichi, Japan.
   [Takeda, Kazuya] IEEE Intelligent Transportat Syst Soc, Piscataway, NJ USA.
   [Takeda, Kazuya] IEEE, Piscataway, NJ USA.
RP Miyajima, C (reprint author), Nagoya Univ, Inst Innovat Future Soc, Nagoya, Aichi, Japan.
EM miyajima@nagoya-u.jp; kazuya.takeda@nagoya-u.jp
CR Angkititrakul P, 2012, 2012 IEEE INTELLIGENT VEHICLES SYMPOSIUM (IV), P426, DOI 10.1109/IVS.2012.6232177
   Angkititrakul P, 2011, IEEE INT VEH SYM, P814, DOI 10.1109/IVS.2011.5940464
   Bando T, 2013, IEEE INT VEH SYM, P177, DOI 10.1109/IVS.2013.6629467
   Benmimoun M., 2013, P FISITA 2012 WORLD, P537
   Dingus T. A., 2006, DOT HS, V810, P593
   Dingus T.A., 2015, S2S06RW1 TRANSP RES
   Eenink R., 2014, TRANSP RES AR C APR
   Fitch G. M., 2009, HS811147 DOT
   Jain A., 2016, P IEEE INT C ROB AUT, P3118
   Kawaguchi N., 2001, P 7 EUR C SPEECH COM, P2027
   Malta L, 2011, IEEE T INTELL TRANSP, V12, P109, DOI 10.1109/TITS.2010.2070839
   Malta L, 2009, IEEE T INTELL TRANSP, V10, P201, DOI 10.1109/TITS.2009.2018321
   Miyajima C., 2013, 2 INT S FUT ACT SAF
   Miyajima C, 2007, P IEEE, V95, P427, DOI 10.1109/JPROC.2006.888405
   Miyajima C, 2015, IEEE INT VEH SYM, P1293, DOI 10.1109/IVS.2015.7225894
   Mori M, 2013, IEEE INT C INTELL TR, P2020, DOI 10.1109/ITSC.2013.6728526
   Nishiwaki Yoshihiro, 2007, Proceedings of the 2007 IEEE Intelligent Vehicles Symposium, P823
   Nishiwaki Y, 2012, DIGITAL SIGNAL PROCESSING FOR IN-VEHICLE SYSTEMS AND SAFETY, P271, DOI 10.1007/978-1-4419-9607-7_19
   Okuda H, 2013, IEEE T INTELL TRANSP, V14, P98, DOI 10.1109/TITS.2012.2207893
   OLIVER N, 2000, P SPIE AER ENH SYNTH, P280
   Raksincharoensak P., 2013, FOT NET WORKSH TOK J
   Regan M. A., 2013, P AUSTR ROAD SAF RES, P1
   Research Institute of Human Engineering for Quality Life, DRIV BEH DAT
   Sakaguchi Y., 2003, P 18 INT C ENH SAF V
   Satzoda RK, 2014, IEEE INT VEH SYM, P293, DOI 10.1109/IVS.2014.6856609
   Takeda K, 2012, IEEE T INTELL TRANSP, V13, P1821, DOI 10.1109/TITS.2012.2205917
   Takeda K, 2011, IEEE T INTELL TRANSP, V12, P1609, DOI 10.1109/TITS.2011.2167680
   Taniguchi T, 2012, 2012 IEEE INTELLIGENT VEHICLES SYMPOSIUM (IV), P849, DOI 10.1109/IVS.2012.6232243
   Terai H, 2015, PROCEDIA MANUF, V3, P3136, DOI 10.1016/j.promfg.2015.07.974
   Yamazaki S., 2016, P IEEE INT VEH S, P642
NR 30
TC 17
Z9 17
U1 3
U2 34
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1053-5888
EI 1558-0792
J9 IEEE SIGNAL PROC MAG
JI IEEE Signal Process. Mag.
PD NOV
PY 2016
VL 33
IS 6
BP 14
EP 21
DI 10.1109/MSP.2016.2602377
PG 8
WC Engineering, Electrical & Electronic
SC Engineering
GA EC6BE
UT WOS:000388220200004
DA 2020-02-19
ER

PT J
AU Song, JK
   Gao, LL
   Zou, FH
   Yan, Y
   Sebe, N
AF Song, Jingkuan
   Gao, Lianli
   Zou, Fuhao
   Yan, Yan
   Sebe, Nicu
TI Deep and fast: Deep learning hashing with semi-supervised graph
   construction
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Deep learning; Hashing; Multimedia retrieval
AB Learning-based hashing methods are becoming the mainstream for approximate scalable multimedia retrieval. They consist of two main components: hash codes learning for training data and hash functions learning for new data points. Tremendous efforts have been devoted to designing novel methods for these two components, i.e., supervised and unsupervised methods for learning hash codes, and different models for inferring hashing functions. However, there is little work integrating supervised and Unsupervised hash codes learning into a single framework. Moreover, the hash function learning component is usually based on hand-crafted visual features extracted from the training images. The performance of a content-based image retrieval system crucially depends on the feature representation and such hand-crafted visual features may degrade the accuracy of the hash functions. In this paper, we propose a semi-supervised deep learning hashing (DLH) method for fast multimedia retrieval. More specifically, in the first component, we utilize both visual and label information to learn an optimal similarity graph that can more precisely encode the relationship among training data, and then generate the hash codes based on the graph. In the second stage, we apply a deep convolutional network to simultaneously learn a good multimedia representation and a set of hash functions. Extensive experiments on five popular datasets demonstrate the superiority of our DLH over both supervised and unsupervised hashing methods. (C) 2016 Published by Elsevier B.V.
C1 [Song, Jingkuan; Yan, Yan; Sebe, Nicu] Univ Trento, Trento, Italy.
   [Gao, Lianli] Univ Elect Sci & Technol China, Chengdu, Peoples R China.
   [Zou, Fuhao] Huazhong Univ Sci & Technol, Wuhan, Peoples R China.
RP Gao, LL (reprint author), 2006,Xiyuan Ave,West Hitech Zone, Chengdu 611731, Peoples R China.
EM jingkuan.song@unitn.it; lianli.gao@uestc.edu.cn
OI Sebe, Niculae/0000-0002-6597-7248
CR Blei D. M., 2011, ICML
   Bohm C, 2001, ACM COMPUT SURV, V33, P322, DOI 10.1145/502807.502809
   Cappelli R, 2011, IEEE T SYST MAN CY B, V41, P1511, DOI 10.1109/TSMCB.2011.2155648
   Chen M., 2013, ICML
   Datta R., ACM COMPUTING SURVEY, V40
   Gao L., 2016, AAAI
   Gao LL, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P903, DOI 10.1145/2733373.2806360
   Gao LL, 2015, PROC CVPR IEEE, P4371, DOI 10.1109/CVPR.2015.7299066
   Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193
   Guillaumin M., 2009, ICCV
   He K., 2013, CVPR
   Jegou H, 2012, IEEE T PATTERN ANAL, V34, P1704, DOI 10.1109/TPAMI.2011.235
   Jia Y., ARXIV14085093
   Krizhevsky A., 2012, NIPS
   Kulis B, 2009, NIPS
   Lai Hanjiang, 2015, CVPR
   Lin G., 2014, CVPR 2014
   Liu L., TCYB
   Liu L., TNNLS
   Liu L, 2015, IEEE T IMAGE PROCESS, V24, P956, DOI 10.1109/TIP.2015.2390975
   Liu Wei, 2012, CVPR
   Nister D., 2006, CVPR
   Norouzi M., 2013, CVPR
   Song J., 2011, ACM MULTIMEDIA
   Song J, 2013, P ACM SIGMOD INT C M, P785, DOI DOI 10.1145/2463676.2465274
   Szegedy C., ABS14094842 CORR
   Wan J., 2014, ACM MULTIMEDIA
   Wang J., ABS14082927 CORR
   Wang Jinjun, 2010, CVPR
   Weiss Y, 2008, NIPS
   Xia Rongkai, 2014, AAAI
   Yu M., TPAMI
   Zhang D., 2010, SIGIR
   Zhang L., 2011, ICC
   Zhang LN, 2012, IEEE T SYST MAN CY B, V42, P282, DOI 10.1109/TSMCB.2011.2165335
   Zhang S., 2012, ECCV
   Zhao F, 2015, CVPR
   Zhou W., 2010, ACM MULTIMEDIA
   Zhu XF, 2014, IEEE T IMAGE PROCESS, V23, P3737, DOI 10.1109/TIP.2014.2332764
NR 39
TC 11
Z9 12
U1 2
U2 18
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD NOV
PY 2016
VL 55
SI SI
BP 101
EP 108
DI 10.1016/j.imavis.2016.02.005
PN 2
PG 8
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
SC Computer Science; Engineering; Optics
GA ED9BE
UT WOS:000389164300008
DA 2020-02-19
ER

PT J
AU Liskowski, P
   Krawiec, K
AF Liskowski, Pawel
   Krawiec, Krzysztof
TI Segmenting Retinal Blood Vessels With Deep Neural Networks
SO IEEE TRANSACTIONS ON MEDICAL IMAGING
LA English
DT Article
DE Classification; deep learning; feature learning; fundus; neural
   networks; retina; retinopathy; structured prediction; vessel
   segmentation
ID COLOR IMAGES; SEGMENTATION; DELINEATION
AB The condition of the vascular network of human eye is an important diagnostic factor in ophthalmology. Its segmentation in fundus imaging is a nontrivial task due to variable size of vessels, relatively low contrast, and potential presence of pathologies like microaneurysms and hemorrhages. Many algorithms, both unsupervised and supervised, have been proposed for this purpose in the past. We propose a supervised segmentation technique that uses a deep neural network trained on a large (up to 400 000) sample of examples preprocessed with global contrast normalization, zero-phase whitening, and augmented using geometric transformations and gamma corrections. Several variants of the method are considered, including structured prediction, where a network classifies multiple pixels simultaneously. When applied to standard benchmarks of fundus imaging, the DRIVE, STARE, and CHASE databases, the networks significantly outperform the previous algorithms on the area under ROC curve measure (up to > 0.99) and accuracy of classification (up to > 0.97). The method is also resistant to the phenomenon of central vessel reflex, sensitive in detection of fine vessels (sensitivity > 0.87), and fares well on pathological cases.
C1 [Liskowski, Pawel; Krawiec, Krzysztof] Poznan Univ Tech, Inst Comp Sci, PL-60965 Poznan, Poland.
RP Liskowski, P (reprint author), Poznan Univ Tech, Inst Comp Sci, PL-60965 Poznan, Poland.
EM pliskowski@cs.put.poznan.pl; kkrawiec@cs.put.poznan.pl
RI Liskowski, Pawel/N-4342-2014; Krawiec, Krzysztof/L-9390-2014
OI Liskowski, Pawel/0000-0002-8165-748X; Krawiec,
   Krzysztof/0000-0001-5439-3231
FU National Centre for Research and Development, Poland [PBS1/A9/20/2013]
FX Research reported in this study was supported by the National Centre for
   Research and Development, Poland, Grant PBS1/A9/20/2013. Asterisk
   indicates corresponding author.
CR Azzopardi G, 2015, MED IMAGE ANAL, V19, P46, DOI 10.1016/j.media.2014.08.002
   Bakir G. H., 2007, PREDICTING STRUCTURE
   Bell A. J., 1996, ADV NEURAL INFORM PR, P831
   Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Dosovitskiy A., 2014, ADV NEURAL INFORM PR, V27, P766
   Fraz MM, 2012, COMPUT METH PROG BIO, V108, P407, DOI 10.1016/j.cmpb.2012.03.009
   Fraz MM, 2014, INT J COMPUT ASS RAD, V9, P795, DOI 10.1007/s11548-013-0965-9
   Fraz MM, 2012, IEEE T BIO-MED ENG, V59, P2538, DOI 10.1109/TBME.2012.2205687
   FUKUSHIMA K, 1980, BIOL CYBERN, V36, P193, DOI 10.1007/BF00344251
   Glorot X., 2010, JLMR P TRACK, p[249, 2010], DOI DOI 10.1177/1753193409103364.
   Hinton G. E, 2012, ARXIV12070580
   Hoover A, 2000, IEEE T MED IMAGING, V19, P203, DOI 10.1109/42.845178
   Hyvarinen A, 2000, NEURAL NETWORKS, V13, P411, DOI 10.1016/S0893-6080(00)00026-5
   Jia Yangqing, 2014, ARXIV14085093
   Jiang XY, 2003, IEEE T PATTERN ANAL, V25, P131, DOI 10.1109/TPAMI.2003.1159954
   Krizhevsky A., 2009, LEARNING MULTIPLE LA, V1, P7
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Le Cun B B, 1990, ADV NEURAL INFORM PR
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Marin D, 2011, IEEE T MED IMAGING, V30, P146, DOI 10.1109/TMI.2010.2064333
   Nair V, 2010, ICML, V27, P807, DOI DOI 10.0RG/PAPERS/432.PDF
   Nam Jinseok, 2014, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2014. Proceedings: LNCS 8725, P437, DOI 10.1007/978-3-662-44851-9_28
   NEKOVEI R, 1995, IEEE T NEURAL NETWOR, V6, P64, DOI 10.1109/72.363449
   Osareh A, 2009, IRAN J SCI TECHNOL B, V33, P191
   Owen CG, 2009, INVEST OPHTH VIS SCI, V50, P2004, DOI 10.1167/iovs.08-3018
   Ricci E, 2007, IEEE T MED IMAGING, V26, P1357, DOI 10.1109/TMI.2007.898551
   Roychowdhury S, 2015, IEEE T BIO-MED ENG, V62, P1738, DOI 10.1109/TBME.2015.2403295
   Roychowdhury S, 2015, IEEE J BIOMED HEALTH, V19, P1118, DOI 10.1109/JBHI.2014.2335617
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Soares JVB, 2006, IEEE T MED IMAGING, V25, P1214, DOI 10.1109/TMI.2006.879967
   Springenberg J. T., 2014, 14126806 ARXIV
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Staal J, 2004, IEEE T MED IMAGING, V23, P501, DOI 10.1109/TMI.2004.825627
   Villalobos-Castaldi FM, 2010, J VISUAL-JAPAN, V13, P263, DOI 10.1007/s12650-010-0037-y
   Xavier Glorot, 2011, P 14 INT C ART INT S, P315, DOI DOI 10.1177/1753193410395357
   Zhang ML, 2006, IEEE T KNOWL DATA EN, V18, P1338, DOI 10.1109/TKDE.2006.162
   Zhao YT, 2015, IEEE T MED IMAGING, V34, P1797, DOI 10.1109/TMI.2015.2409024
NR 39
TC 192
Z9 205
U1 17
U2 89
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 0278-0062
EI 1558-254X
J9 IEEE T MED IMAGING
JI IEEE Trans. Med. Imaging
PD NOV
PY 2016
VL 35
IS 11
BP 2369
EP 2380
DI 10.1109/TMI.2016.2546227
PG 12
WC Computer Science, Interdisciplinary Applications; Engineering,
   Biomedical; Engineering, Electrical & Electronic; Imaging Science &
   Photographic Technology; Radiology, Nuclear Medicine & Medical Imaging
SC Computer Science; Engineering; Imaging Science & Photographic
   Technology; Radiology, Nuclear Medicine & Medical Imaging
GA EC9ZS
UT WOS:000388503400001
PM 27046869
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Annunziata, R
   Trucco, E
AF Annunziata, Roberto
   Trucco, Emanuele
TI Accelerating Convolutional Sparse Coding for Curvilinear Structures
   Segmentation by Refining SCIRD-TS Filter Banks
SO IEEE TRANSACTIONS ON MEDICAL IMAGING
LA English
DT Article
DE Convolutional sparse coding; neurites; retinal blood vessels;
   segmentation
ID VESSEL SEGMENTATION; QUANTIFICATION
AB Deep learning has shown great potential for curvilinear structure (e.g., retinal blood vessels and neurites) segmentation as demonstrated by a recent auto-context regression architecture based on filter banks learned by convolutional sparse coding. However, learning such filter banks is very time-consuming, thus limiting the amount of filters employed and the adaptation to other data sets (i.e., slow re-training). We address this limitation by proposing a novel acceleration strategy to speed-up convolutional sparse coding filter learning for curvilinear structure segmentation. Our approach is based on a novel initialisation strategy (warm start), and therefore it is different from recent methods improving the optimisation itself. Our warm-start strategy is based on carefully designed hand-crafted filters (SCIRD-TS), modelling appearance properties of curvilinear structures which are then refined by convolutional sparse coding. Experiments on four diverse data sets, including retinal blood vessels and neurites, suggest that the proposed method reduces significantly the time taken to learn convolutional filter banks (i.e., up to -82%) compared to conventional initialisation strategies. Remarkably, this speed-up does not worsen performance; in fact, filters learned with the proposed strategy often achieve a much lower reconstruction error and match or exceed the segmentation performance of random and DCT-based initialisation, when used as input to a random forest classifier.
C1 [Annunziata, Roberto; Trucco, Emanuele] Univ Dundee, Sch Sci & Engn Comp, Dundee DD1 4HN, Scotland.
RP Annunziata, R (reprint author), Univ Dundee, Sch Sci & Engn Comp, Dundee DD1 4HN, Scotland.
EM r.annunziata@dundee.ac.uk; e.trucco@dundee.ac.uk
OI Trucco, Emanuele/0000-0002-5055-0794
FU EU Marie Curie ITN REVAMMADEuropean Union (EU) [316990]
FX This work was supported by the EU Marie Curie ITN REVAMMAD 316990.
   Asterisk indicates corresponding author.
CR Al-Diri B, 2009, IEEE T MED IMAGING, V28, P1488, DOI 10.1109/TMI.2009.2017941
   Annunziata R., IEEE J BIOM IN PRESS
   Annunziata R, 2016, MED IMAGE ANAL, V32, P216, DOI 10.1016/j.media.2016.04.006
   Annunziata R, 2016, INVEST OPHTH VIS SCI, V57, P1132, DOI 10.1167/iovs.15-18513
   Annunziata R, 2015, LECT NOTES COMPUT SC, V9351, P588, DOI 10.1007/978-3-319-24574-4_70
   Annunziata R, 2015, LECT NOTES COMPUT SC, V9351, P596, DOI 10.1007/978-3-319-24574-4_71
   Arthur D, 2007, SOC IND APPL MATH, P1027, DOI DOI 10.1145/1283383.1283494
   Bach F., 2011, OPTIMIZAT MACH LEARN, V5
   Bao C., 2015, IEEE T PATT IN PRESS
   Bao CL, 2014, LECT NOTES COMPUT SC, V8694, P302, DOI 10.1007/978-3-319-10599-4_20
   Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Bristow H, 2013, PROC CVPR IEEE, P391, DOI 10.1109/CVPR.2013.57
   Brown KM, 2011, NEUROINFORMATICS, V9, P143, DOI 10.1007/s12021-010-9095-5
   Chalasani R., 2013, P INT JOINT C NEUR N, DOI [10.1109/ijcnn.2013.6706854, DOI 10.1109/IJCNN.2013.6706854]
   Coates Adam, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P561, DOI 10.1007/978-3-642-35289-8_30
   Criminisil A, 2011, FOUND TRENDS COMPUT, V7, P81, DOI [10.1501/0000000035, 10.1561/0600000035]
   Frangi AF, 1998, LECT NOTES COMPUT SC, V1496, P130
   Ganin Y, 2015, LECT NOTES COMPUT SC, V9004, P536, DOI 10.1007/978-3-319-16808-1_36
   Gu L, 2015, IEEE I CONF COMP VIS, P639, DOI 10.1109/ICCV.2015.80
   Heide F, 2015, PROC CVPR IEEE, P5135, DOI 10.1109/CVPR.2015.7299149
   Hoover A, 2000, IEEE T MED IMAGING, V19, P203, DOI 10.1109/42.845178
   Kavukcuoglu K., 2010, ADV NEURAL INFORM PR, V23, P1090
   Kontschieder P., 2015, INT C COMPUT VIS
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lam BSY, 2010, IEEE T MED IMAGING, V29, P1369, DOI 10.1109/TMI.2010.2043259
   Law MWK, 2008, LECT NOTES COMPUT SC, V5305, P368, DOI 10.1007/978-3-540-88693-8_27
   Le Q.V., 2011, P 28 INT C MACH LEAR, P265
   Lee H., 2006, ADV NEURAL INF PROCE, P801
   Li QL, 2016, IEEE T MED IMAGING, V35, P109, DOI 10.1109/TMI.2015.2457891
   Marin D, 2011, IEEE T MED IMAGING, V30, P146, DOI 10.1109/TMI.2010.2064333
   Mendonca AM, 2006, IEEE T MED IMAGING, V25, P1200, DOI 10.1109/TMI.2006.879955
   Parikh Neal, 2014, Foundations and Trends in Optimization, V1, P127, DOI 10.1561/2400000003
   Ricci E, 2007, IEEE T MED IMAGING, V26, P1357, DOI 10.1109/TMI.2007.898551
   Rigamonti R, 2012, LECT NOTES COMPUT SC, V7510, P189, DOI 10.1007/978-3-642-33415-3_24
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Sironi A., 2015, IEEE T PATT IN PRESS
   Sironi A, 2015, IEEE I CONF COMP VIS, P316, DOI 10.1109/ICCV.2015.44
   Sironi A, 2015, IEEE T PATTERN ANAL, V37, P94, DOI 10.1109/TPAMI.2014.2343229
   Soares JVB, 2006, IEEE T MED IMAGING, V25, P1214, DOI 10.1109/TMI.2006.879967
   Staal J, 2004, IEEE T MED IMAGING, V23, P501, DOI 10.1109/TMI.2004.825627
   Szlam A. D, 2011, ADV NEURAL INFORM PR, P1116
   Trucco E, 2013, INVEST OPHTH VIS SCI, V54, P3546, DOI 10.1167/iovs.12-10347
   Zhao YT, 2015, IEEE T MED IMAGING, V34, P1797, DOI 10.1109/TMI.2015.2409024
NR 45
TC 9
Z9 9
U1 0
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 0278-0062
EI 1558-254X
J9 IEEE T MED IMAGING
JI IEEE Trans. Med. Imaging
PD NOV
PY 2016
VL 35
IS 11
BP 2381
EP 2392
DI 10.1109/TMI.2016.2570123
PG 12
WC Computer Science, Interdisciplinary Applications; Engineering,
   Biomedical; Engineering, Electrical & Electronic; Imaging Science &
   Photographic Technology; Radiology, Nuclear Medicine & Medical Imaging
SC Computer Science; Engineering; Imaging Science & Photographic
   Technology; Radiology, Nuclear Medicine & Medical Imaging
GA EC9ZS
UT WOS:000388503400002
PM 27214893
OA Green Published
DA 2020-02-19
ER

PT J
AU Chellappa, R
AF Chellappa, Rama
TI The changing fortunes of pattern recognition and computer vision
SO IMAGE AND VISION COMPUTING
LA English
DT Article; Proceedings Paper
CT Biometrics Conference
CY OCT, 2015
CL London, ENGLAND
DE Convolutional Neural Networks; Deep Learning; Image Recognition;
   Biometrics
AB As someone who had been attending conferences on pattern recognition and computer vision since 1978, I have watched with interest the ups and downs of pattern recognition and computer vision areas and how they have been presented at various conferences such as PRIP, CVPR, ECCV, ICCV, ACCV, ICPR, IjCAI, AAAI, NIPS, ICASSP and ICIP. Given the recent successes of deep learning networks, it appears that the scale is tilting towards pattern recognition as is commonly understood. A good number of papers in recent vision conferences seem to push data through one or other deep learning networks and report improvements over state of the art. While one cannot argue against the remarkable (magical?) performance improvements obtained by deep learning network based approaches, I worry that five years from now, most students in computer vision will only be aware of software that implements some deep learning network. After all, 2-D based detection and recognition problems for which the deep learning networks have shown their mettle are only a subset of the computer vision field. While enjoying the ride, I would like to caution that understanding of scene and image formation, invariants, interaction between light and matter, human vision, 3D recovery, and emerging concepts like common sense reasoning are too important to ignore for the long-term viability of the computer vision field. It will be a dream come true if we manage to integrate these computer vision concepts into deep learning networks so that more robust performance can be obtained with less data and cheaper computers. (C) 2016 Elsevier B.V. All rights reserved.
C1 [Chellappa, Rama] Univ Maryland, Dept Elect & Comp Engn, College Pk, MD 20742 USA.
   [Chellappa, Rama] Univ Maryland, Ctr Automat Res, UMIACS, College Pk, MD 20742 USA.
RP Chellappa, R (reprint author), Univ Maryland, Dept Elect & Comp Engn, College Pk, MD 20742 USA.; Chellappa, R (reprint author), Univ Maryland, Ctr Automat Res, UMIACS, College Pk, MD 20742 USA.
RI Chellappa, Rama/B-6573-2012
CR Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Bruna J, 2013, IEEE T PATTERN ANAL, V35, P1872, DOI 10.1109/TPAMI.2012.230
   Carpenter G. A., 1991, PATTERN RECOGNITION
   FAUGERAS O, 1993, 3 DIMENSIONAL COMPUT
   FU KS, 1981, SYNTACTIC PATTERN RE
   Fukunaga K., 1990, INTRO STAT PATTERN R
   FUKUSHIMA K, 1980, BIOL CYBERN, V36, P193, DOI 10.1007/BF00344251
   Giryes R., ARXIV141258962014
   GRIMSON W, 1990, OBJECT RECOGNITION C
   Haeffele B. D., ARXIV1506075402015
   Hinton G. E., 1986, LEARNING RELEARNING, V1
   Horn BKP, 1986, ROBOT VISION
   Kanade T., 1987, 3 DIMENSIONAL VISION
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   LeCun Y, 2004, PROC CVPR IEEE, P97
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Mallat S., 2014, ARXIV1601049202016
   Rosenfeld A, 1982, PATTERN RECOGN LETT, V1, P2, DOI 10.1016/0167-8655(82)90042-3
   Rumelhart D.E., 1986, PARALLEL DISTRIBUTED, V1, P318
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Sun Y, 2015, PROC CVPR IEEE, P2892, DOI 10.1109/CVPR.2015.7298907
   Szeliski R., 2010, COMPUTER VISION ALGO
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Vapnik VN, 1995, NATURE STAT LEARNING
   WERBOS PJ, 1994, ROOTS BACKPROPAGATIO
   Zhou Y., 1991, ARTIFICIAL NEURAL NE
NR 27
TC 2
Z9 2
U1 0
U2 9
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD NOV
PY 2016
VL 55
BP 3
EP 5
DI 10.1016/j.imavis.2016.04.005
PN 1
PG 3
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
SC Computer Science; Engineering; Optics
GA ED8BO
UT WOS:000389097100002
DA 2020-02-19
ER

PT J
AU Liu, C
   Xu, WS
   Wu, QD
   Yang, GL
AF Liu, Cong
   Xu, Weisheng
   Wu, Qidi
   Yang, Gelan
TI Learning motion and content-dependent features with convolutions for
   action recognition
SO MULTIMEDIA TOOLS AND APPLICATIONS
LA English
DT Article
DE Spatiotemporal; Convolutional neural networks; Multiplicative
   interactions; Deep learning; Action recognition
AB A variety of recognizing architectures based on deep convolutional neural networks have been devised for labeling videos containing human motion with action labels. However, so far, most works cannot properly deal with the temporal dynamics encoded in multiple contiguous frames, which distinguishes action recognition from other recognition tasks. This paper develops a temporal extension of convolutional neural networks to exploit motion-dependent features for recognizing human action in video. Our approach differs from other recent attempts in that it uses multiplicative interactions between convolutional outputs to describe motion information across contiguous frames. Interestingly, the representation of image content arises when we are at work on extracting motion pattern, which makes our model effectively incorporate both of them to analysis video. Additional theoretical analysis proves that motion and content-dependent features arise simultaneously from the developed architecture, whereas previous works mostly deal with the two separately. Our architecture is trained and evaluated on the standard video actions benchmarks of KTH and UCF101, where it matches the state of the art and has distinct advantages over previous attempts to use deep convolutional architectures for action recognition.
C1 [Liu, Cong; Xu, Weisheng; Wu, Qidi] Tongji Univ, Sch Elect & Informat Engn, Shanghai 201804, Peoples R China.
   [Yang, Gelan] Hunan City Univ, Sch Informat Sci & Engn, Yiyang 413000, Peoples R China.
RP Liu, C (reprint author), Tongji Univ, Sch Elect & Informat Engn, Shanghai 201804, Peoples R China.
EM 1210482congliu@tongji.edu.cn; glyang@mail.ustc.edu.cn
OI Yang, Gelan/0000-0002-2472-3436
FU Science Research Foundation of Hunan Provincial Education Department
   [12B023]
FX The research work described in this paper is supported by Science
   Research Foundation of Hunan Provincial Education Department under grant
   number 12B023.
CR ADELSON EH, 1985, J OPT SOC AM A, V2, P284, DOI 10.1364/JOSAA.2.000284
   Aggarwal JK, 2011, ACM COMPUT SURV, V43, DOI 10.1145/1922649.1922653
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bouagar S, 2016, MULTIMED TOOLS APPL, V75, P2989, DOI 10.1007/s11042-014-2417-0
   Dollar P., 2005, Proceedings. 2nd Joint IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance (VS-PETS) (IEEE Cat. No. 05EX1178), P65
   Guo J, 2011, J INF PROCESS SYST, V7, P103, DOI 10.3745/JIPS.2011.7.1.103
   Heider F., 1944, AM J PSYCHOL
   Horn R.A., 2012, MATRIX ANAL
   Hyvarinen A, 2009, COMPUT IMAGING VIS, V39, P1
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Karpathy Andrej, 2014, IEEE C COMP VIS PATT
   Kim H, 2014, THERANOSTICS, V4, P1, DOI 10.7150/thno.7101
   Konda KR, 2013, ARXIVCORR13063162
   Krizhevsky A., 2012, ADV NEURAL INFORM PR
   Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7
   Laptev I., 2008, COMP VIS PATT REC 20
   Le QV, 2011, COMP VIS PATT REC CV
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Liu S., 2015, MULTIMED TOOLS APPL, V76, P5787, DOI [10.1007/s11042-014-2408-1, DOI 10.1007/S11042-014-2408-1]
   Marszalek M., 2009, IEEE C COMP VIS PATT
   Memisevic R, 2011, COMP VIS ICCV 2011 I
   Memisevic R, 2013, IEEE T PATTERN ANAL, V35, P1829, DOI 10.1109/TPAMI.2013.53
   Mobahi H., 2009, P 26 ANN INT C MACH
   Ng C., 2013, J CONVERGENCE, V4, P39
   Olshausen BA, 2003, IM PROC ICIP 2003 P, V1
   Sanin A, 2013, APPL COMP VIS WACV 2
   Schindler K, 2008, COMP VIS PATT REC 20
   Schuldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462
   Simonyan K, 2014, ADV NEURAL INFORM PR
   Soomro K., 2012, ARXIV12120402
   Szegedy C., 2014, ARXIV14094842
   Turaga P, 2008, IEEE T CIRC SYST VID, V18, P1473, DOI 10.1109/TCSVT.2008.2005594
   van Hateren JH, 1998, P ROY SOC B-BIOL SCI, V265, P2315, DOI 10.1098/rspb.1998.0577
   Vishwakarma S, 2013, VISUAL COMPUT, V29, P983, DOI 10.1007/s00371-012-0752-6
   Wang H, 2009, BMVC 2009 BRIT MACH
   Wang H., 2011, COMP VIS PATT REC CV
   Wang Y, 2009, IEEE T PATTERN ANAL, V31, P1762, DOI 10.1109/TPAMI.2009.43
   Willems G, 2008, COMPUTER VISION ECCV
   Wiskott L, 2002, NEURAL COMPUT, V14, P715, DOI 10.1162/089976602317318938
   Zhang Z, 2012, IEEE T PATTERN ANAL, V34, P436, DOI 10.1109/TPAMI.2011.157
NR 40
TC 6
Z9 6
U1 1
U2 18
PU SPRINGER
PI DORDRECHT
PA VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN 1380-7501
EI 1573-7721
J9 MULTIMED TOOLS APPL
JI Multimed. Tools Appl.
PD NOV
PY 2016
VL 75
IS 21
BP 13023
EP 13039
DI 10.1007/s11042-015-2550-4
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic
SC Computer Science; Engineering
GA EA6ZD
UT WOS:000386776800003
DA 2020-02-19
ER

PT J
AU Fang, ZJ
   Fei, FC
   Fang, YM
   Lee, C
   Xiong, NX
   Shu, L
   Chen, S
AF Fang, Zhijun
   Fei, Fengchang
   Fang, Yuming
   Lee, Changhoon
   Xiong, Naixue
   Shu, Lei
   Chen, Sheng
TI Abnormal event detection in crowded scenes based on deep learning
SO MULTIMEDIA TOOLS AND APPLICATIONS
LA English
DT Article
DE Abnormal event detection; Crowd analysis; Saliency information; Optical
   flow; Deep learning
ID SALIENT REGION DETECTION; QUALITY ASSESSMENT; DETECTION MODEL;
   RECOGNITION; IMAGE
C1 [Fang, Zhijun; Fei, Fengchang; Fang, Yuming; Xiong, Naixue; Shu, Lei; Chen, Sheng] Jiangxi Univ Finance & Econ, Sch Informat Technol, Nanchang 330032, Peoples R China.
   [Fang, Zhijun] Shanghai Univ Engn Sci, Sch Elect & Elect Engn, Shanghai 201620, Peoples R China.
   [Fei, Fengchang] Jiangxi Univ Finance & Econ, Modern Econ & Management Coll, Nanchang 330013, Peoples R China.
   [Lee, Changhoon] Seoul Natl Univ Sci & Technol, Dept Computat Sci & Engn, Seoul, South Korea.
RP Fang, YM (reprint author), Jiangxi Univ Finance & Econ, Sch Informat Technol, Nanchang 330032, Peoples R China.
EM fa0001ng@e.ntu.edu.sg; nxiong@coloradotech.edu
RI xiong, naixue/M-4277-2019
OI xiong, naixue/0000-0002-0394-4635; Lee, Changhoon/0000-0003-4292-5792;
   Shu, Lei/0000-0002-6700-9347
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61461021, 61571212]; Key Academic Leader Plan in
   Jiangxi Province [20133BCB22005]; Key Project in Science and Technology
   from the Education Department of Jiangxi Province [GJJ14318]; Foreign
   Cooperation Foundation from the Science and Technology Department of
   Jiangxi Province [20151BDH80003, 20141BDH80003]
FX This research was supported partially by the National Natural Science
   Foundation of China (No. 61461021, 61571212), the Key Academic Leader
   Plan in Jiangxi Province (No. 20133BCB22005), the Key Project in Science
   and Technology from the Education Department of Jiangxi Province (No.
   GJJ14318) and the Foreign Cooperation Foundation from the Science and
   Technology Department of Jiangxi Province (No. 20151BDH80003,
   20141BDH80003).
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Adam A, 2008, IEEE T PATTERN ANAL, V30, P555, DOI 10.1109/TPAMI.2007.70825
   Alexe B, 2010, PROC CVPR IEEE, P73, DOI 10.1109/CVPR.2010.5540226
   Andrade EL, 2006, INT C PATT RECOG, P175
   Baumgartner T, 2013, PROC CVPR IEEE, P3658, DOI 10.1109/CVPR.2013.469
   Benezeth Y., 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2458, DOI 10.1109/CVPRW.2009.5206686
   Chan TH, PCANET SIMP IN PRESS
   Cheng MM, 2013, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2013.193
   Cong Y, 2013, IEEE T INF FOREN SEC, V8, P1590, DOI 10.1109/TIFS.2013.2272243
   Fang YM, 2012, IEEE T MULTIMEDIA, V14, P187, DOI 10.1109/TMM.2011.2169775
   Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231
   Gopalakrishnan V, 2009, IEEE T MULTIMEDIA, V11, P892, DOI 10.1109/TMM.2009.2021726
   Guo C., 2008, HUM NEUROBIOL, P1, DOI DOI 10.1109/CVPR.2008.4587715
   Guo CL, 2010, IEEE T IMAGE PROCESS, V19, P185, DOI 10.1109/TIP.2009.2030969
   Han J, 2012, MOR KAUF D, P1
   Hassner Tal, 2012, COMP VIS PATT REC WO, P1, DOI DOI 10.1109/CVPRW.2012.6239348
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hou X., 2007, IEEE C COMP VIS PATT, V2007, P1, DOI DOI 10.1109/CVPR.2007.383267
   Keyvanrad MA, 2014, INT C LEARN IN PRESS
   Kwon J, 2013, PROC CVPR IEEE, P2355, DOI 10.1109/CVPR.2013.305
   Lee YS, 2012, SENSORS-BASEL, V12, P573, DOI 10.3390/s120100573
   Liu Y, 2014, INT J SIGN PROC IMAG, V7, P115
   Ma RH, 2004, 2004 IEEE CONFERENCE ON CYBERNETICS AND INTELLIGENT SYSTEMS, VOLS 1 AND 2, P170
   Mebran R., 2009, C VIS PATT REC, P20
   Mehran R, 2010, LECT NOTES COMPUT SC, V6313, P439
   Popoola OP, 2012, IEEE T SYST MAN CY C, V42, P865, DOI 10.1109/TSMCC.2011.2178594
   Rasheed N, 2014, 2014 28TH INTERNATIONAL CONFERENCE ON ADVANCED INFORMATION NETWORKING AND APPLICATIONS WORKSHOPS (WAINA), P61, DOI 10.1109/WAINA.2014.18
   Ren XF, 2013, PROC CVPR IEEE, P3246, DOI 10.1109/CVPR.2013.417
   Sang-Hyun Cho, 2012, Proceedings of the 2012 IEEE Southwest Symposium on Image Analysis & Interpretation (SSIAI 2012), P113, DOI 10.1109/SSIAI.2012.6202466
   Shao F, IEEE T BROA IN PRESS
   Shao F, 2014, DIGIT SIGNAL PROCESS, V29, P45, DOI 10.1016/j.dsp.2014.03.003
   Shao F, 2013, IEEE T IMAGE PROCESS, V22, P1940, DOI 10.1109/TIP.2013.2240003
   Shu G, 2013, PROC CVPR IEEE, P3721, DOI 10.1109/CVPR.2013.477
   Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244
   Sun Y, 2013, PROC CVPR IEEE, P3476, DOI 10.1109/CVPR.2013.446
   Supancic JS, 2013, PROC CVPR IEEE, P2379, DOI 10.1109/CVPR.2013.308
   Suriani NS, 2013, SENSORS-BASEL, V13, P9966, DOI 10.3390/s130809966
   Thida M, 2013, IEEE T CYBERNETICS, V43, P2147, DOI 10.1109/TCYB.2013.2242059
   TREISMAN AM, 1980, COGNITIVE PSYCHOL, V12, P97, DOI 10.1016/0010-0285(80)90005-5
   Valenti R, 2009, IEEE I CONF COMP VIS, P2185, DOI 10.1109/ICCV.2009.5459240
   Vapnik V. N., 1998, STAT LEARNING THEORY
   Vapnik VN, 1995, NATURE STAT LEARNING
   Varadarajan Jagannadan, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P1338, DOI 10.1109/ICCVW.2009.5457456
   Wang LJ, 2012, IEEE IMAGE PROC, P2701, DOI 10.1109/ICIP.2012.6467456
   Wang T, 2014, IEEE T INF FOREN SEC, V9, P988, DOI 10.1109/TIFS.2014.2315971
   Wang T, 2013, SENSORS-BASEL, V13, P17130, DOI 10.3390/s131217130
   Xinyi Cui, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3161, DOI 10.1109/CVPR.2011.5995558
   Yang JM, 2012, PROC CVPR IEEE, P2296, DOI 10.1109/CVPR.2012.6247940
   Yuan JS, 2011, IEEE T PATTERN ANAL, V33, P1728, DOI 10.1109/TPAMI.2011.38
   Zhang L, 2013, PROC CVPR IEEE, P1838, DOI 10.1109/CVPR.2013.240
   Zhang YH, 2012, IEEE IMAGE PROC, P2689, DOI 10.1109/ICIP.2012.6467453
   Zhang YH, IEEE T CIRC IN PRESS
NR 52
TC 25
Z9 27
U1 4
U2 28
PU SPRINGER
PI DORDRECHT
PA VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN 1380-7501
EI 1573-7721
J9 MULTIMED TOOLS APPL
JI Multimed. Tools Appl.
PD NOV
PY 2016
VL 75
IS 22
BP 14617
EP 14639
DI 10.1007/s11042-016-3316-3
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic
SC Computer Science; Engineering
GA EA6YQ
UT WOS:000386775500037
DA 2020-02-19
ER

PT J
AU Shi, ZL
   Ye, YD
   Wu, YP
AF Shi, Zenglin
   Ye, Yangdong
   Wu, Yunpeng
TI Rank-based pooling for deep convolutional neural networks
SO NEURAL NETWORKS
LA English
DT Article
DE Pooling; Deep learning; Image classification; Convolutional neural
   network
ID RECOGNITION
AB Pooling is a key mechanism in deep convolutional neural networks (CNNs) which helps to achieve translation invariance. Numerous studies, both empirically and theoretically, show that pooling consistently boosts the performance of the CNNs. The conventional pooling methods are operated on activation values. In this work, we alternatively propose rank-based pooling. It is derived from the observations that ranking list is invariant under changes of activation values in a pooling region, and thus rank-based pooling operation may achieve more robust performance. In addition, the reasonable usage of rank can avoid the scale problems encountered by value-based methods. The novel pooling mechanism can be regarded as an instance of weighted pooling where a weighted sum of activations is used to generate the pooling output. This pooling mechanism can also be realized as rank-based average pooling (RAP), rank-based weighted pooling (RWP) and rank-based stochastic pooling (RSP) according to different weighting strategies. As another major contribution, we present a novel criterion to analyze the discriminant ability of various pooling methods, which is heavily under-researched in machine learning and computer vision community. Experimental results on several image benchmarks show that rank-based pooling outperforms the existing pooling methods in classification performance. We further demonstrate better performance on CIFAR datasets by integrating RSP into Network-in-Network. (C) 2016 Elsevier Ltd. All rights reserved.
C1 [Shi, Zenglin; Ye, Yangdong; Wu, Yunpeng] Zhengzhou Univ, Sch Informat Engn, Zhengzhou 450052, Peoples R China.
RP Ye, YD (reprint author), Zhengzhou Univ, Sch Informat Engn, Zhengzhou 450052, Peoples R China.
EM ieydye@zzu.edu.cn
OI Wu, Yunpeng/0000-0002-0648-868X
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61170223, 61502432, 61502434]
FX This work is supported by the National Natural Science Foundation of
   China under Grant No. 61170223, the National Natural Science Foundation
   of China under Grant No. 61502432 and the National Natural Science
   Foundation of China under Grant No. 61502434.
CR Bing Xu T. C., 2015, CORR
   Boureau Y., 2010, P 27 INT C MACH LEAR, P111, DOI DOI 10.1016/J.NEUNET.2012.02.023
   Boureau YL, 2011, IEEE I CONF COMP VIS, P2651, DOI 10.1109/ICCV.2011.6126555
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Ciresan D, 2012, PROC CVPR IEEE, P3642, DOI 10.1109/CVPR.2012.6248110
   Dalal N, 2005, PROC CVPR IEEE, P886
   Feng J., 2011, 2011 IEEE C COMP VIS, P2609
   FUKUSHIMA K, 1982, PATTERN RECOGN, V15, P455, DOI 10.1016/0031-3203(82)90024-3
   Goodfellow I. J., 2013, JMLR W CP, P1319
   Graham B, 2015, ABS14126071 CORR
   Gulcehre Caglar, 2014, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2014. Proceedings: LNCS 8724, P530, DOI 10.1007/978-3-662-44848-9_34
   He K., 2015, ABS150201852 CORR
   Hinton G. E., 2012, ABS12070580 CORR
   HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837
   Jia Y., 2014, ABS14085093 CORR
   Kahou S. E., 2015, ABS150301800 CORR
   Koenderink JJ, 1999, INT J COMPUT VISION, V31, P159, DOI 10.1023/A:1008065931878
   Krizhevsky A., 2009, LEARNING MULTIPLE LA, V1, P7
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y., 2004, 2004 IEEE COMP SOC C, V2
   Lee C. Y., 2014, DEEPLY SUPERVISED NE, P562
   Lin M, 2014, ABS13124400 CORR
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Maas A. L., 2013, P 27 INT C MACH LEAR, V30
   Meng Cai, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P3266, DOI 10.1109/ICASSP.2014.6854204
   Michalewicz Z, 2013, GENETIC ALGORITHMS D
   Nair V, 2010, ICML, V27, P807, DOI DOI 10.0RG/PAPERS/432.PDF
   Riesenhuber M, 1999, NAT NEUROSCI, V2, P1019
   Sainath TN, 2015, NEURAL NETWORKS, V64, P39, DOI 10.1016/j.neunet.2014.08.005
   Sermanet P, 2012, INT C PATT RECOG, P3288
   Silva LM, 2008, NEURAL NETWORKS, V21, P1302, DOI 10.1016/j.neunet.2008.04.004
   Simonyan K, 2015, ABS14091556 CORR
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Srivastava Nitish, 2013, THESIS
   Srivastava R. K., 2013, ADV NEURAL INFORM PR, P2310
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Wang M, 2014, ABS150604701 CORR
   Zeiler M.D., 2013, ABS13013557 CORR
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
NR 40
TC 15
Z9 17
U1 2
U2 29
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0893-6080
EI 1879-2782
J9 NEURAL NETWORKS
JI Neural Netw.
PD NOV
PY 2016
VL 83
BP 21
EP 31
DI 10.1016/j.neunet.2016.07.003
PG 11
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA EA1AI
UT WOS:000386320300003
PM 27543927
DA 2020-02-19
ER

PT J
AU Yin, JT
   Zhao, WT
AF Yin, Jiateng
   Zhao, Wentian
TI Fault diagnosis network design for vehicle on-board equipments of
   high-speed railway: A deep learning approach
SO ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE
LA English
DT Article
DE Deep learning; High-speed railway; Vehicle on-board equipments; Fault
   diagnosis
ID ALGORITHMS; SYSTEM
AB With the rapid development of high-speed railways (HSRs) throughout the world, the fault diagnosis systems of vehicle on-board equipments (VOBEs) for high speed trains have received increasing attention. Since the faults of VOBEs in HSRs are usually uncertain and complex, the current fault diagnosis methods are mainly based on manual judgement in real-world operations, which is generally inefficient and insecurity with the big rail traffic data. In this paper, we propose an automated diagnosis network of VOBE for high-speed train via a deep learning approach. First, we propose a mathematical model to formulate the fault diagnosis problem in HSRs, involving the definition of fault evidence vectors and reason vectors by analyzing the real-world fault data that are collected in Wuhan-Guangzhou high speed railway. Then, a deep belief network (DBN) and its training procedures are developed on the basis of Restricted Boltzmann Machine (RBM). Finally, the proposed diagnosis network is trained and validated with real-world data. Furthermore, we compare the DBN-based fault diagnosis network with k-nearest neighbor (KNN) and ANN-BP (artificial neural network with back propagations). The results indicate that, the developed DBN outperforms both KNN and ANN-BP, and improves the accuracy of fault diagnosis for VOBEs to 90-95% in HSRs.
C1 [Yin, Jiateng] Beijing Jiaotong Univ, State Key Lab Rail Traff Control & Safety, Beijing 100044, Peoples R China.
   [Yin, Jiateng] Univ Wisconsin, Dept Civil & Environm Engn, Madison, WI 53706 USA.
   [Zhao, Wentian] Beijing Urban Construct Design & Dev Grp Co Ltd, Beijing 100037, Peoples R China.
RP Yin, JT (reprint author), Beijing Jiaotong Univ, State Key Lab Rail Traff Control & Safety, Beijing 100044, Peoples R China.
EM jiatyin@bjtu.edu.cn
FU Fundamental Research Funds for the Central UniversitiesFundamental
   Research Funds for the Central Universities [2015YJS026]; Wisconsin
   Traffic Operations and Safety Laboratory at University of Wisconsin,
   Madison; China Scholarship CouncilChina Scholarship Council
FX This work was supported by the Fundamental Research Funds for the
   Central Universities (No. 2015YJS026), the Wisconsin Traffic Operations
   and Safety Laboratory at University of Wisconsin, Madison, and the China
   Scholarship Council.
CR Abdel-Zaher AM, 2016, EXPERT SYST APPL, V46, P139, DOI 10.1016/j.eswa.2015.10.015
   Bagheri B., 2010, 2010 IEEE INT C EL M
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Chen D., 2015, NAT PROD RES, P1
   Chen DW, 2015, APPL SOFT COMPUT, V30, P758, DOI 10.1016/j.asoc.2015.01.017
   Chen J, 2008, CONTROL ENG PRACT, V16, P585, DOI 10.1016/j.conengprac.2007.06.007
   Dahl GE, 2012, IEEE T AUDIO SPEECH, V20, P30, DOI 10.1109/TASL.2011.2134090
   Dong HR, 2010, IEEE CIRC SYST MAG, V10, P6, DOI 10.1109/MCAS.2010.936782
   Fischer A, 2014, PATTERN RECOGN, V47, P25, DOI 10.1016/j.patcog.2013.05.025
   Givoni M, 2006, TRANSPORT REV, V26, P593, DOI 10.1080/01441640600589319
   Hinton G., 2012, NEURAL NETWORKS TRIC, V9, P599, DOI DOI 10.1007/978-3-642-35289-8_32
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Kollar T, 2010, ACMIEEE INT CONF HUM, P259, DOI 10.1109/HRI.2010.5453186
   LEHTOKANGAS M, 1995, NEURAL COMPUT, V7, P982, DOI 10.1162/neco.1995.7.5.982
   Oukhellou L, 2010, ENG APPL ARTIF INTEL, V23, P117, DOI 10.1016/j.engappai.2009.06.005
   Salakhutdinov R., 2008, P 25 INT C MACH LEAR, P872, DOI [10.1145/1390156.1390266, DOI 10.1145/1390156.1390266]
   Saravanan N, 2010, EXPERT SYST APPL, V37, P4168, DOI 10.1016/j.eswa.2009.11.006
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Tran VT, 2014, EXPERT SYST APPL, V41, P4113, DOI 10.1016/j.eswa.2013.12.026
   Wu XD, 2008, KNOWL INF SYST, V14, P1, DOI 10.1007/s10115-007-0114-2
   Yin JT, 2014, IEEE T INTELL TRANSP, V15, P2561, DOI 10.1109/TITS.2014.2320757
   Zhang Y, 2016, ENG APPL ARTIF INTEL, V50, P245, DOI 10.1016/j.engappai.2016.01.032
   Zhao LH, 2013, P I MECH ENG F-J RAI, V227, P333, DOI 10.1177/0954409713480453
   Zhao Y, 2014, 2014 IEEE 17TH INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS (ITSC), P900, DOI 10.1109/ITSC.2014.6957803
NR 25
TC 41
Z9 42
U1 9
U2 97
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0952-1976
EI 1873-6769
J9 ENG APPL ARTIF INTEL
JI Eng. Appl. Artif. Intell.
PD NOV
PY 2016
VL 56
BP 250
EP 259
DI 10.1016/j.engappai.2016.10.002
PG 10
WC Automation & Control Systems; Computer Science, Artificial Intelligence;
   Engineering, Multidisciplinary; Engineering, Electrical & Electronic
SC Automation & Control Systems; Computer Science; Engineering
GA EC3VH
UT WOS:000388054400018
DA 2020-02-19
ER

PT J
AU Sigtia, S
   Stark, AM
   Krstulovic, S
   Plumbley, MD
AF Sigtia, Siddharth
   Stark, Adam M.
   Krstulovic, Sacha
   Plumbley, Mark D.
TI Automatic Environmental Sound Recognition: Performance Versus
   Computational Cost
SO IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING
LA English
DT Article
DE Automatic environmental sound recognition; computational auditory scene
   analysis; deep learning; machine learning
AB In the context of the Internet of Things, sound sensing applications are required to run on embedded platforms where notions of product pricing and form factor impose hard constraints on the available computing power. Whereas Automatic Environmental Sound Recognition (AESR) algorithms are most often developed with limited consideration for computational cost, this paper seeks which AESR algorithm can make the most of a limited amount of computing power by comparing the sound classification performance as a function of its computational cost. Results suggest that Deep Neural Networks yield the best ratio of sound classification accuracy across a range of computational costs, while Gaussian Mixture Models offer a reasonable accuracy at a consistently small cost, and Support Vector Machines stand between both in terms of compromise between accuracy and computational cost.
C1 [Sigtia, Siddharth; Stark, Adam M.] Queen Mary Univ London, London E1 4NS, England.
   [Krstulovic, Sacha] Audio Analyt Ltd, Cambridge CB2 3AH, England.
   [Plumbley, Mark D.] Univ Surrey, Guildford GU2 7XH, Surrey, England.
RP Sigtia, S (reprint author), Queen Mary Univ London, London E1 4NS, England.
EM s.s.sigtia@qmul.ac.uk; adamstark.uk@gmail.com;
   sacha.krstulovic@audioanalytic.com; m.plumbley@surrey.ac.uk
RI Plumbley, Mark D/A-7298-2008
OI Plumbley, Mark D/0000-0002-9708-1075
FU Innovate U.K.; U.K. Engineering and Physical Sciences Research
   CouncilEngineering & Physical Sciences Research Council (EPSRC)
   [EP/M507088/1, EP/N014111/1]; Audio Analytic Ltd., Cambridge, U.K.;
   Engineering and Physical Sciences Research CouncilEngineering & Physical
   Sciences Research Council (EPSRC) [EP/N014111/1, EP/M507088/1]
FX This work was supported in part by Innovate U.K., in part by the U.K.
   Engineering and Physical Sciences Research Council under Grants
   EP/M507088/1 and EP/N014111/1, and in part by Audio Analytic Ltd.,
   Cambridge, U.K. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Simon Doclo.
CR Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027
   Atrey PK, 2006, INT CONF ACOUST SPEE, P813
   Bengio Y, 2013, INT CONF ACOUST SPEE, P8624, DOI 10.1109/ICASSP.2013.6639349
   Beritelli F., 2008, IEEE INT SIGNAL P CO, P1
   Bishop CM, 2006, PATTERN RECOGNITION
   Bonfigli R, 2014, 2014 6TH EUROPEAN EMBEDDED DESIGN IN EDUCATION AND RESEARCH CONFERENCE (EDERC), P307, DOI 10.1109/EDERC.2014.6924410
   Bonomi F., 2014, BIG DATA INTERNET TH, P169, DOI DOI 10.1007/978-3-319-05029-4_7
   Burges CJC, 1998, DATA MIN KNOWL DISC, V2, P121, DOI 10.1023/A:1009715923555
   Chachada S, 2014, APSIPA TRANS SIGNAL, V3, DOI 10.1017/ATSIP.2014.12
   Chen L, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P781, DOI 10.1109/ICME.2006.262954
   Clavel C, 2005, 2005 IEEE International Conference on Multimedia and Expo (ICME), Vols 1 and 2, P1307
   Foster Peter, 2015, 2015 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA). Proceedings, P1, DOI 10.1109/WASPAA.2015.7336899
   Gencoglu O, 2014, EUR SIGNAL PR CONF, P506
   Glorot X., 2011, P 14 INT C ART INT S, V15, P315
   Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947
   Gubbi J, 2013, FUTURE GENER COMP SY, V29, P1645, DOI 10.1016/j.future.2013.01.010
   Gupta S., 2015, P 32 INT C MACH LEAR, P1737
   Istrate D, 2006, IEEE T INF TECHNOL B, V10, P264, DOI 10.1109/TITB.2005.859889
   Japkowicz N, 2011, EVALUATING LEARNING
   LeCun Y, 1998, LECT NOTES COMPUT SC, V1524, P9
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Martin A., 1997, P EUR 97 RHOD GREEC, P1899
   Medaglia CM, 2010, INTERNET OF THINGS, P389, DOI 10.1007/978-1-4419-1674-7_38
   Mermelstein P., 1976, SR47 HASK LAB
   MONCRIEFF S, 2001, P IEEE INT C MULT EX, P989
   Murphy K. P., 2006, NAIVE BAYES CLASSIFI
   Murphy KP., 2012, MACHINE LEARNING PRO
   Parhami B., 2009, COMPUTER ARITHMETIC
   Portelo J, 2009, INT CONF ACOUST SPEE, P1973, DOI 10.1109/ICASSP.2009.4959998
   Principi E, 2015, EXPERT SYST APPL, V42, P5668, DOI 10.1016/j.eswa.2015.02.036
   Rabiner L. R., 1993, FUNDAMENTALS SPEECH
   Radhakrishnan R, 2005, IEEE WORK APPL SIG, P158, DOI 10.1109/ASPAA.2005.1540194
   Reynolds D., 2015, ENCY BIOMETRICS, P827, DOI [DOI 10.1007/978-1-4899-7488-4_196, 10.1007/978-1-4899-7488-4]
   Reynolds DA, 2000, DIGIT SIGNAL PROCESS, V10, P19, DOI 10.1006/dspr.1999.0361
   Rouas J.-L., 2006, P IEEE INT TRANSP SY, P733, DOI DOI 10.1109/ITSC.2006.1706829
   Salamon J, 2015, EUR SIGNAL PR CONF, P724, DOI 10.1109/EUSIPCO.2015.7362478
   Salamon J, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P1041, DOI 10.1145/2647868.2655045
   Salamon J, 2015, INT CONF ACOUST SPEE, P171, DOI 10.1109/ICASSP.2015.7177954
   Sawhney N., 1997, SITUATIONAL AWARENES
   Senior A, 2015, INT CONF ACOUST SPEE, P4585, DOI 10.1109/ICASSP.2015.7178839
   Sitte R, 2007, PROCEEDINGS OF THE FOURTH IASTED INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING, PATTERN RECOGNITION, AND APPLICATIONS, P281
   SMITH SW, 1997, SCI ENG GUIDE DIGITA
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Stager M, 2003, SEVENTH IEEE INTERNATIONAL SYMPOSIUM ON WEARABLE COMPUTERS, PROCEEDINGS, P12, DOI 10.1109/ISWC.2003.1241387
   Stowell D, 2015, IEEE T MULTIMEDIA, V17, P1733, DOI 10.1109/TMM.2015.2428998
   Stowell D, 2014, PEERJ, V2, DOI 10.7717/peerj.488
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Temko A., 2006, Multimodal Technologies for Perception of Humans. First International Evaluation Workshop on Classification of Events, Activities and Relationships, CLEAR 2006. Revised Selected Papers (Lecture Notes in Computer Science Vol.4122), P311
   Toyoda Y, 2004, FOURTH INTERNATIONAL CONFERENCE ON COMPUTER AND INFORMATION TECHNOLOGY, PROCEEDINGS, P123, DOI 10.1109/CIT.2004.1357184
   Vacher Michel, 2010, 2010 12th IEEE International Conference on e-Health Networking, Applications and Services (Healthcom 2010), P330, DOI 10.1109/HEALTH.2010.5556546
   Valenzise G, 2007, 2007 IEEE CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P21, DOI 10.1109/AVSS.2007.4425280
   Vidal E. G., 2013, P AUSTR C ROB AUT DE
   VIRTANEN T, 2007, [No title captured], P82
   Wang D.L., 2006, COMPUTATIONAL AUDITO
   Yamakawa Nobuhide, 2011, Modern Approaches in Applied Intelligence. Proceedings 24th International Conference on Industrial Engineering and Other Applications of Applied Intelligent Systems (IEA/AIE 2011), P1, DOI 10.1007/978-3-642-21827-9_1
   Zeiler Matthew D, 2012, ARXIV12125701
   ZUE V, 1990, SPEECH COMMUN, V9, P351, DOI 10.1016/0167-6393(90)90010-7
NR 57
TC 17
Z9 17
U1 0
U2 35
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 2329-9290
EI 2329-9304
J9 IEEE-ACM T AUDIO SPE
JI IEEE-ACM Trans. Audio Speech Lang.
PD NOV
PY 2016
VL 24
IS 11
BP 2096
EP 2107
DI 10.1109/TASLP.2016.2592698
PG 12
WC Acoustics; Engineering, Electrical & Electronic
SC Acoustics; Engineering
GA DV1JS
UT WOS:000382677800018
OA Green Accepted
DA 2020-02-19
ER

PT J
AU Zhang, H
   Gong, MG
   Zhang, PZ
   Su, LZ
   Shi, J
AF Zhang, Hui
   Gong, Maoguo
   Zhang, Puzhao
   Su, Linzhi
   Shi, Jiao
TI Feature-Level Change Detection Using Deep Representation and Feature
   Change Analysis for Multispectral Imagery
SO IEEE GEOSCIENCE AND REMOTE SENSING LETTERS
LA English
DT Article
DE Change detection; change vector analysis (CVA); cosine angle distance
   (CAD); deep belief networks (DBNs); multi-spectral images
AB Due to the noise interference and redundancy in multispectral images, it is promising to transform the available spectral channels into a suitable feature space for relieving noise and reducing the redundancy. The booming of deep learning provides a flexible tool to learn abstract and invariant features directly from the data in their raw forms. In this letter, we propose an unsupervised change detection technique for multispectral images, in which we combine deep belief networks (DBNs) and feature change analysis to highlight changes. First, a DBN is established to capture the key information for discrimination and suppress the irrelevant variations. Second, we map bitemporal change feature into a 2-D polar domain to characterize the change information. Finally, an unsupervised clustering algorithm is adopted to distinguish the changed and unchanged pixels, and then, the changed types can be identified by classifying the changed pixels into several classes according to the directions of feature changes. The experimental results demonstrate the effectiveness and robustness of the proposed method.
C1 [Zhang, Hui; Gong, Maoguo; Zhang, Puzhao; Su, Linzhi; Shi, Jiao] Xidian Univ, Minist Educ China, Key Lab Intelligent Percept & Image Understanding, Xian 710071, Peoples R China.
   [Zhang, Hui] Xidian Univ, Sch Microelect, Dept Integrated Circuit Design & Integrated Syst, Xian 710071, Peoples R China.
   [Shi, Jiao] Northwestern Polytech Univ, Sch Elect & Informat, Xian 710072, Peoples R China.
RP Gong, MG (reprint author), Xidian Univ, Minist Educ China, Key Lab Intelligent Percept & Image Understanding, Xian 710071, Peoples R China.
EM gong@ieee.org
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61273317, 61422209]; National Program for the
   Support of Top-Notch Young Professionals of China; Specialized Research
   Fund for the Doctoral Program of Higher EducationSpecialized Research
   Fund for the Doctoral Program of Higher Education (SRFDP)
   [20130203110011]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61273317 and Grant 61422209, by the
   National Program for the Support of Top-Notch Young Professionals of
   China, and by the Specialized Research Fund for the Doctoral Program of
   Higher Education under Grant 20130203110011.
CR Bovolo F, 2007, IEEE T GEOSCI REMOTE, V45, P218, DOI 10.1109/TGRS.2006.885408
   Bovolo F, 2012, IEEE T GEOSCI REMOTE, V50, P2196, DOI 10.1109/TGRS.2011.2171493
   Carvalho OA, 2011, REMOTE SENS-BASEL, V3, P2473, DOI 10.3390/rs3112473
   Deng JS, 2008, INT J REMOTE SENS, V29, P4823, DOI 10.1080/01431160801950162
   Ding K, 2015, IEEE GEOSCI REMOTE S, V12, P577, DOI 10.1109/LGRS.2014.2351807
   Eismann MT, 2008, IEEE T GEOSCI REMOTE, V46, P237, DOI 10.1109/TGRS.2007.907973
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Korenius T, 2007, INFORM SCIENCES, V177, P4893, DOI 10.1016/j.ins.2007.05.027
   Krinidis S, 2010, IEEE T IMAGE PROCESS, V19, P1328, DOI 10.1109/TIP.2010.2040763
   Kuremoto T, 2014, NEUROCOMPUTING, V137, P47, DOI 10.1016/j.neucom.2013.03.047
   Nielsen AA, 2007, IEEE T IMAGE PROCESS, V16, P463, DOI 10.1109/TIP.2006.888195
   Oyedotun Oyebade K., 2015, International Journal of Intelligent Systems and Applications, V7, P1, DOI 10.5815/ijisa.2015.07.01
   Tewkesbury AP, 2015, REMOTE SENS ENVIRON, V160, P1, DOI 10.1016/j.rse.2015.01.006
NR 13
TC 27
Z9 27
U1 6
U2 37
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1545-598X
EI 1558-0571
J9 IEEE GEOSCI REMOTE S
JI IEEE Geosci. Remote Sens. Lett.
PD NOV
PY 2016
VL 13
IS 11
BP 1666
EP 1670
DI 10.1109/LGRS.2016.2601930
PG 5
WC Geochemistry & Geophysics; Engineering, Electrical & Electronic; Remote
   Sensing; Imaging Science & Photographic Technology
SC Geochemistry & Geophysics; Engineering; Remote Sensing; Imaging Science
   & Photographic Technology
GA EA0DL
UT WOS:000386255600016
DA 2020-02-19
ER

PT J
AU Hu, JL
   Lu, JW
   Tan, YP
AF Hu, Junlin
   Lu, Jiwen
   Tan, Yap-Peng
TI Deep Metric Learning for Visual Tracking
SO IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY
LA English
DT Article
DE Deep learning; metric learning; visual tracking
ID OBJECT TRACKING; REPRESENTATIONS; MODEL
AB In this paper, we propose a deep metric learning (DML) approach for robust visual tracking under the particle filter framework. Unlike most existing appearance-based visual trackers, which use hand-crafted similarity metrics, our DML tracker learns a nonlinear distance metric to classify the target object and background regions using a feed-forward neural network architecture. Since there are usually large variations in visual objects caused by varying deformations, illuminations, occlusions, motions, rotations, scales, and cluttered backgrounds, conventional linear similarity metrics cannot work well in such scenarios. To address this, our proposed DML tracker first learns a set of hierarchical nonlinear transformations in the feed-forward neural network to project both the template and particles into the same feature space where the intra-class variations of positive training pairs are minimized and the interclass variations of negative training pairs are maximized simultaneously. Then, the candidate that is most similar to the template in the learned deep network is identified as the true target. Experiments on the benchmark data set including 51 challenging videos show that our DML tracker achieves a very competitive performance with the state-of-the-art trackers.
C1 [Hu, Junlin; Tan, Yap-Peng] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
   [Lu, Jiwen] Tsinghua Univ, Dept Automat, Beijing 100084, Peoples R China.
RP Lu, JW (reprint author), Tsinghua Univ, Dept Automat, Beijing 100084, Peoples R China.
EM jhu007@e.ntu.edu.sg; elujiwen@gmail.com; eyptan@ntu.edu.sg
RI Tan, Yap-Peng/A-5158-2011; Lu, Jiwen/C-5291-2009
OI Lu, Jiwen/0000-0002-6121-5529
FU Rapid-Rich Object Search Laboratory through the National Research
   Foundation, Singapore
FX This work was supported by the Rapid-Rich Object Search Laboratory
   through the National Research Foundation, Singapore, under its
   Interactive Digital Media within the Strategic Research Programme. This
   paper was recommended by Associate Editor P. Salembier.
CR Andrew G., 2013, P 30 INT C MACH LEAR, P1247
   Babenko B, 2009, PROC CVPR IEEE, P983, DOI 10.1109/CVPRW.2009.5206737
   Bao CL, 2012, PROC CVPR IEEE, P1830, DOI 10.1109/CVPR.2012.6247881
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Black MJ, 1998, INT J COMPUT VISION, V26, P63, DOI 10.1023/A:1007939232436
   Cai X, 2012, P 20 ACM INT C MULT, P749
   Comaniciu D, 2003, IEEE T PATTERN ANAL, V25, P564, DOI 10.1109/TPAMI.2003.1195991
   Davis J. V., 2007, P 24 INT C MACH LEAR, P209, DOI DOI 10.1145/1273496.1273523
   Elgammal A, 2003, PROC CVPR IEEE, P781
   Fan JL, 2010, IEEE T NEURAL NETWOR, V21, P1610, DOI 10.1109/TNN.2010.2066286
   Gao J, 2014, LECT NOTES COMPUT SC, V8691, P188, DOI 10.1007/978-3-319-10578-9_13
   Glorot X., 2010, JLMR P TRACK, p[249, 2010], DOI DOI 10.1177/1753193409103364.
   Gordon N, 2001, SEQUENTIAL MONTE CAR
   Grabner Helmut, 2006, P BMVC, V1, P6, DOI DOI 10.5244/C.20.6
   Hager GD, 2004, PROC CVPR IEEE, P790
   Hare S, 2011, IEEE I CONF COMP VIS, P263, DOI 10.1109/ICCV.2011.6126251
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Henriques JF, 2012, LECT NOTES COMPUT SC, V7575, P702, DOI 10.1007/978-3-642-33765-9_50
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hu JL, 2014, PROC CVPR IEEE, P1875, DOI 10.1109/CVPR.2014.242
   Huang GB, 2012, PROC CVPR IEEE, P2518, DOI 10.1109/CVPR.2012.6247968
   Jepson AD, 2003, IEEE T PATTERN ANAL, V25, P1296, DOI 10.1109/TPAMI.2003.1233903
   Jia X, 2012, PROC CVPR IEEE, P1822, DOI 10.1109/CVPR.2012.6247880
   Jiang N, 2012, PROC CVPR IEEE, P1956, DOI 10.1109/CVPR.2012.6247897
   Jin J, 2013, INF SCI SYST CISS 20, P1
   Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kwon J, 2010, PROC CVPR IEEE, P1269, DOI 10.1109/CVPR.2010.5539821
   Le Q. V., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3361, DOI 10.1109/CVPR.2011.5995496
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee H., 2009, P 26 ANN INT C MACH, P609, DOI DOI 10.1145/1553374.1553453
   Li H, 2015, P BMVC, P1, DOI [10.5244/C.28.56, DOI 10.5244/C.28.56]
   Li HX, 2015, LECT NOTES COMPUT SC, V9007, P194, DOI 10.1007/978-3-319-16814-2_13
   Li HX, 2011, PROC CVPR IEEE, P1305, DOI 10.1109/CVPR.2011.5995483
   Li X, 2013, DRUG ALCOHOL DEPEN, V1, P1, DOI DOI 10.1039/C2TC00283C)
   Li X, 2012, PROC CVPR IEEE, P1760, DOI 10.1109/CVPR.2012.6247872
   Mei X, 2009, IEEE I CONF COMP VIS, P1436, DOI 10.1109/ICCV.2009.5459292
   Nowlan S. J., 1994, P ADV NEUR INF PROC, P901
   Ranzato MA, 2007, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2007.383157
   Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7
   Shen CH, 2010, IEEE T CIRC SYST VID, V20, P119, DOI 10.1109/TCSVT.2009.2031393
   Smeulders AWM, 2014, IEEE T PATTERN ANAL, V36, P1442, DOI 10.1109/TPAMI.2013.230
   Tsagkatakis G, 2011, IEEE T CIRC SYST VID, V21, P1810, DOI 10.1109/TCSVT.2011.2133970
   Vincent P, 2008, P 25 INT C MACH LEAR, P1096, DOI DOI 10.1145/1390156.1390294
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   VIOLA P, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P16, DOI 10.1109/ICCV.1995.466930
   Wang D, 2013, PROC CVPR IEEE, P2371, DOI 10.1109/CVPR.2013.307
   Wang N., 2013, ADV NEURAL INFORM PR, P809
   Wang NY, 2013, IEEE I CONF COMP VIS, P657, DOI 10.1109/ICCV.2013.87
   Wang XY, 2010, LECT NOTES COMPUT SC, V6313, P200
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Wu YW, 2014, IEEE T CIRC SYST VID, V24, P865, DOI 10.1109/TCSVT.2013.2291283
   Xiao ZY, 2014, IEEE T CIRC SYST VID, V24, P1301, DOI 10.1109/TCSVT.2013.2291355
   Yan SC, 2007, IEEE T PATTERN ANAL, V29, P40, DOI 10.1109/TPAMI.2007.250598
   Yang F, 2014, IEEE T CIRC SYST VID, V24, P242, DOI 10.1109/TCSVT.2013.2276145
   Zhang KH, 2013, IEEE T IMAGE PROCESS, V22, P4664, DOI 10.1109/TIP.2013.2277800
   Zhang KH, 2012, LECT NOTES COMPUT SC, V7574, P864, DOI 10.1007/978-3-642-33712-3_62
   Zhang L, 2014, IEEE T PATTERN ANAL, V36, P756, DOI 10.1109/TPAMI.2013.221
   Zhang TZ, 2012, PROC CVPR IEEE, P2042, DOI 10.1109/CVPR.2012.6247908
   Zhong W, 2014, IEEE T IMAGE PROCESS, V23, P2356, DOI 10.1109/TIP.2014.2313227
NR 60
TC 37
Z9 37
U1 2
U2 42
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1051-8215
EI 1558-2205
J9 IEEE T CIRC SYST VID
JI IEEE Trans. Circuits Syst. Video Technol.
PD NOV
PY 2016
VL 26
IS 11
BP 2056
EP 2068
DI 10.1109/TCSVT.2015.2477936
PG 13
WC Engineering, Electrical & Electronic
SC Engineering
GA EC1US
UT WOS:000387894100007
DA 2020-02-19
ER

PT J
AU Han, ZZ
   Liu, ZB
   Han, JW
   Vong, CM
   Bu, SH
   Li, XL
AF Han, Zhizhong
   Liu, Zhenbao
   Han, Junwei
   Vong, Chi-Man
   Bu, Shuhui
   Li, Xuelong
TI Unsupervised 3D Local Feature Learning by Circle Convolutional
   Restricted Boltzmann Machine
SO IEEE TRANSACTIONS ON IMAGE PROCESSING
LA English
DT Article
DE Circle convolutional restricted Boltzmann machine; deep learning;
   projection distance distribution; geometry processing; fourier transform
   modulus; 3D shapes
ID 3-D OBJECT RETRIEVAL; MODEL; DESCRIPTORS; RECOGNITION; ROBUST;
   REPRESENTATION; SIMILARITY
AB Extracting local features from 3D shapes is an important and challenging task that usually requires carefully designed 3D shape descriptors. However, these descriptors are hand-crafted and require intensive human intervention with prior knowledge. To tackle this issue, we propose a novel deep learning model, namely circle convolutional restricted Boltzmann machine (CCRBM), for unsupervised 3D local feature learning. CCRBM is specially designed to learn from raw 3D representations. It effectively overcomes obstacles such as irregular vertex topology, orientation ambiguity on the 3D surface, and rigid or slightly non-rigid transformation invariance in the hierarchical learning of 3D data that cannot be resolved by the existing deep learning models. Specifically, by introducing the novel circle convolution, CCRBM holds a novel ring-like multi-layer structure to learn 3D local features in a structure preserving manner. Circle convolution convolves across 3D local regions via rotating a novel circular sector convolution window in a consistent circular direction. In the process of circle convolution, extra points are sampled in each 3D local region and projected onto the tangent plane of the center of the region. In this way, the projection distances in each sector window are employed to constitute a novel local raw 3D representation called projection distance distribution (PDD). In addition, to eliminate the initial location ambiguity of a sector window, the Fourier transform modulus is used to transform the PDD into the Fourier domain, which is then conveyed to CCRBM. Experiments using the learned local features are conducted on three aspects: global shape retrieval, partial shape retrieval, and shape correspondence. The experimental results show that the learned local features outperform other state-of-the-art 3D shape descriptors.
C1 [Han, Zhizhong; Liu, Zhenbao; Han, Junwei; Bu, Shuhui] Northwestern Polytech Univ, Xian 710072, Peoples R China.
   [Vong, Chi-Man] Univ Macau, Dept Comp & Informat Sci, Macau 999078, Peoples R China.
   [Li, Xuelong] Chinese Acad Sci, State Key Lab Transient Opt & Photon, Ctr OPT IMagery Anal & Learning, Xian Inst Opt & Precis Mech, Xian 710119, Peoples R China.
RP Liu, ZB; Han, JW (reprint author), Northwestern Polytech Univ, Xian 710072, Peoples R China.
EM h312h@mail.nwpu.edu.cn; liuzhenbao@nwpu.edu.cn; jhan@nwpu.edu.cn;
   cmvong@umac.mo; bushuhui@nwpu.edu.cn; xuelongli@opt.ac.cn
RI Li, Xuelong/Z-3785-2019
OI Li, Xuelong/0000-0002-0019-4197
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61672430, 61522207, 61573284, 61473231]; NWPU Basic
   Research Fund [3102016JKBJJGZ08]; State Key Laboratory of CAD and CG in
   Zhejiang University [A1509]; Open Research Foundation of State Key
   Laboratory of Digital Manufacturing Equipment and Technology in Huazhong
   University of Science and Technology [DMETKF2015009]; Fund of National
   Engineering and Research Center for Commercial Aircraft Manufacturing
   [SAMC14-JS-15-045]; Shaanxi Natural Science Fund [2015JM6344];
   University of Macau [MYRG2016-00134-FST]; FDCT Macau [050/2015/A]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61672430, Grant 61522207, Grant
   61573284, and Grant 61473231, in part by NWPU Basic Research Fund under
   Grant 3102016JKBJJGZ08, in part by the Open Fund of State Key Laboratory
   of CAD and CG in Zhejiang University under Grant A1509, in part by the
   Open Research Foundation of State Key Laboratory of Digital
   Manufacturing Equipment and Technology in Huazhong University of Science
   and Technology under Grant DMETKF2015009, in part by the Fund of
   National Engineering and Research Center for Commercial Aircraft
   Manufacturing under Grant SAMC14-JS-15-045, in part by the Shaanxi
   Natural Science Fund under Grant 2015JM6344, in part by the University
   of Macau Research Funding under Grant MYRG2016-00134-FST, and in part by
   FDCT Macau under Grant 050/2015/A. The associate editor coordinating the
   review of this manuscript and approving it for publication was Dr.
   Weisheng Dong. (Corresponding authors: Zhenbao Liu; Junwei Han.)
CR Anguelov D., 2004, NIPS, V17, P33
   Aubry M, 2011, IEEE I CONF COMP VIS, P1411, DOI 10.1109/ICCV.2011.6126396
   Bai X, 2015, IEEE T PATTERN ANAL, V37, P2361, DOI 10.1109/TPAMI.2015.2424863
   Bai X, 2014, IEEE T IMAGE PROCESS, V23, P3935, DOI 10.1109/TIP.2014.2336542
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Biasotti S, 2006, COMPUT AIDED DESIGN, V38, P1002, DOI 10.1016/j.cad.2006.07.003
   Boscaini D, 2016, COMPUT GRAPH FORUM, V35, P431, DOI 10.1111/cgf.12844
   Boscaini D, 2015, COMPUT GRAPH FORUM, V34, P13, DOI 10.1111/cgf.12693
   Bronstein AM, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1899404.1899405
   Bronstein MM, 2010, PROC CVPR IEEE, P1704, DOI 10.1109/CVPR.2010.5539838
   Bu SH, 2014, IEEE T MULTIMEDIA, V16, P2154, DOI 10.1109/TMM.2014.2351788
   Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669
   Cheng G, 2016, ISPRS J PHOTOGRAMM, V117, P11, DOI 10.1016/j.isprsjprs.2016.03.014
   Cheng G, 2015, IEEE T GEOSCI REMOTE, V53, P4238, DOI 10.1109/TGRS.2015.2393857
   Cornea ND, 2005, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDINGS, P366, DOI 10.1109/SMI.2005.1
   Darom T, 2012, IEEE T IMAGE PROCESS, V21, P2758, DOI 10.1109/TIP.2012.2183142
   Elad A, 2003, IEEE T PATTERN ANAL, V25, P1285, DOI 10.1109/TPAMI.2003.1233902
   Fang Y, 2015, PROC CVPR IEEE, P2319, DOI 10.1109/CVPR.2015.7298845
   Gao Y, 2014, IEEE T IND ELECTRON, V61, P2088, DOI 10.1109/TIE.2013.2262760
   Gao Y, 2012, IEEE T IMAGE PROCESS, V21, P4290, DOI 10.1109/TIP.2012.2199502
   GIORGI D, 2007, [No title captured], P5
   Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947
   Han JW, 2016, IEEE T CYBERNETICS, V46, P487, DOI 10.1109/TCYB.2015.2404432
   Han JW, 2015, IEEE T CIRC SYST VID, V25, P1309, DOI 10.1109/TCSVT.2014.2381471
   Han ZZ, 2017, IEEE T NEUR NET LEAR, V28, P2268, DOI 10.1109/TNNLS.2016.2582532
   Heider P., 2011, P 4 EUR C 3D OBJ RET, P49, DOI DOI 10.2312/3DOR/3DOR11/049-056
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655
   Kazhdan M., 2003, Symposium on Geometry Processing, P156
   Kim J, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1966394.1966403
   Kokkinos I., 2008, IEEE C COMP VIS PATT, P1, DOI DOI 10.1109/CVPR.2008.4587798
   Kokkinos I, 2012, PROC CVPR IEEE, P159, DOI 10.1109/CVPR.2012.6247671
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kuhn HW, 1955, NAV RES LOG, V2, P83, DOI DOI 10.1002/NAV.3800020109
   Lavoue G, 2012, VISUAL COMPUT, V28, P931, DOI 10.1007/s00371-012-0724-x
   Lee H., 2009, P 26 ANN INT C MACH, P609, DOI DOI 10.1145/1553374.1553453
   Leng BA, 2016, INFORM SCIENCES, V366, P188, DOI 10.1016/j.ins.2015.08.007
   Leng B, 2015, SIGNAL PROCESS, V112, P119, DOI 10.1016/j.sigpro.2014.09.005
   Leng B, 2015, NEUROCOMPUTING, V151, P593, DOI 10.1016/j.neucom.2014.06.084
   Leng B, 2015, IEEE T IMAGE PROCESS, V24, P94, DOI 10.1109/TIP.2014.2372618
   Liu YS, 2011, IEEE T PATTERN ANAL, V33, P2538, DOI 10.1109/TPAMI.2011.116
   Liu Z. M., 2014, ADV GEOSCI, V1, P1, DOI DOI 10.1002/ADMI.201400155
   Liu ZM, 2014, INT J ANTENN PROPAG, DOI 10.1155/2014/959386
   Liu ZB, 2016, IEEE T NEUR NET LEAR, V27, P1150, DOI 10.1109/TNNLS.2015.2495148
   Lodder J, 2003, AM MATH MON, V110, P593, DOI 10.2307/3647744
   Lu K, 2014, IEEE T IMAGE PROCESS, V23, P4553, DOI 10.1109/TIP.2014.2343460
   Marini S., 2007, P IEEE SHAP MOD INT, P13
   Masci J., 2015, P IEEE INT C COMP VI, P830
   Osada R, 2002, ACM T GRAPHIC, V21, P807, DOI 10.1145/571647.571648
   Papadakis P, 2010, INT J COMPUT VISION, V89, P177, DOI 10.1007/s11263-009-0281-6
   Peyre G, 2006, INT J COMPUT VISION, V69, P145, DOI 10.1007/s11263-006-6859-3
   Pickup D, 2015, P 8 EUR WORKSH 3D OB, P99
   Reuter M, 2005, P ACM S SOL PHYS MOD, P101, DOI DOI 10.1145/1060244.1060256
   Shapira L, 2008, VISUAL COMPUT, V24, P249, DOI 10.1007/s00371-007-0197-5
   Shi BG, 2015, IEEE SIGNAL PROC LET, V22, P2339, DOI 10.1109/LSP.2015.2480802
   Siddiqi K, 2008, MACH VISION APPL, V19, P261, DOI 10.1007/s00138-007-0097-8
   Socher R., 2012, ADV NEURAL INFORM PR, V3, P665, DOI DOI 10.1002/2014GB005021
   Steinbach M., 2004, NEW DIRECTIONS STAT, P273, DOI DOI 10.1007/978-3-662-08968-2_16
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x
   Tabia H, 2014, PROC CVPR IEEE, P4185, DOI 10.1109/CVPR.2014.533
   Tabia H, 2011, IEEE T PATTERN ANAL, V33, P852, DOI 10.1109/TPAMI.2010.202
   Tierny J, 2009, COMPUT GRAPH FORUM, V28, P41, DOI 10.1111/j.1467-8659.2008.01190.x
   Toldo R., 2009, P 2 EUR C 3D OBJ RET, P21, DOI DOI 10.2312/3DOR/3DOR09/021-028
   van Kaick O, 2011, COMPUT GRAPH FORUM, V30, P1681, DOI 10.1111/j.1467-8659.2011.01884.x
   Wang X., 2010, P INT S 3D DAT PROC, V10, P17
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Zhang WC, 2014, IEEE T IMAGE PROCESS, V23, P5374, DOI 10.1109/TIP.2014.2364113
   Zokai S, 2005, IEEE T IMAGE PROCESS, V14, P1422, DOI 10.1109/TIP.2005.854501
NR 71
TC 14
Z9 14
U1 1
U2 29
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1057-7149
EI 1941-0042
J9 IEEE T IMAGE PROCESS
JI IEEE Trans. Image Process.
PD NOV
PY 2016
VL 25
IS 11
BP 5331
EP 5344
DI 10.1109/TIP.2016.2605920
PG 14
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
SC Computer Science; Engineering
GA DY8LC
UT WOS:000385380500001
PM 28113374
DA 2020-02-19
ER

PT J
AU Liao, LX
   Jin, WJ
   Pavel, R
AF Liao, Linxia
   Jin, Wenjing
   Pavel, Radu
TI Enhanced Restricted Boltzmann Machine With Prognosability Regularization
   for Prognostics and Health Assessment
SO IEEE TRANSACTIONS ON INDUSTRIAL ELECTRONICS
LA English
DT Article
DE Deep learning; feature extraction; prognostics and health management
   (PHM); regularization; remaining useful life (RUL) prediction;
   restricted Boltzmann machine (RBM)
AB In the Internet-of-Things environment, it is critical to bridge the gap between business decision-making and real-time factory data to let companies transfer from condition-based maintenance service to predictive maintenance service. Condition monitoring systems have been widely applied to many industries to acquire operation and equipment related data, through which machine health state can be evaluated. One of the challenges of predicting future machine health lies in extracting the right features that are correlated well with the fault progression/degradation. We propose an enhanced restricted Boltzmann machine with a novel regularization term to automatically generate features that are suitable for remaining useful life prediction. The regularization term tries to maximize the trendability of the output features, which potentially better represent the degradation pattern of a system. The proposed method is benchmarked with regular restricted Boltzmann machine algorithm and principal component analysis. The generated features are used as input to a similarity-based method for life prediction. Run-to-failure datasets collected from two rotating systems are used for validation.
C1 [Liao, Linxia; Jin, Wenjing] Palo Alto Res Ctr, Syst Sci Lab, Palo Alto, CA 94303 USA.
   [Jin, Wenjing] Univ Cincinnati, Cincinnati, OH 45221 USA.
   [Pavel, Radu] TechSolve Inc, Cincinnati, OH 45237 USA.
RP Liao, LX (reprint author), Palo Alto Res Ctr, Syst Sci Lab, Palo Alto, CA 94303 USA.
EM linxia.liao@parc.com; jinwi@mail.uc.edu; pavel@techsolve.org
FU Palo Alto Research Center (PARC, a Xerox Company)
FX This work was supported by the Palo Alto Research Center (PARC, a Xerox
   Company).
CR [Anonymous], 2015, TIMKEN SUPER PRECISI
   Coble J., 2009, P ANN C PROGN HLTH M, P1
   Hinton G., 2012, NEURAL NETWORKS TRIC, V9, P599, DOI DOI 10.1007/978-3-642-35289-8_32
   Jin W., 2014, THESIS
   KOHONEN T, 1990, P IEEE, V78, P1464, DOI 10.1109/5.58325
   KyungHyun Cho, 2012, Artificial Neural Networks and Machine Learning - ICANN 2012. Proceedings of the 22nd International Conference on Artificial Neural Networks, P81, DOI 10.1007/978-3-642-33269-2_11
   Liao LX, 2014, IEEE T RELIAB, V63, P191, DOI 10.1109/TR.2014.2299152
   MacGillivray C., 2013, IDC Q1 DOC, P1
   Nectoux P., 2012, IEEE INT C PROGN HLT, P1
   Pan GY, 2014, IEEE IJCNN, P2935, DOI 10.1109/IJCNN.2014.6889458
   Pavel R., 2012, PROGN HLTH MAN SOL C
   Rauber TW, 2015, IEEE T IND ELECTRON, V62, P637, DOI 10.1109/TIE.2014.2327589
   Singleton RK, 2015, IEEE T IND ELECTRON, V62, P1781, DOI 10.1109/TIE.2014.2336616
   Sutrisno, 2012, 2012 IEEE CONFERENCE ON CONTROL, SYSTEMS & INDUSTRIAL INFORMATICS (ICCSII), P1, DOI 10.1109/CCSII.2012.6470462
   Tomczak JM, 2015, ADV INTELL SYST, V366, P181, DOI 10.1007/978-3-319-08422-0_27
   Wang T., 2010, THESIS
   Xie GS, 2014, IEEE IJCNN, P1622, DOI 10.1109/IJCNN.2014.6889447
   Xue F, 2008, J FAIL ANAL PREV, V8, P199, DOI 10.1007/s11668-008-9118-9
   Yan W, 2015, P ANN C PROGN HLTH M, P440
   Yin S, 2015, IEEE T IND ELECTRON, V62, P1651, DOI 10.1109/TIE.2014.2345331
NR 20
TC 47
Z9 53
U1 17
U2 94
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 0278-0046
EI 1557-9948
J9 IEEE T IND ELECTRON
JI IEEE Trans. Ind. Electron.
PD NOV
PY 2016
VL 63
IS 11
BP 7076
EP 7083
DI 10.1109/TIE.2016.2586442
PG 8
WC Automation & Control Systems; Engineering, Electrical & Electronic;
   Instruments & Instrumentation
SC Automation & Control Systems; Engineering; Instruments & Instrumentation
GA ED1RH
UT WOS:000388622100043
DA 2020-02-19
ER

PT J
AU Zhu, F
   Shao, L
   Xie, J
   Fang, Y
AF Zhu, Fan
   Shao, Ling
   Xie, Jin
   Fang, Yi
TI From handcrafted to learned representations for human action
   recognition: A survey
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Human action recognition; Handcrafted features; Deep learning;
   Convolutional neural network; Dictionary learning
ID IMAGE CLASSIFICATION; ALGORITHM; MODEL
AB Human action recognition is an important branch among the studies of both human perception and computer vision systems. Along with the development of artificial intelligence, deep learning techniques have gained remarkable reputation when dealing with image categorization tasks (e.g., object detection and classification). However, since human actions normally present in the form of sequential image frames, analyzing human action data requires significantly increased computational power than still images when deep learning techniques are employed. Such a challenge has been the bottleneck for the migration of learning based image representation techniques to action sequences, so that the old fashioned handcrafted human action representations are still widely used for human action recognition tasks. On the other hand, since handcrafted representations are usually ad-hoc and overfit to specific data, they are incapable of being generalized to deal with various realistic scenarios. Consequently, resorting to deep learning action representations for human action recognition tasks is eventually a natural option. In this work, we provide a detailed overview of recent advancements in human action representations. As the first survey that covers both handcrafted and learning-based action representations, we explicitly discuss the superiorities and limitations of exiting techniques from both kinds. The ultimate goal of this survey is to provide comprehensive analysis and comparisons between learning-based and handcrafted action representations respectively, so as to inspire action recognition researchers towards the study of both kinds of representation techniques. (C) 2016 Elsevier B.V. All rights reserved.
C1 [Zhu, Fan; Xie, Jin; Fang, Yi] NYU, Multimedia & Visual Comp Ldb, New York, NY 10003 USA.
   [Zhu, Fan; Xie, Jin; Fang, Yi] New York Univ Abu Dhabi, Elect & Comp Engn, Abu Dhabi, U Arab Emirates.
   [Shao, Ling] Northumbria Univ, Dept Comp Sci & Digital Technol, Newcastle Upon Tyne NE1 8ST, Tyne & Wear, England.
RP Fang, Y (reprint author), NYU, Multimedia & Visual Comp Ldb, New York, NY 10003 USA.; Fang, Y (reprint author), New York Univ Abu Dhabi, Elect & Comp Engn, Abu Dhabi, U Arab Emirates.
EM yfang@nyu.edu
RI Shao, Ling/D-3535-2011
CR Aggarwal JK, 2011, ACM COMPUT SURV, V43, DOI 10.1145/1922649.1922653
   Aggarwal JK, 1999, COMPUT VIS IMAGE UND, V73, P428, DOI 10.1006/cviu.1998.0744
   Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   Baccouche Moez, 2011, Human Behavior Unterstanding. Proceedings Second International Workshop, HBU 2011, P29, DOI 10.1007/978-3-642-25446-8_4
   Banzhaf Wolfgang, 1998, GENETIC PROGRAMMING, V1
   Baum LE, 1966, ANN MATH STAT
   Bengio Y., 2007, LARGE SCALE KERNEL M, V34
   Brox T., 2004, EUR C COMP VIS
   Buccino G, 2004, BRAIN LANG, V89, P370, DOI 10.1016/S0093-934X(03)00356-0
   BURRUS CS, 1997, INTRO WAVELETS WAVEL
   Cadieu C., 2008, ADV NEURAL INFORM PR
   Cai Z, 2014, IEEE C COMP VIS PATT
   CEDRAS C, 1995, IMAGE VISION COMPUT, V13, P129, DOI 10.1016/0262-8856(95)93154-K
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chatfield K., 2014, ARXIV14053531
   Chaudhry R., 2009, IEEE C COMP VIS PATT
   Cheng G., 2015, ARXIV150105964
   Collobert R., 2008, ACM INT C MACH LEARN
   Dalai N., 2005, IEEE C COMP VIS PATT
   Dalal N, 2006, EUR C COMP VIS
   Deng J., 2009, IEEE C COMP VIS PATT
   Diwadkar VA, 1997, PSYCHOL SCI, V8, P302, DOI 10.1111/j.1467-9280.1997.tb00442.x
   Dollar P., 2005, IEEE INT WORKSH VIS
   Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714
   Fernando B, 2015, IEEE C COMP VIS PATT
   Forstner W., 1987, INT C FAST PROC PHOT
   FUKUSHIMA K, 1980, BIOL CYBERN, V36, P193, DOI 10.1007/BF00344251
   Ganapathi V, 2010, IEEE C COMP VIS PATT
   Gavrila DM, 1999, COMPUT VIS IMAGE UND, V73, P82, DOI 10.1006/cviu.1998.0716
   Gers FA, 2003, J MACH LEARN RES, V3, P115, DOI 10.1162/153244303768966139
   Gilbert A, 2008, EUR C COMP VIS
   Graves A, 2009, IEEE T PATTERN ANAL, V31, P855, DOI 10.1109/TPAMI.2008.137
   Guha T, 2012, IEEE T PATTERN ANAL, V34, P1576, DOI 10.1109/TPAMI.2011.253
   Harris C., 1988, ALV VIS C MANCH UK, V15, P50, DOI DOI 10.5244/C
   Harris ZS, 1954, WORD, V10, P146, DOI 10.1080/00437956.1954.11659520
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hyvarinen A, 2009, COMPUT IMAGING VIS, V39, P1
   Jarrett K, 2009, IEEE I CONF COMP VIS, P2146, DOI 10.1109/ICCV.2009.5459469
   Jhuang H, 2007, IEEE I CONF COMP VIS, P1253
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Jia Y., ARXIV14085093
   Jiang Y., 2012, EUR C COMP VIS
   JONES JP, 1987, J NEUROPHYSIOL, V58, P1233
   Jones S, 2014, PROC CVPR IEEE, P604, DOI 10.1109/CVPR.2014.84
   Karpathy Andrej, 2014, IEEE C COMP VIS PATT
   Keysers C, 2003, EXP BRAIN RES, V153, P628, DOI 10.1007/s00221-003-1603-5
   Kim H.-J., 2007, ADV NEURAL INFORM PR
   Kim HJ, 2006, LECT NOTES COMPUT SC, V4233, P177
   Klaser A., 2008, BRIT MACH VIS C
   Krizhevsky A., 2012, ADV NEURAL INFORM PR
   Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543
   Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7
   Le QV, 2011, PROC CVPR IEEE
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y, 2010, IEEE INT SYMP CIRC S, P253, DOI 10.1109/ISCAS.2010.5537907
   Liu J., 2011, IEEE C COMP VIS PATT
   Liu J., 2009, IEEE C COMP VIS PATT
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   MARCELJA S, 1980, J OPT SOC AM, V70, P1297, DOI 10.1364/JOSA.70.001297
   Memisevic R, 2007, IEEE C COMP VIS PATT
   Moeslund TB, 2006, COMPUT VIS IMAGE UND, V104, P90, DOI 10.1016/j.cviu.2006.08.002
   Mutch J., 2006, IEEE C COMP VIS PATT
   Ning F, 2005, IEEE T IMAGE PROCESS, V14, P1360, DOI 10.1109/TIP.2005.852470
   Oquab M., 2014, IEEE C COMP VIS PATT
   Parameswaran V, 2006, INT J COMPUT VISION, V66, P83, DOI 10.1007/s11263-005-3671-4
   Peng X., 2014, ARXIV14054506
   Peng XJ, 2014, LECT NOTES COMPUT SC, V8693, P581, DOI 10.1007/978-3-319-10602-1_38
   Perronnin F., 2010, EUR C COMP VIS
   Poppe R, 2010, IMAGE VISION COMPUT, V28, P976, DOI 10.1016/j.imavis.2009.11.014
   Pulvermuller F, 2005, J COGNITIVE NEUROSCI, V17, P884, DOI 10.1162/0898929054021111
   Rodriguez MD, 2008, IEEE C COMP VIS PATT
   Sanchez J, 2013, INT J COMPUT VISION, V105, P222, DOI 10.1007/s11263-013-0636-x
   Schacter D, 2011, PSYCHOLOGY
   Schindler K., 2008, IEEE C COMP VIS PATT
   Schuldt C., 2004, IEEE INT C PATT REC
   Scovanner P., 2007, ACM INT C MULT
   Sermanet P., ARXIV13126229
   Serre T., 2005, IEEE C COMP VIS PATT
   Shao L, 2014, IEEE T NEUR NET LEAR, V25, P1359, DOI 10.1109/TNNLS.2013.2293418
   Shao L, 2014, IEEE T CYBERNETICS, V44, P817, DOI 10.1109/TCYB.2013.2273174
   Shotton J, 2013, COMMUN ACM, V56, P116, DOI 10.1145/2398356.2398381
   Siddiqui M., 2010, IEEE C COMP VIS PATT
   Simonyan K, 2014, ADV NEURAL INFORM PR
   Sivic J., 2003, IEEE INT C COMP VIS
   Soomro K., 2012, ARXIV12120402
   SOTAK GE, 1989, COMPUT VISION GRAPH, V48, P147, DOI 10.1016/S0734-189X(89)80036-2
   SOUVENIR R, 2008, IEEE C COMP VIS PATT
   Spencer TE, 2002, FRONT BIOSCI-LANDMRK, V7, pD1879, DOI 10.2741/spencer
   Taylor G., 2006, ADV NEURAL INFORM PR
   Taylor G., 2010, EUR C COMP VIS
   Tuytelaars Tinne, 2006, EUR C COMP VIS
   Vig E., 2012, EUR C COMP VIS
   Wang H., 2009, BRIT MACH VIS C
   Wang H, 2013, IEEE INT C COMP VIS
   Wang H, 2013, INT J COMPUT VISION, V103, P60, DOI 10.1007/s11263-012-0594-8
   WANG J., 2010, IEEE C COMP VIS PATT
   Wang L., 2015, IEEE C COMP VIS PATT
   Wang L., 2013, IEEE C COMP VIS PATT
   Weinland D., 2010, EUR C COMP VIS
   Weinland D, 2007, IEEE I CONF COMP VIS, P170
   Weinland D, 2006, COMPUT VIS IMAGE UND, V104, P249, DOI 10.1016/j.cviu.2006.07.013
   WILLEMS G, 2008, EUR C COMP VIS
   Wilson J.L., 2010, PC MAG COM
   WOLD S, 1987, CHEMOMETR INTELL LAB, V2, P37, DOI 10.1016/0169-7439(87)80084-9
   Wu D., 2014, IEEE C COMP VIS PATT
   Xia L, 2012, IEEE C COMP VIS PATT, P20
   Yan Y., 2013, IEEE INT C IM PROC
   Yang Q., 2009, INT JOINT C ART INT
   Zeiler Matthew D., 2014, EUR C COMP VIS
   Zhen XT, 2014, INFORM SCIENCES, V281, P295, DOI 10.1016/j.ins.2014.05.021
   Zheng JJ, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.125
   Zheng V.W., 2009, ACM INT C UB COMP
   Zhu F, 2014, INT J COMPUT VISION, V109, P42, DOI 10.1007/s11263-014-0703-y
   Zhu Y., 2007, P AS C COMP VIS
NR 114
TC 34
Z9 35
U1 1
U2 30
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD NOV
PY 2016
VL 55
SI SI
BP 42
EP 52
DI 10.1016/j.imavis.2016.06.007
PN 2
PG 11
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
SC Computer Science; Engineering; Optics
GA ED9BE
UT WOS:000389164300002
DA 2020-02-19
ER

PT J
AU Liu, Z
   Zhang, CY
   Tian, YL
AF Liu, Zhi
   Zhang, Chenyang
   Tian, Yingli
TI 3D-based Deep Convolutional Neural Network for action recognition with
   depth sequences
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Action recognition; Deep learning; Convolutional neural network; Depth
   sequences; 3D convolution
ID REPRESENTATION; POSE
AB Traditional algorithms to design hand-crafted features for action recognition have been a hot research area in the last decade. Compared to RGB video, depth sequence is more insensitive to lighting changes and more discriminative due to its capability to catch geometric information of object. Unlike many existing methods for action recognition which depend on well-designed features, this paper studies deep learning based action recognition using depth sequences and the corresponding skeleton joint information. Firstly, we construct a 3D-based Deep Convolutional Neural Network (3D(2)CNN) to directly learn spatio-temporal features from raw depth sequences, then compute a joint based feature vector named JointVector for each sequence by taking into account the simple position and angle information between skeleton joints. Finally, support vector machine (SVM) classification results from 3D(2)CNN learned features and JointVector are fused to take action recognition. Experimental results demonstrate that our method can learn feature representation which is time-invariant and viewpoint-invariant from depth sequences. The proposed method achieves comparable results to the state-of-the-art methods on the UTKinect-Action3D dataset and achieves superior performance in comparison to baseline methods on the MSR-Action3D dataset. We further investigate the generalization of the trained model by transferring the learned features from one dataset (MSR-Action3D) to another dataset (UTKinect-Action3D) without retraining and obtain very promising classification accuracy. (C) 2016 Elsevier B.V. All rights reserved.
C1 [Liu, Zhi] Chongqing Univ Technol, Coll Comp Sci & Engn, Chongqing 400050, Peoples R China.
   [Zhang, Chenyang; Tian, Yingli] CUNY City Coll, Dept Elect Engn, New York, NY 10031 USA.
RP Tian, YL (reprint author), CUNY City Coll, Dept Elect Engn, New York, NY 10031 USA.
EM liuzhi@cqut.edu.cn; czhang10@ccny.cuny.edu; ytian@ccny.cuny.edu
FU NSFNational Science Foundation (NSF) [EFRI-1137172, IIS-1400802];
   Scientific and Technological Research Program of Chongqing Municipal
   Education Commission [KJ1400926]; Chongqing National Science Foundation
   [cstc2013jcyjA40038]
FX This work was supported in part by NSF grants EFRI-1137172 and
   IIS-1400802, Scientific and Technological Research Program of Chongqing
   Municipal Education Commission (KJ1400926), and Chongqing National
   Science Foundation (cstc2013jcyjA40038).
CR Valle EA, 2013, 2013 10TH INTERNATIONAL CONFERENCE ON ELECTRICAL ENGINEERING, COMPUTING SCIENCE AND AUTOMATIC CONTROL (CCE), P239, DOI 10.1109/ICEEE.2013.6676005
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Burger HC, 2012, PROC CVPR IEEE, P2392, DOI 10.1109/CVPR.2012.6247952
   Chrungoo A, 2014, LECT NOTES ARTIF INT, V8755, P84, DOI 10.1007/978-3-319-11973-1_9
   Collobert R, 2011, NIPS WORKSH
   Davis JW, 1997, PROC CVPR IEEE, P928, DOI 10.1109/CVPR.1997.609439
   Devanne M, 2013, LECT NOTES COMPUT SC, V8158, P456, DOI 10.1007/978-3-642-41190-8_49
   Donahue J., ARXIV13101531
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Han JG, 2013, IEEE T CYBERNETICS, V43, P1318, DOI 10.1109/TCYB.2013.2265378
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hunter D, 2012, IEEE T IND INFORM, V8, P228, DOI 10.1109/TII.2012.2187914
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Jin L, 2014, 2014 IEEE INTERNATIONAL SYMPOSIUM ON MULTIMEDIA (ISM), P311, DOI 10.1109/ISM.2014.56
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Klaser A., 2008, BMVC 2008 19 BRIT MA, P275
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   Laptev I, 2008, PROC CVPR IEEE, P3222
   Le Q. V., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3361, DOI 10.1109/CVPR.2011.5995496
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Li WB, 2010, 2010 THE 3RD INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND INDUSTRIAL APPLICATION (PACIIA2010), VOL I, P9, DOI 10.1109/CVPRW.2010.5543273
   Liu L., 2013, P 23 INT JOINT C ART, P1493
   Liu L., IEEE T CYBERN
   Lowe D., 1999, P INT C COMP VIS, V2, P1150, DOI [10.1109/ICCV.1999.790410, DOI 10.1109/ICCV.1999.790410]
   Mao Ye, 2013, Time-Of-Flight and Depth Imaging. Sensors, Algorithms and Applications. Dagstuhl 2012 Seminar on Time-of-Flight Imaging and GCPR 2013 Workshop on Imaging New Modalities: LNCS 8200, P149, DOI 10.1007/978-3-642-44964-2_8
   Molchanov P., 2015, P IEEE C COMP VIS PA
   Murray R. M., 1994, MATH INTRO ROBOTIC M
   Ni BB, 2013, IEEE T CYBERNETICS, V43, P1383, DOI 10.1109/TCYB.2013.2276433
   O'Hara S, 2012, IMAGE VISION COMPUT, V30, P206, DOI 10.1016/j.imavis.2011.11.001
   Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222
   Oreifej O, 2013, PROC CVPR IEEE, P716, DOI 10.1109/CVPR.2013.98
   Peng XJ, 2014, IMAGE VISION COMPUT, V32, P616, DOI 10.1016/j.imavis.2014.06.011
   Roshtkhari MJ, 2013, IMAGE VISION COMPUT, V31, P864, DOI 10.1016/j.imavis.2013.08.005
   Sermanet P, 2013, PROC CVPR IEEE, P3626, DOI 10.1109/CVPR.2013.465
   Shao L, 2016, INT J COMPUT VISION, V118, P115, DOI 10.1007/s11263-015-0861-6
   Shotton J, 2011, PROC CVPR IEEE, P1297, DOI 10.1109/CVPR.2011.5995316
   Sun L, 2014, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2014.336
   Thi TH, 2012, IMAGE VISION COMPUT, V30, P1, DOI 10.1016/j.imavis.2011.12.006
   Vemulapalli R, 2014, PROC CVPR IEEE, P588, DOI 10.1109/CVPR.2014.82
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang J, 2012, LECT NOTES COMPUT SC, V7573, P872, DOI 10.1007/978-3-642-33709-3_62
   Wang J, 2012, PROC CVPR IEEE, P1290, DOI 10.1109/CVPR.2012.6247813
   Wu D, 2014, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2014.98
   Xia L., 2012, IEEE COMP SOC C COMP, P20, DOI DOI 10.1109/CVPRW.2012.6239233
   Xia L, 2013, PROC CVPR IEEE, P2834, DOI 10.1109/CVPR.2013.365
   Yang X, 2012, P 20 ACM INT C MULT, P1057, DOI DOI 10.1145/2393347.2396382
   Yang XD, 2014, PROC CVPR IEEE, P804, DOI 10.1109/CVPR.2014.108
   Ye GZ, 2013, IEEE T CYBERNETICS, V43, P1370, DOI 10.1109/TCYB.2013.2272321
   Yu M., IEEE T PATTERN ANAL
   Zanfir M, 2013, IEEE I CONF COMP VIS, P2752, DOI 10.1109/ICCV.2013.342
   Zhang CY, 2013, IEEE COMPUT SOC CONF, P500, DOI 10.1109/CVPRW.2013.80
   Zhu Y, 2014, IMAGE VISION COMPUT, V32, P453, DOI 10.1016/j.imavis.2014.04.005
NR 53
TC 44
Z9 44
U1 1
U2 40
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD NOV
PY 2016
VL 55
SI SI
BP 93
EP 100
DI 10.1016/j.imavis.2016.04.004
PN 2
PG 8
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
SC Computer Science; Engineering; Optics
GA ED9BE
UT WOS:000389164300007
OA Bronze
DA 2020-02-19
ER

PT J
AU Ortiz, A
   Munilla, J
   Gorriz, JM
   Ramirez, J
AF Ortiz, Andres
   Munilla, Jorge
   Gorriz, Juan M.
   Ramirez, Javier
TI Ensembles of Deep Learning Architectures for the Early Diagnosis of the
   Alzheimer's Disease
SO INTERNATIONAL JOURNAL OF NEURAL SYSTEMS
LA English
DT Article
DE Deep learning; ensemble; Alzheimer's disease classification
ID MILD COGNITIVE IMPAIRMENT; BRAIN ATROPHY; CLASSIFICATION; MRI;
   PREDICTION; VOLUME; SPECT; CONNECTIVITY; CLASSIFIERS; COMPUTATION
AB Computer Aided Diagnosis (CAD) constitutes an important tool for the early diagnosis of Alzheimer's Disease (AD), which, in turn, allows the application of treatments that can be simpler and more likely to be effective. This paper explores the construction of classification methods based on deep learning architectures applied on brain regions defined by the Automated Anatomical Labeling (AAL). Gray Matter (GM) images from each brain area have been split into 3D patches according to the regions defined by the AAL atlas and these patches are used to train different deep belief networks. An ensemble of deep belief networks is then composed where the final prediction is determined by a voting scheme. Two deep learning based structures and four different voting schemes are implemented and compared, giving as a result a potent classification architecture where discriminative features are computed in an unsupervised fashion. The resulting method has been evaluated using a large dataset from the Alzheimer's disease Neuroimaging Initiative (ADNI). Classification results assessed by cross-validation prove that the proposed method is not only valid for differentiate between controls (NC) and AD images, but it also provides good performances when tested for the more challenging case of classifying Mild Cognitive Impairment (MCI) Subjects. In particular, the classification architecture provides accuracy values up to 0.90 and AUC of 0.95 for NC/AD classification, 0.84 and AUC of 0.91 for stable MCI/AD classification and 0.83 and AUC of 0.95 for NC/MCI converters classification.
C1 [Ortiz, Andres; Munilla, Jorge] Univ Malaga, Commun Engn Dept, E-29071 Malaga, Spain.
   [Gorriz, Juan M.; Ramirez, Javier] Univ Granada, Dept Signal Theory Commun & Networking, Granada 18060, Spain.
RP Ortiz, A (reprint author), Univ Malaga, Commun Engn Dept, E-29071 Malaga, Spain.
EM aortiz@ic.uma.es; munilla@ic.uma.es; gorriz@ugr.es; javierrp@ugr.es
RI Ramirez, Javier/B-1836-2012; Ortiz, Andres/K-4886-2014; Gorriz, Juan
   M./C-2385-2012
OI Ramirez, Javier/0000-0002-6229-2921; Gorriz, Juan M./0000-0001-7069-1714
FU MINECO [TEC2012-34306, TEC2015-64718-R, PSI2015-65848-R]; Consejeria de
   Innovacion, Ciencia y Empresa (Junta de Andalucia, Spain)Junta de
   Andalucia [P09-TIC-4530, P11-TIC-7103]; Universidad de Malaga; Programa
   de fortalecimiento de las capacidades de I+D+I en las Universidades, de
   la Consejeria de Economia, Innovacion, Ciencia y Empleo, cofinanciado
   por el fondo europeo de desarrollo regional (FEDER) [FC14-SAF30];
   Alzheimer's Disease Neuroimaging Initiative (ADNI) (National Institutes
   of Health)United States Department of Health & Human ServicesNational
   Institutes of Health (NIH) - USANIH National Institute of Neurological
   Disorders & Stroke (NINDS) [U01 AG024904]; DOD ADNI (Department of
   Defense) [W81XWH-12-2-0012]; National Institute on AgingUnited States
   Department of Health & Human ServicesNational Institutes of Health (NIH)
   - USANIH National Institute on Aging (NIA); National Institute of
   Biomedical Imaging and BioengineeringUnited States Department of Health
   & Human ServicesNational Institutes of Health (NIH) - USANIH National
   Institute of Biomedical Imaging & Bioengineering (NIBIB); Canadian
   Institutes of Health ResearchCanadian Institutes of Health Research
   (CIHR)
FX This work was partly supported by the MINECO under the projects
   TEC2012-34306, TEC2015-64718-R and PSI2015-65848-R, and the Consejeria
   de Innovacion, Ciencia y Empresa (Junta de Andalucia, Spain) under the
   Excellence Projects P09-TIC-4530, P11-TIC-7103 and the Universidad de
   Malaga. Programa de fortalecimiento de las capacidades de I+D+I en las
   Universidades 2014-2015, de la Consejeria de Economia, Innovacion,
   Ciencia y Empleo, cofinanciado por el fondo europeo de desarrollo
   regional (FEDER) under the project FC14-SAF30. Data collection and
   sharing for this project was funded by the Alzheimer's Disease
   Neuroimaging Initiative (ADNI) (National Institutes of Health Grant U01
   AG024904) and DOD ADNI (Department of Defense award number
   W81XWH-12-2-0012). ADNI is funded by the National Institute on Aging,
   the National Institute of Biomedical Imaging and Bioengineering, and
   through generous contributions from the following: AbbVie, Alzheimer's
   Association; Alzheimer's Drug Discovery Foundation; Araclon Biotech;
   BioClinica, Inc.; Biogen; Bristol-Myers Squibb Company; CereSpir, Inc.;
   Eisai Inc.; Elan Pharmaceuticals, Inc.; Eli Lilly and Company;
   EuroImmun; F. Hoffmann-La Roche Ltd and its affiliated company
   Genentech, Inc.; Fujirebio; GE Healthcare; IXICO Ltd.; Janssen Alzheimer
   Immunotherapy Research & Development, LLC.; Johnson & Johnson
   Pharmaceutical Research & Development LLC.; Lumosity; Lundbeck; Merck &
   Co., Inc.; Meso Scale Diagnostics, LLC.; NeuroRx Research; Neurotrack
   Technologies; Novartis Pharmaceuticals Corporation; Pfizer Inc.; Piramal
   Imaging; Servier; Takeda Pharmaceutical Company; and Transition
   Therapeutics. The Canadian Institutes of Health Research is providing
   funds to support ADNI clinical sites in Canada. Private sector
   contributions are facilitated by the Foundation for the National
   Institutes of Health (www.fnih.org). The grantee organization is the
   Northern California Institute for Research and Education, and the study
   is coordinated by the Alzheimer's Disease Cooperative Study at the
   University of California, San Diego. ADNI data are disseminated by the
   Laboratory for Neuro Imaging at the University of Southern California.
CR Abdi H, 2010, WIRES COMPUT STAT, V2, P433, DOI 10.1002/wics.101
   Adeli H, 2005, J ALZHEIMERS DIS, V7, P187
   Adeli H, 2005, CLIN EEG NEUROSCI, V36, P131, DOI 10.1177/155005940503600303
   Adeli H, 2010, AUTOMATED EEG-BASED DIAGNOSIS OF NEUROLOGICAL DISORDERS: INVENTING THE FUTURE OF NEUROLOGY, P1
   Adeli H., 1994, MACHINE LEARNING NEU
   Adeli H, 2008, NEUROSCI LETT, V444, P190, DOI 10.1016/j.neulet.2008.08.008
   Ahmadlou M, 2014, CLIN NEUROPHYSIOL, V125, P694, DOI 10.1016/j.clinph.2013.08.033
   Ahmadlou M, 2011, ALZ DIS ASSOC DIS, V25, P85, DOI 10.1097/WAD.0b013e3181ed1160
   Ahmadlou M, 2010, J NEURAL TRANSM, V117, P1099, DOI 10.1007/s00702-010-0450-3
   Akhand MAH, 2009, INT J NEURAL SYST, V19, P67, DOI 10.1142/S0129065709001859
   Ashburner J., 2011, SPM8 MANUEL, V12
   Aston JAD, 2002, J CEREBR BLOOD F MET, V22, P1019, DOI 10.1097/00004647-200208000-00014
   Barnes J, 2010, NEUROIMAGE, V53, P1244, DOI 10.1016/j.neuroimage.2010.06.025
   Baruque B, 2011, INT J NEURAL SYST, V21, P505, DOI 10.1142/S0129065711003012
   Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Castillo E, 2015, INT J NEURAL SYST, V25, DOI 10.1142/S012906571550029X
   Chyzhyk D, 2012, NEUROCOMPUTING, V75, P72, DOI 10.1016/j.neucom.2011.02.024
   Colon-Lopez V, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0083209
   Cuingnet R, 2011, NEUROIMAGE, V56, P766, DOI 10.1016/j.neuroimage.2010.06.013
   Duin RPW, 2000, INT C PATT RECOG, P1, DOI 10.1109/ICPR.2000.906006
   Dukart J., ALZHEIMERS DIS NEURO
   Dura-Bernal S., 2013, INT J NEURAL SYST, V13, P1
   Fawcett T, 2006, PATTERN RECOGN LETT, V27, P861, DOI 10.1016/j.patrec.2005.10.010
   Flandin G, 2002, 2002 IEEE INTERNATIONAL SYMPOSIUM ON BIOMEDICAL IMAGING, PROCEEDINGS, P907, DOI 10.1109/ISBI.2002.1029408
   Galar M, 2011, PATTERN RECOGN, V44, P1761, DOI 10.1016/j.patcog.2011.01.017
   Giovacchini G, 2004, J NUCL MED, V45, P1471
   Gorriz JM, 2011, APPL SOFT COMPUT, V11, P2313, DOI 10.1016/j.asoc.2010.08.012
   Hamal P., 2010, ISMIR, P339
   Hastie T., 2003, ELEMENTS STAT LEARNI
   HAYKIN S, 1999, NEURAL NETWORKS
   Hebbo H., 2013, PROJECT REPORT
   Hidalgo-Munoz AR, 2014, FRONT AGING NEUROSCI, V6, DOI 10.3389/fnagi.2014.00020
   Hinrichs C, 2011, NEUROIMAGE, V55, P574, DOI 10.1016/j.neuroimage.2010.10.081
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Huang SA, 2010, NEUROIMAGE, V50, P935, DOI 10.1016/j.neuroimage.2009.12.120
   HUGHES GF, 1968, IEEE T INFORM THEORY, V14, P55, DOI 10.1109/TIT.1968.1054102
   Illan IA, 2011, INFORM SCIENCES, V181, P903, DOI 10.1016/j.ins.2010.10.027
   Jack CR, 1999, NEUROLOGY, V52, P1397, DOI 10.1212/WNL.52.7.1397
   Jagust WJ, 2010, ALZHEIMERS DEMENT, V6, P221, DOI 10.1016/j.jalz.2010.03.003
   Kittler J, 1998, IEEE T PATTERN ANAL, V20, P226, DOI 10.1109/34.667881
   Kusy B, 2009, 2009 INTERNATIONAL CONFERENCE ON INFORMATION PROCESSING IN SENSOR NETWORKS (IPSN 2009), P109
   Lin KA, 2015, FRONT NEUROL, V5, DOI 10.3389/fneur.2014.00288
   Liu MH, 2012, NEUROIMAGE, V60, P1106, DOI 10.1016/j.neuroimage.2012.01.055
   Liu Y, 2011, PATTERN RECOGN, V44, P2287, DOI 10.1016/j.patcog.2010.12.012
   Lopez M, 2011, NEUROCOMPUTING, V74, P1260, DOI 10.1016/j.neucom.2010.06.025
   MINOSHIMA S, 1994, LANCET, V344, P895, DOI 10.1016/S0140-6736(94)92871-1
   Morabito FC, 2015, INT J NEURAL SYST, V25, DOI 10.1142/S0129065715500057
   Murphy KP., 2012, MACHINE LEARNING PRO
   Murphy MP, 2010, J ALZHEIMERS DIS, V19, P311, DOI [10.3233/JAD-2009-1221, 10.3233/JAD-2010-1221]
   Navidi W, 2010, STAT ENG SCI
   Nestor PJ, 2004, NAT MED, V10, pS34, DOI 10.1038/nrn1433
   Ng S, 2007, J NUCL MED, V48, P547, DOI 10.2967/jnumed.106.037762
   Ortiz A, 2013, PATTERN RECOGN LETT, V34, P1725, DOI 10.1016/j.patrec.2013.04.014
   PERANI D, 1988, J NUCL MED, V29, P1507
   Plant C, 2010, NEUROIMAGE, V50, P162, DOI 10.1016/j.neuroimage.2009.11.046
   Powers D. M. W., 2011, J MACH LEARN TECHNOL, V2, P37, DOI DOI 10.9735/2229-3981
   Ramirez J, 2009, IEEE NUCL SCI CONF R, P2738, DOI 10.1109/NSSMIC.2009.5401968
   Ranzato M, 2008, ADV NEURAL INFORM PR, P1185
   Ranzato M., 2006, ADV NEURAL INFORM PR, V19, P1137
   Rojas R., 1996, NEURAL NETWORKS SYST
   Sammut C., 2010, STAT LEARNING THEORY
   Sankari Z, 2012, CLIN EEG NEUROSCI, V43, P268, DOI 10.1177/1550059412444970
   Sankari Z, 2011, CLIN NEUROPHYSIOL, V122, P897, DOI 10.1016/j.clinph.2010.09.008
   Sankari Z, 2011, J NEUROSCI METH, V197, P165, DOI 10.1016/j.jneumeth.2011.01.027
   Savio A, 2013, LECT NOTES COMPUT SC, V7903, P107
   Segovia F, 2012, NEUROCOMPUTING, V75, P64, DOI 10.1016/j.neucom.2011.03.050
   Smith S. L., 2011, GENETIC EVOLUTIONARY
   Smolensky P., 1986, INFORM PROCESSING DY, V1, P194
   Stoeckel J., 2005, P 5 IEEE INT DAT MIN, P243
   Suk HI, 2013, LECT NOTES COMPUT SC, V8150, P583, DOI 10.1007/978-3-642-40763-5_72
   Teipel SJ, 2007, NEUROIMAGE, V38, P13, DOI 10.1016/j.neuroimage.2007.07.008
   Teipel SJ, 2015, NEUROIMAGE-CLIN, V8, P583, DOI 10.1016/j.nicl.2015.05.006
   Theodoridis S, 2009, PATTERN RECOGNITION, 4RTH EDITION, P1
   Vapnik V. N., 1998, STAT LEARNING THEORY
   Welling Max, 2004, ADV NEURAL INFORM PR, P1481
   Zhang DQ, 2011, NEUROIMAGE, V55, P856, DOI 10.1016/j.neuroimage.2011.01.008
   Zhang YL, 2015, INT J NEURAL SYST, V25, DOI 10.1142/S0129065715500203
NR 79
TC 74
Z9 76
U1 5
U2 111
PU WORLD SCIENTIFIC PUBL CO PTE LTD
PI SINGAPORE
PA 5 TOH TUCK LINK, SINGAPORE 596224, SINGAPORE
SN 0129-0657
EI 1793-6462
J9 INT J NEURAL SYST
JI Int. J. Neural Syst.
PD NOV
PY 2016
VL 26
IS 7
AR 1650025
DI 10.1142/S0129065716500258
PG 23
WC Computer Science, Artificial Intelligence
SC Computer Science
GA DW7CL
UT WOS:000383808700004
PM 27478060
DA 2020-02-19
ER

PT J
AU Cusano, C
   Napoletano, P
   Schettini, R
AF Cusano, Claudio
   Napoletano, Paolo
   Schettini, Raimondo
TI Combining multiple features for color texture classification
SO JOURNAL OF ELECTRONIC IMAGING
LA English
DT Article
DE color texture classification; color texture features; color texture
   database; ensemble of classifiers
ID DESCRIPTORS
AB The analysis of color and texture has a long history in image analysis and computer vision. These two properties are often considered as independent, even though they are strongly related in images of natural objects and materials. Correlation between color and texture information is especially relevant in the case of variable illumination, a condition that has a crucial impact on the effectiveness of most visual descriptors. We propose an ensemble of hand-crafted image descriptors designed to capture different aspects of color textures. We show that the use of these descriptors in a multiple classifiers framework makes it possible to achieve a very high classification accuracy in classifying texture images acquired under different lighting conditions. A powerful alternative to hand-crafted descriptors is represented by features obtained with deep learning methods. We also show how the proposed combining strategy hand-crafted and convolutional neural networks features can be used together to further improve the classification accuracy. Experimental results on a food database ( raw food texture) demonstrate the effectiveness of the proposed strategy. (C) 2016 SPIE and IS&T
C1 [Cusano, Claudio] Univ Pavia, Dept Elect Comp & Biomed Engn, Via Ferrata 1, I-27100 Pavia, Italy.
   [Napoletano, Paolo; Schettini, Raimondo] Univ Milano Bicocca, Dept Informat Syst & Commun, Viale Sarca 336, I-20126 Milan, Italy.
RP Napoletano, P (reprint author), Univ Milano Bicocca, Dept Informat Syst & Commun, Viale Sarca 336, I-20126 Milan, Italy.
EM napoletano@disco.unimib.it
RI Cusano, Claudio/T-2948-2019; Cusano, Claudio/AAH-1115-2019
OI Cusano, Claudio/0000-0001-9365-8167; SCHETTINI,
   RAIMONDO/0000-0001-7461-1451
CR Bianco S, 2015, DIGIT SIGNAL PROCESS, V44, P1, DOI 10.1016/j.dsp.2015.06.001
   Bianconi F, 2011, J ELECTRON IMAGING, V20, DOI 10.1117/1.3651210
   Bianconi F, 2009, J ELECTRON IMAGING, V18, DOI 10.1117/1.3273946
   Cimpoi M, 2015, PROC CVPR IEEE, P3828, DOI 10.1109/CVPR.2015.7299007
   Cusano C, 2016, J OPT SOC AM A, V33, P17, DOI 10.1364/JOSAA.33.000017
   Cusano C, 2015, LECT NOTES COMPUT SC, V9281, P111, DOI 10.1007/978-3-319-23222-5_14
   Cusano C, 2014, J OPT SOC AM A, V31, P1453, DOI 10.1364/JOSAA.31.001453
   Drimbarean A, 2001, PATTERN RECOGN LETT, V22, P1161, DOI 10.1016/S0167-8655(01)00058-7
   Finlayson GD, 2004, 12TH COLOR IMAGING CONFERENCE: COLOR SCIENCE AND ENGINEERING SYSTEMS, TECHNOLOGIES, APPLICATIONS, P37
   Gevers T, 1999, PATTERN RECOGN, V32, P453, DOI 10.1016/S0031-3203(98)00036-3
   Gevers T, 2003, IEEE T MULTIMEDIA, V5, P237, DOI 10.1109/TMM.2003.811620
   Iakovidis D., 2005, P IEEE INT WORKSH SY, P205
   Khan FS, 2013, LECT NOTES COMPUT SC, V8047, P154, DOI 10.1007/978-3-642-40261-6_18
   Kittler J, 1998, IEEE T PATTERN ANAL, V20, P226, DOI 10.1109/34.667881
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kuncheva L., 2004, COMBINING PATTERN CL
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lee D., 2014, ADV LOW LEVEL COLOR, P55
   Maenpaa T, 2004, PATTERN RECOGN, V37, P1629, DOI 10.1016/j.patcog.2003.11.011
   Palm C, 2004, PATTERN RECOGN, V37, P965, DOI 10.1016/j.patcog.2003.09.010
   Porebski A, 2010, IMAGE PROC THEORY TO, P32
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Seifi M., 2010, C COL GRAPH IM VIS, P332
   Simonyan K., 2014, P ICLR
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470
   Tax DMJ, 2000, PATTERN RECOGN, V33, P1475, DOI 10.1016/S0031-3203(99)00138-7
NR 26
TC 7
Z9 7
U1 1
U2 10
PU IS&T & SPIE
PI BELLINGHAM
PA 1000 20TH ST, BELLINGHAM, WA 98225 USA
SN 1017-9909
EI 1560-229X
J9 J ELECTRON IMAGING
JI J. Electron. Imaging
PD NOV
PY 2016
VL 25
IS 6
AR 061410
DI 10.1117/1.JEI.25.6.061410
PG 9
WC Engineering, Electrical & Electronic; Optics; Imaging Science &
   Photographic Technology
SC Engineering; Optics; Imaging Science & Photographic Technology
GA EP0EI
UT WOS:000397059200011
DA 2020-02-19
ER

PT J
AU Baldi, P
   Sadowski, P
AF Baldi, Pierre
   Sadowski, Peter
TI A theory of local learning, the learning channel, and the optimality of
   backpropagation
SO NEURAL NETWORKS
LA English
DT Article
DE Deep learning; Backpropagation; Hebbian learning; Learning channel;
   Supervised learning; Unsupervised learning
ID NEURAL-NETWORKS; BINOCULAR INTERACTION; SYNAPTIC PLASTICITY;
   RECEPTIVE-FIELDS; MODEL; ALGORITHM
AB In a physical neural system, where storage and processing are intimately intertwined, the rules for adjusting the synaptic weights can only depend on variables that are available locally, such as the activity of the pre- and post-synaptic neurons, resulting in local learning rules. A systematic framework for studying the space of local learning rules is obtained by first specifying the nature of the local variables, and then the functional form that ties them together into each learning rule. Such a framework enables also the systematic discovery of new learning rules and exploration of relationships between learning rules and group symmetries. We study polynomial local learning rules stratified by their degree and analyze their behavior and capabilities in both linear and non-linear units and networks. Stacking local learning rules in deep feedforward networks leads to deep local learning. While deep local learning can learn interesting representations, it cannot learn complex input-output functions, even when targets are available for the top layer. Learning complex input-output functions requires local deep learning where target information is communicated to the deep layers through a backward learning channel. The nature of the communicated information about the targets and the structure of the learning channel partition the space of learning algorithms. For any learning algorithm, the capacity of the learning channel can be defined as the number of bits provided about the error gradient per weight, divided by the number of required operations per weight. We estimate the capacity associated with several learning algorithms and show that backpropagation outperforms them by simultaneously maximizing the information rate and minimizing the computational cost. This result is also shown to be true for recurrent networks, by unfolding them in time. The theory clarifies the concept of Hebbian learning, establishes the power and limitations of local learning rules, introduces the learning channel which enables a formal analysis of the optimality of backpropagation, and explains the sparsity of the space of learning rules discovered so far. (C) 2016 Elsevier Ltd. All rights reserved.
C1 [Baldi, Pierre; Sadowski, Peter] Univ Calif Irvine, Dept Comp Sci, Irvine, CA 92697 USA.
RP Baldi, P (reprint author), Univ Calif Irvine, Dept Comp Sci, Irvine, CA 92697 USA.
EM pfbaldi@uci.edu; psadowsk@uci.edu
FU NSFNational Science Foundation (NSF) [IIS-1550705]; Google Faculty
   Research Award
FX Work supported in part by NSF grant IIS-1550705 and a Google Faculty
   Research Award to PB. We are also grateful for a hardware gift from
   NVDIA Corporation. This work was presented as a keynote talk at the 2015
   ICLR Conference and a preliminary version was posted on ArXiv under the
   title "The Ebb and Flow of Deep Learning".
CR ACKLEY DH, 1985, COGNITIVE SCI, V9, P147
   Anselmi F., 2014, ARXIV13114158V5
   BALDI P, 1988, IEEE T INFORM THEORY, V34, P523, DOI 10.1109/18.6032
   Baldi P, 2014, NAT COMMUN, V5, DOI 10.1038/ncomms5308
   Baldi P, 2012, DESIGN CODE CRYPTOGR, V65, P383, DOI 10.1007/s10623-012-9719-x
   BALDI P, 1988, J COMPUT SYST SCI, V36, P1, DOI 10.1016/0022-0000(88)90017-7
   BALDI P, 1987, PHYS REV LETT, V59, P1976, DOI 10.1103/PhysRevLett.59.1976
   Baldi P., 2012, NIPS 2012
   Baldi P., 2012, J MACH LEARN RES P T, V27, P37
   Baldi P, 2014, ARTIF INTELL, V210, P78, DOI 10.1016/j.artint.2014.02.004
   Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153
   Bengio Y., 2007, LARGE SCALE KERNEL M
   BIENENSTOCK EL, 1982, J NEUROSCI, V2, P32
   Block H., 1970, BOUNDEDNESS ITERATIV
   Bottou L, 2004, LECT NOTES ARTIF INT, V3176, P146
   Bottou L., 1998, ONLINE LEARNING NEUR
   Coles S., 2001, INTRO STAT MODELING
   COVER TM, 1965, IEEE TRANS ELECTRON, VEC14, P326, DOI 10.1109/PGEC.1965.264137
   Di Lena P, 2012, BIOINFORMATICS, V28, P2449, DOI 10.1093/bioinformatics/bts475
   Erhan D, 2010, J MACH LEARN RES, V11, P625
   Felleman DJ, 1991, CEREB CORTEX, V1, P1, DOI 10.1093/cercor/1.1.1
   FUKUSHIMA K, 1980, BIOL CYBERN, V36, P193, DOI 10.1007/BF00344251
   GALAMBOS J, 1987, ASYMPTOTIC THEORY EX
   Gelfand A., 2010, ADV NEURAL INFORM PR, P694
   Guan ZH, 2002, CELL, V111, P483, DOI 10.1016/S0092-8674(02)01074-7
   Hebb D., 1949, ORG BEHAV NEUROPHYCH
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554
   HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837
   Hyvarinen A, 1998, SIGNAL PROCESS, V64, P301, DOI 10.1016/S0165-1684(97)00197-7
   INTRATOR N, 1992, NEURAL NETWORKS, V5, P3, DOI 10.1016/S0893-6080(05)80003-6
   KLEITMAN D, 1969, P AM MATH SOC, V21, P677, DOI 10.2307/2036446
   KOHONEN T, 1995, SELF ORGANIZING MAPS
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   LAHIRI S, 2013, ADV NEURAL INF PROCE, V26, P1034
   LAW CC, 1994, P NATL ACAD SCI USA, V91, P7797, DOI 10.1073/pnas.91.16.7797
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y., 1990, ADV NEURAL INFORMATI, P598
   Lillicrap T. P., 2014, ARXIV14110247
   Mayford M, 2012, CSH PERSPECT BIOL, V4, DOI 10.1101/cshperspect.a005751
   Minsky M, 1969, PERCEPTRONS
   MUROGA S, 1965, IEEE TRANS ELECTRON, VEC14, P136, DOI 10.1109/PGEC.1965.263958
   OJA E, 1982, J MATH BIOL, V15, P267, DOI 10.1007/BF00275687
   Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0
   Polsky A, 2004, NAT NEUROSCI, V7, P621, DOI 10.1038/nn1253
   Polyanin A. D., 2002, HDB EXACT EOLUTIONS
   Robbins R., 1971, OPTIMIZING METHODS S, P233
   ROSENBLATT F, 1958, PSYCHOL REV, V65, P386, DOI 10.1037/h0042519
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Sutton R. S., 1998, REINFORCEMENT LEARNI, V1
   Valiant LG, 2012, NEURAL COMPUT, V24, P2873, DOI 10.1162/NECO_a_00357
   Vogel-Ciernia A, 2013, NAT NEUROSCI, V16, P552, DOI 10.1038/nn.3359
   Widrow B., 1960, IRE Wescon Convention Record, V4, P96
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1023/A:1022672621406
   Xie XH, 2003, NEURAL COMPUT, V15, P441, DOI 10.1162/089976603762552988
   Yamins DLK, 2016, NAT NEUROSCI, V19, P356, DOI 10.1038/nn.4244
   Zenke F, 2015, NAT COMMUN, V6, DOI 10.1038/ncomms7922
   ZIPSER D, 1988, NATURE, V331, P679, DOI 10.1038/331679a0
   Zuev Y. A., 1989, SOV MATH DOKL, V39, P512
NR 61
TC 20
Z9 20
U1 0
U2 13
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0893-6080
EI 1879-2782
J9 NEURAL NETWORKS
JI Neural Netw.
PD NOV
PY 2016
VL 83
BP 51
EP 74
DI 10.1016/j.neunet.2016.07.006
PG 24
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA EA1AI
UT WOS:000386320300006
PM 27584574
OA Other Gold
DA 2020-02-19
ER

PT J
AU Park, JG
   Jo, S
AF Park, Jung-Guk
   Jo, Sungho
TI Approximate Bayesian MLP regularization for regression in the presence
   of noise
SO NEURAL NETWORKS
LA English
DT Article
DE Bayesian method; Multilayer perceptron training; Non-smooth regression;
   Regularization; Weight-decay
ID NEURAL-NETWORK; WEIGHT DECAY; ALGORITHM; MACHINE; FRAMEWORK
AB We present a novel regularization method for a multilayer perceptron (MLP) that learns a regression function in the presence of noise regardless of how smooth the function is. Unlike general MLP regularization methods assuming that a regression function is smooth, the proposed regularization method is also valid when a regression function has discontinuities (non-smoothness). Since a true regression function to be learned is unknown, we examine a training set with our Bayesian approach that identifies non-smooth data, analyzing discontinuities in a regression function. The use of a Bayesian probability distribution identifies the non-smooth data. These identified data is used in a proposed objective function to fit an MLP response to the desired regression function regardless of its smoothness and noise. Experimental simulations show that the MLP with our presented training method yields more accurate fits to non-smooth functions than other MLP training methods. Further, we show that the suggested training methodology can be incorporated with deep learning models. (C) 2016 Elsevier Ltd. All rights reserved.
C1 [Park, Jung-Guk; Jo, Sungho] Korea Adv Inst Sci & Technol, Sch Comp, 291 Daehak Ro, Daejeon 305701, South Korea.
RP Jo, S (reprint author), Korea Adv Inst Sci & Technol, Sch Comp, 291 Daehak Ro, Daejeon 305701, South Korea.
EM jgparknn@kaist.ac.kr; shjo@kaist.ac.kr
RI Jo, Sungho/C-1691-2011
FU Technology Innovation Program - Ministry of Trade, Industry and Energy
   in Republic of Korea [10045252]
FX This work was supported by the Technology Innovation Program, 10045252,
   funded by the Ministry of Trade, Industry and Energy in Republic of
   Korea. The authors would like to thank the anonymous reviewers for their
   helpful and insightful comments.
CR Apostolopoulou MS, 2009, IEEE INTL CONF IND I, P216, DOI 10.1109/INDIN.2009.5195806
   Belsley D. A., 2007, REGRESSION DIAGNOSTI, P244
   Bowman A. W., 2008, STAT COMPUT, V16, P377
   Chamjangali MA, 2012, J CHIN CHEM SOC-TAIP, V59, P743, DOI 10.1002/jccs.201100417
   Connor P, 2015, NEURAL NETWORKS, V67, P121, DOI 10.1016/j.neunet.2015.03.005
   Esposito A, 2000, NEURAL NETWORKS, V13, P651, DOI 10.1016/S0893-6080(00)00035-6
   Foresee FD, 1997, 1997 IEEE INTERNATIONAL CONFERENCE ON NEURAL NETWORKS, VOLS 1-4, P1930, DOI 10.1109/ICNN.1997.614194
   Graf F, 2011, LECT NOTES COMPUT SC, V6892, P607, DOI 10.1007/978-3-642-23629-7_74
   Hagan M. T., 2014, NEURAL NETWORK DESIG
   HAGAN MT, 1994, IEEE T NEURAL NETWOR, V5, P989, DOI 10.1109/72.329697
   HORNIK K, 1989, NEURAL NETWORKS, V2, P359, DOI 10.1016/0893-6080(89)90020-8
   Kucuk N, 2013, RADIAT PHYS CHEM, V86, P10, DOI 10.1016/j.radphyschem.2013.01.021
   Larsen J., 1994, Neural Networks for Signal Processing IV. Proceedings of the 1994 IEEE Workshop (Cat. No.94TH0688-2), P42, DOI 10.1109/NNSP.1994.366065
   Lauer F, 2011, AUTOMATICA, V47, P608, DOI 10.1016/j.automatica.2011.01.020
   Little MA, 2009, IEEE T BIO-MED ENG, V56, P1015, DOI 10.1109/TBME.2008.2005954
   Llanas B, 2008, NEURAL PROCESS LETT, V27, P209, DOI 10.1007/s11063-007-9070-9
   Ludwig O, 2014, NEUROCOMPUTING, V124, P33, DOI 10.1016/j.neucom.2013.08.005
   MACKAY DJC, 1992, NEURAL COMPUT, V4, P448, DOI 10.1162/neco.1992.4.3.448
   Murphy K., 2007, TECHNICAL REPORT
   Oliver JF, 2013, IEEE T NUCL SCI, V60, P3399, DOI 10.1109/TNS.2013.2274702
   Paoletti S, 2010, IEEE T AUTOMAT CONTR, V55, P60, DOI 10.1109/TAC.2009.2034224
   Piliougine M, 2013, APPL ENERG, V112, P610, DOI 10.1016/j.apenergy.2013.05.053
   Pinzolas M, 2006, NEURAL COMPUT, V18, P1987, DOI 10.1162/neco.2006.18.8.1987
   Raiko T., 2012, P 15 INT C ART INT S
   Roll J, 2004, AUTOMATICA, V40, P37, DOI 10.1016/j.automatica.2003.08.006
   Sum J, 2009, LECT NOTES COMPUT SC, V5863, P494, DOI 10.1007/978-3-642-10677-4_56
   Tsanas A, 2012, ENERG BUILDINGS, V49, P560, DOI 10.1016/j.enbuild.2012.03.003
   Tufekci P, 2014, INT J ELEC POWER, V60, P126, DOI 10.1016/j.ijepes.2014.02.027
   Wilamowski BM, 2010, IEEE T NEURAL NETWOR, V21, P930, DOI 10.1109/TNN.2010.2045657
   Yang YM, 2015, IEEE T NEUR NET LEAR, V26, P1855, DOI 10.1109/TNNLS.2014.2357683
NR 30
TC 4
Z9 5
U1 1
U2 17
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0893-6080
EI 1879-2782
J9 NEURAL NETWORKS
JI Neural Netw.
PD NOV
PY 2016
VL 83
BP 75
EP 85
DI 10.1016/j.neunet.2016.07.010
PG 11
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA EA1AI
UT WOS:000386320300007
PM 27584575
DA 2020-02-19
ER

PT J
AU Lee, B
   Erdenee, E
   Jin, S
   Rhee, PK
AF Lee, Byungjae
   Erdenee, Enkhbayar
   Jin, Songguo
   Rhee, Phill Kyu
TI Efficient object detection using convolutional neural network-based
   hierarchical feature modeling
SO SIGNAL IMAGE AND VIDEO PROCESSING
LA English
DT Article
DE Object detection; Deep learning; Convolutional neural network;
   Hierarchical feature modeling
ID MIXTURE; SVMS
AB A hierarchical data-driven object detection framework is addressed considering a deep feature hierarchy of object appearances. The performance of many object detectors is degraded due to ambiguities in inter-class appearances and variations in intra-class appearances, but deep features extracted from visual objects show a strong hierarchical clustering property. Deep features were partitioned into unsupervised super-categories at the inter-class level, and augmented categories at the object level, to discover deep feature-driven information. A hierarchical feature model is built using a latent topic model algorithm, assembling a one-versus-all support vector machine at each node to constitute a hierarchical classification ensemble. Extensive experiments show that the proposed method is superior to state-of-the-art techniques using the PASCAL VOC 2007 and VOC 2012 datasets.
C1 [Lee, Byungjae; Erdenee, Enkhbayar; Jin, Songguo; Rhee, Phill Kyu] Inha Univ, 235 Yong Hyun Dong, Inchon, South Korea.
RP Rhee, PK (reprint author), Inha Univ, 235 Yong Hyun Dong, Inchon, South Korea.
EM jaylee@inha.edu; enkhbayar@inha.edu; sgkim735@inha.edu;
   pkrhee@inha.ac.kr
FU Inha University
FX This work was supported by an Inha University research grant. A GPU used
   in this research was generously donated by NVIDIA Corporation.
CR Aghazadeh O, 2012, LECT NOTES COMPUT SC, V7577, P115, DOI 10.1007/978-3-642-33783-3_9
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Bourdev L, 2010, LECT NOTES COMPUT SC, V6316, P168, DOI 10.1007/978-3-642-15567-3_13
   Cheng D, 2015, NEUROCOMPUTING, V149, P473, DOI 10.1016/j.neucom.2014.08.048
   Cinaroglu I, 2016, SIGNAL IMAGE VIDEO P, V10, P413, DOI 10.1007/s11760-015-0768-2
   Ding K, 2015, IEEE GEOSCI REMOTE S, V12, P577, DOI 10.1109/LGRS.2014.2351807
   Divvala SK, 2012, LECT NOTES COMPUT SC, V7585, P31, DOI 10.1007/978-3-642-33885-4_4
   Dong J, 2015, IEEE T CIRC SYST VID, V25, P1322, DOI 10.1109/TCSVT.2014.2355697
   Everingham M, 2012, PASCAL VISUAL OBJECT
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Fusek R, 2016, SIGNAL IMAGE VIDEO P, V10, P479, DOI 10.1007/s11760-015-0777-1
   Gidaris S., 2015, ARXIV151107763
   Gidaris S, 2015, IEEE I CONF COMP VIS, P1134, DOI 10.1109/ICCV.2015.135
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Goh KS, 2005, IEEE T KNOWL DATA EN, V17, P1333, DOI 10.1109/TKDE.2005.170
   Gu CH, 2012, LECT NOTES COMPUT SC, V7575, P445, DOI 10.1007/978-3-642-33765-9_32
   Gu CH, 2010, LECT NOTES COMPUT SC, V6315, P408
   He KM, 2015, IEEE T PATTERN ANAL, V37, P1904, DOI 10.1109/TPAMI.2015.2389824
   Hoiem D, 2012, LECT NOTES COMPUT SC, V7574, P340, DOI 10.1007/978-3-642-33712-3_25
   Kong T., 2016, ARXIV160400600
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Malisiewicz T, 2011, IEEE I CONF COMP VIS, P89, DOI 10.1109/ICCV.2011.6126229
   Park D, 2010, LECT NOTES COMPUT SC, V6314, P241, DOI 10.1007/978-3-642-15561-1_18
   Platt JC, 2000, ADV NEUR IN, P61
   Ruan ZW, 2015, IEEE SIGNAL PROC LET, V22, P244, DOI 10.1109/LSP.2014.2349940
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Song Z, 2011, PROC CVPR IEEE, P1585, DOI 10.1109/CVPR.2011.5995330
   Takarli F, 2016, SIGNAL IMAGE VIDEO P, V10, P93, DOI 10.1007/s11760-014-0706-8
   Wang LM, 2014, IEEE T IMAGE PROCESS, V23, P810, DOI 10.1109/TIP.2013.2295753
   Yu XY, 2015, IEEE SIGNAL PROC LET, V22, P1472, DOI 10.1109/LSP.2014.2299571
   Zhu XX, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.80
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
NR 33
TC 7
Z9 7
U1 0
U2 43
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 1863-1703
EI 1863-1711
J9 SIGNAL IMAGE VIDEO P
JI Signal Image Video Process.
PD NOV
PY 2016
VL 10
IS 8
BP 1503
EP 1510
DI 10.1007/s11760-016-0962-x
PG 8
WC Engineering, Electrical & Electronic; Imaging Science & Photographic
   Technology
SC Engineering; Imaging Science & Photographic Technology
GA DX7TP
UT WOS:000384592600017
DA 2020-02-19
ER

PT J
AU Wei, YC
   Liang, XD
   Chen, YP
   Jie, ZQ
   Xiao, YH
   Zhao, Y
   Yan, SC
AF Wei, Yunchao
   Liang, Xiaodan
   Chen, Yunpeng
   Jie, Zequn
   Xiao, Yanhui
   Zhao, Yao
   Yan, Shuicheng
TI Learning to segment with image-level annotations
SO PATTERN RECOGNITION
LA English
DT Article
DE Semantic segmentation; Weakly supervised; Deep learning
ID SEMANTIC SEGMENTATION
AB Recently, deep convolutional neural networks (DCNNs) have significantly promoted the development of semantic image segmentation. However, previous works on learning the segmentation network often rely on a large number of ground-truths with pixel-level annotations, which usually require considerable human effort. In this paper, we explore a more challenging problem by learning to segment under image level annotations. Specifically, our framework consists of two components. First, reliable hypotheses based localization maps are generated by incorporating the hypotheses-aware classification and cross image contextual refinement. Second, the segmentation network can be trained in a supervised manner by these generated localization maps. We explore two network training strategies for achieving good segmentation performance. For the first strategy, a novel multi-label cross-entropy loss is proposed to train the network by directly using multiple localization maps for all classes, where each pixel contributes to each class with different weights. For the second strategy, the rough segmentation mask can be inferred from the localization maps, and then the, network is optimized based on the single-label cross-entropy loss with the produced masks. We evaluate our methods on the PASCAL VOC 2012 segmentation benchmark. Extensive experimental results demonstrate the effectiveness of the proposed methods compared with the state-of-the-arts. (C) 2016 Elsevier Ltd. All rights reserved.
C1 [Wei, Yunchao; Zhao, Yao] Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.
   [Wei, Yunchao; Zhao, Yao] Beijing Key Lab Adv Informat Sci & Network Techno, Beijing 100044, Peoples R China.
   [Liang, Xiaodan] Sun Yat Sen Univ, Sch Adv Comp, Guangzhou 510006, Guangdong, Peoples R China.
   [Wei, Yunchao; Liang, Xiaodan; Chen, Yunpeng; Jie, Zequn; Yan, Shuicheng] Natl Univ Singapore, Vis & Machine Learning Lab, Singapore 117583, Singapore.
   [Xiao, Yanhui] Peoples Publ Secur Univ China, Beijing 100038, Peoples R China.
RP Zhao, Y (reprint author), Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.; Zhao, Y (reprint author), Beijing Key Lab Adv Informat Sci & Network Techno, Beijing 100044, Peoples R China.
EM yzhao@bjtu.edu.cn
FU National Basic Research Program of ChinaNational Basic Research Program
   of China [2012CB316400]; National NSF of ChinaNational Natural Science
   Foundation of China [61210006, 61532005, 61502506]; Program for
   Changjiang Scholars, Innovative Research Team in UniversityProgram for
   Changjiang Scholars & Innovative Research Team in University (PCSIRT)
   [IRT201206]; National High Technology Research and Development Program
   of ChinaNational High Technology Research and Development Program of
   China [2013AA013801]
FX This work is supported in part by National Basic Research Program of
   China (No. 2012CB316400), National NSF of China (61210006, 61532005,
   61502506), the Program for Changjiang Scholars, Innovative Research Team
   in University under Grant IRT201206 and National High Technology
   Research and Development Program of China (No. 2013AA013801).
CR Chen L. C., ARXIV14127062
   Chen LC, 2014, PROC CVPR IEEE, P3198, DOI 10.1109/CVPR.2014.409
   Dai J., ARXIV150301640
   Dai JF, 2015, PROC CVPR IEEE, P3992, DOI 10.1109/CVPR.2015.7299025
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Felzenszwalb Pedro, 2008, P IEEE C COMP VIS PA, V08, P1, DOI DOI 10.1109/CVPR.2008.4587597
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Felzenszwalb PF, 2010, PROC CVPR IEEE, P2241, DOI 10.1109/CVPR.2010.5539906
   Guillaumin M, 2014, INT J COMPUT VISION, V110, P328, DOI 10.1007/s11263-014-0713-9
   Hariharan B., ARXIV14115752
   Hariharan B, 2011, IEEE I CONF COMP VIS, P991, DOI 10.1109/ICCV.2011.6126343
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   jin X., ARXIV151207030
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Liang X., ARXIV151104517
   Liang X., ARXIV150902636
   Liang XD, 2015, IEEE I CONF COMP VIS, P1386, DOI 10.1109/ICCV.2015.163
   Lin L, 2015, IEEE T PATTERN ANAL, V37, P959, DOI 10.1109/TPAMI.2014.2359888
   Liu F., PATTERN REC IN PRESS
   Liu X., IEEE MULTIM IN PRESS
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lu JS, 2015, PROC CVPR IEEE, P3762, DOI 10.1109/CVPR.2015.7299000
   Maron O, 1998, ADV NEUR IN, V10, P570
   Mottaghi R, 2014, PROC CVPR IEEE, P891, DOI 10.1109/CVPR.2014.119
   Papandreou G., ARXIV150202734
   Pathak D., ARXIV14127144
   Pathak D., ARXIV150603648
   Pinheiro PO, 2015, PROC CVPR IEEE, P1713, DOI 10.1109/CVPR.2015.7298780
   Pont-Tuset J., ARXIV150300848
   Simonyan K., ARXIV14091556
   Szegedy C., ARXIV14094842
   Verbeek J., 2007, P IEEE C COMP VIS PA, V2007, P1, DOI DOI 10.1109/CVPR.2007.383098
   Vezhnevets A, 2011, IEEE I CONF COMP VIS, P643, DOI 10.1109/ICCV.2011.6126299
   Vezhnevets A, 2010, PROC CVPR IEEE, P3249, DOI 10.1109/CVPR.2010.5540060
   Wei Y., 2014, ARXIV14065726
   Xia W, 2013, IEEE I CONF COMP VIS, P2176, DOI 10.1109/ICCV.2013.271
   Xu J, 2015, PROC CVPR IEEE, P3781, DOI 10.1109/CVPR.2015.7299002
   Xu J, 2014, PROC CVPR IEEE, P3190, DOI 10.1109/CVPR.2014.408
   Yang W, 2014, PROC CVPR IEEE, P3182, DOI 10.1109/CVPR.2014.407
   Zheng S., ARXIV150203240
   Zhu J., 2014, NIPS, P1125
   Zuo Z., PATTERN REC IN PRESS
NR 43
TC 27
Z9 27
U1 1
U2 34
PU ELSEVIER SCI LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND
SN 0031-3203
EI 1873-5142
J9 PATTERN RECOGN
JI Pattern Recognit.
PD NOV
PY 2016
VL 59
SI SI
BP 234
EP 244
DI 10.1016/j.patcog.2016.01.015
PG 11
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
SC Computer Science; Engineering
GA DV5YY
UT WOS:000383007800021
DA 2020-02-19
ER

PT J
AU Chandrasekhar, V
   Lin, J
   Morere, O
   Goh, H
   Veillard, A
AF Chandrasekhar, Vijay
   Lin, Jie
   Morere, Olivier
   Goh, Hanlin
   Veillard, Antoine
TI A practical guide to CNNs and Fisher Vectors for image instance
   retrieval
SO SIGNAL PROCESSING
LA English
DT Article
DE Convolutional neural networks; Fisher Vectors; Image instance retrieval
ID SEARCH; FEATURES
AB With deep learning becoming the dominant approach in computer vision, the use of representations extracted from Convolutional Neural Nets (CNNs) is quickly gaining ground on Fisher Vectors (FVs) as favoured state-of-the-art global image descriptors for image instance retrieval. While the good performance of CNNs for image classification are unambiguously recognised, which of the two has the upper hand in the image retrieval context is not entirely clear yet.
   We propose a comprehensive study that systematically evaluates FVs and CNNs for image instance retrieval. The first part compares the performances of FVs and CNNs on multiple publicly available data sets and for multiple criteria. We show that no descriptor is systematically better than the other and that performance gains can usually be obtained by using both types together. The second part of the study focuses on the impact of geometrical transformations. We show that performance of CNNs can quickly degrade in the presence of certain transformations and propose a number of ways to incorporate the required invariances in the CNN pipeline.
   Our findings are organised as a reference guide offering practically useful and simply implementable guidelines to anyone looking for state-of-the-art global descriptors best suited to their specific image instance retrieval problem. (C) 2016 Elsevier B.V. All rights reserved.
C1 [Chandrasekhar, Vijay; Lin, Jie; Morere, Olivier; Goh, Hanlin] Inst Infocomm Res A STAR, 1 Fusionolopis Way,21-01 Connexis, Singapore 138632, Singapore.
   [Morere, Olivier; Veillard, Antoine] Univ Paris 06, 4 Pl Jussieu, F-75252 Paris, France.
RP Lin, J (reprint author), Inst Infocomm Res A STAR, 1 Fusionolopis Way,21-01 Connexis, Singapore 138632, Singapore.
EM lin-j@i2r.a-star.edu.sg
CR Arandjelovic R, 2013, PROC CVPR IEEE, P1578, DOI 10.1109/CVPR.2013.207
   Babenko A, 2014, LECT NOTES COMPUT SC, V8689, P584, DOI 10.1007/978-3-319-10590-1_38
   Chandrasekhar V. R., 2011, P 2 ANN ACM C MULT S, P117, DOI DOI 10.1145/1943552.1943568
   Chandrasekhar V, 2015, IEEE DATA COMPR CONF, P333, DOI 10.1109/DCC.2015.54
   Chen D, 2013, SIGNAL PROCESS, V93, P2316, DOI 10.1016/j.sigpro.2012.06.005
   Chen DM, 2015, IEEE T MULTIMEDIA, V17, P1019, DOI 10.1109/TMM.2015.2427744
   Chilimbi Trishul M, 2014, P OSDI, V14, P571
   Chum O, 2007, IEEE I CONF COMP VIS, P496
   Collobert R., 2011, NEUR INF PROC SYST W
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Duan LY, 2015, IEEE T MULTIMEDIA, V17, P828, DOI 10.1109/TMM.2015.2419973
   Duan LY, 2014, IEEE T MULTIMEDIA, V16, P346, DOI 10.1109/TMM.2013.2293063
   Francini G, 2013, SIGNAL PROCESS-IMAGE, V28, P311, DOI 10.1016/j.image.2012.11.002
   Ge TZ, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.132
   Gong YC, 2014, LECT NOTES COMPUT SC, V8695, P392, DOI 10.1007/978-3-319-10584-0_26
   Gordo A, 2012, PROC CVPR IEEE, P3045, DOI 10.1109/CVPR.2012.6248035
   Iscen A, 2015, IEEE T IMAGE PROCESS, V24, P2369, DOI 10.1109/TIP.2015.2423557
   Jegou H, 2008, LECT NOTES COMPUT SC, V5302, P304, DOI 10.1007/978-3-540-88682-2_24
   Jegou H, 2012, IEEE T PATTERN ANAL, V34, P1704, DOI 10.1109/TPAMI.2011.235
   Jegou H, 2014, PROC CVPR IEEE, P3310, DOI 10.1109/CVPR.2014.417
   Jegou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039
   Ji RR, 2013, IEEE T MULTIMEDIA, V15, P153, DOI 10.1109/TMM.2012.2225035
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Jiang YG, 2013, IEEE T MULTIMEDIA, V15, P442, DOI 10.1109/TMM.2012.2231061
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   Lin J., 2013, INT C AC SIGN PROC I
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mikolajczyk K., 2010, SOFTWARE COMPUTING H
   Nister D, 2006, IEEE COMP SOC C COMP, V2, P2161, DOI DOI 10.1109/CVPR.2006.264
   Ouyang WL, 2013, IEEE I CONF COMP VIS, P2056, DOI 10.1109/ICCV.2013.257
   Perronnin F, 2010, PROC CVPR IEEE, P3384, DOI 10.1109/CVPR.2010.5540009
   Quattoni A, 2009, PROC CVPR IEEE, P413, DOI 10.1109/CVPRW.2009.5206537
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Researcher MPEG, 2014, COMP DESCR VIS SEARC
   Sanchez J, 2013, INT J COMPUT VISION, V105, P222, DOI 10.1007/s11263-013-0636-x
   Simonyan K., 2015, INT C LEARN REPR ICL
   Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Takacs G, 2010, PROC CVPR IEEE, P934, DOI 10.1109/CVPR.2010.5540116
   Thomee B, 2016, COMMUN ACM, V59, P64, DOI 10.1145/2812802
   Tolias G, 2013, IEEE I CONF COMP VIS, P1401, DOI 10.1109/ICCV.2013.177
   Toshev A, 2014, PROC CVPR IEEE, P1653, DOI 10.1109/CVPR.2014.214
   Tuytelaars T, 2010, PROC CVPR IEEE, P2281, DOI 10.1109/CVPR.2010.5539911
   Vedaldi A, 2010, P 18 ACM INT C MULT, P1469, DOI DOI 10.1145/1873951.1874249
   Winder S, 2009, PROC CVPR IEEE, P178, DOI 10.1109/CVPRW.2009.5206839
   Wu R., ARXIV150102876
   XIAO JX, 2010, PROC CVPR IEEE, P3485, DOI DOI 10.1109/CVPR.2010.5539970
   Xu ZW, 2015, PROC CVPR IEEE, P1798, DOI 10.1109/CVPR.2015.7298789
   Yang F., 2014, P AS C COMP VIS, P19
   Zhou B, 2014, ADV NEURAL INFORM PR, P487, DOI DOI 10.1162/153244303322533223
NR 51
TC 26
Z9 27
U1 0
U2 57
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0165-1684
EI 1872-7557
J9 SIGNAL PROCESS
JI Signal Process.
PD NOV
PY 2016
VL 128
BP 426
EP 439
DI 10.1016/j.sigpro.2016.05.021
PG 14
WC Engineering, Electrical & Electronic
SC Engineering
GA DR2AH
UT WOS:000379706500041
DA 2020-02-19
ER

PT J
AU Wang, ZB
   Cui, XH
   Gao, L
   Yin, Q
   Ke, L
   Zhang, SR
AF Wang, Zhibo
   Cui, Xiaohui
   Gao, Lu
   Yin, Qi
   Ke, Lei
   Zhang, Shurong
TI A hybrid model of sentimental entity recognition on mobile social media
SO EURASIP JOURNAL ON WIRELESS COMMUNICATIONS AND NETWORKING
LA English
DT Article
DE Feature selection; Sentiment analysis; Sentiment classification; Entity
   recognition
AB With new forms of media such as Twitter becoming increasingly popular, the Internet is now the main conduit of individual and interpersonal messages. A considerable amount of people express their personal opinions about news-related subject through Twitter, a popular SNS platform based on human relationships. It provides us a data source that we can use to extract peoples' opinions which are important for product review and public opinion monitoring. In this paper, a hybrid sentimental entity recognition model (HSERM) has been designed. Utilizing 100 million collected messages from Twitter, the hashtag is regarded as the label for sentimental classification. In the meanwhile, features as emoji and N-grams have been extracted and classified the collected topic messages into four different sentiment categories based on the circumplex sentimental model. Finally, machine learning methods are used to classify the sentimental data set, and an 89 % precise result has been achieved. Further, entities that are behind emotions could be gotten with the help of SENNA deep learning model.
C1 [Wang, Zhibo; Cui, Xiaohui; Gao, Lu; Yin, Qi; Ke, Lei; Zhang, Shurong] Wuhan Univ, Int Sch Software, Wuhan 430079, Peoples R China.
   [Wang, Zhibo] East China Univ Technol, Sch Software, Nanchang 330013, Peoples R China.
RP Cui, XH (reprint author), Wuhan Univ, Int Sch Software, Wuhan 430079, Peoples R China.
EM xcui@whu.edu.cn
FU National Nature Science Foundation of ChinaNational Natural Science
   Foundation of China [61440054]; Fundamental Research Funds for the
   Central Universities of ChinaFundamental Research Funds for the Central
   Universities [216274213]; Nature Science Foundation of Hubei, China
   [2014CFA048]; Outstanding Academic Talents Startup Funds of Wuhan
   University [216-410100003]; National Natural Science Foundation of
   ChinaNational Natural Science Foundation of China [61462004]; Natural
   Science Foundation of Jiangxi ProvinceNatural Science Foundation of
   Jiangxi Province [20151BAB207042]; Youth Funds of Science and Technology
   in Jiangxi Province Department of Education [GJJ150572]
FX This research is supported in part by the National Nature Science
   Foundation of China No. 61440054, Fundamental Research Funds for the
   Central Universities of China No. 216274213, and Nature Science
   Foundation of Hubei, China No. 2014CFA048. Outstanding Academic Talents
   Startup Funds of Wuhan University, No. 216-410100003. National Natural
   Science Foundation of China (No. 61462004). Natural Science Foundation
   of Jiangxi Province (No. 20151BAB207042). Youth Funds of Science and
   Technology in Jiangxi Province Department of Education (No. GJJ150572).
CR Agarwal Apoorv, 2011, P WORKSH LANG SOC ME, P30
   Coh CL, 2003, P 41 ANN M ASS COMP, P197
   Cui XH, 2015, PERS UBIQUIT COMPUT, V19, P1125, DOI 10.1007/s00779-015-0877-5
   Degen H, 2001, J CHIN INFORM PROCES, V02
   Derczynski L, 2015, INFORM PROCESS MANAG, V51, P32, DOI 10.1016/j.ipm.2014.10.006
   Go Alec, 2009, TWITTER SENTIMENT CL, P1, DOI DOI 10.1016/J.SEDGEO.2006.07.004
   Kucuk D., 2014, LANG RES EV C
   [廖祥文 Liao Xiangwen], 2013, [中文信息学报, Journal of Chinese Information Processing], V27, P56
   Pak A., 2010, 7 C INT LANG RES EV
   Pavitra R, 2015, 2015 2ND INTERNATIONAL CONFERENCE ON ELECTRONICS AND COMMUNICATION SYSTEMS (ICECS), P889, DOI 10.1109/ECS.2015.7125042
   Read J., 2005, P ACL STUD RES WORKS, P43, DOI DOI 10.3115/1628960.1628969
   Wang ZB, 2015, 2015 INTERNATIONAL CONFERENCE ON IDENTIFICATION, INFORMATION, AND KNOWLEDGE IN THE INTERNET OF THINGS (IIKI), P62, DOI 10.1109/IIKI.2015.20
   Xu G, 2015, Patent No. [US 9009134, 9009134]
   Zhibo W, 2015, CCBD2015
NR 14
TC 3
Z9 3
U1 1
U2 55
PU SPRINGEROPEN
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 1687-1472
EI 1687-1499
J9 EURASIP J WIREL COMM
JI EURASIP J. Wirel. Commun. Netw.
PD OCT 24
PY 2016
AR 253
DI 10.1186/s13638-016-0745-7
PG 12
WC Engineering, Electrical & Electronic; Telecommunications
SC Engineering; Telecommunications
GA EB0IB
UT WOS:000387026400001
OA DOAJ Gold
DA 2020-02-19
ER

PT J
AU Ronao, CA
   Cho, SB
AF Ronao, Charissa Ann
   Cho, Sung-Bae
TI Human activity recognition with smartphone sensors using deep learning
   neural networks
SO EXPERT SYSTEMS WITH APPLICATIONS
LA English
DT Article
DE Human activity recognition; Deep learning; Convolutional neural network;
   Smartphone; Sensors
ID MOTION
AB Human activities are inherently translation invariant and hierarchical. Human activity recognition (HAR), a field that has garnered a lot of attention in recent years due to its high demand in various application domains, makes use of time-series sensor data to infer activities. In this paper, a deep convolutional neural network (convnet) is proposed to perform efficient and effective HAR using smartphone sensors by exploiting the inherent characteristics of activities and 1D time-series signals, at the same time providing a way to automatically and data-adaptively extract robust features from raw data. Experiments show that convnets indeed derive relevant and more complex features with every additional layer, although difference of feature complexity level decreases with every additional layer. A wider time span of temporal local correlation can be exploited (1 x 9-1 x 14) and a low pooling size (1 x 2-1 x 3) is shown to be beneficial. Convnets also achieved an almost perfect classification on moving activities, especially very similar ones which were previously perceived to be very difficult to classify. Lastly, convnets outperform other state-of-the-art data mining techniques in HAR for the benchmark dataset collected from 30 volunteer subjects, achieving an overall performance of 94.79% on the test set with raw sensor data, and 95.75% with additional information of temporal fast Fourier transform of the HAR data set. (C) 2016 Published by Elsevier Ltd.
C1 [Ronao, Charissa Ann; Cho, Sung-Bae] Yonsei Univ, Dept Comp Sci, 50 Yonsei Ro, Seoul 120749, South Korea.
RP Cho, SB (reprint author), Yonsei Univ, Dept Comp Sci, 50 Yonsei Ro, Seoul 120749, South Korea.
EM cvronao@sclab.yonsei.ac.kr; sbcho@yonsei.ac.kr
FU MSIP (Ministry of Science, ICT and Future Planning), Korea, under the
   ITRC (Information Technology Research Center) support program
   [IITP-2016-R0992-15-1011]
FX This research was supported by the MSIP (Ministry of Science, ICT and
   Future Planning), Korea, under the ITRC (Information Technology Research
   Center) support program (IITP-2016-R0992-15-1011) supervised by the IITP
   (Institute for Information & communications Technology Promotion).
CR Anguita D., 2013, P EUR S ART NEUR NET, P437
   Anguita D., 2012, LECT NOTES COMPUTER, P216, DOI DOI 10.1007/978-3-642-35395-6_30
   Bao L, 2004, LECT NOTES COMPUT SC, V3001, P1
   Bengio Yoshua, 2012, ARXIV12065533V2
   Bergstra J., 2010, P PYTH SCI COMP C SC, P3
   Bhattacharya S, 2014, PERVASIVE MOB COMPUT, V15, P242, DOI 10.1016/j.pmcj.2014.05.006
   Chen LM, 2012, IEEE T SYST MAN CY C, V42, P790, DOI 10.1109/TSMCC.2012.2198883
   Duffner Stefan, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P5432, DOI 10.1109/ICASSP.2014.6854641
   Duong T, 2009, ARTIF INTELL, V173, P830, DOI 10.1016/j.artint.2008.12.005
   Foerster F, 1999, COMPUT HUM BEHAV, V15, P571, DOI 10.1016/S0747-5632(99)00037-0
   Goodfellow I. J., 2013, ARXIV13084214
   Hinton G. E, 2012, ARXIV12070580
   KHAN AM, 2013, INT J INFORM ED TECH, V3, P60, DOI DOI 10.7763/IJIET.2013.V3.234
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kwapisz J. R., 2011, ACM SIGKDD EXPLORATI, V12, P74, DOI DOI 10.1145/1964897.1964918
   Lara OD, 2013, IEEE COMMUN SURV TUT, V15, P1192, DOI 10.1109/SURV.2012.110112.00192
   LeCun Y., 1998, HDB BRAIN THEORY NEU, V3361, P255
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lee YS, 2011, LECT NOTES ARTIF INT, V6678, P460, DOI 10.1007/978-3-642-21219-2_58
   Plotz T., 2011, IJCAI P INT JOINT C, VTwo, P1729, DOI DOI 10.5591/978-1-57735-516-8/LICA111-290
   Sharma A, 2008, THIRD 2008 INTERNATIONAL CONFERENCE ON CONVERGENCE AND HYBRID INFORMATION TECHNOLOGY, VOL 1, PROCEEDINGS, P430, DOI 10.1109/ICCIT.2008.394
   Shoaib M, 2014, SENSORS-BASEL, V14, P10146, DOI 10.3390/s140610146
   Swietojanski P, 2014, IEEE SIGNAL PROC LET, V21, P1120, DOI 10.1109/LSP.2014.2325781
   Vollmer C, 2013, LECT NOTES COMPUT SC, V8131, P367, DOI 10.1007/978-3-642-40728-4_46
   Wu WM, 2012, J MED INTERNET RES, V14, DOI 10.2196/jmir.2208
   Yang JB, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3995
   Yongmou Li, 2014, Mining Intelligence and Knowledge Exploration. Second International Conference, MIKE 2014. Proceedings: LNCS 8891, P99, DOI 10.1007/978-3-319-13817-6_11
   Zeng M, 2014, 2014 6TH INTERNATIONAL CONFERENCE ON MOBILE COMPUTING, APPLICATIONS AND SERVICES (MOBICASE), P197, DOI 10.4108/icst.mobicase.2014.257786
   Zheng Y, 2014, LECT NOTES COMPUT SC, V8485, P298, DOI 10.1007/978-3-319-08010-9_33
NR 29
TC 167
Z9 183
U1 20
U2 326
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0957-4174
EI 1873-6793
J9 EXPERT SYST APPL
JI Expert Syst. Appl.
PD OCT 15
PY 2016
VL 59
BP 235
EP 244
DI 10.1016/j.eswa.2016.04.032
PG 10
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic; Operations Research & Management Science
SC Computer Science; Engineering; Operations Research & Management Science
GA DN7BO
UT WOS:000377230500018
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Liu, NAF
   Zhang, M
   Li, HQ
   Sun, ZA
   Tan, TN
AF Liu, Nianfeng
   Zhang, Man
   Li, Haiqing
   Sun, Zhenan
   Tan, Tieniu
TI DeepIris: Learning pairwise filter bank for heterogeneous iris
   verification
SO PATTERN RECOGNITION LETTERS
LA English
DT Article
DE Biometrics; Iris verification; Convolutional neural networks; Deep
   learning; Iris recognition
AB Heterogeneous iris recognition (HIR) is in great demand for a large-scale identity management system. Iris images acquired in heterogeneous environment have large intra-class variations, such as different resolutions or different sensor optics, etc. Therefore, it is challenging to manually design a robust encoding filter to face the complex intra-class variations of heterogeneous iris images. This paper proposes a deep learning based framework for heterogeneous iris verification, namely DeepIris, which learns relational features to measure the similarity between pairs of iris images based on convolutional neural networks. DeepIris is a novel solution to iris recognition in two main aspects. (1) DeepIris learns a pairwise filter bank to establish the relationship between heterogeneous iris images, where pairs of filters are learned from two heterogeneous sources. (2) Different from two separate steps in terms of handcrafted feature extraction and feature matching in conventional solutions, DeepIris directly learns a nonlinear mapping function between pairs of iris images and their identity supervision with a pairwise filter bank (PFB) from different sources. Thus, the learned pairwise filters can adapt to new sources when given new training data. Extensive experimental results on the Q-FIRE and the CASIA cross sensor datasets demonstrate that EER (Equal Error Rate) of heterogeneous iris verification is reduced by 90% using DeepIris compared to traditional methods. (C) 2015 Elsevier B.V. All rights reserved.
C1 [Liu, Nianfeng; Zhang, Man; Li, Haiqing; Sun, Zhenan; Tan, Tieniu] Chinese Acad Sci, CAS Ctr Excellence Brain Sci & Intelligence Techn, Ctr Res Intelligent Percept & Comp, Natl Lab Pattern Recognit,Inst Automat, Beijing, Peoples R China.
RP Zhang, M (reprint author), Chinese Acad Sci, CAS Ctr Excellence Brain Sci & Intelligence Techn, Ctr Res Intelligent Percept & Comp, Natl Lab Pattern Recognit,Inst Automat, Beijing, Peoples R China.
EM zhangman@nlpr.ia.ac.cn
OI Wang, Yunlong/0000-0002-3535-308X
FU National Basic Research Program of ChinaNational Basic Research Program
   of China [2012CB316300]; National Natural Science Foundation of
   ChinaNational Natural Science Foundation of China [61273272]; Beijing
   Baidu Netcom Science Technology Co., Ltd.
FX This work is supported by National Basic Research Program of China
   (Grant no. 2012CB316300), National Natural Science Foundation of China
   (Grant no. 61273272), and Beijing Baidu Netcom Science Technology Co.,
   Ltd.
CR Arora S., 2012, P INT C BIOM THEOR A, DOI 10.1109/BTAS.2012.6374599
   Ben XY, 2012, NEUROCOMPUTING, V97, P44, DOI 10.1016/j.neucom.2012.06.022
   Biswas S, 2012, IEEE T PATTERN ANAL, V34, P2019, DOI 10.1109/TPAMI.2011.278
   Blitzer J., 2005, ADV NEURAL INFORM PR, P1473
   Bowyer Kevin W., 2009, Identity in the Information Society, V2, P327, DOI 10.1007/s12394-009-0037-z
   Connaughton R., 2011, P IEEE COMP SOC C CO, DOI [10.1109/CVPRW.2011.5981814, DOI 10.1109/CVPRW.2011.5981814.]
   Connaughton R, 2012, IEEE T INF FOREN SEC, V7, P919, DOI 10.1109/TIFS.2012.2190575
   DAUGMAN JG, 1993, IEEE T PATTERN ANAL, V15, P1148, DOI 10.1109/34.244676
   Davis J.V., 2007, P INT C MACH LEARN
   He ZF, 2009, IEEE T PATTERN ANAL, V31, P1670, DOI 10.1109/TPAMI.2008.183
   Johnson P., 2010, P INT C BIOM THEOR A
   Nguyen K, 2011, IEEE T INF FOREN SEC, V6, P1248, DOI 10.1109/TIFS.2011.2159597
   Krizhevsky Alex, 2012, P ADV NEUR INF PROC
   Liu J., 2014, NEUROCOMPUTING
   Liu J., 2013, P INT C BIOM THEOR A, DOI [10.1109/BTAS.2013.6712692, DOI 10.1109/BTAS.2013.6712692.]
   Matey JR, 2006, P IEEE, V94, P1936, DOI 10.1109/JPROC.2006.884091
   Nguyen K., 2012, P IEEE C COMP VIS PA, DOI [10.1109/CVPR.2012.6247984, DOI 10.1109/CVPR.2012.6247984.]
   Shin K. Y., 2009, P INT C UB INF TECHN, DOI [10.1109/ICUT.2009.5405701, DOI 10.1109/ICUT.2009.5405701.]
   Xiao L., 2013, P INT C BIOM THEOR A, DOI [10.1109/BTAS.2013.6712752, DOI 10.1109/BTAS.2013.6712752.]
   Xiao LH, 2013, 2013 SECOND IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION (ACPR 2013), P246, DOI 10.1109/ACPR.2013.34
   Xing E P, 2002, ADV NEURAL INFORM PR, P505
NR 21
TC 34
Z9 36
U1 2
U2 14
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0167-8655
EI 1872-7344
J9 PATTERN RECOGN LETT
JI Pattern Recognit. Lett.
PD OCT 15
PY 2016
VL 82
BP 154
EP 161
DI 10.1016/j.patrec.2015.09.016
PN 2
PG 8
WC Computer Science, Artificial Intelligence
SC Computer Science
GA EA8GZ
UT WOS:000386874600008
DA 2020-02-19
ER

PT J
AU Duan, GL
   Hu, WX
   Wang, JR
AF Duan, Ganglong
   Hu, Wenxiu
   Wang, Jianren
TI Research on the natural image super-resolution reconstruction algorithm
   based on compressive perception theory and deep learning model
SO NEUROCOMPUTING
LA English
DT Article
DE Image super-resolution; Compressive perception and sensing; Deep
   learning model; Neural network structure; Optimization; Natural images;
   Image restoration
ID CLASSIFICATION; REGULARIZATION; RECOGNITION; FRAMEWORK; MACHINE
AB With the bursting development of machine learning and artificial intelligence, the pattern recognition based image processing techniques are growing faster than ever before. In this paper, we conduct theoretical analysis on the natural image super-resolution reconstruction algorithm based on compressive perception theory and deep learning model. The image restoration is the purpose of the degraded image processing which make its recovery as it had been before the degradation of ideal image. According to the views of Fourier optics, optical imaging system is a low pass filter, due to the general influence of optical diffraction. The deep neural network with hierarchical unsupervised training method stratified greed training beforehand matter will be the result of the training as the novel learning supervision probability model of the initial value to make good use of the optical imaging system. The adopted compressed sensing theory points out that as long as signal is compressible or sparse, so, if there is a transformation matrix is not related observation matrix on signal can directly obtain compressed form of the original signal. Our research adopts the advances of the mentioned technique, in the training step, we use deep neural network to automatically capture the features and in the reconstruction procedure we use the compressive sensing and dictionary learning theory to reconstruct the high resolution image. By enhancing both of the steps, our experimental result indicates the feasibility of the novel algorithm. The prospect is also discussed in the final part. (C) 2016 Elsevier B.V. All rights reserved.
C1 [Duan, Ganglong; Hu, Wenxiu; Wang, Jianren] Xian Univ Technol, Xian 710054, Shaanxi, Peoples R China.
RP Duan, GL (reprint author), Xian Univ Technol, Xian 710054, Shaanxi, Peoples R China.
EM gl-duan@xaut.edu.cn
FU project of Xi'an Science and Technology fund: Technology Research of
   Urban Intelligent Transportation System Based on big data [cxy1509(14)];
   Natural Science Foundation of the Shaanxi Province education department:
   flexible recommendation system based on hybrid strategy research and
   development [14JK1522]
FX This research is supported by the project of Xi'an Science and
   Technology fund: Technology Research of Urban Intelligent Transportation
   System Based on big data (No. cxy1509(14)) and the project supported by
   the Natural Science Foundation of the Shaanxi Province education
   department: flexible recommendation system based on hybrid strategy
   research and development (No. 14JK1522).
CR Bi CJ, 2014, INT CONF CLOUD COMPU, P327, DOI 10.1109/CCIS.2014.7175753
   Bi CJ, 2015, PROCEEDINGS OF 2015 INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTING AND INTERNET OF THINGS, P55, DOI 10.1109/ICAIOT.2015.7111537
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Caron J, 2014, CELL ADHES MIGR, V8, P430, DOI 10.4161/cam.29358
   Chen XX, 2014, IEEE SIGNAL PROC LET, V21, P79, DOI 10.1109/LSP.2013.2286417
   Chooi W.K., 2014, BR J RADIOL
   Ding Lu, 2015, EUR C BIOM OPT INT S
   Domhan Tobias, 2014, ICML 2014 AUTOML WOR
   Dong Wang, 2015, Image and Graphics. 8th International Conference, ICIG 2015. Proceedings: LNCS 9218, P373, DOI 10.1007/978-3-319-21963-9_34
   Esser Ernie, 2014, TECH REP
   Gao J, 2015, NANOSCALE, V7, P2511, DOI 10.1039/c4nr04962d
   Gao SH, 2014, IEEE T MULTIMEDIA, V16, P762, DOI 10.1109/TMM.2014.2299516
   Goyal S., 2014, ARXIV14123684
   Gulcehre Caglar, 2014, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2014. Proceedings: LNCS 8724, P530, DOI 10.1007/978-3-662-44848-9_34
   Han Xixuan, 2015, PATTERN RECOGNIT
   Hill MRH, 2015, J NEUROPHYSIOL, V113, P1260, DOI 10.1152/jn.00595.2014
   Hirvonen Toni, 2015, AUDIO ENG SOC CONVEN, V138
   Honda O., 2014, BR J RADIOL
   Hou Le, 2015, ARXIV150407947
   Hou SG, 2014, SCI CHINA CHEM, V57, P100, DOI 10.1007/s11426-013-5014-6
   Ijjina Earnest, 2014, COMP VIS ACCV 2014 W
   Lee MK, 2014, J AM CHEM SOC, V136, P14003, DOI 10.1021/ja508028h
   Leng Biao, 2015, INF SCI
   Li Deng, 2014, P 2014 IEEE INT C AC
   Li H., 2014, ARXIV14124526
   Li P, 2014, ANN IEEE SYM FIELD P, P172, DOI 10.1109/FCCM.2014.54
   Li Xiaodong, 2014, SEL TOP APPL EARTH O, V7
   Li Yuming, 2015, NO REFERENCE VIDEO Q
   Long MS, 2014, IEEE T KNOWL DATA EN, V26, P1076, DOI 10.1109/TKDE.2013.111
   Lu J., 2014, IEEE T INF FOREN SEC, V9, P41
   Lu JW, 2015, IEEE T PATTERN ANAL, V37, P2041, DOI 10.1109/TPAMI.2015.2408359
   Lu JW, 2014, IEEE T PATTERN ANAL, V36, P331, DOI 10.1109/TPAMI.2013.134
   Lu JW, 2013, IEEE T INF FOREN SEC, V8, P510, DOI 10.1109/TIFS.2013.2243146
   Lu JW, 2013, IEEE T PATTERN ANAL, V35, P39, DOI 10.1109/TPAMI.2012.70
   Lu JW, 2010, IEEE T SYST MAN CY B, V40, P958, DOI 10.1109/TSMCB.2009.2032926
   Maiseli BJ, 2015, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-015-0077-2
   Mozaffari A, 2014, NEUROCOMPUTING, V131, P143, DOI 10.1016/j.neucom.2013.10.030
   Niu G, 2014, NEURAL COMPUT, V26, P1717, DOI 10.1162/NECO_a_00614
   Peleg T, 2014, IEEE T IMAGE PROCESS, V23, P2569, DOI 10.1109/TIP.2014.2305844
   Proppert S, 2014, OPT EXPRESS, V22, P10304, DOI 10.1364/OE.22.010304
   Revelo NH, 2014, J CELL BIOL, V205, P591, DOI 10.1083/jcb.201402066
   Schmidt Uwe, 2014, ARXIV14042086
   Sidky Emil Y., 2014, TRANSLATIONAL ENG HL, V2, P1
   Toth Laszlo, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P190, DOI 10.1109/ICASSP.2014.6853584
   Wang HX, 2014, PROC INT C TOOLS ART, P853, DOI 10.1109/ICTAI.2014.131
   Wang ZY, 2015, IEEE T GEOSCI REMOTE, V53, P1161, DOI 10.1109/TGRS.2014.2335177
   William R, 2015, P 4 JOINT C LEX COMP, P279
   Xiao YL, 2014, 2014 INTERNATIONAL CONFERENCE ON CLOUD COMPUTING AND INTERNET OF THINGS (CCIOT), P109, DOI 10.1109/CCIOT.2014.7062516
   Yan Y, 2015, IEEE T IMAGE PROCESS, V24, P2984, DOI 10.1109/TIP.2015.2438540
   Yan Y, 2015, IEEE T IMAGE PROCESS, V24, P1867, DOI 10.1109/TIP.2015.2413294
   Yan Y, 2013, IEEE INT C BIOINFORM
   Yan Y, 2014, IEEE T IMAGE PROCESS, V23, P5599, DOI 10.1109/TIP.2014.2365699
   Yan Y, 2014, COMPUT VIS IMAGE UND, V124, P99, DOI 10.1016/j.cviu.2014.02.006
   Yang H, 2014, SMALL, V10, P1712, DOI 10.1002/smll.201302942
   Yu JF, 2014, IEEE T NEUR NET LEAR, V25, P780, DOI 10.1109/TNNLS.2013.2281313
   Yuan Q, 2014, INT J NEURAL SYST, V24, DOI 10.1142/S0129065714500154
   Zeng D., 2014, COLING, P2335
   Zhang YD, 2014, PROG ELECTROMAGN RES, V144, P171, DOI 10.2528/PIER13121310
   Zhao Q, 2015, IEEE T NEUR NET LEAR, V26, P825, DOI 10.1109/TNNLS.2014.2387376
   Zhao YQ, 2014, IEEE J-STARS, V7, P2671, DOI 10.1109/JSTARS.2013.2292824
NR 60
TC 10
Z9 10
U1 4
U2 112
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0925-2312
EI 1872-8286
J9 NEUROCOMPUTING
JI Neurocomputing
PD OCT 5
PY 2016
VL 208
SI SI
BP 117
EP 126
DI 10.1016/j.neucom.2015.12.125
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA DV3AV
UT WOS:000382794300011
DA 2020-02-19
ER

PT J
AU Shafiee, MJ
   Siva, P
   Scharfenberger, C
   Fieguth, P
   Wong, A
AF Shafiee, Mohammad Javad
   Siva, Parthipan
   Scharfenberger, Christian
   Fieguth, Paul
   Wong, Alexander
TI NeRD: A Neural Response Divergence Approach to Visual Saliency Detection
SO IEEE SIGNAL PROCESSING LETTERS
LA English
DT Article
DE Deep learning; neural response distinctiveness; neural response
   divergence (NeRD); saliency detection; StochasticNet
ID OBJECT DETECTION; ATTENTION; MODEL
AB In this letter, a novel approach to visual saliency detection via neural response divergence (NeRD) is proposed, where synaptic portions of deep neural networks, previously trained for complex object recognition, are leveraged to compute low-level cues that can be used to compute image region distinctiveness. Based on this concept, an efficient visual saliency detection framework is proposed using deep convolutional StochasticNets. Experimental results using complex scene saliency dataset and MSRA10k natural image datasets show that the proposed NeRD approach can achieve improved performance when compared to state-of-the-art image saliency approaches, while attaining low computational complexity necessary for near-real-time computer vision applications.
C1 [Shafiee, Mohammad Javad; Scharfenberger, Christian; Fieguth, Paul; Wong, Alexander] Univ Waterloo, Dept Syst Design Engn, Waterloo, ON N2L 3G1, Canada.
   [Siva, Parthipan] Aimetis Corp, Waterloo, ON N2L 4E9, Canada.
RP Shafiee, MJ (reprint author), Univ Waterloo, Dept Syst Design Engn, Waterloo, ON N2L 3G1, Canada.
EM mjshafiee@uwaterloo.ca; parthipan.siva@aimetis.com;
   cscharfenberger@uwaterloo.ca; pfieguth@uwaterloo.ca;
   alexander.wong@uwaterloo.ca
OI Shafiee, Mohammad Javad/0000-0001-5989-8255
FU Natural Sciences and Engineering Research Council of CanadaNatural
   Sciences and Engineering Research Council of Canada; Canada Research
   Chairs ProgramCanada Research Chairs; Ontario Ministry of Research and
   InnovationMinistry of Research and Innovation, Ontario
FX This work was supported by the Natural Sciences and Engineering Research
   Council of Canada, Canada Research Chairs Program, and the Ontario
   Ministry of Research and Innovation.
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Achanta R, 2010, IEEE IMAGE PROC, P2653, DOI 10.1109/ICIP.2010.5652636
   Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Adelson EH, 2001, PROC SPIE, V4299, P1, DOI 10.1117/12.429489
   Albertazzi L., 2013, HDB EXPT PHENOMENOLO
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Cheng MM, 2015, IEEE T PATTERN ANAL, V37, P569, DOI 10.1109/TPAMI.2014.2345401
   Deb A, 2016, PR IEEE COMP DESIGN, P17, DOI 10.1109/ICCD.2016.7753256
   Fang YM, 2014, IEEE T IMAGE PROCESS, V23, P3910, DOI 10.1109/TIP.2014.2336549
   Fang YM, 2014, IEEE T CIRC SYST VID, V24, P27, DOI 10.1109/TCSVT.2013.2273613
   Fang YM, 2012, IEEE T IMAGE PROCESS, V21, P3888, DOI 10.1109/TIP.2012.2199126
   Goferman S, 2012, IEEE T PATTERN ANAL, V34, P1915, DOI 10.1109/TPAMI.2011.272
   Han JW, 2015, IEEE T CIRC SYST VID, V25, P1309, DOI 10.1109/TCSVT.2014.2381471
   Han JW, 2013, IEEE T CIRC SYST VID, V23, P2009, DOI 10.1109/TCSVT.2013.2242594
   Hou X., 2007, IEEE C COMP VIS PATT, V2007, P1, DOI DOI 10.1109/CVPR.2007.383267
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jiang HZ, 2013, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2013.271
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   LeCun Y, 2010, IEEE INT SYMP CIRC S, P253, DOI 10.1109/ISCAS.2010.5537907
   Lee H., 2009, P 26 ANN INT C MACH, P609, DOI DOI 10.1145/1553374.1553453
   Lin JY, 2009, CURR BIOL, V19, P1118, DOI 10.1016/j.cub.2009.05.021
   Liu NA, 2016, PROC CVPR IEEE, P678, DOI 10.1109/CVPR.2016.80
   Mai L, 2014, LECT NOTES COMPUT SC, V8691, P76, DOI 10.1007/978-3-319-10578-9_6
   Mai L, 2013, PROC CVPR IEEE, P1131, DOI 10.1109/CVPR.2013.150
   Margolin R, 2013, PROC CVPR IEEE, P1139, DOI 10.1109/CVPR.2013.151
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Rahtu E, 2010, LECT NOTES COMPUT SC, V6315, P366, DOI 10.1007/978-3-642-15555-0_27
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Scharfenberger C, 2015, IEEE T IMAGE PROCESS, V24, P457, DOI 10.1109/TIP.2014.2380351
   Scharfenberger C, 2013, PROC CVPR IEEE, P979, DOI 10.1109/CVPR.2013.131
   Shafiee M, 2015, IEEE T MED IMAGING, V1, P1
   Shafiee M. J., 2016, P C COMP VIS PATT RE, P28
   Shafiee MJ, 2016, IEEE ACCESS, V4, P1915, DOI 10.1109/ACCESS.2016.2551458
   Shen XH, 2012, PROC CVPR IEEE, P853, DOI 10.1109/CVPR.2012.6247758
   Shi KY, 2013, PROC CVPR IEEE, P2115, DOI 10.1109/CVPR.2013.275
   Siva P, 2013, PROC CVPR IEEE, P3238, DOI 10.1109/CVPR.2013.416
   Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412
   Wang LJ, 2015, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR.2015.7298938
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Yun Z., 2006, P 14 ANN ACM INT C M, P815, DOI DOI 10.1145/1180639.1180824
   Zhao R, 2015, PROC CVPR IEEE, P1265, DOI 10.1109/CVPR.2015.7298731
   Zhu WJ, 2014, PROC CVPR IEEE, P2814, DOI 10.1109/CVPR.2014.360
NR 43
TC 0
Z9 0
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1070-9908
EI 1558-2361
J9 IEEE SIGNAL PROC LET
JI IEEE Signal Process. Lett.
PD OCT
PY 2016
VL 23
IS 10
BP 1404
EP 1408
DI 10.1109/LSP.2016.2600592
PG 5
WC Engineering, Electrical & Electronic
SC Engineering
GA EM0NB
UT WOS:000395013900014
DA 2020-02-19
ER

PT J
AU Wang, YX
   Widrow, B
   Zadeh, LA
   Howard, N
   Wood, S
   Bhavsar, VC
   Budin, G
   Chan, C
   Fiorini, RA
   Gavrilova, ML
   Shell, DF
AF Wang, Yingxu
   Widrow, Bernard
   Zadeh, Lotfi A.
   Howard, Newton
   Wood, Sally
   Bhavsar, Virendrakumar C.
   Budin, Gerhard
   Chan, Christine
   Fiorini, Rodolfo A.
   Gavrilova, Marina L.
   Shell, Duane F.
TI Cognitive Intelligence: Deep Learning, Thinking, and Reasoning by
   Brain-Inspired Systems
SO INTERNATIONAL JOURNAL OF COGNITIVE INFORMATICS AND NATURAL INTELLIGENCE
LA English
DT Article
DE Applications; Artificial Intelligence; Brain-Inspired Systems; Cognitive
   Computers; Cognitive Engineering; Cognitive Informatics; Cognitive
   Robotics; Cognitive Systems; Computational Intelligence; CWW; Deep
   Learning; Deep Reasoning; Deep Thinking; Denotational Mathematics;
   Knowledge Learning
ID MATHEMATICAL-MODELS; INFORMATICS; SCIENCE; ALGORITHM; NETWORKS; ALGEBRA;
   NOISE
AB The theme of IEEE ICCI*CC'16 on Cognitive Informatics (CI) and Cognitive Computing (CC) was on cognitive computers, big data cognition, and machine learning. CI and CC are a contemporary field not only for basic studies on the brain, computational intelligence theories, and denotational mathematics, but also for engineering applications in cognitive systems towards deep learning, deep thinking, and deep reasoning. This paper reports a set of position statements presented in the plenary panel (Part I) in IEEE ICCI*CC'16 at Stanford University. The summary is contributed by invited panelists who are part of the world's renowned scholars in the transdisciplinary field of CI and CC.
C1 [Wang, Yingxu] Univ Calgary, Dept Elect & Comp Engn,Schulich Sch Engn, Int Inst Cognit Informat & Cognit Comp ICIC, Lab Computat Intelligence Denotat Math & Software, Calgary, AB, Canada.
   [Wang, Yingxu] Univ Calgary, Hotchkiss Brain Inst, Calgary, AB, Canada.
   [Widrow, Bernard] Stanford Univ, Stanford, CA 94305 USA.
   [Zadeh, Lotfi A.] Univ Calif Berkeley, Berkeley, CA USA.
   [Howard, Newton] Univ Oxford, Oxford, England.
   [Wood, Sally] Santa Clara Univ, Santa Carla, CA USA.
   [Bhavsar, Virendrakumar C.] Univ New Brunswick, Fredericton, NB, Canada.
   [Budin, Gerhard] Univ Vienna, Ctr Translat Studies, Vienna, Austria.
   [Chan, Christine] Univ Regina, Regina, SK, Canada.
   [Fiorini, Rodolfo A.] Politecn Milano Univ, DEIB, Milan, Italy.
   [Gavrilova, Marina L.] Univ Calgary, Dept Comp Sci, Calgary, AB, Canada.
   [Shell, Duane F.] Univ Nebraska Lincoln, Dept Educ Psychol, Lincoln, NE USA.
RP Wang, YX (reprint author), Univ Calgary, Dept Elect & Comp Engn,Schulich Sch Engn, Int Inst Cognit Informat & Cognit Comp ICIC, Lab Computat Intelligence Denotat Math & Software, Calgary, AB, Canada.; Wang, YX (reprint author), Univ Calgary, Hotchkiss Brain Inst, Calgary, AB, Canada.
RI Fiorini, Rodolfo/C-8464-2011
OI Fiorini, Rodolfo/0000-0001-5344-7218
CR Adali T, 2015, P IEEE, V103, P1494, DOI 10.1109/JPROC.2015.2461601
   Akgun I, 2016, PR IEEE COMP DESIGN, P33, DOI 10.1109/ICCD.2016.7753258
   Anderson J. R., 1983, ARCHITECTURE COGNITI
   [Anonymous], 2016, REUTERS HLTH NE 0831
   Azam S., 2016, P ICCI CC 16 STANF U
   Baciu G., 2009, P 8 IEEE INT C COGN
   Bender E.A., 1996, MATH METHODS ARTIFIC
   Bhavsar VC, 2004, COMPUT INTELL, V20, P584, DOI 10.1111/j.0824-7935.2004.00255.x
   Boley H, 2005, INTERACT TECHNOL SMA, V2, P171, DOI 10.1108/17415650580000042
   Boucenna S, 2014, IEEE T AUTON MENT DE, V6, P42, DOI 10.1109/TAMD.2013.2284065
   Byrd R. J., 1987, COMPUTATIONAL LINGUI, V13, P219
   CHAN C, 2004, P 3 IEEE INT C COGN
   Chan V, 2016, 2016 IEEE 15TH INTERNATIONAL CONFERENCE ON COGNITIVE INFORMATICS & COGNITIVE COMPUTING (ICCI*CC), P479, DOI 10.1109/ICCI-CC.2016.7862080
   Chiriacescu V, 2013, INT J COGN INFORM NA, V7, P1, DOI 10.4018/ijcini.2013100101
   Dietterich TG, 2008, MACH LEARN, V73, P3, DOI 10.1007/s10994-008-5079-1
   Fiorini R. A., 2015, P 2 INT EL C ENTR IT
   Fiorini R.A., 2015, INT J BIOL BIOMED EN, V9, P29
   Fiorini R. A., 2016, P 15 IEEE INT C COGN
   Fiorini RA, 2015, FUND INFORM, V141, P115, DOI 10.3233/FI-2015-1267
   Fiorini RA, 2014, FUND INFORM, V135, P135, DOI 10.3233/FI-2014-1116
   Ge N., 2015, P 14 IEEE INT C COGN
   Gers FA, 2001, IEEE T NEURAL NETWOR, V12, P1333, DOI 10.1109/72.963769
   Gharbieh W., 2016, P 12 WORKSH MULT EXP, P112
   Gilat A., 2010, NUMERICAL METHODS EN
   Gobet F., 2012, ENCY SCI LEARNING, P541
   Goldberg D.E, 1989, GENETIC ALGORITHMS S
   Hebb D. O, 1949, ORG BEHAV NEUROPSYCH
   HINTON GE, 1995, SCIENCE, V268, P1158, DOI 10.1126/science.7761831
   Hsu D. F., 2013, P 12 IEEE INT C COGN
   Jain A. K., 2004, SPECIAL ISSUE IMAGE, V14, P420, DOI DOI 10.1109/TCSVT.2003.818349.HTTP://DX.D0I.0RG/10.1109/TCSVT.2003.818349
   Jain P., 2013, P NAT C E LEARN E LE
   Jain P, 2014, INT CONF COMM SYST, P886, DOI 10.1109/CSNT.2014.183
   Joshi M., 2010, ADV KNOWLEDGE BASED, P29
   Joshi M., 2010, P 12 INT C EL COMM R, P148
   Kahneman D, 2011, THINKING FAST SLOW
   Karasuyama M, 2010, IEEE T NEURAL NETWOR, V21, P1048, DOI 10.1109/TNN.2010.2048039
   Kiani M, 2016, 2016 IEEE 15TH INTERNATIONAL CONFERENCE ON COGNITIVE INFORMATICS & COGNITIVE COMPUTING (ICCI*CC), P244, DOI 10.1109/ICCI-CC.2016.7862043
   Kiani M, 2015, LECT NOTES COMPUT SC, V9371, P150, DOI 10.1007/978-3-319-25087-8_14
   Kiani M, 2014, 2014 IEEE 13TH INTERNATIONAL CONFERENCE ON COGNITIVE INFORMATICS & COGNITIVE COMPUTING (ICCI-CC), P203, DOI 10.1109/ICCI-CC.2014.6921461
   Kiani M, 2014, IEEE INT C SEMANT CO, P100, DOI 10.1109/ICSC.2014.33
   Kober J, 2010, IEEE ROBOT AUTOM MAG, V17, P55, DOI 10.1109/MRA.2010.936952
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lu Y., 2016, TREE STRUCTURE UNPUB
   Maulik U, 2010, IEEE T SYST MAN CY A, V40, P664, DOI 10.1109/TSMCA.2010.2041225
   Mehryar M., 2012, FDN MACHINE LEARNING
   Mohri M., 2012, FDN MACHINE LEARNING
   Moors A, 2016, ANNU REV PSYCHOL, V67, P263, DOI 10.1146/annurev-psych-122414-033550
   Morimoto J, 2010, IEEE ROBOT AUTOM MAG, V17, P17, DOI 10.1109/MRA.2010.937374
   Patel S., 2014, P 13 IEEE INT C COGN
   Patel S., 2003, P 2 IEEE INT C COGN
   Paul PP, 2015, IEEE SYS MAN CYBERN, P1676, DOI 10.1109/SMC.2015.295
   Paul PP, 2014, VISUAL COMPUT, V30, P1059, DOI 10.1007/s00371-013-0907-0
   Poole D., 1997, COMPUTATIONAL INTELL
   REISBERG D, 2001, [No title captured]
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Segalin C., 2014, P 16 INT C MULT INT, P180, DOI [10.1145/2663204.2663259, DOI 10.1145/2663204.2663259]
   SHANNON CE, 1948, BELL SYST TECH J, V27, P623, DOI 10.1002/j.1538-7305.1948.tb00917.x
   Shaw CC, 2010, P IEEE, V98, P2, DOI 10.1109/JPROC.2009.2035158
   Shell DF, 2016, 2016 IEEE 15TH INTERNATIONAL CONFERENCE ON COGNITIVE INFORMATICS & COGNITIVE COMPUTING (ICCI*CC), P77, DOI 10.1109/ICCI-CC.2016.7862098
   Shell DF, 2015, INT J COGN INFORM NA, V9, P1, DOI 10.4018/IJCINI.2015070101
   Shell DF, 2010, UNIFIED LEARNING MODEL, P205, DOI 10.1007/978-90-481-3215-7_17
   Sheu PCY, 2007, INT J SEMANT COMPUT, V1, P1, DOI 10.1142/S1793351X07000068
   Sourin A, 2016, LECT NOTES COMPUT SC, V9590, P1, DOI 10.1007/978-3-662-53090-0_1
   Sugawara K., 2012, P 11 IEEE INT C COGN
   Sultana M, 2016, LECT NOTES COMPUT SC, V9550, P111, DOI 10.1007/978-3-662-49247-5_7
   Sultana M, 2015, INT J PATTERN RECOGN, V29, DOI 10.1142/S0218001415560133
   Sultana M, 2014, 2014 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW), P293, DOI 10.1109/CW.2014.47
   Sultana M, 2014, 2014 INTERNATIONAL CONFERENCE ON CYBERWORLDS (CW), P271, DOI 10.1109/CW.2014.44
   Sun F., 2010, P 9 IEEE INT C COGN
   Tomar A. A., 2013, P NAT C PAR COMP TEC, P223, DOI [10.1109/ParCompTech.2013.6621391, DOI 10.1109/PARCOMPTECH.2013.6621391]
   Tomar A. A., 2013, P NAT C PAR COMP TEC, P249, DOI [10.1109/ParCompTech.2013.6621407, DOI 10.1109/PARCOMPTECH.2013.6621407]
   Tomar A, 2014, 2014 FOURTH INTERNATIONAL CONFERENCE ON ADVANCES IN COMPUTING AND COMMUNICATIONS (ICACC), P203, DOI 10.1109/ICACC.2014.56
   Vargas J. G., 2015, CARTAN EINSTEIN KAHL
   Wang Y., 2009, FUNDAMENTA INFORM, V90, P282, DOI DOI 10.4018/JSSCI.2009062501
   Wang Y., 2016, INT J SOFTWARE SCI C, V8
   Wang Y., 2015, J GEOL RES, V4, P1
   Wang Y., 2010, ADV COGNITIVE INFORM, DOI [10.1007/978-3-642-16083-7, DOI 10.1007/978-3-642-16083-7]
   Wang Y., 2008, T COMPUTATIONAL SCI
   Wang Y., 2012, COGNITIVE INFORM REV
   Wang Y., 2016, P 15 IEEE INT C COGN
   Wang Y., 2007, CRC SERIES SOFTWARE, VII
   Wang Y., 2015, INT J SOFTWARE SCI C, V7, P52, DOI [10.4018/IJSSCI.2015040103, DOI 10.4018/IJSSCI.2015040103]
   Wang Y., 2015, INT J COGNITIVE INFO, V9, P41, DOI [10.4018/IJCINI.2015070103, DOI 10.4018/IJCINI.2015070103]
   Wang Y., 2016, J ADV MATH IN PRESS, V5
   WANG Y, 2002, P 1 IEEE INT C COGN
   WANG Y, 2003, BRAIN MIND TRANSDISC, V4, P151, DOI DOI 10.1023/A:1025401527570
   Wang Y., 2008, P 7 IEEE INT C COGN
   Wang Y., 2017, P 8 WORLD C NEUROTAL
   Wang Y., 2012, PRINCIPLES PRACTICE, P3
   Wang Y., 2009, IEEE T SYST MAN CY B, V39, P1
   Wang Y., 2016, P 8 INT C BRAIN INSP, P2
   Wang YX, 2007, INT J COGN INFORM NA, V1, P66
   Wang YX, 2006, PROCEEDINGS OF THE FIFTH IEEE INTERNATIONAL CONFERENCE ON COGNITIVE INFORMATICS, VOLS 1 AND 2, P3
   Wang YX, 2007, INT J COGN INFORM NA, V1, P1
   Wang YX, 2012, J ADV MATH APPL, V1, P121, DOI 10.1166/jama.2012.1009
   Wang YX, 2011, INT J COGN INFORM NA, V5, P1, DOI [10.4018/jcini.2011070101, 10.4018/ijcini.2011070101]
   Wang YX, 2016, INT J COGN INFORM NA, V10, P1, DOI 10.4018/IJCINI.2016040101
   Wang YX, 2015, INT J COGN INFORM NA, V9, P37, DOI [10.4018/ijcini.2015040103, 10.4018/IJCINI.2015040103]
   Wang YX, 2012, INT J COGN INFORM NA, V6, P54, DOI 10.4018/jcini.2012100103
   Wang YX, 2012, J ADV MATH APPL, V1, P206, DOI 10.1166/jama.2012.1015
   Wang YX, 2013, J ADV MATH APPL, V2, P182, DOI 10.1166/jama.2013.1042
   Wang YX, 2014, J ADV MATH APPL, V3, P130, DOI 10.1166/jama.2014.1060
   Wang YX, 2013, J ADV MATH APPL, V2, P145, DOI 10.1166/jama.2013.1039
   Wang YX, 2012, J ADV MATH APPL, V1, P1, DOI 10.1166/jama.2012.1001
   Wang YX, 2015, COMPLEX INTELL SYST, V1, P1, DOI 10.1007/s40747-015-0001-5
   Wang YX, 2015, J ADV MATH APPL, V4, P37, DOI 10.1166/jama.2015.1071
   Wang YX, 2007, INT J COGN INFORM NA, V1, P73
   Wang YC, 2012, J ADV MATH APPL, V1, P250, DOI 10.1166/jama.2012.1019
   Wang Y, 2013, INT J COGN INFORM NA, V7, P98, DOI 10.4018/jcini.2013010105
   Wang Y, 2011, INT J COGN INFORM NA, V5, P61, DOI 10.4018/jcini.2011100105
   Wang YX, 2010, INT J SEMANT COMPUT, V4, P203, DOI 10.1142/S1793351X10000833
   Wang YX, 2011, INT J COGN INFORM NA, V5, P75, DOI 10.4018/jcini.2011010105
   Wang YX, 2010, IEEE ROBOT AUTOM MAG, V17, P54, DOI 10.1109/MRA.2010.938842
   Wang YZ, 2011, BOUND VALUE PROBL, DOI 10.1186/1687-2770-2011-11
   Wang YQ, 2015, Proceedings of 2015 International Symposium - Open Economy & Financial Engineering, P3
   Wang YX, 2006, IEEE T SYST MAN CY C, V36, P203, DOI 10.1109/TSMCC.2006.871151
   Wang YX, 2006, IEEE T SYST MAN CY C, V36, P121, DOI 10.1109/TSMCC.2006.871120
   Wang YX, 2002, ANN SOFTW ENG, V14, P235, DOI 10.1023/A:1020561826073
   Widrow B., 2016, P IEEE 15 INT C COGN, P2
   Widrow B, 2015, IEEE COMPUT INTELL M, V10, P37, DOI 10.1109/MCI.2015.2471216
   Widrow B, 2013, NEURAL NETWORKS, V41, P3, DOI 10.1016/j.neunet.2013.01.016
   Widrow B, 2013, NEURAL NETWORKS, V37, P180, DOI 10.1016/j.neunet.2012.09.020
   Wilson RA, 2001, MIT ENCY COGNITIVE S
   Yampolskiy RV, 2012, IEEE ROBOT AUTOM MAG, V19, P48, DOI 10.1109/MRA.2012.2201574
   Yang L, 2005, J BUSINESS TECHNOLOG, V1, P42, DOI DOI 10.1515/JISYS.2008.17.1-3.247
   YAO Y, 2006, P 5 IEEE INT C COGN
   Yingxu Wang, 2009, International Journal of Software Science and Computational Intelligence, V1, P1
   Yingxu Wang, 2014, International Journal of Software Science and Computational Intelligence, V6, P41, DOI 10.4018/ijssci.2014040103
   Zadeh L. A., 2016, P IEEE 15 INT C COGN, P1
   Zadeh L. A., 1975, SYNTHESE, V30, P407, DOI DOI 10.1007/BF00485052
   Zadeh LA, 2004, AI MAG, V25, P74
   Zadeh LA, 1997, FUZZY SET SYST, V90, P111, DOI 10.1016/S0165-0114(97)00077-8
   Zadeh LA, 1999, IEEE T CIRCUITS-I, V46, P105, DOI 10.1109/81.739259
   ZADEH LA, 1965, INFORM CONTROL, V8, P338, DOI 10.1016/S0019-9958(65)90241-X
   Zhang D., 2007, P 6 IEEE INT C COGN
   Zhou Q, 2012, CARBON MANAG, V3, P81, DOI 10.4155/CMT.11.77
NR 136
TC 16
Z9 16
U1 8
U2 40
PU IGI GLOBAL
PI HERSHEY
PA 701 E CHOCOLATE AVE, STE 200, HERSHEY, PA 17033-1240 USA
SN 1557-3958
EI 1557-3966
J9 INT J COGN INFORM NA
JI Int. J. Cogn. Inform. Nat. Intell.
PD OCT-DEC
PY 2016
VL 10
IS 4
BP 1
EP 20
DI 10.4018/IJCINI.2016100101
PG 20
WC Computer Science, Artificial Intelligence
SC Computer Science
GA EE7MR
UT WOS:000389803200001
DA 2020-02-19
ER

PT J
AU Zhao, GP
   Zhao, CH
   Jia, XP
AF Zhao, Genping
   Zhao, Chunhui
   Jia, Xiuping
TI Multilayer Unmixing for Hyperspectral Imagery With Fast Kernel
   Archetypal Analysis
SO IEEE GEOSCIENCE AND REMOTE SENSING LETTERS
LA English
DT Article
DE Hyperspectral imagery; kernel archetypal analysis (KAA); multilayer
   network; Nystrom method; spectral unmixing
ID SPARSE REGRESSION; CLASSIFICATION
AB The multilayer network in deep learning provides a promising means for rich data representation. Inspired by this approach, we investigate multilayer unmixing for spectral decomposition with fast kernel archetypal analysis (KAA). KAA is used for endmember extraction and abundance estimation simultaneously. To refine the initial unmixing results, a multilayer process is utilized to provide final unmixing results at the end of the network. Moreover, a fast implementation of KAA is proposed via using the Nystrom method to relieve KAA's memory issue and decrease the processing time. The proposed method is tested on both synthetic and real hyperspectral image data sets. The results demonstrate that the multilayer unmixing algorithm outperforms the conventional unmixing techniques.
C1 [Zhao, Genping; Zhao, Chunhui] Harbin Engn Univ, Coll Informat & Telecommun, Harbin 150001, Peoples R China.
   [Jia, Xiuping] Univ New South Wales, Sch Informat Technol & Engn, Canberra, ACT 2600, Australia.
RP Zhao, CH (reprint author), Harbin Engn Univ, Coll Informat & Telecommun, Harbin 150001, Peoples R China.
EM zhaogenping@gmail.com; zhaochunhui@hrbeu.edu.cn; X.JIA@adfa.edu.au
OI Jia, Xiuping/0000-0001-9916-6382
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61405041, 61571145]; Key Program of Heilongjiang
   Natural Science Foundation [ZD201216]; Program Excellent Academic
   Leaders of Harbin [RC2013XK009003]; Fundamental Research Funds for the
   Central UniversitiesFundamental Research Funds for the Central
   Universities [HEUCF1508]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61405041 and 61571145, by the Key
   Program of Heilongjiang Natural Science Foundation under Grant ZD201216,
   by the Program Excellent Academic Leaders of Harbin under Grant
   RC2013XK009003, and by the Fundamental Research Funds for the Central
   Universities under Grant HEUCF1508.
CR Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153
   Bioucas-Dias JM, 2012, IEEE J-STARS, V5, P354, DOI 10.1109/JSTARS.2012.2194696
   Bioucas-Dias JM, 2009, 2009 FIRST WORKSHOP ON HYPERSPECTRAL IMAGE AND SIGNAL PROCESSING: EVOLUTION IN REMOTE SENSING, P1, DOI 10.1109/WHISPERS.2009.5289072
   Chen YS, 2014, PROC CVPR IEEE, P1478, DOI 10.1109/CVPR.2014.192
   Chen YS, 2015, IEEE J-STARS, V8, P2381, DOI 10.1109/JSTARS.2015.2388577
   Chen YS, 2014, IEEE J-STARS, V7, P2094, DOI 10.1109/JSTARS.2014.2329330
   CUTLER A, 1994, TECHNOMETRICS, V36, P338, DOI 10.2307/1269949
   Guo R., 2015, P 7 WORKSH HYP IM SI, P1, DOI DOI 10.1109/WHISPERS.2015.8075378
   Iordache MD, 2014, IEEE T GEOSCI REMOTE, V52, P341, DOI 10.1109/TGRS.2013.2240001
   Lee H., 2007, ADV NEURAL INF PROCE, P801
   Miao LD, 2007, IEEE T GEOSCI REMOTE, V45, P765, DOI 10.1109/TGRS.2006.888466
   Morup M, 2012, NEUROCOMPUTING, V80, P54, DOI 10.1016/j.neucom.2011.06.033
   Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0
   Rajabi R, 2015, IEEE GEOSCI REMOTE S, V12, P38, DOI 10.1109/LGRS.2014.2325874
   Sun SL, 2015, INFORM FUSION, V26, P36, DOI 10.1016/j.inffus.2015.03.001
   Villa A, 2013, PATTERN RECOGN, V46, P1556, DOI 10.1016/j.patcog.2012.10.030
   Williams C., 2000, INT C P 17 MACH LEAR, P1159
   Zhao GP, 2015, INT GEOSCI REMOTE SE, P5039, DOI 10.1109/IGARSS.2015.7326965
NR 18
TC 5
Z9 5
U1 0
U2 31
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1545-598X
EI 1558-0571
J9 IEEE GEOSCI REMOTE S
JI IEEE Geosci. Remote Sens. Lett.
PD OCT
PY 2016
VL 13
IS 10
BP 1532
EP 1536
DI 10.1109/LGRS.2016.2595102
PG 5
WC Geochemistry & Geophysics; Engineering, Electrical & Electronic; Remote
   Sensing; Imaging Science & Photographic Technology
SC Geochemistry & Geophysics; Engineering; Remote Sensing; Imaging Science
   & Photographic Technology
GA EA0CS
UT WOS:000386253500029
DA 2020-02-19
ER

PT J
AU Ghamisi, P
   Chen, YS
   Zhu, XX
AF Ghamisi, Pedram
   Chen, Yushi
   Zhu, Xiao Xiang
TI A Self-Improving Convolution Neural Network for the Classification of
   Hyperspectral Data
SO IEEE GEOSCIENCE AND REMOTE SENSING LETTERS
LA English
DT Article
DE Convolutional neural network (CNN); deep learning; feature selection;
   fractional order Darwinian particle swarm optimization (FODPSO);
   hyperspectral image classification
ID SPECTRAL-SPATIAL CLASSIFICATION; ALGORITHM
AB In this letter, a self-improving convolutional neural network (CNN) based method is proposed for the classification of hyperspectral data. This approach solves the so-called curse of dimensionality and the lack of available training samples by iteratively selecting the most informative bands suitable for the designed network via fractional order Darwinian particle swarm optimization. The selected bands are then fed to the classification system to produce the final classification map. Experimental results have been conducted with two well-known hyperspectral data sets: Indian Pines and Pavia University. Results indicate that the proposed approach significantly improves a CNN-based classification method in terms of classification accuracy. In addition, this letter uses the concept of dither for the first time in the remote sensing community to tackle overfitting.
C1 [Ghamisi, Pedram; Zhu, Xiao Xiang] German Aerosp Ctr DLR, Remote Sensing Technol Inst IMF, D-82234 Wessling, Germany.
   [Ghamisi, Pedram; Zhu, Xiao Xiang] Tech Univ Munich, Signal Proc Earth Observat, D-80333 Munich, Germany.
   [Chen, Yushi] Harbin Inst Technol, Dept Informat Engn, Harbin 150001, Peoples R China.
RP Ghamisi, P (reprint author), German Aerosp Ctr DLR, Remote Sensing Technol Inst IMF, D-82234 Wessling, Germany.; Ghamisi, P (reprint author), Tech Univ Munich, Signal Proc Earth Observat, D-80333 Munich, Germany.
EM pedram.ghamisi@dlr.de; chenyushi@hit.edu.cn; xiao.zhu@dlr.de
OI Zhu, Xiaoxiang/0000-0001-5530-3613
FU Alexander von Humboldt FellowshipAlexander von Humboldt Foundation;
   Helmholtz Young Investigators Group "SiPEO"
FX This work was supported in part by Alexander von Humboldt Fellowship for
   postdoctoral researchers and Helmholtz Young Investigators Group
   "SiPEO."
CR Bazi Y, 2006, IEEE T GEOSCI REMOTE, V44, P3374, DOI 10.1109/TGRS.2006.880628
   Benediktsson JA, 2015, ARTECH HSE REMOTE SE, P1
   Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153
   Chen YS, 2015, IEEE J-STARS, V8, P2381, DOI 10.1109/JSTARS.2015.2388577
   Chen YS, 2014, IEEE J-STARS, V7, P2094, DOI 10.1109/JSTARS.2014.2329330
   Ghamisi P, 2015, IEEE T GEOSCI REMOTE, V53, P2935, DOI 10.1109/TGRS.2014.2367010
   Ghamisi P, 2015, IEEE GEOSCI REMOTE S, V12, P309, DOI 10.1109/LGRS.2014.2337320
   Ghamisi P, 2014, IEEE J-STARS, V7, P2147, DOI 10.1109/JSTARS.2014.2298876
   Ghamisi P, 2012, EXPERT SYST APPL, V39, P12407, DOI 10.1016/j.eswa.2012.04.078
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Salakhutdinov R, 2009, ARTIF INTELL, P448, DOI DOI 10.1109/CVPRW.2009.5206577
   Simpson A. J. R., DITHER IS BETT UNPUB
   Simpsons A., OVER SAMPLING UNPUB
NR 14
TC 47
Z9 50
U1 6
U2 92
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1545-598X
EI 1558-0571
J9 IEEE GEOSCI REMOTE S
JI IEEE Geosci. Remote Sens. Lett.
PD OCT
PY 2016
VL 13
IS 10
BP 1537
EP 1541
DI 10.1109/LGRS.2016.2595108
PG 5
WC Geochemistry & Geophysics; Engineering, Electrical & Electronic; Remote
   Sensing; Imaging Science & Photographic Technology
SC Geochemistry & Geophysics; Engineering; Remote Sensing; Imaging Science
   & Photographic Technology
GA EA0CS
UT WOS:000386253500030
OA Green Accepted
DA 2020-02-19
ER

PT J
AU Jiang, P
   Hu, ZX
   Liu, J
   Yu, SN
   Wu, F
AF Jiang, Peng
   Hu, Zhixin
   Liu, Jun
   Yu, Shanen
   Wu, Feng
TI Fault Diagnosis Based on Chemical Sensor Data with an Active Deep Neural
   Network
SO SENSORS
LA English
DT Article
DE fault diagnosis; deep learning; deep neural network; active learning;
   big sensor data
ID EMPIRICAL MODE DECOMPOSITION; ROTATING MACHINERY; CLASSIFICATION;
   ALGORITHM
AB Big sensor data provide significant potential for chemical fault diagnosis, which involves the baseline values of security, stability and reliability in chemical processes. A deep neural network (DNN) with novel active learning for inducing chemical fault diagnosis is presented in this study. It is a method using large amount of chemical sensor data, which is a combination of deep learning and active learning criterion to target the difficulty of consecutive fault diagnosis. DNN with deep architectures, instead of shallow ones, could be developed through deep learning to learn a suitable feature representation from raw sensor data in an unsupervised manner using stacked denoising auto-encoder (SDAE) and work through a layer-by-layer successive learning process. The features are added to the top Softmax regression layer to construct the discriminative fault characteristics for diagnosis in a supervised manner. Considering the expensive and time consuming labeling of sensor data in chemical applications, in contrast to the available methods, we employ a novel active learning criterion for the particularity of chemical processes, which is a combination of Best vs. Second Best criterion (BvSB) and a Lowest False Positive criterion (LFP), for further fine-tuning of diagnosis model in an active manner rather than passive manner. That is, we allow models to rank the most informative sensor data to be labeled for updating the DNN parameters during the interaction phase. The effectiveness of the proposed method is validated in two well-known industrial datasets. Results indicate that the proposed method can obtain superior diagnosis accuracy and provide significant performance improvement in accuracy and false positive rate with less labeled chemical sensor data by further active learning compared with existing methods.
C1 [Jiang, Peng; Hu, Zhixin; Yu, Shanen; Wu, Feng] Hangzhou Dianzi Univ, Coll Automat, Hangzhou 310018, Peoples R China.
   [Liu, Jun] Zhejiang Univ, Inst Ind Proc Control, State Key Lab Ind Control Technol, Hangzhou 310027, Peoples R China.
RP Jiang, P (reprint author), Hangzhou Dianzi Univ, Coll Automat, Hangzhou 310018, Peoples R China.
EM pjiang@hdu.edu.cn; hduhzx@163.com; liujun@163.com; shanen_yu@hdu.edu.cn;
   fengwu@hdu.edu.cn
FU National Key Research and Development Program of China [2016YFC0201400];
   National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [NSFC61273072]; National Natural Science Foundation
   of ChinaNational Natural Science Foundation of China; Zhejiang Joint
   Fund for Integrating of Informatization and Industrialization [U1509217]
FX This paper was supported by the National Key Research and Development
   Program of China (2016YFC0201400), the National Natural Science
   Foundation of China (NSFC61273072), the National Natural Science
   Foundation of China and Zhejiang Joint Fund for Integrating of
   Informatization and Industrialization (U1509217).
CR Al Rahhal MM, 2016, INFORM SCIENCES, V345, P340, DOI 10.1016/j.ins.2016.01.082
   Amar M, 2015, IEEE T IND ELECTRON, V62, P494, DOI 10.1109/TIE.2014.2327555
   Azadeh A, 2013, APPL SOFT COMPUT, V13, P1478, DOI 10.1016/j.asoc.2012.06.020
   Bellala G, 2013, IEEE T PATTERN ANAL, V35, P2078, DOI 10.1109/TPAMI.2013.30
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Bin GF, 2012, MECH SYST SIGNAL PR, V27, P696, DOI 10.1016/j.ymssp.2011.08.002
   Bordes A, 2005, J MACH LEARN RES, V6, P1579
   Dai XW, 2013, IEEE T IND INFORM, V9, P2226, DOI 10.1109/TII.2013.2243743
   DOWNS JJ, 1993, COMPUT CHEM ENG, V17, P245, DOI 10.1016/0098-1354(93)80018-I
   Ellingson BM, 2012, J MAGN RESON IMAGING, V35, P1472, DOI 10.1002/jmri.23600
   Eslamloueyan R, 2011, APPL SOFT COMPUT, V11, P1407, DOI 10.1016/j.asoc.2010.04.012
   Gan M, 2016, MECH SYST SIGNAL PR, V72-73, P92, DOI 10.1016/j.ymssp.2015.11.014
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Isermann R, 2005, ANNU REV CONTROL, V29, P71, DOI 10.1016/j.arcontrol.2004.12.002
   Jia F, 2016, MECH SYST SIGNAL PR, V72-73, P303, DOI 10.1016/j.ymssp.2015.10.025
   Kambatla K, 2014, J PARALLEL DISTR COM, V74, P2561, DOI 10.1016/j.jpdc.2014.01.003
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   KULLBACK S, 1951, ANN MATH STAT, V22, P79, DOI 10.1214/aoms/1177729694
   Larochelle H, 2009, J MACH LEARN RES, V10, P1
   Li F, 2009, ISA T, V48, P213, DOI 10.1016/j.isatra.2008.10.014
   Lu W, 2015, P IEEE 27 CHIN CONTR
   Mahadevan S, 2009, J PROCESS CONTR, V19, P1627, DOI 10.1016/j.jprocont.2009.07.011
   Marti L, 2015, SENSORS-BASEL, V15, P2774, DOI 10.3390/s150202774
   Monroy I, 2010, COMPUT CHEM ENG, V34, P631, DOI 10.1016/j.compchemeng.2009.12.008
   Nair V, 2010, ICML, V27, P807, DOI DOI 10.0RG/PAPERS/432.PDF
   NAJAFABADI M, 2015, J BIG DATA, V2, P1, DOI DOI 10.1186/S40537-014-0007-7
   Ruiz D, 2000, COMPUT CHEM ENG, V24, P777, DOI 10.1016/S0098-1354(00)00371-9
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Settles B., 2010, ACTIVE LEARNING LIT, V52, P11, DOI DOI 10.1111/J.1467-7687.2012.01135.X
   Shang C, 2014, J PROCESS CONTR, V24, P223, DOI 10.1016/j.jprocont.2014.01.012
   Socher R., 2012, P TUT ACL 2012 ASS C
   Sun WJ, 2016, MEASUREMENT, V89, P171, DOI 10.1016/j.measurement.2016.04.007
   Tamilselvan P, 2013, RELIAB ENG SYST SAFE, V115, P124, DOI 10.1016/j.ress.2013.02.022
   Tuia D, 2011, REMOTE SENS ENVIRON, V115, P2232, DOI 10.1016/j.rse.2011.04.022
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Xie DF, 2015, 2015 IEEE 14TH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA), P745, DOI 10.1109/ICMLA.2015.208
   Yelamos I, 2009, COMPUT CHEM ENG, V33, P244, DOI 10.1016/j.compchemeng.2008.08.008
   Zhang LW, 2015, RELIAB ENG SYST SAFE, V142, P482, DOI 10.1016/j.ress.2015.05.025
   Zhang XY, 2013, MECH SYST SIGNAL PR, V41, P127, DOI 10.1016/j.ymssp.2013.07.006
   Zhao XK, 2011, EXPERT SYST APPL, V38, P10199, DOI 10.1016/j.eswa.2011.02.078
   Zhu X, 2008, COMPUTER SCI, V37, P63, DOI DOI 10.7551/MITPRESS/9780262033589.003.0001
NR 41
TC 3
Z9 4
U1 4
U2 64
PU MDPI
PI BASEL
PA ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
SN 1424-8220
J9 SENSORS-BASEL
JI Sensors
PD OCT
PY 2016
VL 16
IS 10
AR 1695
DI 10.3390/s16101695
PG 22
WC Chemistry, Analytical; Engineering, Electrical & Electronic; Instruments
   & Instrumentation
SC Chemistry; Engineering; Instruments & Instrumentation
GA DZ8OY
UT WOS:000386131600147
OA DOAJ Gold, Green Published
DA 2020-02-19
ER

PT J
AU Sun, Y
   Wang, XG
   Tang, XO
AF Sun, Yi
   Wang, Xiaogang
   Tang, Xiaoou
TI Hybrid Deep Learning for Face Verification
SO IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
LA English
DT Article
DE Convolutional networks; deep learning; face recognition
ID RECOGNITION; DIMENSIONALITY; REPRESENTATION; ALGORITHM
AB This paper proposes a hybrid convolutional network (ConvNet)-Restricted Boltzmann Machine (RBM) model for face verification. A key contribution of this work is to learn high-level relational visual features with rich identity similarity information. The deep ConvNets in our model start by extracting local relational visual features from two face images in comparison, which are further processed through multiple layers to extract high-level and global relational features. To keep enough discriminative information, we use the last hidden layer neuron activations of the ConvNet as features for face verification instead of those of the output layer. To characterize face similarities from different aspects, we concatenate the features extracted from different face region pairs by different deep ConvNets. The resulting high-dimensional relational features are classified by an RBM for face verification. After pre-training each ConvNet and the RBM separately, the entire hybrid network is jointly optimized to further improve the accuracy. Various aspects of the ConvNet structures, relational features, and face verification classifiers are investigated. Our model achieves the state-of-the-art face verification performance on the challenging LFW dataset under both the unrestricted protocol and the setting when outside data is allowed to be used for training.
C1 [Sun, Yi; Tang, Xiaoou] Chinese Univ Hong Kong, Dept Informat Engn, Hong Kong, Hong Kong, Peoples R China.
   [Wang, Xiaogang] Chinese Univ Hong Kong, Dept Elect Engn, Hong Kong, Hong Kong, Peoples R China.
RP Sun, Y (reprint author), Chinese Univ Hong Kong, Dept Informat Engn, Hong Kong, Hong Kong, Peoples R China.
EM sy011@ie.cuhk.edu.hk; xgwang@ee.cuhk.edu.hk; xtang@ie.cuhk.edu.hk
FU CUHK Computer Vision Cooperation grant from Huawei; General Research
   Fund - Research Grants Council of Hong KongHong Kong Research Grants
   Council [CUHK 416510, 416312, 14207814]; National Natural Science
   Foundation of ChinaNational Natural Science Foundation of China
   [91320101]; Guangdong Innovative Research Team Program
   [201001D0104648280]; Shenzhen Basic Research Program
   [JCYJ20130402113127496]
FX This work is partially supported by CUHK Computer Vision Cooperation
   grant from Huawei, the General Research Fund sponsored by the Research
   Grants Council of Hong Kong (Project Nos. CUHK 416510, 416312, and
   14207814), National Natural Science Foundation of China (91320101),
   Guangdong Innovative Research Team Program (No. 201001D0104648280), and
   Shenzhen Basic Research Program (JCYJ20130402113127496).
CR Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   Barkan O, 2013, IEEE I CONF COMP VIS, P1960, DOI 10.1109/ICCV.2013.246
   Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228
   Berg T, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.129
   Berg T, 2013, PROC CVPR IEEE, P955, DOI 10.1109/CVPR.2013.128
   Bissacco A, 2013, IEEE I CONF COMP VIS, P785, DOI 10.1109/ICCV.2013.102
   Cai X, 2012, P 20 ACM INT C MULT, P749
   Cao Q, 2013, IEEE I CONF COMP VIS, P2408, DOI 10.1109/ICCV.2013.299
   Cao XD, 2013, IEEE I CONF COMP VIS, P3208, DOI 10.1109/ICCV.2013.398
   Cao ZM, 2010, PROC CVPR IEEE, P2707, DOI 10.1109/CVPR.2010.5539992
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chen D., 2012, P EUR C COMPUT VIS
   Chen D, 2013, PROC CVPR IEEE, P3025, DOI 10.1109/CVPR.2013.389
   CHOPRA S, 2005, PROC CVPR IEEE, P539, DOI DOI 10.1109/CVPR.2005.202
   Ciresan D, 2012, PROC CVPR IEEE, P3642, DOI 10.1109/CVPR.2012.6248110
   Cox D, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P8, DOI 10.1109/FG.2011.5771385
   Cui Z, 2013, PROC CVPR IEEE, P3554, DOI 10.1109/CVPR.2013.456
   Deniz O, 2011, PATTERN RECOGN LETT, V32, P1598, DOI 10.1016/j.patrec.2011.01.004
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231
   Guillaumin M, 2009, IEEE I CONF COMP VIS, P498, DOI 10.1109/ICCV.2009.5459197
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton G. E., 2012, CORR
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hu JL, 2014, PROC CVPR IEEE, P1875, DOI 10.1109/CVPR.2014.242
   Huang C., 2011, TR115 NEC
   Huang GB, 2012, PROC CVPR IEEE, P2518, DOI 10.1109/CVPR.2012.6247968
   Huang Gary B., 2007, 0749 U MASS
   Huang ZW, 2013, IEEE I CONF COMP VIS, P3296, DOI 10.1109/ICCV.2013.409
   Jarrett K, 2009, IEEE I CONF COMP VIS, P2146, DOI 10.1109/ICCV.2009.5459469
   Kan M, 2014, PROC CVPR IEEE, P1883, DOI 10.1109/CVPR.2014.243
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kumar N, 2009, IEEE I CONF COMP VIS, P365, DOI 10.1109/ICCV.2009.5459250
   Larochelle H, 2012, J MACH LEARN RES, V13, P643
   Le Q. V., 2012, P 29 INT C MACH LEAR, P81, DOI DOI 10.1109/MSP.2011.940881
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li HX, 2013, PROC CVPR IEEE, P3499, DOI 10.1109/CVPR.2013.449
   Li P, 2012, IEEE T PATTERN ANAL, V34, P144, DOI 10.1109/TPAMI.2011.104
   Luo P, 2013, IEEE I CONF COMP VIS, P2864, DOI 10.1109/ICCV.2013.356
   Luo P, 2012, PROC CVPR IEEE, P2480, DOI 10.1109/CVPR.2012.6247963
   Nair V, 2010, ICML, V27, P807, DOI DOI 10.0RG/PAPERS/432.PDF
   Nguyen Hieu V., 2010, Computer Vision - ACCV 2010. 10th Asian Conference on Computer Vision. Revised Selected Papers, P709, DOI 10.1007/978-3-642-19309-5_55
   Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222
   Poon H., 2011, P 27 C UNC ART INT, p[337, 346]
   Qi YX, 2011, CONGRES INT FROID, V23, P497, DOI 10.1109/CVPR.2011.5995494
   Sermanet P., 2014, P INT C LEARN REPR
   Simonyan K, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.8
   Sun Y, 2013, IEEE I CONF COMP VIS, P1489, DOI 10.1109/ICCV.2013.188
   Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244
   Sun Y, 2013, PROC CVPR IEEE, P3476, DOI 10.1109/CVPR.2013.446
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Wagner A, 2012, IEEE T PATTERN ANAL, V34, P372, DOI 10.1109/TPAMI.2011.112
   Wang XG, 2004, IEEE T PATTERN ANAL, V26, P1222, DOI 10.1109/TPAMI.2004.57
   Wiskott L, 1997, IEEE T PATTERN ANAL, V19, P775, DOI 10.1109/34.598235
   Wolf L., 2008, P WORKSH FAC REAL LI
   Wolf L, 2011, IEEE T PATTERN ANAL, V33, P1978, DOI 10.1109/TPAMI.2010.230
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Yang M, 2010, LECT NOTES COMPUT SC, V6316, P448, DOI 10.1007/978-3-642-15567-3_33
   Zhang L, 2011, IEEE I CONF COMP VIS, P471, DOI 10.1109/ICCV.2011.6126277
   Zhu ZY, 2013, IEEE I CONF COMP VIS, P113, DOI 10.1109/ICCV.2013.21
NR 61
TC 34
Z9 39
U1 4
U2 50
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 0162-8828
EI 1939-3539
J9 IEEE T PATTERN ANAL
JI IEEE Trans. Pattern Anal. Mach. Intell.
PD OCT
PY 2016
VL 38
IS 10
BP 1997
EP 2009
DI 10.1109/TPAMI.2015.2505293
PG 13
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
SC Computer Science; Engineering
GA DX2YV
UT WOS:000384240600006
PM 26660699
OA Green Published
DA 2020-02-19
ER

PT J
AU Ntalampiras, S
AF Ntalampiras, Stavros
TI Automatic identification of integrity attacks in cyber-physical systems
SO EXPERT SYSTEMS WITH APPLICATIONS
LA English
DT Article
DE Critical infrastructure protection; Fault diagnosis; Cyber security;
   Cyber-physical systems; Probabilistic modelling; Deep learning
ID BAD DATA; SMART; SECURITY
AB Modern society relies on the availability and smooth operation of complex engineering systems, such as electric power systems, water distributions networks, etc. which due to the recent advancements in information and communication technologies (ICT) are usually controlled by means of a cyber-layer. This design may potentially improve the usage of the components of the cyber-physical system (CPS), however further protection is needed due to the emerging threat of cyber-attacks. These may degrade the quality of the communicated information which is of fundamental importance in the decision making process.
   This paper proposes a novel methodology for automatic identification of the type of the integrity attack affecting a CPS. We designed a feature set for capturing the characteristics of each attack in the spectral and wavelet domains while its distribution is learned by pattern recognition algorithms of different modelling properties customized for the specific application scenario. In addition a novelty detection component is incorporated for dealing with previously unseen types of attacks. The proposed approach is applied onto data coming from the IEEE-9 bus model achieving promising identification performance. (C) 2016 Elsevier Ltd. All rights reserved.
C1 [Ntalampiras, Stavros] Politecn Milan, Dept Elect Informat & Bioengn, I-20133 Milan, Italy.
RP Ntalampiras, S (reprint author), Politecn Milan, Dept Elect Informat & Bioengn, I-20133 Milan, Italy.
EM stavros.ntalampiras@polimi.it
RI Ntalampiras, Stavros/W-5636-2019
OI Ntalampiras, Stavros/0000-0003-3482-9215
FU Politecnico di Milano International Fellowship Program
FX This work was supported by the Politecnico di Milano International
   Fellowship Program.
CR Almalawi A, 2014, COMPUT SECUR, V46, P94, DOI 10.1016/j.cose.2014.07.005
   Ansari MR, 2014, IET GENER TRANSM DIS, V8, P1900, DOI 10.1049/iet-gtd.2014.0145
   Anwar A., 2014, CORR
   Arafat Yasir, 2014, P IEEE PES T D C EXP, P1
   Berizzi A, 2004, 2004 IEEE POWER ENGINEERING SOCIETY GENERAL MEETING, VOLS 1 AND 2, P1673
   Bi T, 2013, INNOVATIVE SMART GRI, P1, DOI DOI 10.1109/ISGT.2013.6497876
   Caro E, 2011, IEEE T POWER SYST, V26, P1953, DOI 10.1109/TPWRS.2011.2157366
   Cole S, 2011, IEEE T POWER SYST, V26, P1129, DOI 10.1109/TPWRS.2010.2071888
   Deng R, 2015, INDUSTRIAL INFORMATI, VPP, P1
   Dong Zhaoyang, 2014, 2014 13th International Conference on Control Automation Robotics & Vision (ICARCV). Proceedings, P1, DOI 10.1109/ICARCV.2014.7064485
   Elhag S, 2015, EXPERT SYST APPL, V42, P193, DOI 10.1016/j.eswa.2014.08.002
   Gerard V., 2011, 21 INT C EXH EL DIST, P6
   Granelli GP, 2008, ELECTR POW SYST RES, V78, P806, DOI 10.1016/j.epsr.2007.05.021
   Gulisano V, 2015, EXPERT SYST APPL, V42, P9620, DOI 10.1016/j.eswa.2015.07.027
   Hall M., 2009, SIGKDD EXPLORATIONS, V11, P10, DOI DOI 10.1145/1656274.1656278
   He M, 2010, INT CONF SMART GRID, P43, DOI 10.1109/SMARTGRID.2010.5622016
   Jaeger H., 2004, HARNESSING NONLINEAR
   Jaeger H., 2009, ECHOSTATE NETWORKS T
   Jaeger H., 2002, TECHNICAL REPORT
   Karnouskos S, 2011, IEEE IND ELEC, P4490
   Koc L, 2012, EXPERT SYST APPL, V39, P13492, DOI 10.1016/j.eswa.2012.07.009
   Kumar V., 2012, P 2 IEEE INT C PAR D, P290
   LeMay M., 2008, HAW INT C SYST SCI P, P174
   Lin J., 2007, P 19 AS PAC MICR C, V1, P1
   Lukosevicius M, 2009, COMPUT SCI REV, V3, P127, DOI 10.1016/j.cosrev.2009.03.005
   Mallat S., 2008, WAVELET TOUR SIGNAL
   Manandhar K, 2014, IEEE T CONTROL NETW, V1, P370, DOI 10.1109/TCNS.2014.2357531
   Metke A. R., 2010, INNOVATIVE SMART GRI, P1
   Mo YL, 2014, IEEE T CONTR SYST T, V22, P1396, DOI 10.1109/TCST.2013.2280899
   Ntalampiras S, 2015, IEEE T NEUR NET LEAR, V26, P1939, DOI 10.1109/TNNLS.2014.2362015
   Ntalampiras S, 2015, IEEE T IND INFORM, V11, P104, DOI 10.1109/TII.2014.2367322
   Ntalampiras S, 2011, IEEE T MULTIMEDIA, V13, P713, DOI 10.1109/TMM.2011.2122247
   Pasqualetti F, 2013, IEEE T AUTOMAT CONTR, V58, P2715, DOI 10.1109/TAC.2013.2266831
   Pramod T. C., 2013, COMP COMM NETW TECHN, P1
   Rabiner L. R., 1986, IEEE ASSP Magazine, V3, P4, DOI 10.1109/MASSP.1986.1165342
   REYNOLDS DA, 1995, IEEE T SPEECH AUDI P, V3, P72, DOI 10.1109/89.365379
   Richard M., 1991, NEURAL COMPUTATIONS, V3
   Saluja Riya, 2013, IEEE SOUTHEASTCON, P1
   Sancho-Asensio A, 2014, EXPERT SYST APPL, V41, P5832, DOI 10.1016/j.eswa.2014.03.035
   Shin S, 2013, EXPERT SYST APPL, V40, P315, DOI 10.1016/j.eswa.2012.07.057
   Sou KC, 2012, P AMER CONTR CONF, P3651
   Torrence C, 1998, B AM METEOROL SOC, V79, P61, DOI 10.1175/1520-0477(1998)079<0061:APGTWA>2.0.CO;2
   Verstraeten D, 2007, NEURAL NETWORKS, V20, P391, DOI 10.1016/j.neunet.2007.04.003
   Verstraeten D, 2006, IEEE IJCNN, P1050
   Wang D, 2014, ENERGIES, V7, P1517, DOI 10.3390/en7031517
   Wang WY, 2013, COMPUT NETW, V57, P1344, DOI 10.1016/j.comnet.2012.12.017
   Wu Y, 2013, IEEE GREEN TECHNOL, P183, DOI 10.1109/GreenTech.2013.35
   Zhang MD, 2012, P AS COMM PHOT C NOV, P1
   Zhou HJ, 2010, POW EN SOC GEN M, P1
   Zimmerman RD, 2011, IEEE T POWER SYST, V26, P12, DOI 10.1109/TPWRS.2010.2051168
NR 50
TC 18
Z9 18
U1 4
U2 173
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0957-4174
EI 1873-6793
J9 EXPERT SYST APPL
JI Expert Syst. Appl.
PD OCT 1
PY 2016
VL 58
BP 164
EP 173
DI 10.1016/j.eswa.2016.04.006
PG 10
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic; Operations Research & Management Science
SC Computer Science; Engineering; Operations Research & Management Science
GA DM7PN
UT WOS:000376552600014
DA 2020-02-19
ER

PT J
AU Zhang, KP
   Zhang, ZP
   Li, ZF
   Qiao, Y
AF Zhang, Kaipeng
   Zhang, Zhanpeng
   Li, Zhifeng
   Qiao, Yu
TI Joint Face Detection and Alignment Using Multitask Cascaded
   Convolutional Networks
SO IEEE SIGNAL PROCESSING LETTERS
LA English
DT Article
DE Cascaded convolutional neural network (CNN); face alignment; face
   detection
AB Face detection and alignment in unconstrained environment are challenging due to various poses, illuminations, and occlusions. Recent studies show that deep learning approaches can achieve impressive performance on these two tasks. In this letter, we propose a deep cascaded multitask framework that exploits the inherent correlation between detection and alignment to boost up their performance. In particular, our framework leverages a cascaded architecture with three stages of carefully designed deep convolutional networks to predict face and landmark location in a coarse-to-fine manner. In addition, we propose a new online hard sample mining strategy that further improves the performance in practice. Our method achieves superior accuracy over the state-of-the-art techniques on the challenging face detection dataset and benchmark and WIDER FACE benchmarks for face detection, and annotated facial landmarks in the wild benchmark for face alignment, while keeps real-time performance.
C1 [Zhang, Kaipeng; Li, Zhifeng; Qiao, Yu] Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen 518055, Peoples R China.
   [Zhang, Zhanpeng] Chinese Univ Hong Kong, Dept Informat Engn, Hong Kong, Hong Kong, Peoples R China.
RP Zhang, KP (reprint author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen 518055, Peoples R China.
EM kp.zhang@siat.ac.cn; zz013@ie.cuhk.edu.hk; zhifeng.li@siat.ac.cn;
   yu.qiao@siat.ac.cn
RI zhang, zhanpeng/AAE-7897-2019
FU External Cooperation Program of BIC; Chinese Academy of SciencesChinese
   Academy of Sciences [172644KYSB20160033, 172644KYSB20150019]; Shenzhen
   Research Program [KQCX2015033117354153, JSGG20150925164740726,
   CXZZ20150930104115529, CYJ20150925163005055, JCYJ201 60510154736343];
   Guangdong Research Program [2014B050505017, 2015B010129013]; Natural
   Science Foundation of Guangdong ProvinceNational Natural Science
   Foundation of Guangdong Province [2014A030313688]; Key Laboratory of
   Human Machine Intelligence-Synergy Systems through the Chinese Academy
   of Sciences
FX This work was supported in part by External Cooperation Program of BIC,
   in part by Chinese Academy of Sciences (172644KYSB20160033,
   172644KYSB20150019), in part by Shenzhen Research Program under Grant
   KQCX2015033117354153, Grant JSGG20150925164740726, Grant
   CXZZ20150930104115529, Grant CYJ20150925163005055, and Grant JCYJ201
   60510154736343, in part by Guangdong Research Program under Grant
   2014B050505017 and Grant 2015B010129013, in part by the Natural Science
   Foundation of Guangdong Province under Grant 2014A030313688, and in part
   by the Key Laboratory of Human Machine Intelligence-Synergy Systems
   through the Chinese Academy of Sciences. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Alexandre X. Falcao.
CR Burgos-Artizzu XP, 2013, IEEE I CONF COMP VIS, P1513, DOI 10.1109/ICCV.2013.191
   Cao X., 2012, INT J COMPUT VISION, V107, P177
   Chen D, 2014, LECT NOTES COMPUT SC, V8694, P109, DOI 10.1007/978-3-319-10599-4_8
   Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467
   Farfade SS, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P643, DOI 10.1145/2671188.2749408
   Ghiasi G., ARXIV150608347
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   Jain V., 2010, UMCS2010009
   Koestinger M., 2011, COMP VIS WORKSH ICCV, P2144, DOI DOI 10.1109/ICCVW.2011.6130513
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li HX, 2015, PROC CVPR IEEE, P5325, DOI 10.1109/CVPR.2015.7299170
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Mathias M, 2014, LECT NOTES COMPUT SC, V8692, P720, DOI 10.1007/978-3-319-10593-2_47
   Pham MT, 2010, PROC CVPR IEEE, P942, DOI 10.1109/CVPR.2010.5540117
   Ranjan Rajeev, 2017, Proceedings of the Indian National Science Academy Part B Biological Sciences, V87, P377, DOI 10.1007/s40011-015-0618-6
   Sun Y, 2014, ADV NEURAL INFORM PR, V60, P1988
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Xiong XH, 2013, PROC CVPR IEEE, P532, DOI 10.1109/CVPR.2013.75
   Yan JJ, 2014, PROC CVPR IEEE, P2497, DOI 10.1109/CVPR.2014.320
   Yang B, 2015, IEEE I CONF COMP VIS, P82, DOI 10.1109/ICCV.2015.18
   Yang H, 2014, C IND ELECT APPL, P1, DOI 10.1109/ICIEA.2014.6931121
   Yang  S., ARXIV151106523
   Yang S, 2015, IEEE I CONF COMP VIS, P3676, DOI 10.1109/ICCV.2015.419
   Yu X, 2013, IEEE I CONF COMP VIS, P1944, DOI 10.1109/ICCV.2013.244
   Zhang C, 2014, IEEE WINT CONF APPL, P1036, DOI 10.1109/WACV.2014.6835990
   Zhang J., 2014, INT J DISTRIB SENS N, V2014, P1, DOI DOI 10.1371/J0URNAL.P0NE.0110734
   ZHANG Z, 2014, LECT NOTES COMPUT SC, P94
   Zhu Q., 2006, IEEE COMP SOC C COMP, P1491, DOI DOI 10.1109/CVPR.2006.119
   Zhu XX, 2012, PROC CVPR IEEE, P2879, DOI 10.1109/CVPR.2012.6248014
NR 29
TC 440
Z9 440
U1 40
U2 150
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1070-9908
EI 1558-2361
J9 IEEE SIGNAL PROC LET
JI IEEE Signal Process. Lett.
PD OCT
PY 2016
VL 23
IS 10
BP 1499
EP 1503
DI 10.1109/LSP.2016.2603342
PG 5
WC Engineering, Electrical & Electronic
SC Engineering
GA DX3SN
UT WOS:000384293500003
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Amoh, J
   Odame, K
AF Amoh, Justice
   Odame, Kofi
TI Deep Neural Networks for Identifying Cough Sounds
SO IEEE TRANSACTIONS ON BIOMEDICAL CIRCUITS AND SYSTEMS
LA English
DT Article
DE Deep learning; machine learning; medical devices; wearables
AB In this paper, we consider two different approaches of using deep neural networks for cough detection. The cough detection task is cast as a visual recognition problem and as a sequence-to-sequence labeling problem. A convolutional neural network and a recurrent neural network are implemented to address these problems, respectively. We evaluate the performance of the two networks and compare them to other conventional approaches for identifying cough sounds. In addition, we also explore the effect of the network size parameters and the impact of long-term signal dependencies in cough classifier performance. Experimental results show both network architectures outperform traditional methods. Between the two, our convolutional network yields a higher specificity 92.7% whereas the recurrent attains a higher sensitivity of 87.7%.
C1 [Amoh, Justice; Odame, Kofi] Dartmouth Coll, Sch Engn, Hanover, NH 03755 USA.
RP Amoh, J (reprint author), Dartmouth Coll, Sch Engn, Hanover, NH 03755 USA.
EM justice.amoh.jr.th@dartmouth.edu
RI Amoh, Justice/P-9105-2019
OI Amoh, Justice/0000-0002-0359-2828
FU Neukom Institute for Computational Science at Dartmouth College
FX This work was supported by the Neukom Institute for Computational
   Science at Dartmouth College. This paper was recommended by Associate
   Editor S. Ostadabbas.
CR Amoh Justice, 2015, 2015 IEEE Biomedical Circuits and Systems Conference (BioCAS), P1, DOI 10.1109/BioCAS.2015.7348395
   Amoh Justice, 2013, Critical Reviews in Biomedical Engineering, V41, P457
   [Anonymous], 1969, IEEE T ACOUST SPEECH, VAU17, P225
   Barry Samantha J, 2006, Cough, V2, P8, DOI 10.1186/1745-9974-2-8
   Bengio Y, 1995, J ARTIF INTELL RES, V3, P249, DOI 10.1613/jair.233
   Bengio Y, 2013, INT CONF ACOUST SPEE, P8624, DOI 10.1109/ICASSP.2013.6639349
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bergstra J, 2011, ADV NEURAL INFORM PR, V24, P2546
   Birring SS, 2008, EUR RESPIR J, V31, P1013, DOI 10.1183/09031936.00057407
   Birring SS, 2006, RESP MED, V100, P1105, DOI 10.1016/j.rmed.2005.09.023
   Cho K., 2014, ARXIV14061078, P1724, DOI DOI 10.3115/V1/D14-1179
   Chorowski J., 2015, ATTENTION BASED MODE, P1
   Dauphin Yann N, 2015, ARXIV150204390
   DIELEMAN S, 2015, LASAGNE 1 RELEASE
   Drugman T., 2011, P 19 EUR SIGN PROC C, V32
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Graves A, 2013, ARXIV13080850, P1
   Hihi S., 1995, ADV NEURAL INFORM PR, P493
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1, DOI 10.1162/neco.1997.9.1.1
   Ioffe S, 2015, BATCH NORMALIZATION
   Ittichaichareon C., 2012, INT C COMP GRAPH SIM, P135
   Jim KC, 1996, IEEE T NEURAL NETWOR, V7, P1424, DOI 10.1109/72.548170
   Kraman SS, 2006, J APPL PHYSIOL, V101, P469, DOI 10.1152/japplphysiol.00273.2006
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kusy B, 2009, 2009 INTERNATIONAL CONFERENCE ON INFORMATION PROCESSING IN SENSOR NETWORKS (IPSN 2009), P109
   Larson EC, 2011, UBICOMP'11: PROCEEDINGS OF THE 2011 ACM INTERNATIONAL CONFERENCE ON UBIQUITOUS COMPUTING, P375
   Laurent Cesar, 2015, ARXIV151001378
   Le Cun B. B., 1990, ADV NEURAL INF PROCE
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lu H, 2009, MOBISYS'09: PROCEEDINGS OF THE 7TH ACM INTERNATIONAL CONFERENCE ON MOBILE SYSTEMS, APPLICATIONS, AND SERVICES, P165
   Matos S, 2007, IEEE T BIO-MED ENG, V54, P1472, DOI 10.1109/TBME.2007.900811
   Matos S, 2006, IEEE T BIO-MED ENG, V53, P1078, DOI 10.1109/TBME.2006.873548
   Olia P M, 2000, Respirology, V5, P271, DOI 10.1046/j.1440-1843.2000.00259.x
   PASCANU R, 2012, ARXIV E PRINTS, V1211, P5063
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   Snoek J., 2015, P 32 INT C MACH LEAR
   TOOP LJ, 1989, FAM PRACT, V6, P83, DOI 10.1093/fampra/6.2.83
   Verschelden P, 1996, EUR RESPIR J, V9, P880, DOI 10.1183/09031936.96.09050880
   WERBOS PJ, 1990, P IEEE, V78, P1550, DOI 10.1109/5.58337
   Zeiler M. D., 2012, ADADELTA ADAPTIVE LE, P6
NR 40
TC 8
Z9 8
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1932-4545
EI 1940-9990
J9 IEEE T BIOMED CIRC S
JI IEEE Trans. Biomed. Circuits Syst.
PD OCT
PY 2016
VL 10
IS 5
SI SI
BP 1003
EP 1011
DI 10.1109/TBCAS.2016.2598794
PG 9
WC Engineering, Biomedical; Engineering, Electrical & Electronic
SC Engineering
GA EE8BJ
UT WOS:000389849000009
PM 27654978
DA 2020-02-19
ER

PT J
AU Chen, YS
   Jiang, HL
   Li, CY
   Jia, XP
   Ghamisi, P
AF Chen, Yushi
   Jiang, Hanlu
   Li, Chunyang
   Jia, Xiuping
   Ghamisi, Pedram
TI Deep Feature Extraction and Classification of Hyperspectral Images Based
   on Convolutional Neural Networks
SO IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING
LA English
DT Article
DE Convolutional neural network (CNN); deep learning; feature extraction
   (FE); hyperspectral image (HSI) classification
ID SPECTRAL-SPATIAL CLASSIFICATION; DIMENSIONALITY REDUCTION;
   REPRESENTATIONS
AB Due to the advantages of deep learning, in this paper, a regularized deep feature extraction (FE) method is presented for hyperspectral image (HSI) classification using a convolutional neural network (CNN). The proposed approach employs several convolutional and pooling layers to extract deep features from HSIs, which are nonlinear, discriminant, and invariant. These features are useful for image classification and target detection. Furthermore, in order to address the common issue of imbalance between high dimensionality and limited availability of training samples for the classification of HSI, a few strategies such as L2 regularization and dropout are investigated to avoid overfitting in class data modeling. More importantly, we propose a 3-D CNN-based FE model with combined regularization to extract effective spectral-spatial features of hyperspectral imagery. Finally, in order to further improve the performance, a virtual sample enhanced method is proposed. The proposed approaches are carried out on three widely used hyperspectral data sets: Indian Pines, University of Pavia, and Kennedy Space Center. The obtained results reveal that the proposed models with sparse constraints provide competitive results to state-of-the-art methods. In addition, the proposed deep FE opens a new window for further research.
C1 [Chen, Yushi; Jiang, Hanlu; Li, Chunyang] Harbin Inst Technol, Dept Informat Engn, Sch Elect & Informat Engn, Harbin 150001, Peoples R China.
   [Jia, Xiuping] Univ New South Wales, Sch Engn & Informat Technol, Canberra, ACT 2600, Australia.
   [Ghamisi, Pedram] Tech Univ Munich, Signal Proc Earth Observat, D-80333 Munich, Germany.
   [Ghamisi, Pedram] German Aerosp Ctr DLR, Remote Sensing Technol Inst IMF, D-82234 Wessling, Germany.
RP Chen, YS (reprint author), Harbin Inst Technol, Dept Informat Engn, Sch Elect & Informat Engn, Harbin 150001, Peoples R China.
EM chenyushi@hit.edu.cn; halo91@163.com; lcy_buzz@mail.dlut.edu.cn;
   x.jia@adfa.edu.au; p.ghamisi@gmail.com
OI Chen, Yushi/0000-0003-2421-0996; Jia, Xiuping/0000-0001-9916-6382
FU Fundamental Research Funds for the Central UniversitiesFundamental
   Research Funds for the Central Universities [HIT.NSRIF.2013028];
   National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61301206]
FX This work was supported in part by the Fundamental Research Funds for
   the Central Universities under Grant HIT.NSRIF.2013028 and in part by
   the National Natural Science Foundation of China under Grant 61301206.
   (Corresponding author: Yushi Chen.)
CR Bachmann CM, 2006, IEEE T GEOSCI REMOTE, V44, P2786, DOI 10.1109/TGRS.2006.881801
   Bandos TV, 2009, IEEE T GEOSCI REMOTE, V47, P862, DOI 10.1109/TGRS.2008.2005729
   Bartholomew D. J., 2011, STRUCT EQU MODELING, V18, P686
   Benediktsson JA, 2005, IEEE T GEOSCI REMOTE, V43, P480, DOI 10.1109/TGRS.2004.842478
   Benediktsson JA, 2015, ARTECH HSE REMOTE SE, P1
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bioucas-Dias JM, 2013, IEEE GEOSC REM SEN M, V1, P6, DOI 10.1109/MGRS.2013.2244672
   Bruce LM, 2002, IEEE T GEOSCI REMOTE, V40, P2331, DOI 10.1109/TGRS.2002.804721
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chen Y, 2011, IEEE T GEOSCI REMOTE, V49, P3973, DOI 10.1109/TGRS.2011.2129595
   Chen YS, 2015, IEEE J-STARS, V8, P2381, DOI 10.1109/JSTARS.2015.2388577
   Chen YS, 2014, IEEE J-STARS, V7, P2094, DOI 10.1109/JSTARS.2014.2329330
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Edwards RE, 2013, 2013 12TH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA 2013), VOL 1, P58, DOI 10.1109/ICMLA.2013.18
   Fauvel M, 2013, P IEEE, V101, P652, DOI 10.1109/JPROC.2012.2197589
   Fauvel M, 2008, IEEE T GEOSCI REMOTE, V46, P3804, DOI 10.1109/TGRS.2008.922034
   Girshick R., 2014, P IEEE CVPR, P581
   Gomez-Chova L, 2015, P IEEE, V103, P1560, DOI 10.1109/JPROC.2015.2449668
   Han T, 2008, IEEE T GEOSCI REMOTE, V46, P2840, DOI 10.1109/TGRS.2008.2002952
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton G. E., 2012, RESEARCHGATE, V3, P212, DOI DOI 10.1106/CS.2012.521531
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   HOERL AE, 1970, TECHNOMETRICS, V12, P55
   HUGHES GF, 1968, IEEE T INFORM THEORY, V14, P55, DOI 10.1109/TIT.1968.1054102
   Jia XP, 2013, P IEEE, V101, P676, DOI 10.1109/JPROC.2012.2229082
   Jimenez LO, 1999, IEEE T GEOSCI REMOTE, V37, P2653, DOI 10.1109/36.803413
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kruger N, 2013, IEEE T PATTERN ANAL, V35, P1847, DOI 10.1109/TPAMI.2012.272
   Kuo BC, 2009, IEEE T GEOSCI REMOTE, V47, P1139, DOI 10.1109/TGRS.2008.2008308
   Le Roux N, 2010, NEURAL COMPUT, V22, P2192, DOI 10.1162/neco.2010.08-09-1081
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y., MNIST DATABASE HANDW
   Li J, 2013, IEEE T GEOSCI REMOTE, V51, P844, DOI 10.1109/TGRS.2012.2205263
   Licciardi G, 2012, IEEE GEOSCI REMOTE S, V9, P447, DOI 10.1109/LGRS.2011.2172185
   Lunga D, 2014, IEEE SIGNAL PROC MAG, V31, P55, DOI 10.1109/MSP.2013.2279894
   Nair V, 2010, ICML, V27, P807, DOI DOI 10.0RG/PAPERS/432.PDF
   Plaza A., 2009, MACHINE LEARNING SIG, P1
   Romero A, 2016, IEEE T GEOSCI REMOTE, V54, P1349, DOI 10.1109/TGRS.2015.2478379
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Scholkopf B., 2002, LEARNING KERNELS
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Song BQ, 2014, IEEE T GEOSCI REMOTE, V52, P5122, DOI 10.1109/TGRS.2013.2286953
   Swain P.H., 1981, IEEE T PATTERN ANAL, V6, P713, DOI [DOI 10.1109/TPAMI.1981.4767177, 10.1109/TPAMI.1981.4767177]
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tao C, 2015, IEEE GEOSCI REMOTE S, V12, P2438, DOI 10.1109/LGRS.2015.2482520
   Tarabalka Y, 2010, IEEE GEOSCI REMOTE S, V7, P736, DOI 10.1109/LGRS.2010.2047711
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
   Villa A, 2011, IEEE T GEOSCI REMOTE, V49, P4865, DOI 10.1109/TGRS.2011.2153861
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Yang HH, 2007, SPECTROSC SPECT ANAL, V27, P1955
   Zuo Z, 2016, IEEE T IMAGE PROCESS, V25, P2983, DOI 10.1109/TIP.2016.2548241
NR 51
TC 441
Z9 441
U1 94
U2 376
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 0196-2892
EI 1558-0644
J9 IEEE T GEOSCI REMOTE
JI IEEE Trans. Geosci. Remote Sensing
PD OCT
PY 2016
VL 54
IS 10
BP 6232
EP 6251
DI 10.1109/TGRS.2016.2584107
PG 20
WC Geochemistry & Geophysics; Engineering, Electrical & Electronic; Remote
   Sensing; Imaging Science & Photographic Technology
SC Geochemistry & Geophysics; Engineering; Remote Sensing; Imaging Science
   & Photographic Technology
GA DY5XT
UT WOS:000385178700043
OA Green Accepted
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Brahma, PP
   Wu, DP
   She, YY
AF Brahma, Pratik Prabhanjan
   Wu, Dapeng
   She, Yiyuan
TI Why Deep Learning Works: A Manifold Disentanglement Perspective
SO IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
LA English
DT Article
DE Deep learning; disentanglement; manifold learning; unsupervised feature
   transformation
ID DIMENSIONALITY
AB Deep hierarchical representations of the data have been found out to provide better informative features for several machine learning applications. In addition, multilayer neural networks surprisingly tend to achieve better performance when they are subject to an unsupervised pretraining. The booming of deep learning motivates researchers to identify the factors that contribute to its success. One possible reason identified is the flattening of manifold-shaped data in higher layers of neural networks. However, it is not clear how to measure the flattening of such manifold-shaped data and what amount of flattening a deep neural network can achieve. For the first time, this paper provides quantitative evidence to validate the flattening hypothesis. To achieve this, we propose a few quantities for measuring manifold entanglement under certain assumptions and conduct experiments with both synthetic and real-world data. Our experimental results validate the proposition and lead to new insights on deep learning.
C1 [Brahma, Pratik Prabhanjan; Wu, Dapeng] Univ Florida, Dept Elect & Comp Engn, Gainesville, FL 32611 USA.
   [She, Yiyuan] Florida State Univ, Dept Stat, Tallahassee, FL 32306 USA.
RP Wu, DP (reprint author), Univ Florida, Dept Elect & Comp Engn, Gainesville, FL 32611 USA.
EM prprbr@ufl.edu; wu@ece.ufl.edu; yshe@stat.fsu.edu
FU National Science FoundationNational Science Foundation (NSF)
   [DMS-1352259]; National Natural Science Foundation of ChinaNational
   Natural Science Foundation of China [61529101]
FX This work was supported in part by the National Science Foundation under
   Grant DMS-1352259 and in part by the National Natural Science Foundation
   of China under Grant 61529101. (Corresponding author: Dapeng Wu.)
CR Achlioptas D, 2003, J COMPUT SYST SCI, V66, P671, DOI 10.1016/S0022-0000(03)00025-4
   Basri R, 2003, IEEE T PATTERN ANAL, V25, P218, DOI 10.1109/TPAMI.2003.1177153
   Bengio Y., 2013, P 30 INT C MACH LEAR, P552
   Bengio Y., 2005, ADV NEURAL INFORM PR, V18, P107
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   CAYTON L, 2005, CS20080923 U CAL
   Chen H, 2003, IEE P-VIS IMAGE SIGN, V150, P153, DOI 10.1049/ip-vis:20030362
   Dahl GE, 2013, INT CONF ACOUST SPEE, P3422, DOI 10.1109/ICASSP.2013.6638293
   Dauphin Y. N., 2014, ADV NEURAL INFORM PR, P2933
   Deng L, 2013, FOUND TRENDS SIGNAL, V7, pI, DOI 10.1561/2000000039
   Glorot X., 2010, JLMR P TRACK, p[249, 2010], DOI DOI 10.1177/1753193409103364.
   Goodfellow I., 2009, ADV NEURAL INFORM PR, V22, P646
   Hinton G, 2005, AISTATS, V10, P33
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Huang QY, 2013, IEEE SIGNAL PROC MAG, V30, P132, DOI 10.1109/MSP.2013.2266960
   Joachims T., 1998, Machine Learning: ECML-98. 10th European Conference on Machine Learning. Proceedings, P137
   Knyazev AV, 2002, SIAM J SCI COMPUT, V23, P2008, DOI 10.1137/S1064827500377332
   Kreyszig E., 1991, DIFFERENTIAL GEOMETR
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee J. M., 2002, INTRO SMOOTH MANIFOL
   Salah R, 2011, P 28 INT C MACH LEAR, P833, DOI 10.1016/j.wasman.2010.10.006
   Scholkopf B., 2002, LEARNING KERNELS SUP
   She Y., 2014, SELECTABLE FACTOR EX
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
   Wan L., 2013, P 30 INT C MACH LEAR, P1058
   Weston J, 2008, P 25 INT C MACH LEAR, P1168, DOI DOI 10.1145/1390156.1390303
   Yijun Sun, 2009, Statistical Analysis and Data Mining, V2, P34, DOI 10.1002/sam.10028
   Zhang ZY, 2004, SIAM J SCI COMPUT, V26, P313, DOI 10.1137/S1064827502419154
NR 29
TC 36
Z9 38
U1 0
U2 29
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 2162-237X
EI 2162-2388
J9 IEEE T NEUR NET LEAR
JI IEEE Trans. Neural Netw. Learn. Syst.
PD OCT
PY 2016
VL 27
IS 10
BP 1997
EP 2008
DI 10.1109/TNNLS.2015.2496947
PG 12
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Computer Science, Theory & Methods; Engineering,
   Electrical & Electronic
SC Computer Science; Engineering
GA DX8MV
UT WOS:000384644000001
PM 26672049
DA 2020-02-19
ER

PT J
AU Wu, CH
   Wang, JW
   Liu, JT
   Liu, WY
AF Wu, Caihua
   Wang, Junwei
   Liu, Juntao
   Liu, Wenyu
TI Recurrent neural network based recommendation for time heterogeneous
   feedback
SO KNOWLEDGE-BASED SYSTEMS
LA English
DT Article
DE Recommender system; Collaborative filtering; Time heterogeneous
   feedback; Recurrent neural network; Deep learning
ID PROBABILISTIC MATRIX FACTORIZATION; SYSTEM
AB In recommender systems, several kinds of user feedback with time stamps are collected and used for recommendation, which is called the time heterogeneous feedback recommendation problem. The existing recommendation methods can handle only one kind of feedback or ignore the time stamps of the feedback. To solve the time heterogeneous feedback recommendation problem, in this paper, we propose a recurrent neural network model to predict the probability that the user will access an item given the time heterogeneous feedback of this user. To our best knowledge, it is the first time to solve the time heterogeneous feedback recommendation problem by deep neural network model. The proposed model is learned by back propagation algorithm and back propagation through time algorithm. The comparison results on four real-life datasets indicate that the proposed method outperforms the compared state-of-the-art approaches. (C) 2016 Elsevier B.V. All rights reserved.
C1 [Wu, Caihua] Air Force Early Warning Acad, Huang Pi NCO Sch, Sect Automat Command, Wuhan 430345, Peoples R China.
   [Wang, Junwei; Liu, Juntao] China Shipbuilding Ind Corp, Res Inst 709th, Wuhan 430074, Peoples R China.
   [Liu, Wenyu] Huazhong Univ Sci & Technol, Dept Elect & Informat Engn, Wuhan 430074, Peoples R China.
RP Liu, JT (reprint author), China Shipbuilding Ind Corp, Res Inst 709th, Wuhan 430074, Peoples R China.
EM wucaihua2009@163.com; wjw.20012001@163.com; prolay@163.com;
   liuwy@hust.edu.cn
RI Liu, Wenyu/AAG-1426-2019
OI Liu, Wenyu/0000-0002-4582-7488
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61403350, 61401228]
FX This work is supported by National Natural Science Foundation of China
   (No. 61403350 and No.61401228).
CR Adomavicius G, 2005, IEEE T KNOWL DATA EN, V17, P734, DOI 10.1109/TKDE.2005.99
   Bengio Y, 2003, J MACH LEARN RES, V3, P1137, DOI 10.1162/153244303322533223
   Cantador I., 2011, P 5 ACM C REC SYST, P387, DOI DOI 10.1145/2043932.2044016
   Chen T., 2013, P 30 INT C MACH LEAR, V1, P436
   Cho Kyunghyun, 2014, 8 WORKSH SYNT SEM ST
   Gantner Z., 2011, P KNOWL DISC DAT MIN
   Gantner Z., 2011, P 5 ACM C REC SYST R, P305, DOI [10.1145/2043932.2043989, DOI 10.1145/2043932.2043989]
   Guo G., 2014, P 2014 INT C ADV SOC
   Hanhuai Shan, 2010, Proceedings 2010 10th IEEE International Conference on Data Mining (ICDM 2010), P1025, DOI 10.1109/ICDM.2010.116
   Hu YF, 2008, IEEE DATA MINING, P263, DOI 10.1109/ICDM.2008.22
   Irsoy O., 2014, ADV NEURAL INFORM PR, V27, P2096
   Jamali M., 2011, P 22 INT JOINT C ART, P2644
   Kalchbrenner N., 2014, P 52 ANN M ASS COMP
   Kim Y., 2014, P 2014 C EMP METH NA, P1746, DOI DOI 10.3115/V1/D14-1181
   Koren Y, 2011, RECOMMENDER SYSTEMS HANDBOOK, P145, DOI 10.1007/978-0-387-85820-3_5
   Lai SW, 2015, PROCEEDINGS OF THE TWENTY-NINTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2267
   Li WJ, 2009, 21ST INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-09), PROCEEDINGS, P1126
   Lim Y., 2007, P KDD CUP WORKSH, P15
   Lin C, 2014, INFORM SCIENCES, V254, P1, DOI 10.1016/j.ins.2013.08.034
   Liu JT, 2016, INFORM SCIENCES, V327, P1, DOI 10.1016/j.ins.2015.08.001
   Liu JT, 2014, INFORM SCIENCES, V278, P434, DOI 10.1016/j.ins.2014.03.063
   Liu JT, 2013, DECIS SUPPORT SYST, V55, P838, DOI 10.1016/j.dss.2013.04.002
   Liu WY, 2015, EXPERT SYST APPL, V42, P774, DOI 10.1016/j.eswa.2014.08.044
   Lu J, 2015, DECIS SUPPORT SYST, V74, P12, DOI 10.1016/j.dss.2015.03.008
   Ma H, 2008, P 17 ACM C INF KNOWL, P931, DOI DOI 10.1145/1458082.1458205
   Ma H, 2011, ACM T INFORM SYST, V29, DOI 10.1145/1961209.1961212
   Ma Hao, 2011, P 4 ACM INT C WEB SE, P287, DOI DOI 10.1145/1935826.1935877
   Mikolov T, 2011, INT CONF ACOUST SPEE, P5528
   Mikolov T, 2009, INT CONF ACOUST SPEE, P4725, DOI 10.1109/ICASSP.2009.4960686
   Mikolov Tomas, 2010, P INT
   Mnih A., 2008, P INT C MACH LEARN, V25, P880, DOI DOI 10.1145/1390156.1390267
   Pan WK, 2015, KNOWL-BASED SYST, V73, P173, DOI 10.1016/j.knosys.2014.09.013
   Rendle S., 2009, P 25 C UNC ART INT, V09, P452, DOI DOI 10.1145/1772690.1772773
   Salakhutdinov R., 2007, P INT C MACH LEARN, V24, P791, DOI DOI 10.1145/1273496.1273596
   Salakhutdinov R. R., 2008, ADV NEURAL INFORM PR, V20, P1257, DOI DOI 10.1145/1390156.1390267
   Schwenk  H., 2005, P C HUM LANG TECHN E, P201
   Shambour Q, 2011, INT J INTELL SYST, V26, P814, DOI 10.1002/int.20495
   Shi Y., 2012, P 6 ACM C REC SYST, P139, DOI DOI 10.1145/2365952.2365981
   Singh A. P., 2008, P 14 ACM SIGKDD INT, P650, DOI DOI 10.1145/1401890.1401969
   Singh A. P., 2010, P 26 C ANN C UNC ART, P556
   Socher R., 2012, P 2012 JOINT C EMP M, P1201
   Socher R., 2011, P C EMP METH NAT LAN, P151
   Socher Richard, 2013, EMNLP 13
   Srebro N., 2005, ADV NEURAL INFORM PR, V17, P1329
   van den Oord Aaron, 2013, ADV NEURAL INFORM PR, P2643
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Wang C, 2011, P 17 ACM SIGKDD INT, P448, DOI DOI 10.1145/2020408.2020480
   Wang H, 2014, COLLABORATIVE DEEP L
   Weimer M, 2008, MACH LEARN, V72, P263, DOI 10.1007/s10994-008-5073-7
   Wu L, 2012, P 21 ACM INT C INF K, P1854, DOI DOI 10.1145/2396761.2398531
   Yuan Q., 2011, P 5 ACM C REC SYST, P245, DOI DOI 10.1145/2043932.2043975
   Zeng W., 2013, P 28 ANN ACM S APPL, P237
   Zhang J., 2015, ABS150204163 CORR
   Zhang Z, 2013, INFORM SCIENCES, V235, P117, DOI 10.1016/j.ins.2013.01.025
   Zheng FH, 2012, PROCEEDINGS OF 2012 INTERNATIONAL CONFERENCE ON PUBLIC ADMINISTRATION (8TH), VOL III, P403
   Zhu J., 2011, P 25 AAAI C ART INT, P158
NR 56
TC 20
Z9 20
U1 5
U2 40
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0950-7051
EI 1872-7409
J9 KNOWL-BASED SYST
JI Knowledge-Based Syst.
PD OCT 1
PY 2016
VL 109
BP 90
EP 103
DI 10.1016/j.knosys.2016.06.028
PG 14
WC Computer Science, Artificial Intelligence
SC Computer Science
GA DW0AV
UT WOS:000383304200009
DA 2020-02-19
ER

PT J
AU Erfani, SM
   Rajasegarar, S
   Karunasekera, S
   Leckie, C
AF Erfani, Sarah M.
   Rajasegarar, Sutharshan
   Karunasekera, Shanika
   Leckie, Christopher
TI High-dimensional and large-scale anomaly detection using a linear
   one-class SVM with deep learning
SO PATTERN RECOGNITION
LA English
DT Article
DE Anomaly detection; Outlier detection; High-dimensional data; Deep belief
   net; Deep learning; One-class SVM; Feature extraction
ID SUPPORT VECTOR MACHINES; FEATURE-SELECTION; CLASSIFICATION; DIAGNOSIS
AB High-dimensional problem domains pose significant challenges for anomaly detection. The presence of irrelevant features can conceal the presence of anomalies. This problem, known as the 'curse of dimensionality', is an obstacle for many anomaly detection techniques. Building a robust anomaly detection model for use in high-dimensional spaces requires the combination of an unsupervised feature extractor and an anomaly detector. While one-class support vector machines are effective at producing decision surfaces from well-behaved feature vectors, they can be inefficient at modelling the variation in large, high-dimensional datasets. Architectures such as deep belief networks (DBNs) are a promising technique for learning robust features. We present a hybrid model where an unsupervised DBN is trained to extract generic underlying features, and a one-class SVM is trained from the features learned by the DBN. Since a linear kernel can be substituted for nonlinear ones in our hybrid model without loss of accuracy, our model is scalable and computationally efficient. The experimental results show that our proposed model yields comparable anomaly detection performance with a deep autoencoder, while reducing its training and testing time by a factor of 3 and 1000, respectively. (C) 2016 Elsevier Ltd. All rights reserved.
C1 [Erfani, Sarah M.; Rajasegarar, Sutharshan; Karunasekera, Shanika; Leckie, Christopher] Univ Melbourne, NICTA Victoria Res Lab, Dept Comp & Informat Syst, Room 7-14,Dough MacDonell Bldg, Melbourne, Vic 3010, Australia.
   [Rajasegarar, Sutharshan] Deakin Univ, Sch Informat Technol, Geelong, Vic 3217, Australia.
RP Erfani, SM (reprint author), Univ Melbourne, NICTA Victoria Res Lab, Dept Comp & Informat Syst, Room 7-14,Dough MacDonell Bldg, Melbourne, Vic 3010, Australia.
EM sarah.erfani@unimelb.edu.au
RI erfani, sedigheh/D-3998-2017
OI Rajasegarar, Sutharshan/0000-0002-6559-6736; LECKIE,
   CHRISTOPHER/0000-0002-4388-0517; Karunasekera,
   Shanika/0000-0001-7080-5064
FU NICTA; ARCFondation ARC pour la Recherche sur le CancerAustralian
   Research Council [LP120100529, LE120100129]; University of Melbourne
   Early Career Research (ECR) grant; EU FP7 SocloTal grantEuropean Union
   (EU)
FX We thank the support from NICTA; the ARC Grants LP120100529 and
   LE120100129; University of Melbourne Early Career Research (ECR) grant;
   and the EU FP7 SocloTal grant.
CR Abe S., 2005, ADV PTRN RECOGNIT
   Auria L, 2008, TECHNICAL REPORT
   Bashah N, 2005, PROCEEDINGS OF WORLD ACADEMY OF SCIENCE, ENGINEERING AND TECHNOLOGY, VOL 6, P291
   Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153
   Bengio Y., 2007, LARGE SCALE KERNEL M, V34, P1
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Brugge K, 2013, MACH LEARN, V93, P53, DOI 10.1007/s10994-013-5390-3
   Campbell C, 2001, ADV NEUR IN, V13, P395
   Cao LJ, 2003, NEUROCOMPUTING, V55, P321, DOI 10.1016/S0925-2312(03)00433-8
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Demsar J, 2006, J MACH LEARN RES, V7, P1
   Erfani S. M., 2016, P INT JOINT C ART IN
   Erfani S. M., 2016, P SIAM INT C DAT MIN
   Erfani SM, 2015, PROCEEDINGS OF THE TWENTY-NINTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, P432
   Friedman M, 1937, J AM STAT ASSOC, V32, P675, DOI 10.2307/2279372
   Galar M, 2011, PATTERN RECOGN, V44, P1761, DOI 10.1016/j.patcog.2011.01.017
   Garcia S, 2010, INFORM SCIENCES, V180, P2044, DOI 10.1016/j.ins.2009.12.010
   Garcia S, 2008, J MACH LEARN RES, V9, P2677
   Gubbi J, 2013, FUTURE GENER COMP SY, V29, P1645, DOI 10.1016/j.future.2013.01.010
   Hamal P., 2010, ISMIR, P339
   Hinton G., 2012, NEURAL NETWORKS TRIC, V9, P599, DOI DOI 10.1007/978-3-642-35289-8_32
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hsu C.W., PRACTICAL GUIDE SUPP
   Huang F. J., 2006, C COMP VIS PATT REC, P284, DOI [10.1109/CVPR.2006.164, DOI 10.1109/CVPR.2006.164]
   Kassab R, 2009, MACH LEARN, V74, P191, DOI 10.1007/s10994-008-5092-4
   Larochelle H, 2009, J MACH LEARN RES, V10, P1
   Laskov P., 2004, PIK PRAXIS INFORM KO, V27, P228
   Lee JA, 2007, INFORM SCI STAT, P1
   Liu DH, 2013, PATTERN RECOGN, V46, P2531, DOI 10.1016/j.patcog.2013.02.007
   Liu T, 2010, LECT NOTES COMPUT SC, V6443, P314, DOI 10.1007/978-3-642-17537-4_39
   Liu Y, 2011, PATTERN RECOGN, V44, P2287, DOI 10.1016/j.patcog.2010.12.012
   Manevitz LM, 2002, J MACH LEARN RES, V2, P139, DOI 10.1162/15324430260185574
   Nair V, 2009, ADV NEURAL INF PROCE, V22, P1339
   Neumann J, 2005, MACH LEARN, V61, P129, DOI 10.1007/s10994-005-1505-9
   Rajasegarar S, 2007, IEEE ICC, P3864, DOI 10.1109/ICC.2007.637
   Rajasegarar S, 2008, IEEE ICC, P1610, DOI 10.1109/ICC.2008.311
   Rajasegarar S, 2010, IEEE T INF FOREN SEC, V5, P518, DOI 10.1109/TIFS.2010.2051543
   Scholkopf B, 2001, NEURAL COMPUT, V13, P1443, DOI 10.1162/089976601750264965
   SCHOLKOPF Bernhard, 2001, LEARNING KERNELS SUP
   SHAFFER JP, 1986, J AM STAT ASSOC, V81, P826, DOI 10.2307/2289016
   Shen KQ, 2008, MACH LEARN, V70, P1, DOI 10.1007/s10994-007-5025-7
   Sheskin D., 2007, HDB PARAMETRIC NONPA
   Subramaniam S., 2006, P 32 INT C VER LARG, P187
   Tang Y., 2013, ICML WORKSH CHALL RE
   Tax DMJ, 2004, MACH LEARN, V54, P45, DOI 10.1023/B:MACH.0000008084.60811.49
   Vapnik V. N., 1998, STAT LEARNING THEORY
   Vempati S., 2010, P BRIT MACH VIS C, P1
   Wang DF, 2006, IEEE T SYST MAN CY B, V36, P1283, DOI 10.1109/TSMCB.2006.876189
   Wang LA, 2010, LECT NOTES ARTIF INT, V6118, P16
   Wang YX, 2013, IEEE T AUDIO SPEECH, V21, P1381, DOI 10.1109/TASL.2013.2250961
   WARD JH, 1963, J AM STAT ASSOC, V58, P236, DOI 10.2307/2282967
   Widodo A, 2007, EXPERT SYST APPL, V33, P241, DOI 10.1016/j.eswa.2006.04.020
   Widodo A, 2007, EXPERT SYST APPL, V32, P299, DOI 10.1016/j.eswa.2005.11.031
   WILCOXON F, 1945, BIOMETRICS BULL, V1, P80, DOI 10.2307/3001968
   Wulsin DF, 2011, J NEURAL ENG, V8, DOI 10.1088/1741-2560/8/3/036015
   Zimek Arthur, 2012, Statistical Analysis and Data Mining, V5, P363, DOI 10.1002/sam.11161
NR 59
TC 144
Z9 160
U1 29
U2 248
PU ELSEVIER SCI LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND
SN 0031-3203
EI 1873-5142
J9 PATTERN RECOGN
JI Pattern Recognit.
PD OCT
PY 2016
VL 58
BP 121
EP 134
DI 10.1016/j.patcog.2016.03.028
PG 14
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
SC Computer Science; Engineering
GA DO4BN
UT WOS:000377726900010
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Ouyang, WL
   Zeng, XY
   Wang, XG
AF Ouyang, Wanli
   Zeng, Xingyu
   Wang, Xiaogang
TI Learning Mutual Visibility Relationship for Pedestrian Detection with a
   Deep Model
SO INTERNATIONAL JOURNAL OF COMPUTER VISION
LA English
DT Article
DE Deep model; Deep learning; Pedestrian detection; Object detection
AB Detecting pedestrians in cluttered scenes is a challenging problem in computer vision. The difficulty is added when several pedestrians overlap in images and occlude each other. We observe, however, that the occlusion/visibility statuses of overlapping pedestrians provide useful mutual relationship for visibility estimation-the visibility estimation of one pedestrian facilitates the visibility estimation of another. In this paper, we propose a mutual visibility deep model that jointly estimates the visibility statuses of overlapping pedestrians. The visibility relationship among pedestrians is learned from the deep model for recognizing co-existing pedestrians. Then the evidence of co-existing pedestrians is used for improving the single pedestrian detection results. Compared with existing image-based pedestrian detection approaches, our approach has the lowest average miss rate on the Caltech-Train dataset and the ETH dataset. Experimental results show that the mutual visibility deep model effectively improves the pedestrian detection results. The mutual visibility deep model leads to 6-15 % improvements on multiple benchmark datasets.
C1 [Ouyang, Wanli; Zeng, Xingyu; Wang, Xiaogang] Chinese Univ Hong Kong, Dept Elect Engn, Hong Kong, Hong Kong, Peoples R China.
RP Ouyang, WL (reprint author), Chinese Univ Hong Kong, Dept Elect Engn, Hong Kong, Hong Kong, Peoples R China.
EM wlouyang@ee.cuhk.edu.hk; xyzeng@ee.cuhk.edu.hk; xgwang@ee.cuhk.edu.hk
RI Ouyang, Wanli/I-7135-2018; Ouyang, Wanli/AAB-1196-2020
OI Ouyang, Wanli/0000-0002-9163-2761; 
FU General Research Fund - Research Grants Council of Hong KongHong Kong
   Research Grants Council [CUHK 417110, CUHK 417011]; National Natural
   Science Foundation of ChinaNational Natural Science Foundation of China
   [61005057]; Guangdong Innovative Research Team Program
   [201001D0104648280]
FX This work is supported by the General Research Fund sponsored by the
   Research Grants Council of Hong Kong (Project No. CUHK 417110 and CUHK
   417011), National Natural Science Foundation of China (Project No.
   61005057), and Guangdong Innovative Research Team Program (No.
   201001D0104648280).
CR Bar-Hillel A., 2010, P ECCV
   Benenson R., 2012, CVPR
   Benenson R., 2013, P CVPR
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Chen G., 2013, P CVPR
   Dai S., 2007, IEEE C CVPR
   Dalal N., 2005, IEEE C CVPR
   Dean T., 2013, P IEEE C CVPR NEW YO
   Deng J., 2009, IEEE C CVPR
   Desai C., 2009, ICCV
   Desai C., 2012, IEEE INT C ECCV
   Doll'ar P., 2012, ECCV
   Dollar P., 2010, BMVC
   Dollar P, 2009, BMVC
   Dollar P., 2014, CALTECH PEDESTRIAN D
   Dollar P, 2014, IEEE T PATTERN ANAL, V36, P1532, DOI 10.1109/TPAMI.2014.2300479
   Dollar P, 2012, IEEE T PATTERN ANAL, V34, P743, DOI 10.1109/TPAMI.2011.155
   Duan G., 2010, ECCV
   Enzweiler M., 2010, CVPR
   Erhan D, 2010, J MACH LEARN RES, V11, P625
   Ess A., 2007, ICCV
   Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hu J., 2014, CVPR
   Jarrett K., 2009, CVPR
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Krizhevsky A., 2012, NIPS
   Le Q. V., 2012, ICML
   Lee H., 2009, ICML
   Leibe B., 2005, CVPR
   Li C., 2011, ICCV
   Lin Z., 2007, ICCV
   Liu P., 2014, CVPR
   Luo P., 2012, CVPR
   Mann J., 2013, CVPR
   Mathias M., 2013, CVPR
   Norouzi M, 2009, CVPR
   Ouyang  W., 2013, CVPR
   Ouyang W., 2012, CVPR
   Ouyang WL, 2016, IEEE T CIRC SYST VID, V26, P2123, DOI 10.1109/TCSVT.2015.2501940
   Ouyang WL, 2015, IEEE T PATTERN ANAL, V37, P1875, DOI 10.1109/TPAMI.2014.2377734
   Paisitkriangkrai S., 2013, ICCV
   Park D., 2013, CVPR
   Park  D., 2010, ECCV
   Pepik B, 2013, PROC CVPR IEEE, P3286, DOI 10.1109/CVPR.2013.422
   Ranzato M., 2011, CVPR
   Sadeghi MA, 2011, PROC CVPR IEEE, P1745, DOI 10.1109/CVPR.2011.5995711
   Schwartz W., 2009, ICCV
   Sermanet P., 2013, CVPR
   Sermanet P., 2013, ARXIV13126229
   Shen CH, 2013, INT J COMPUT VISION, V103, P326, DOI 10.1007/s11263-013-0608-1
   Shet V., 2007, CVPR
   Sun L., 2014, CVPR
   Sun Y., 2014, CVPR
   Tang S., 2013, P ICCV
   Tang S., 2012, BMVC
   Viola P, 2005, INT J COMPUT VISION, V63, P153, DOI 10.1007/s11263-005-6644-8
   Walk S., 2010, CVPR
   Wang X., 2009, CVPR
   Wanli Ouyang, 2013, ICCV
   Wojek C., 2008, DAGM
   Woonhyun Nam, 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P1801, DOI 10.1109/ICCVW.2011.6130467
   Wu B., 2005, ICCV
   Wu B, 2009, INT J COMPUT VISION, V82, P185, DOI 10.1007/s11263-008-0194-9
   Wu TF, 2011, INT J COMPUT VISION, V93, P226, DOI 10.1007/s11263-010-0346-6
   Xiao J., 2012, CVPR
   Yan J., 2012, CVPR
   Yan Jianzhou, 2013, CVPR
   Yang Y., 2012, CVPR
   Yang Y., 2011, CVPR
   Yao Bangpeng, 2010, CVPR
   Zeng  X., 2013, ICCV
   Zhu L., 2010, CVPR
NR 77
TC 17
Z9 18
U1 0
U2 33
PU SPRINGER
PI DORDRECHT
PA VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN 0920-5691
EI 1573-1405
J9 INT J COMPUT VISION
JI Int. J. Comput. Vis.
PD OCT
PY 2016
VL 120
IS 1
BP 14
EP 27
DI 10.1007/s11263-016-0890-9
PG 14
WC Computer Science, Artificial Intelligence
SC Computer Science
GA DU3DX
UT WOS:000382092100002
DA 2020-02-19
ER

PT J
AU Jie, ZQ
   Liang, XD
   Feng, JS
   Lu, WF
   Tay, EHF
   Yan, SC
AF Jie, Zequn
   Liang, Xiaodan
   Feng, Jiashi
   Lu, Wen Feng
   Tay, Eng Hock Francis
   Yan, Shuicheng
TI Scale-Aware Pixelwise Object Proposal Networks
SO IEEE TRANSACTIONS ON IMAGE PROCESSING
LA English
DT Article
DE Object proposal; convolutional neural networks; deep learning
AB Object proposal is essential for current state-of-theart object detection pipelines. However, the existing proposal methods generally fail in producing results with satisfying localization accuracy. The case is even worse for small objects, which, however, are quite common in practice. In this paper, we propose a novel scale-aware pixelwise object proposal network (SPOP-net) to tackle the challenges. The SPOP-net can generate proposals with high recall rate and average best overlap, even for small objects. In particular, in order to improve the localization accuracy, a fully convolutional network is employed which predicts locations of object proposals for each pixel. The produced ensemble of pixelwise object proposals enhances the chance of hitting the object significantly without incurring heavy extra computational cost. To solve the challenge of localizing objects at small scale, two localization networks, which are specialized for localizing objects with different scales are introduced, following the divide-and-conquer philosophy. Location outputs of these two networks are then adaptively combined to generate the final proposals by a large-/small-size weighting network. Extensive evaluations on PASCAL VOC 2007 and COCO 2014 show the SPOP network is superior over the state-of-the-art models. The high-quality proposals from SPOP-net also significantly improve the mean average precision of object detection with Fast-Regions with CNN features framework. Finally, the SPOP-net (trained on PASCAL VOC) shows great generalization performance when testing it on ILSVRC 2013 validation set.
C1 [Jie, Zequn; Lu, Wen Feng; Tay, Eng Hock Francis] Natl Univ Singapore, Dept Mech Engn, Singapore 119077, Singapore.
   [Liang, Xiaodan] Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA.
   [Feng, Jiashi] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 119077, Singapore.
   [Yan, Shuicheng] Qihoo 360 Technol Co Ltd, Artificial Intelligence Inst, Beijing 100015, Peoples R China.
RP Jie, ZQ (reprint author), Natl Univ Singapore, Dept Mech Engn, Singapore 119077, Singapore.
EM jiezequn@u.nus.edu; xdliang328@gmail.com; jshfeng@gmail.com;
   mpelwf@nus.edu.sg; mpetayeh@nus.edu.sg; eleyans@nus.edu.sg
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Alexe B, 2012, IEEE T PATTERN ANAL, V34, P2189, DOI 10.1109/TPAMI.2012.28
   Alexe B, 2010, PROC CVPR IEEE, P73, DOI 10.1109/CVPR.2010.5540226
   Arbelaez P, 2014, PROC CVPR IEEE, P328, DOI 10.1109/CVPR.2014.49
   Chen L.-C., 2015, P INT C LEARN REPR
   Chen XZ, 2015, PROC CVPR IEEE, P2587, DOI 10.1109/CVPR.2015.7298874
   Cheng MM, 2014, PROC CVPR IEEE, P3286, DOI 10.1109/CVPR.2014.414
   Erhan D, 2014, PROC CVPR IEEE, P2155, DOI 10.1109/CVPR.2014.276
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Ghodrati A, 2015, IEEE I CONF COMP VIS, P2578, DOI 10.1109/ICCV.2015.296
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Hariharan B, 2011, IEEE I CONF COMP VIS, P991, DOI 10.1109/ICCV.2011.6126343
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI 10.1007/978-3-319-10578-9_23
   Hosang J, 2016, IEEE T PATTERN ANAL, V38, P814, DOI 10.1109/TPAMI.2015.2465908
   Huang L., 2015, DENSEBOX UNIFYING LA
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Jie Z., IEEE T CIRC IN PRESS, VPP
   Krahenbuhl P, 2014, LECT NOTES COMPUT SC, V8693, P725, DOI 10.1007/978-3-319-10602-1_47
   Krahenbuhl P, 2015, PROC CVPR IEEE, P1574, DOI 10.1109/CVPR.2015.7298765
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Liang  X., 2015, PROPOSAL FREE NETWOR
   Liang XD, 2016, PROC CVPR IEEE, P633, DOI 10.1109/CVPR.2016.75
   Liang XD, 2015, IEEE I CONF COMP VIS, P999, DOI 10.1109/ICCV.2015.120
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Manen S, 2013, IEEE I CONF COMP VIS, P2536, DOI 10.1109/ICCV.2013.315
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Simonyan Karen, 2015, P INT C LEARN REPR
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Wei Y., PATTERN REC IN PRESS
   Wei Y., 2015, STC SIMPLE COMPLEX F
   Wei Y., IEEE T PATT IN PRESS
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang ZM, 2011, PROC CVPR IEEE, P1497, DOI 10.1109/CVPR.2011.5995411
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
NR 39
TC 9
Z9 9
U1 2
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1057-7149
EI 1941-0042
J9 IEEE T IMAGE PROCESS
JI IEEE Trans. Image Process.
PD OCT
PY 2016
VL 25
IS 10
BP 4525
EP 4539
DI 10.1109/TIP.2016.2593342
PG 15
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
SC Computer Science; Engineering
GA DT5EY
UT WOS:000381505500003
PM 27448357
DA 2020-02-19
ER

PT J
AU Yang, WX
   Jin, LW
   Tao, DC
   Xie, ZC
   Feng, ZY
AF Yang, Weixin
   Jin, Lianwen
   Tao, Dacheng
   Xie, Zecheng
   Feng, Ziyong
TI DropSample: A new training method to enhance deep convolutional neural
   networks for large-scale unconstrained handwritten Chinese character
   recognition
SO PATTERN RECOGNITION
LA English
DT Article
DE Convolutional neural network; Deep learning; Handwritten character
   recognition; Domain-specific knowledge
ID DIRECTION-CHANGE FEATURES; ONLINE
AB Inspired by the theory of Leitner's learning box from the field of psychology, we propose DropSample, a new method for training deep convolutional neural networks (DCNNs), and apply it to large-scale online handwritten Chinese character recognition (HCCR). According to the principle of DropSample, each training sample is associated with a quota function that is dynamically adjusted on the basis of the classification confidence given by the DCNN softmax output. After a learning iteration, samples with low confidence will have a higher frequency of being selected as training data; in contrast, well-trained and well-recognized samples with very high confidence will have a lower frequency of being involved in the ongoing training and can be gradually eliminated. As a result, the learning process becomes more efficient as it progresses. Furthermore, we investigate the use of domain-specific knowledge to enhance the performance of DCNN by adding a domain knowledge layer before the traditional CNN. By adopting DropSample together with different types of domain-specific knowledge, the accuracy of HCCR can be improved efficiently. Experiments on the CASIA-OLHDWB 1.0, CASIA-OLHWDB 1.1, and ICDAR 2013 online HCCR competition datasets yield outstanding recognition rates of 9733%, 97.06%, and 97.51% respectively, all of which are significantly better than the previous best results reported in the literature. (C) 2016 Elsevier Ltd. All rights reserved.
C1 [Yang, Weixin; Jin, Lianwen; Xie, Zecheng; Feng, Ziyong] S China Univ Technol, Coll Elect & Informat Engn, Guangzhou 510640, Guangdong, Peoples R China.
   [Tao, Dacheng] Univ Technol Sydney, Ctr Quantum Computat & Intelligent Syst, Sydney, NSW 2007, Australia.
RP Jin, LW (reprint author), S China Univ Technol, Coll Elect & Informat Engn, Guangzhou 510640, Guangdong, Peoples R China.
EM lianwen.jin@gmail.com
RI Yang, Weixin/AAE-1112-2019
OI Yang, Weixin/0000-0002-6028-3597
FU NSFCNational Natural Science Foundation of China [61472144]; GDSTP
   [2013B010202004, 2014A010103012, 2015B010101004, 2015B010130003,
   2015B010131004]; GDUPS
FX This research is supported in part by NSFC (Grant No.: 61472144), GDSTP
   (Grant No.: 2013B010202004, 2014A010103012, 2015B010101004,
   2015B010130003, 2015B010131004), GDUPS (2011). The authors thank all
   reviewers for their valuable comments on improving the quality of this
   paper.
CR Aiquan Yuan, 2012, Proceedings of the 10th IAPR International Workshop on Document Analysis Systems (DAS 2012), P125, DOI 10.1109/DAS.2012.61
   Baddeley A., 1997, HUMAN MEMORY THEORY
   Bai ZL, 2005, PROC INT CONF DOC, P262
   Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153
   Biem A, 1997, IEEE T SIGNAL PROCES, V45, P500, DOI 10.1109/78.554319
   Bissacco A, 2013, IEEE I CONF COMP VIS, P785, DOI 10.1109/ICCV.2013.102
   Chen G, 2006, PROCEEDINGS OF 2006 INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND CYBERNETICS, VOLS 1-7, P3329
   Ciresan D.C., 2013, ARXIV13090261
   Ciresan D, 2012, PROC CVPR IEEE, P3642, DOI 10.1109/CVPR.2012.6248110
   Ciresan DC, 2011, PROC INT CONF DOC, P1135, DOI 10.1109/ICDAR.2011.229
   Coates A, 2011, PROC INT CONF DOC, P440, DOI 10.1109/ICDAR.2011.95
   Ebbinghaus H., 1913, MEMORY CONTRIBUTION
   Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504
   Fujisawa H, 2008, PATTERN RECOGN, V41, P2435, DOI 10.1016/j.patcog.2008.03.015
   FUKUSHIMA K, 1980, BIOL CYBERN, V36, P193, DOI 10.1007/BF00344251
   Gao Xue, 2002, Acta Electronica Sinica, V30, P651
   Graham B., 2013, ARXIV13080371
   Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947
   Hambly B, 2010, ANN MATH, V171, P109, DOI 10.4007/annals.2010.171.109
   He MJ, 2015, PROC INT CONF DOC, P61, DOI 10.1109/ICDAR.2015.7333726
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton G. E, 2012, ARXIV12070580
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Horiuchi T, 1997, PROC INT CONF DOC, P511, DOI 10.1109/ICDAR.1997.620551
   Jaderberg M., 2014, ARXIV14062227
   Jaderberg M, 2014, LECT NOTES COMPUT SC, V8692, P512, DOI 10.1007/978-3-319-10593-2_34
   Jin LW, 2011, INT J DOC ANAL RECOG, V14, P53, DOI 10.1007/s10032-010-0116-6
   [金连文 Jin Lianwen], 2002, [中国图象图形学报. A, Journal of image and graphics], V7, P170
   Kai Ding, 2009, 2009 10th International Conference on Document Analysis and Recognition (ICDAR), P531, DOI 10.1109/ICDAR.2009.29
   Kim HJ, 1997, PATTERN RECOGN, V30, P1489, DOI 10.1016/S0031-3203(96)00161-6
   KIMURA F, 1987, IEEE T PATTERN ANAL, V9, P149, DOI 10.1109/TPAMI.1987.4767881
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   LeCun Y, 1998, LECT NOTES COMPUT SC, V1524, P9
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y., 1990, ADV NEURAL INF PROCE
   Leitner S, 1972, SO LERNT MAN LERNEN
   Leung K C, 2009, 2009 10th International Conference on Document Analysis and Recognition (ICDAR), P1026, DOI 10.1109/ICDAR.2009.48
   Lin M., 2013, ARXIV13124400
   Liu CL, 2013, PATTERN RECOGN, V46, P155, DOI 10.1016/j.patcog.2012.06.021
   Liu CL, 2011, PROC INT CONF DOC, P37, DOI 10.1109/ICDAR.2011.17
   Liu CL, 2005, PROC INT CONF DOC, P846
   Liu CL, 2004, NINTH INTERNATIONAL WORKSHOP ON FRONTIERS IN HANDWRITING RECOGNITION, PROCEEDINGS, P300
   Liu CL, 2005, PATTERN RECOGN, V38, P2242, DOI 10.1016/j.patcog.2005.04.019
   Long T, 2008, PATTERN RECOGN, V41, P2916, DOI 10.1016/j.patcog.2008.02.009
   Ma L.L., 2010, TIBETAN COMPONENT RE
   Mehrotra K., 2013, ACM P 4 INT WORKSH M
   Miyao H, 2006, LECT NOTES COMPUT SC, V3872, P96
   MONTAVON G, 2012, LNCS, V7700
   Nair V, 2010, P 27 INT C MACH LEAR
   Okamoto M, 1998, INT C PATT RECOG, P1747, DOI 10.1109/ICPR.1998.712064
   Okamoto M, 1999, PATTERN RECOGN, V32, P1115, DOI 10.1016/S0031-3203(98)00153-8
   Okamoto M, 1997, PROC INT CONF DOC, P926, DOI 10.1109/ICDAR.1997.620646
   Owens JD, 2008, P IEEE, V96, P879, DOI 10.1109/JPROC.2008.917757
   Ozkan C, 2003, PHOTOGRAMM ENG REM S, V69, P1225
   Prechelt L, 1998, LECT NOTES COMPUT SC, V1524, P55
   [钱跃良 Qian Yueliang], 2005, [高技术通讯, Chinese High Technology Letters], V15, P107
   Ryu S, 2014, INT J DOC ANAL RECOG, V17, P79, DOI 10.1007/s10032-013-0206-3
   Simard P.Y., 2003, P 12 INT C DOC AN RE, V2
   Sun Y, 2014, ADV NEURAL INFORM PR, V60, P1988
   Szegedy C., 2014, ARXIV14094842
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Toshev A., 2013, ARXIV13124659
   Vincent P, 2008, P 25 INT C MACH LEAR, P1096, DOI DOI 10.1145/1390156.1390294
   Wan L., 2013, P 30 INT C MACH LEAR, P1058
   Wang T, 2012, INT C PATT RECOG, P3304
   Wshah S, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2865, DOI 10.1109/ICPR.2010.702
   Wu C., 2014, ICFHR
   Yang WX, 2015, PROC INT CONF DOC, P551, DOI 10.1109/ICDAR.2015.7333822
   Yin F, 2013, PROC INT CONF DOC, P1051, DOI 10.1109/ICDAR.2013.210
   Yin F, 2013, PROC INT CONF DOC, P1464, DOI 10.1109/ICDAR.2013.218
NR 70
TC 43
Z9 44
U1 2
U2 89
PU ELSEVIER SCI LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND
SN 0031-3203
EI 1873-5142
J9 PATTERN RECOGN
JI Pattern Recognit.
PD OCT
PY 2016
VL 58
BP 190
EP 203
DI 10.1016/j.patcog.2016.04.007
PG 14
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
SC Computer Science; Engineering
GA DO4BN
UT WOS:000377726900015
DA 2020-02-19
ER

PT J
AU Wang, XG
   Duan, X
   Bai, X
AF Wang, Xinggang
   Duan, Xiong
   Bai, Xiang
TI Deep sketch feature for cross-domain image retrieval
SO NEUROCOMPUTING
LA English
DT Article
DE Sketch recognition; Image retrieval; Deep learning
ID RECOGNITION
AB Deep learning has been proven to be very effective for various image recognition tasks, e.g., image classification, semantic segmentation, image retrieval, shape classification, etc. However, existing works on deep learning for image recognition mainly focus on either natural image data or binary shape data. In this paper, we show that deep convolutional neural networks (DCNN) is also suitable for cross-domain image recognition, i.e., using sketch as query to retrieve natural images in a large dataset. To solve this kind of cross-domain problem, we propose to train CNN jointly using image data and sketch data in a novel way. The learned deep feature is effective for cross-domain image retrieval - using simple Eucli-dean distance on the learned feature can significantly outperform the previous state-of-the-arts. In addition, we find that pre-training and a feasible data-argumentation for DCNN can largely surpass human-level performance in the standard sketch classification benchmark. (C) 2016 Elsevier B.V. All rights reserved.
C1 [Wang, Xinggang; Duan, Xiong; Bai, Xiang] Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, 1037 Luoyu Rd, Wuhan 430074, Hubei Province, Peoples R China.
RP Bai, X (reprint author), Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, 1037 Luoyu Rd, Wuhan 430074, Hubei Province, Peoples R China.
EM xbai@hust.edu.cn
RI Wang, Xinggang/W-4374-2019
FU National Natural Science Foundation of China (NSFC)National Natural
   Science Foundation of China [61503145, 61222308, 61573160]
FX This work was supported by National Natural Science Foundation of China
   (NSFC) (No. 61503145, No. 61222308 and No. 61573160).
CR [Anonymous], 2010, INT SKETCH BAS IM SE
   Babenko A, 2014, LECT NOTES COMPUT SC, V8689, P584, DOI 10.1007/978-3-319-10590-1_38
   Bai X, 2015, IEEE T PATTERN ANAL, V37, P2361, DOI 10.1109/TPAMI.2015.2424863
   Bai XA, 2009, PROC CVPR IEEE, P1335, DOI 10.1109/CVPRW.2009.5206543
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   Bozas K, 2012, LECT NOTES COMPUT SC, V7431, P210, DOI 10.1007/978-3-642-33179-4_21
   Cao Y, 2011, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2011.5995460
   Chen T, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618470
   Cheng MM, 2014, VISUAL COMPUT, V30, P443, DOI 10.1007/s00371-013-0867-4
   Cheng MM, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778820
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Dalal N, 2005, PROC CVPR IEEE, P886
   Dong WS, 2015, INT J COMPUT VISION, V114, P217, DOI 10.1007/s11263-015-0808-y
   Eitz M., IEEE T VIS COMPUT GR, V17
   Eitz M., 2009, P 6 EUR S SKETCH BAS, P29
   Eitz M, 2012, ACM T GRAPHIC, V31, DOI [10.1145/2185520.2185527, 10.1145/2185520.2185540]
   Everingham M., 2007, PASCAL VISUAL OBJECT
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li Y, 2015, COMPUT VIS IMAGE UND, V137, P1, DOI 10.1016/j.cviu.2015.02.003
   Li Yi, 2013, BRIT MACH VIS C BMVC
   Liu Y, 2015, IEEE T BROADCAST, V61, P651, DOI 10.1109/TBC.2015.2460611
   Liu Y, 2015, IEEE T CIRC SYST VID, V25, P1960, DOI 10.1109/TCSVT.2015.2450175
   Liu Z, 2015, NEUROCOMPUTING, V168, P1144, DOI 10.1016/j.neucom.2015.05.008
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Luo X., 2011, WORDS OF INTEREST MO
   Ma C., 2013, BRIT MACH VIS C, V2, P3
   Mahendran A., 2015, P IEEE C COMP VIS PA
   Olsen L, 2009, COMPUT GRAPH-UK, V33, P85, DOI 10.1016/j.cag.2008.09.013
   Parui S., 2014, ECCV 2014
   Saavedra JM, 2014, MULTIMED TOOLS APPL, V73, P2033, DOI 10.1007/s11042-013-1689-0
   Sarvadevabhatla R. K., 2015, ARXIV150200254
   Schneider RG, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661231
   Shen W, 2015, PROC CVPR IEEE, P3982, DOI 10.1109/CVPR.2015.7299024
   Shrivastava A., 2011, ACM T GRAPHIC, V30, P61
   Shuang Liang, 2014, Advances in Multimedia Information Processing - PCM 2014. 15th Pacific-Rim Conference on Multimedia. Proceedings: LNCS 8879, P133, DOI 10.1007/978-3-319-13168-9_14
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Thayananthan A, 2003, P 2003 IEEE COMP SOC, P1
   Tian S, 2016, NEUROCOMPUTING, V171, P768, DOI 10.1016/j.neucom.2015.07.028
   Wang XG, 2014, PATTERN RECOGN, V47, P2116, DOI 10.1016/j.patcog.2013.12.008
   Xia GS, 2010, INT J COMPUT VISION, V88, P382, DOI 10.1007/s11263-009-0312-3
   Yang F, 2016, NEUROCOMPUTING, V173, P1310, DOI 10.1016/j.neucom.2015.09.004
   [于谦 Yu Qian], 2015, [高分子通报, Polymer Bulletin], P1
   Zhang ZX, 2015, NEUROCOMPUTING, V166, P151, DOI 10.1016/j.neucom.2015.03.083
   Zhang ZX, 2014, NEUROCOMPUTING, V140, P146, DOI 10.1016/j.neucom.2014.03.028
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
NR 46
TC 9
Z9 9
U1 1
U2 18
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0925-2312
EI 1872-8286
J9 NEUROCOMPUTING
JI Neurocomputing
PD SEP 26
PY 2016
VL 207
BP 387
EP 397
DI 10.1016/j.neucom.2016.04.046
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA DV3AX
UT WOS:000382794500035
DA 2020-02-19
ER

PT J
AU Poria, S
   Cambria, E
   Gelbukh, A
AF Poria, Soujanya
   Cambria, Erik
   Gelbukh, Alexander
TI Aspect extraction for opinion mining with a deep convolutional neural
   network
SO KNOWLEDGE-BASED SYSTEMS
LA English
DT Article
DE Sentiment analysis; Aspect extraction; Opinion mining; CNN; RBM; DNN
AB In this paper, we present the first deep learning approach to aspect extraction in opinion mining. Aspect extraction is a subtask of sentiment analysis that consists in identifying opinion targets in opinionated text, i.e., in detecting the specific aspects of a product or service the opinion holder is either praising or complaining about. We used a 7-layer deep convolutional neural network to tag each word in opinionated sentences as either aspect or non-aspect word. We also developed a set of linguistic patterns for the same purpose and combined them with the neural network. The resulting ensemble classifier, coupled with a word-embedding model for sentiment analysis, allowed our approach to obtain significantly better accuracy than state-of-the-art methods. (C) 2016 Elsevier B.V. All rights reserved.
C1 [Poria, Soujanya] Nanyang Technol Univ, Temasek Labs, Singapore, Singapore.
   [Cambria, Erik] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore, Singapore.
   [Gelbukh, Alexander] Inst Politecn Nacl, CIC, Mexico City 07738, DF, Mexico.
RP Cambria, E (reprint author), Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore, Singapore.
EM cambria@ntu.edu.sg
RI Cambria, Erik/C-2103-2013; Poria, Soujanya/AAC-6549-2020; Gelbukh,
   Alexander/A-8979-2008; Poria, Soujanya/L-8361-2015
OI Cambria, Erik/0000-0002-3030-1280; Poria, Soujanya/0000-0001-6924-7931;
   Gelbukh, Alexander/0000-0001-7845-9039; 
CR Andrzejewski David, 2009, Proc Int Conf Mach Learn, V382, P25
   Arjun Mukherjee, 2012, P 50 ANN M ASS COMP, P339
   Blair-Goldensohn S., 2008, WWW WORKSH NLP INF E, P14
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Branavan SRK, 2009, J ARTIF INTELL RES, V34, P569, DOI 10.1613/jair.2633
   Cambria E, 2015, SENTIC COMPUTING COM
   Cambria E, 2016, IEEE INTELL SYST, V31, P102, DOI 10.1109/MIS.2016.31
   Cambria E, 2014, PROCEEDINGS OF THE TWENTY-EIGHTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1515
   Cambria E, 2014, KNOWL-BASED SYST, V69, P1, DOI 10.1016/j.knosys.2014.07.002
   Cambria E, 2013, IEEE INTELL SYST, V28, P6, DOI 10.1109/MIS.2013.68
   Cambria E, 2009, LECT NOTES COMPUT SC, V5707, P252, DOI 10.1007/978-3-642-04391-8_33
   Chen Z, 2014, P 20 ACM SIGKDD INT, P1116
   Collobert R, 2011, J MACH LEARN RES, V12, P2493
   Ding X., 2008, P 2008 INT C WEB SEA, P231, DOI DOI 10.1145/1341531.1341561
   Fonseca Erick R, 2013, P 2013 INT JOINT C N, P1
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hofmann T, 1999, SIGIR'99: PROCEEDINGS OF 22ND INTERNATIONAL CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P50, DOI 10.1145/312624.312649
   Hu M., 2004, P 10 ACM SIGKDD INT, V04, P168, DOI DOI 10.1145/1014052.1014073
   Hu YN, 2014, MACH LEARN, V95, P423, DOI 10.1007/s10994-013-5413-0
   Jagarlamudi J, 2012, P 13 C EUR CHAPT ASS, P204
   Jakob N., 2010, P 2010 C EMP METH NA, P1035
   Lu Y., 2008, P 17 INT C WORLD WID, P121, DOI DOI 10.1145/1367497.1367514
   Lu Y., 2009, P 18 INT C WORLD WID, P131, DOI DOI 10.1145/1526709.1526728
   McAuley J., 2013, P RECSYS 13 HONG KON
   Mcauliffe J.D., 2008, ADV NEURAL INFORM PR, P121
   Mikolov T., 2013, P NAACL 2013, P746
   Popescu Ana-Maria, 2005, P C EMP METH NAT LAN, P3
   Poria S, 2015, IEEE COMPUT INTELL M, V10, P26, DOI 10.1109/MCI.2015.2471215
   Poria S, 2012, INT CONF SIGN PROCES, P1251, DOI 10.1109/ICoSP.2012.6491803
   Poria S, 2012, INT CONF DAT MIN WOR, P709, DOI 10.1109/ICDMW.2012.142
   Poria Soujanya, 2016, IJCNN
   Poria Soujanya, 2015, P 2015 C EMP METH NA, P2539
   Qiu GA, 2011, COMPUT LINGUIST, V37, P9, DOI 10.1162/coli_a_00034
   Rill S, 2014, KNOWL-BASED SYST, V69, P24, DOI 10.1016/j.knosys.2014.05.008
   Scaffidi C, 2007, EC'07: PROCEEDINGS OF THE EIGHTH ANNUAL CONFERENCE ON ELECTRONIC COMMERCE, P182
   Taylor G.W., 2007, ADV NEURAL INFORM PR, V19, P1345
   Titov  Ivan, 2008, P 17 INT C WORLD WID, P111, DOI DOI 10.1145/1367497.1367513
   Wang H., 2010, P 16 ACM SIGKDD INT, P783, DOI DOI 10.1145/1835804.1835903
   Wang T, 2014, KNOWL-BASED SYST, V71, P86, DOI 10.1016/j.knosys.2014.05.018
   Zhao W.X., 2010, P 2010 C EMP METH NA, P56
   Zhiqiang T., 2014, P 8 INT WORKSH SEM E, P235, DOI DOI 10.3115/V1/S14-2038
NR 41
TC 192
Z9 199
U1 45
U2 275
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0950-7051
EI 1872-7409
J9 KNOWL-BASED SYST
JI Knowledge-Based Syst.
PD SEP 15
PY 2016
VL 108
SI SI
BP 42
EP 49
DI 10.1016/j.knosys.2016.06.009
PG 8
WC Computer Science, Artificial Intelligence
SC Computer Science
GA DV0FE
UT WOS:000382592600006
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Atzori, M
   Cognolato, M
   Muller, H
AF Atzori, Manfredo
   Cognolato, Matteo
   Mueller, Henning
TI Deep Learning with Convolutional Neural Networks Applied to
   Electromyography Data: A Resource for the Classification of Movements
   for Prosthetic Hands
SO FRONTIERS IN NEUROROBOTICS
LA English
DT Article
DE electromyography; prosthetics; rehabilitation robotics; machine
   learning; deep learning; convolutional neural networks
ID OF-THE-ART; SURFACE EMG; MYOELECTRIC CONTROL; SIGNALS
AB Natural control methods based on surface electromyography (sEMG) and pattern recognition are promising for hand prosthetics. However, the control robustness offered by scientific research is still not sufficient for many real life applications, and commercial prostheses are capable of offering natural control for only a few movements. In recent years deep learning revolutionized several fields of machine learning, including computer vision and speech recognition. Our objective is to test its methods for natural control of robotic hands via sEMG using a large number of intact subjects and amputees. We tested convolutional networks for the classification of an average of 50 hand movements in 67 intact subjects and 11 transradial amputees. The simple architecture of the neural network allowed to make several tests in order to evaluate the effect of preprocessing, layer architecture, data augmentation and optimization. The classification results are compared with a set of classical classification methods applied on the same datasets. The classification accuracy obtained with convolutional neural networks using the proposed architecture is higher than the average results obtained with the classical classification methods, but lower than the results obtained with the best reference methods in our tests. The results show that convolutional neural networks with a very simple architecture can produce accurate results comparable to the average classical classification methods. They show that several factors (including pre-processing, the architecture of the net and the optimization parameters) can be fundamental for the analysis of sEMG data. Larger networks can achieve higher accuracy on computer vision and object recognition tasks. This fact suggests that it may be interesting to evaluate if larger networks can increase sEMG classification accuracy too.
C1 [Atzori, Manfredo; Cognolato, Matteo; Mueller, Henning] Univ Appl Sci Western Switzerland, Inst Informat Syst, HES SO Valais Wallis, Sierre, Switzerland.
RP Atzori, M (reprint author), Univ Appl Sci Western Switzerland, Inst Informat Syst, HES SO Valais Wallis, Sierre, Switzerland.
EM manfredo.atzori@hevs.ch
OI atzori, manfredo/0000-0001-5397-2063
FU Swiss National Science Foundation Sinergia project [160837 Megane Pro]
FX This work is partially supported by the Swiss National Science
   Foundation Sinergia project # 160837 Megane Pro.
CR Atzori M, 2016, J REHABIL RES DEV, V53, P345, DOI 10.1682/JRRD.2014.09.0218
   Atzori M, 2014, SCI DATA, V1, DOI 10.1038/sdata.2014.53
   Atzori M, 2015, FRONT SYST NEUROSCI, V9, DOI 10.3389/fnsys.2015.00162
   Atzori M, 2015, IEEE T NEUR SYS REH, V23, P73, DOI 10.1109/TNSRE.2014.2328495
   Atzori M, 2014, IEEE ENG MED BIO, P4362, DOI 10.1109/EMBC.2014.6944590
   Atzori M, 2014, IEEE ENG MED BIO, P3545, DOI 10.1109/EMBC.2014.6944388
   Bengio Y., 2015, DEEP LEARNI IN PRESS
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Castellini C, 2009, J NEUROENG REHABIL, V6, DOI 10.1186/1743-0003-6-41
   Castellini C, 2009, BIOL CYBERN, V100, P35, DOI 10.1007/s00422-008-0278-1
   Chestek CA, 2011, J NEURAL ENG, V8, DOI 10.1088/1741-2560/8/4/045005
   Chicco D., 2014, P 5 ACM C BIOINF COM, V14, P533, DOI [10.1145/2649387.2649442, DOI 10.1145/2649387.2649442]
   Ciresan D, 2012, NEURAL NETWORKS, V32, P333, DOI 10.1016/j.neunet.2012.02.023
   Crawford B., 2005, P AAAI, V20, P523
   Cristianini N., 2000, INTRO SUPPORT VECTOR
   CUTKOSKY MR, 1989, IEEE T ROBOTIC AUTOM, V5, P269, DOI 10.1109/70.34763
   Dahl G., 2010, ADV NEURAL INFORM PR, P469
   Dahl G, 2014, ARXIV14061231, P1
   De Luca CJ, 1997, J APPL BIOMECH, V13, P135, DOI 10.1123/jab.13.2.135
   Deng J, 2010, LECT NOTES COMPUT SC, V6315, P71, DOI 10.1007/978-3-642-15555-0_6
   Deng L, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 3 AND 4, P1692
   Deng L, 2013, IEEE INT NEW CIRC
   Dosen S, 2010, J NEUROENG REHABIL, V7, DOI 10.1186/1743-0003-7-42
   Duda R.O., 2001, PATTERN CLASSIFICATI
   Edwards S. J., 2002, DEV FUNCTIONAL HAND
   Engdahl SM, 2015, J NEUROENG REHABIL, V12, DOI 10.1186/s12984-015-0044-2
   Engelhart K, 1999, MED ENG PHYS, V21, P431, DOI 10.1016/S1350-4533(99)00066-1
   Englehart K, 2003, IEEE T BIO-MED ENG, V50, P848, DOI 10.1109/TBME.2003.813539
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Farina D, 2014, IEEE T NEUR SYS REH, V22, P797, DOI 10.1109/TNSRE.2014.2305111
   Feix T, 2009, ROB SCI SYST WORKSH, V2, P2
   Fougner A, 2012, IEEE T NEUR SYS REH, V20, P663, DOI 10.1109/TNSRE.2012.2196711
   Fukuda O, 2003, IEEE T ROBOTIC AUTOM, V19, P210, DOI 10.1109/TRA.2003.808873
   Gijsberts A, 2014, IEEE T NEUR SYS REH, V22, P735, DOI 10.1109/TNSRE.2014.2303394
   Goodfellow I., 2016, DEEP LEARNING UNPUB
   Hargrove L, 2007, P ANN INT IEEE EMBS, P4842, DOI 10.1109/IEMBS.2007.4353424
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Jiang NF, 2013, IEEE ENG MED BIO, P1603, DOI 10.1109/EMBC.2013.6609822
   Jiang N, 2014, IEEE T NEUR SYS REH, V22, P549, DOI 10.1109/TNSRE.2013.2287383
   KAMAKURA N, 1980, AM J OCCUP THER, V34, P437, DOI 10.5014/ajot.34.7.437
   Kato R, 2006, INTELLIGENT AUTONOMOUS SYSTEMS 9, P946
   Koiva R, 2012, IEEE ENG MED BIO, P531, DOI 10.1109/EMBC.2012.6345985
   Krasoulis A, 2015, I IEEE EMBS C NEUR E, P631, DOI 10.1109/NER.2015.7146702
   Krizhevsky A., 2010, ADV NEURAL INFORM PR, P1
   Kuiken TA, 2009, JAMA-J AM MED ASSOC, V301, P619, DOI 10.1001/jama.2009.116
   Kuzborskij I, 2012, IEEE ENG MED BIO, P4931, DOI 10.1109/EMBC.2012.6347099
   LeCun Y., 1995, ICANN '95. International Conference on Artificial Neural Networks. Neuronimes '95 Scientific Conference, P53
   Li GL, 2010, IEEE T NEUR SYS REH, V18, P185, DOI 10.1109/TNSRE.2009.2039619
   Lucas MF, 2008, BIOMED SIGNAL PROCES, V3, P169, DOI 10.1016/j.bspc.2007.09.002
   Markovic M, 2014, J NEURAL ENG, V11, DOI 10.1088/1741-2560/11/4/046001
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Muller H, 2009, P 10 INT C CROSS LAN
   Ortiz-Catalan M, 2013, IEEE ENG MED BIO, P6651, DOI 10.1109/EMBC.2013.6611081
   Park K.-H., 2016, P 4 INT WINT C BRAIN, P1, DOI DOI 10.1109/IWW-BCI.2016.7457459
   Peerdeman B, 2011, J REHABIL RES DEV, V48, P719, DOI 10.1682/JRRD.2010.08.0161
   Ramsundar B, 2015, ARXIV150202072
   Scheme E, 2011, J REHABIL RES DEV, V48, P643, DOI 10.1682/JRRD.2010.09.0177
   Sebelius FCP, 2005, J HAND SURG-AM, V30A, P780, DOI 10.1016/j.jhsa.2005.01.002
   Sermanet P, 2013, PROC CVPR IEEE, P3626, DOI 10.1109/CVPR.2013.465
   Tenore FVG, 2009, IEEE T BIO-MED ENG, V56, P1427, DOI 10.1109/TBME.2008.2005485
   Urbanchek MG, 2012, PLAST RECONSTR SURG, V130, P55, DOI DOI 10.1097/01.PRS.0000421762.53265.54
   Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412
   Young AJ, 2013, IEEE T BIO-MED ENG, V60, P1250, DOI 10.1109/TBME.2012.2232293
   Zardoshti-Kermani M., 1995, IEEE Transactions on Rehabilitation Engineering, V3, P324, DOI 10.1109/86.481972
NR 65
TC 65
Z9 66
U1 11
U2 74
PU FRONTIERS MEDIA SA
PI LAUSANNE
PA AVENUE DU TRIBUNAL FEDERAL 34, LAUSANNE, CH-1015, SWITZERLAND
SN 1662-5218
J9 FRONT NEUROROBOTICS
JI Front. Neurorobotics
PD SEP 7
PY 2016
VL 10
AR 9
DI 10.3389/fnbot.2016.00009
PG 10
WC Computer Science, Artificial Intelligence; Robotics; Neurosciences
SC Computer Science; Robotics; Neurosciences & Neurology
GA DU9IH
UT WOS:000382529600001
PM 27656140
OA DOAJ Gold, Green Published
DA 2020-02-19
ER

PT J
AU Xue, HY
   Liu, Y
   Cai, D
   He, XF
AF Xue, Hongyang
   Liu, Yao
   Cai, Deng
   He, Xiaofei
TI Tracking people in RGBD videos using deep learning and motion clues
SO NEUROCOMPUTING
LA English
DT Article; Proceedings Paper
CT IEEE International Conference on Security, Pattern Analysis, and
   Cybernetics (SPAC)
CY OCT 18-19, 2014
CL Huazhong Univ Sci & Technol, Wuhan, PEOPLES R CHINA
SP IEEE, IEEE Syst Man & Cybernet Soc, Univ Macau, Wuhan Assoc Sci & Technol
HO Huazhong Univ Sci & Technol
DE RGB-D; Human tracking; Pedestrian trajectory; Convolutional neural
   network; Deep learning
AB Tracking people in videos is an important topic in surveillance. We consider the problem of human tracking in RGBD videos filmed by sensors such as MS Kinect and Primesense. Our goal is to track persons where the crowd of people is known in advance or all persons in the video have appeared in the very beginning. Thus we can train a classifier to help classify and track persons across the video. A deep learning model trained with big data has been proved to be an effective classifier for various kinds of objects. We propose to train a deep convolutional neural network, which improves tracking performance, to classify people. And a motion model based on spatial and kinetic clues is combined with the network to track people in the scene. We demonstrate the effectiveness of our method by evaluating it on several datasets and comparing with traditional methods like SVM. (C) 2016 Elsevier B.V. All rights reserved.
C1 [Xue, Hongyang; Liu, Yao; Cai, Deng; He, Xiaofei] Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310058, Zhejiang, Peoples R China.
RP Cai, D (reprint author), Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310058, Zhejiang, Peoples R China.
EM hyxue@outlook.com; xuanyao0111@gmail.com; dengcai@gmail.com;
   xiaofei_h@qq.com
CR Benenson R, 2012, PROC CVPR IEEE, P2903, DOI 10.1109/CVPR.2012.6248017
   Berclaz J, 2011, IEEE T PATTERN ANAL, V33, P1806, DOI 10.1109/TPAMI.2011.21
   Black J, 2002, IEEE WORKSHOP ON MOTION AND VIDEO COMPUTING (MOTION 2002), PROCEEDINGS, P169, DOI 10.1109/MOTION.2002.1182230
   Comaniciu D, 2000, PROC CVPR IEEE, P142, DOI 10.1109/CVPR.2000.854761
   Cui XY, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P1263
   Dalal N, 2005, PROC CVPR IEEE, P886
   Fleuret F, 2008, IEEE T PATTERN ANAL, V30, P267, DOI 10.1109/TPAMI.2007.1174
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   HAN M, 2004, [No title captured], V1, P1
   Isard M, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P34, DOI 10.1109/ICCV.2001.937594
   Jafari OH, 2014, IEEE INT CONF ROBOT, P5636, DOI 10.1109/ICRA.2014.6907688
   Jia Y., ARXIV14085093
   Kang J., 2004, AS C COMP VIS, V7, P15
   Khanloo Bahman Yari Saeed, 2010, Proceedings of the 2010 Seventh Canadian Conference on Computer and Robot Vision (CRV 2010), P347, DOI 10.1109/CRV.2010.52
   Khanloo BYS, 2012, COMPUT VIS IMAGE UND, V116, P676, DOI 10.1016/j.cviu.2012.01.004
   Liu J, 2013, IEEE IMAGE PROC, P3088, DOI 10.1109/ICIP.2013.6738636
   Mazzon R, 2013, NEUROCOMPUTING, V100, P41, DOI 10.1016/j.neucom.2011.09.038
   Mittal A, 2003, INT J COMPUT VISION, V51, P189, DOI 10.1023/A:1021849801764
   Munaro M, 2014, AUTON ROBOT, V37, P227, DOI 10.1007/s10514-014-9385-0
   Qi ZQ, 2011, NEUROCOMPUTING, V74, P1769, DOI 10.1016/j.neucom.2011.02.011
   Spinello L, 2011, IEEE INT C INT ROBOT, P3838, DOI 10.1109/IROS.2011.6048835
   Sun Y, 2014, ADV NEURAL INFORM PR, V60, P1988
   Vedaldi A, 2008, LECT NOTES COMPUT SC, V5305, P705, DOI 10.1007/978-3-540-88693-8_52
   Wang Q, 2011, NEUROCOMPUTING, V74, P1035, DOI 10.1016/j.neucom.2010.11.020
   Wei Yichen, 2007, IEEE 11 INT C COMP V, P1, DOI DOI 10.1109/ICCV.2007.4408949
   Zhang SP, 2013, NEUROCOMPUTING, V100, P31, DOI 10.1016/j.neucom.2011.11.031
NR 26
TC 34
Z9 36
U1 0
U2 78
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0925-2312
EI 1872-8286
J9 NEUROCOMPUTING
JI Neurocomputing
PD SEP 5
PY 2016
VL 204
SI SI
BP 70
EP 76
DI 10.1016/j.neucom.2015.06.112
PG 7
WC Computer Science, Artificial Intelligence
SC Computer Science
GA DQ1HR
UT WOS:000378952600010
DA 2020-02-19
ER

PT J
AU Zhu, ZT
   Wang, XG
   Bai, S
   Yao, C
   Bai, X
AF Zhu, Zhuotun
   Wang, Xinggang
   Bai, Song
   Yao, Cong
   Bai, Xiang
TI Deep Learning Representation using Autoencoder for 3D Shape Retrieval
SO NEUROCOMPUTING
LA English
DT Article; Proceedings Paper
CT IEEE International Conference on Security, Pattern Analysis, and
   Cybernetics (SPAC)
CY OCT 18-19, 2014
CL Huazhong Univ Sci & Technol, Wuhan, PEOPLES R CHINA
SP IEEE, IEEE Syst Man & Cybernet Soc, Univ Macau, Wuhan Assoc Sci & Technol
HO Huazhong Univ Sci & Technol
DE 3D Shape Matching; 3D Shape Retrieval; Autoencoder; Shape Representation
AB We study the problem of how to build a deep learning representation for 3D shape. Deep learning has shown to be very effective in variety of visual applications, such as image classification and object detection. However, it has not been successfully applied to 3D shape recognition. This is because 3D shape has complex structure in 3D space and there are limited number of 3D shapes for feature learning. To address these problems, we project 3D shapes into 2D space and use autoencoder for feature learning on the 2D images. High accuracy 3D shape retrieval performance is obtained by aggregating the features learned on 2D images. In addition, we show the proposed deep learning feature is complementary to conventional local image descriptors. By combing the global deep learning representation and the local descriptor representation, our method can obtain the state-of-the-art performance on 3D shape retrieval benchmarks. (C) 2016 Published by Elsevier B.V.
C1 [Zhu, Zhuotun; Wang, Xinggang; Bai, Song; Yao, Cong; Bai, Xiang] Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, 1037 Luoyu Rd, Wuhan 430074, Hubei Province, Peoples R China.
RP Wang, XG (reprint author), Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, 1037 Luoyu Rd, Wuhan 430074, Hubei Province, Peoples R China.
EM zhuotun@gmail.com; xgwang@hust.edu.cn; songbai@hust.edu.cn;
   yaocong2010@gmail.com; xbai@hust.edu.cn
CR Bai X, 2014, IEEE T IMAGE PROCESS, V23, P3935, DOI 10.1109/TIP.2014.2336542
   Bai X, 2010, IEEE T PATTERN ANAL, V32, P861, DOI 10.1109/TPAMI.2009.85
   Bengio Y., 2006, NIPS
   Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669
   Cyr CM, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P254, DOI 10.1109/ICCV.2001.937526
   Fang Y, 2015, PROC CVPR IEEE, P2319, DOI 10.1109/CVPR.2015.7298845
   Fei-Fei L, 2005, PROC CVPR IEEE, P524
   Girshick R., 2013, ARXIV13112524
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554
   Jayanti S, 2006, COMPUT AIDED DESIGN, V38, P939, DOI 10.1016/j.cad.2006.06.007
   Kazhdan M., 2003, Symposium on Geometry Processing, P156
   KHOTANZAD A, 1990, IEEE T PATTERN ANAL, V12, P489, DOI 10.1109/34.55109
   Krizhevsky A., 2011, ESANN
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, V1, P4
   Lazebnik Svetlana, 2006, IEEE C COMP VIS PATT, V2
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Lian ZH, 2010, INT J COMPUT VISION, V89, P130, DOI 10.1007/s11263-009-0295-0
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Ma Shugao, 2014, EUR C COMP VIS
   Ohbuchi R, 2008, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2008, PROCEEDINGS, P93, DOI 10.1109/SMI.2008.4547955
   Osada R, 2002, ACM T GRAPHIC, V21, P807, DOI 10.1145/571647.571648
   Papadakis P, 2010, INT J COMPUT VISION, V89, P177, DOI 10.1007/s11263-009-0281-6
   Papadakis Panagiotis, 3D OBJECT RETRIEVAL, V8, P9
   Shilane P, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P167, DOI 10.1109/SMI.2004.1314504
   Vranic DV, 2005, 2005 IEEE International Conference on Multimedia and Expo (ICME), Vols 1 and 2, P963
   Zeng MY, 2015, IEEE COMPUT SOC CONF
   Zhang DS, 2002, SIGNAL PROCESS-IMAGE, V17, P825, DOI 10.1016/S0923-5965(02)00084-X
   Zhang Z., 2010, P AS C COMP VIS
   Zhouhui Lian, 2010, Proceedings of the Shape Modeling International (SMI 2010), P25, DOI 10.1109/SMI.2010.20
   Zhu ZT, 2014, 2014 INTERNATIONAL CONFERENCE ON SECURITY, PATTERN ANALYSIS, AND CYBERNETICS (SPAC), P279, DOI 10.1109/SPAC.2014.6982699
NR 32
TC 37
Z9 43
U1 2
U2 38
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0925-2312
EI 1872-8286
J9 NEUROCOMPUTING
JI Neurocomputing
PD SEP 5
PY 2016
VL 204
SI SI
BP 41
EP 50
DI 10.1016/j.neucom.2015.08.127
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA DQ1HR
UT WOS:000378952600007
DA 2020-02-19
ER

PT J
AU Ergun, H
   Akyuz, YC
   Sert, M
   Liu, JQ
AF Ergun, Hilal
   Akyuz, Yusuf Caglar
   Sert, Mustafa
   Liu, Jianquan
TI Early and Late Level Fusion of Deep Convolutional Neural Networks for
   Visual Concept Recognition
SO INTERNATIONAL JOURNAL OF SEMANTIC COMPUTING
LA English
DT Article
DE Deep learning; convolutional neural networks; image classification;
   visual concept recognition; fusion
ID IMAGE; CLASSIFICATION; FEATURES; SETS
AB Visual concept recognition is an active research field in the last decade. Related to this attention, deep learning architectures are showing great promise in various computer vision domains including image classification, object detection, event detection and action recognition in videos. In this study, we investigate various aspects of convolutional neural networks for visual concept recognition. We analyze recent studies and different network architectures both in terms of running time and accuracy. In our proposed visual concept recognition system, we first discuss various important properties of popular convolutional network architecture under consideration. Then we describe our method for feature extraction at different levels of abstraction. We present extensive empirical information along with best practices for big data practitioners. Using these best practices we propose efficient fusion mechanisms both for single and multiple network models. We present state-of-the-art results on benchmark datasets while keeping computational costs at low level. Our results show that these state-of-the-art results can be reached without using extensive data augmentation techniques.
C1 [Ergun, Hilal; Sert, Mustafa] Baskent Univ, Dept Comp Engn, Ankara, Turkey.
   [Akyuz, Yusuf Caglar] Bilkon Ltd, Ankara, Turkey.
   [Liu, Jianquan] NEC Corp Ltd, Syst Platform Res Labs, Minato, Tokyo, Japan.
RP Sert, M (reprint author), Baskent Univ, Dept Comp Engn, Ankara, Turkey.
EM 21020005@mail.baskent.edu.tr; caglar@bilkon-kontrol.com.tr;
   msert@baskent.edu.tr; j-liu@ct.jp.nec.com
RI SERT, Mustafa/AAB-8673-2019; Sert, Mustafa/D-3080-2015
OI SERT, Mustafa/0000-0002-7056-4245; Sert, Mustafa/0000-0002-7056-4245
CR Arora S., 2013, ARXIV13106343
   Avila S, 2013, COMPUT VIS IMAGE UND, V117, P453, DOI 10.1016/j.cviu.2012.09.007
   Babenko A, 2015, IEEE I CONF COMP VIS, P1269, DOI 10.1109/ICCV.2015.150
   Babenko A, 2014, LECT NOTES COMPUT SC, V8689, P584, DOI 10.1007/978-3-319-10590-1_38
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1016/j.cviu.2007.09.014
   Boureau Y., 2010, P 27 INT C MACH LEAR, P111, DOI DOI 10.1016/J.NEUNET.2012.02.023
   Boureau YL, 2011, IEEE I CONF COMP VIS, P2651, DOI 10.1109/ICCV.2011.6126555
   Boureau YL, 2010, PROC CVPR IEEE, P2559, DOI 10.1109/CVPR.2010.5539963
   Chatfield K., 2014, ARXIV14053531
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76
   Cimpoi M, 2015, PROC CVPR IEEE, P3828, DOI 10.1109/CVPR.2015.7299007
   Csurka G., 2004, WORKSH STAT LEARN CO, V1, P1, DOI DOI 10.1234/12345678
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Donahue J., 2013, ARXIV13101531
   Ergun H, 2016, ADV INTELL SYST, V400, P389, DOI 10.1007/978-3-319-26154-6_30
   Everingham M., PASCAL VISUAL OBJECT
   Grauman K, 2005, IEEE I CONF COMP VIS, P1458
   Griffin G., 2007, CALTECH 256 OBJECT C
   Guo J, 2015, ABS150607224 CORR
   Haghighat M, 2016, EXPERT SYST APPL, V47, P23, DOI 10.1016/j.eswa.2015.10.047
   Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814
   He K., 2015, ARXIV151203385
   Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.2307/2333955
   Jegou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   Lazebnik S., 2006, P IEEE INT C COMP VI, V2, P2169, DOI DOI 10.1109/CVPR.2006.68
   Li FF, 2007, COMPUT VIS IMAGE UND, V106, P59, DOI 10.1016/j.cviu.2005.09.012
   Lin M., 2013, ABS13124400 CORR
   Liu LQ, 2015, PROC CVPR IEEE, P4749, DOI 10.1109/CVPR.2015.7299107
   Lowe D., 1999, P INT C COMP VIS, V2, P1150, DOI [10.1109/ICCV.1999.790410, DOI 10.1109/ICCV.1999.790410]
   PERRONNIN F, 2007, IEEE C COMP VIS PATT, V1, P1
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Russell BC, 2008, INT J COMPUT VISION, V77, P157, DOI 10.1007/s11263-007-0090-8
   Sermanet P., 2013, ARXIV13126229
   Sert M, 2014, SIG PROCESS COMMUN, P1946, DOI 10.1109/SIU.2014.6830637
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Simonyan K., 2014, ADV NEURAL INFORM PR, P568, DOI DOI 10.1109/ICCVW.2017.368
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470
   Snoek C. G. M., 2005, 13th Annual ACM International Conference on Multimedia, P399
   Sun QS, 2005, PATTERN RECOGN, V38, P2437, DOI 10.1016/j.patcog.2004.12.013
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   van Gemert JC, 2010, IEEE T PATTERN ANAL, V32, P1271, DOI 10.1109/TPAMI.2009.132
   Wang JJ, 2010, PROC CVPR IEEE, P3360, DOI 10.1109/CVPR.2010.5540018
   Wang L, 2015, ARXIV150702159
   Welinder P., 2010, CNSTR2010001 CALTECH
   Xu ZW, 2015, PROC CVPR IEEE, P1798, DOI 10.1109/CVPR.2015.7298789
   Yang JC, 2009, PROC CVPR IEEE, P1794, DOI 10.1109/CVPRW.2009.5206757
   Yang Y, 2013, IEEE T MULTIMEDIA, V15, P572, DOI 10.1109/TMM.2012.2234731
   Ye Hao, 2015, ARXIV150401920
   Zeiler Matthew D., 2013, ABS13112901 CORR
   Zha S., 2015, ARXIV150304144
   Zhang J, 2007, INT J COMPUT VISION, V73, P213, DOI 10.1007/s11263-006-9794-4
   Zhou B, 2014, ADV NEURAL INFORM PR, P487, DOI DOI 10.1162/153244303322533223
NR 54
TC 7
Z9 7
U1 1
U2 5
PU WORLD SCIENTIFIC PUBL CO PTE LTD
PI SINGAPORE
PA 5 TOH TUCK LINK, SINGAPORE 596224, SINGAPORE
SN 1793-351X
EI 1793-7108
J9 INT J SEMANT COMPUT
JI Int. J. Semant. Comput.
PD SEP
PY 2016
VL 10
IS 3
SI SI
BP 379
EP 397
DI 10.1142/S1793351X16400158
PG 19
WC Computer Science, Artificial Intelligence
SC Computer Science
GA EE5NW
UT WOS:000389655900006
DA 2020-02-19
ER

PT J
AU Nguyen, A
   Yosinski, J
   Clune, J
AF Nguyen, A.
   Yosinski, J.
   Clune, J.
TI Understanding Innovation Engines: Automated Creativity and Improved
   Stochastic Optimization via Deep Learning
SO EVOLUTIONARY COMPUTATION
LA English
DT Article
DE Genetic algorithms; deep neural networks; CPPNs; MAP-Elites
ID MODULARITY; NETWORKS; DESIGN
AB The Achilles Heel of stochastic optimization algorithms is getting trapped on local optima. Novelty Search mitigates this problem by encouraging exploration in all interesting directions by replacing the performance objective with a reward for novel behaviors. This reward for novel behaviors has traditionally required a human-crafted, behavioral distance function. While Novelty Search is a major conceptual breakthrough and outperforms traditional stochastic optimization on certain problems, it is not clear how to apply it to challenging, high-dimensional problems where specifying a useful behavioral distance function is difficult. For example, in the space of images, how do you encourage novelty to produce hawks and heroes instead of endless pixel static? Here we propose a new algorithm, the Innovation Engine, that builds on Novelty Search by replacing the human-crafted behavioral distance with a Deep Neural Network (DNN) that can recognize interesting differences between phenotypes. The key insight is that DNNs can recognize similarities and differences between phenotypes at an abstract level, wherein novelty means interesting novelty. For example, a DNN-based novelty search in the image space does not explore in the low-level pixel space, but instead creates a pressure to create new types of images (e.g., churches, mosques, obelisks, etc.). Here, we describe the long-term vision for the Innovation Engine algorithm, which involves many technical challenges that remain to be solved. We then implement a simplified version of the algorithm that enables us to explore some of the algorithm's key motivations. Our initial results, in the domain of images, suggest that Innovation Engines could ultimately automate the production of endless streams of interesting solutions in any domain: for example, producing intelligent software, robot controllers, optimized physical components, and art.
C1 [Nguyen, A.; Clune, J.] Univ Wyoming, Laramie, WY 82071 USA.
   [Yosinski, J.] Cornell Univ, Ithaca, NY 14853 USA.
   [Yosinski, J.] Geometr Intelligence, New York, NY USA.
RP Nguyen, A (reprint author), Univ Wyoming, Laramie, WY 82071 USA.
EM anguyen8@uwyo.edu; jason@geometricintelligence.com; jeffclune@uwyo.edu
FU NSF CAREERNational Science Foundation (NSF)NSF - Office of the Director
   (OD) [CAREER: 1453549]; NASANational Aeronautics & Space Administration
   (NASA); NSFNational Science Foundation (NSF) [1527232]
FX We thank Joost Huizinga, Christopher Stanton, Henok Mengistu, and
   Jean-Baptiste Mouret for useful conversations. Jeff Clune was supported
   by an NSF CAREER award (CAREER: 1453549) and a hardware donation from
   the NVIDIA Corporation, and Jason Yosinski by the NASA Space Technology
   Research Fellowship and NSF grant 1527232.
CR Nguyen A, 2015, PROC CVPR IEEE, P427, DOI 10.1109/CVPR.2015.7298640
   Auerbach J. E., 2012, ARTIF LIFE, P629
   Belew R. K., 1995, FDN GENETIC ALGORITH
   Bengio Y., 2014, P 31 INT C MACH LEAR, P226
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Bruna J., 2014, INT C LEARN REPR, P1
   Cheney N, 2013, GECCO'13: PROCEEDINGS OF THE 2013 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P167
   Clune J, 2011, ACM SIGEVOLUTION, V5, P2
   Clune J, 2013, P ROY SOC B-BIOL SCI, V280, DOI 10.1098/rspb.2012.2863
   Clune J, 2011, IEEE T EVOLUT COMPUT, V15, P346, DOI 10.1109/TEVC.2010.2104157
   Clune J, 2009, IEEE C EVOL COMPUTAT, P2764, DOI 10.1109/CEC.2009.4983289
   Cuccu G., 2011, NOVELTY IS NOT ENOUG, P234
   Cully A, 2015, NATURE, V521, P503, DOI 10.1038/nature14422
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Donahue J., 2015, P 2015 IEEE C COMP V, P427
   FLOREANO D, 2008, BIOINSPIRED ARTIFICI
   Floreano D, 2010, PLOS BIOL, V8, DOI 10.1371/journal.pbio.1000292
   Fogel DB, 1996, IEEE C EVOL COMPUTAT, P11, DOI 10.1109/ICEC.1996.542328
   Getreuer P, 2012, IMAGE PROCESS ON LIN, V2, P74, DOI 10.5201/ipol.2012.g-tvd
   Grefenstette J. J., 2014, FDN GENETIC ALGORITH, V3, P139
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hornby GS, 2003, IEEE T ROBOTIC AUTOM, V19, P703, DOI 10.1109/TRA.2003.814502
   Igel C, 1999, FROM ANIM ANIMAT, P191
   Igel C., 1999, P 1 IEEE C EV COMP, P1909
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Karpathy A, 2014, WHAT I LEARNED COMPE
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kashtan N, 2005, P NATL ACAD SCI USA, V102, P13773, DOI 10.1073/pnas.0503610102
   Kashtan N, 2007, P NATL ACAD SCI USA, V104, P13711, DOI 10.1073/pnas.0611630104
   Kompella V. R., 2015, CONTINUAL CURIOSITY
   Koza J. R., 2005, GENETIC PROGRAMMING, V6, P231
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y., 1998, MNIST DATABASE HANDW
   Lee S., 2013, APPL EVOLUTIONARY CO, P540, DOI DOI 10.1007/978-3-642-37192-954
   Lehman J, 2011, GENET EVOL COMPUT, P37
   Lehman J, 2011, EVOL COMPUT, V19, P189, DOI 10.1162/EVCO_a_00025
   Lehman Joel, 2008, ALIFE, P329
   Leicht EA, 2008, PHYS REV LETT, V100, DOI 10.1103/PhysRevLett.100.118703
   Li J, 2014, GECCO'14: PROCEEDINGS OF THE 2014 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P193, DOI 10.1145/2576768.2598222
   Liapis A., 2013, P 4 INT C COMP CREAT, P56
   Mouret J. -B., 2010, EV COMP CEC 2010 IEE, P1
   Mouret J. -B., 2015, ILLUMINATING SEARCH
   Mouret JB, 2011, STUD COMPUT INTELL, V341, P139
   Newman MEJ, 2006, P NATL ACAD SCI USA, V103, P8577, DOI 10.1073/pnas.0601602103
   Nguyen A., 2016, VIS WORKSH INT C MAC
   Nguyen  A., 2016, SYNTHESIZING PREFERR
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Schmidhuber J, 2006, CONNECT SCI, V18, P173, DOI 10.1080/09540090600768658
   Secretan J, 2011, EVOL COMPUT, V19, P373, DOI 10.1162/EVCO_a_00030
   Simonyan K., 2014, ADV NEURAL INFORM PR, P568, DOI DOI 10.1109/ICCVW.2017.368
   Stallkamp J, 2012, NEURAL NETWORKS, V32, P323, DOI 10.1016/j.neunet.2012.02.016
   Stanley K., 2016, EMERGENCE CANA UNPUB
   Stanley K. O., 2013, CSTR1305 U CENTR FLO
   Stanley K. O., 2015, WHY GREATNESS CANNOT
   Stanley KO, 2007, GENET PROGRAM EVOL M, V8, P131, DOI 10.1007/s10710-007-9028-8
   Stanley KO, 2002, EVOL COMPUT, V10, P99, DOI 10.1162/106365602320169811
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Woolley BG, 2011, GECCO-2011: PROCEEDINGS OF THE 13TH ANNUAL GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P957
   Yosinski J., 2015, DEEP LEARN WORKSH IN
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
NR 62
TC 10
Z9 10
U1 1
U2 9
PU MIT PRESS
PI CAMBRIDGE
PA ONE ROGERS ST, CAMBRIDGE, MA 02142-1209 USA
SN 1063-6560
EI 1530-9304
J9 EVOL COMPUT
JI Evol. Comput.
PD FAL
PY 2016
VL 24
IS 3
BP 545
EP 572
DI 10.1162/EVCO_a_00189
PG 28
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods
SC Computer Science
GA EC9FQ
UT WOS:000388450300007
PM 27367139
DA 2020-02-19
ER

PT J
AU Zhao, L
   Wang, ZC
   Wang, XJ
   Qi, YZ
   Liu, Q
   Zhang, GX
AF Zhao, Lei
   Wang, Zengcai
   Wang, Xiaojin
   Qi, Yazhou
   Liu, Qing
   Zhang, Guoxin
TI Human fatigue expression recognition through image-based dynamic
   multi-information and bimodal deep learning
SO JOURNAL OF ELECTRONIC IMAGING
LA English
DT Article
DE fatigue expression recognition; texture; landmark; bimodal learning;
   dynamic multi-information
ID DRIVERS FATIGUE; CLASSIFICATION; FEATURES; SYSTEM
AB Human fatigue is an important cause of traffic accidents. To improve the safety of transportation, we propose, in this paper, a framework for fatigue expression recognition using image-based facial dynamic multi-information and a bimodal deep neural network. First, the landmark of face region and the texture of eye region, which complement each other in fatigue expression recognition, are extracted from facial image sequences captured by a single camera. Then, two stacked autoencoder neural networks are trained for landmark and texture, respectively. Finally, the two trained neural networks are combined by learning a joint layer on top of them to construct a bimodal deep neural network. The model can be used to extract a unified representation that fuses landmark and texture modalities together and classify fatigue expressions accurately. The proposed system is tested on a human fatigue dataset obtained from an actual driving environment. The experimental results demonstrate that the proposed method performs stably and robustly, and that the average accuracy achieves 96.2%. (C) 2016 SPIE and IS&T
C1 [Zhao, Lei; Wang, Zengcai; Wang, Xiaojin; Qi, Yazhou; Liu, Qing; Zhang, Guoxin] Shandong Univ, Sch Mech Engn, Vehicle Engn Res Inst, 17923 Jingshi Rd, Jinan 250061, Peoples R China.
   [Wang, Zengcai] Shandong Univ, Sch Mech Engn, Minist Educ, Key Lab High Efficiency & Clean Mech Manufacture, 17923 Jingshi Rd, Jinan 250061, Peoples R China.
RP Wang, ZC (reprint author), Shandong Univ, Sch Mech Engn, Vehicle Engn Res Inst, 17923 Jingshi Rd, Jinan 250061, Peoples R China.; Wang, ZC (reprint author), Shandong Univ, Sch Mech Engn, Minist Educ, Key Lab High Efficiency & Clean Mech Manufacture, 17923 Jingshi Rd, Jinan 250061, Peoples R China.
EM wangzc@sdu.edu.cn
OI Wang, Zengcai/0000-0003-1733-4114
FU Open Foundation of State Key Laboratory of Automotive Simulation and
   Control (China) [20121107]
FX This work was supported by the Open Foundation of State Key Laboratory
   of Automotive Simulation and Control (China, Grant No. 20121107). All
   participants involved in the experiment were volunteers who participated
   and understood the content and purpose of the study. The subjects shown
   in Figs. 2, 5, and 6 agreed to and authorized the publication of their
   facial images in this journal.
CR Asthana A, 2014, PROC CVPR IEEE, P1859, DOI 10.1109/CVPR.2014.240
   Barr L, 2005, REV EVALUATION EMERG
   Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153
   Bergasa LM, 2006, IEEE T INTELL TRANSP, V7, P63, DOI 10.1109/TITS.2006.869598
   Chieh TC, 2003, IEEE ST CONF RES DEV, P45
   Ciodaro T, 2012, J PHYS CONF SER, V368, DOI 10.1088/1742-6596/368/1/012030
   Cyganek B, 2014, NEUROCOMPUTING, V126, P78, DOI 10.1016/j.neucom.2013.01.048
   Dinges DF, 1998, PERCLOS VALID PSYCHO
   Dong WH, 2005, Proceedings of 2005 IEEE International Workshop on VLSI Design and Video Technology, P365
   Fan X, 2008, IEEE INT C NETW SENS, P905
   Fan X, 2010, PATTERN RECOGN LETT, V31, P234, DOI 10.1016/j.patrec.2009.08.014
   Fan XJ, 2015, PATTERN RECOGN, V48, P3407, DOI 10.1016/j.patcog.2015.04.025
   Fasel B, 2003, PATTERN RECOGN, V36, P259, DOI 10.1016/S0031-3203(02)00052-3
   Gu HS, 2004, SIXTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P111
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Ibrahim MM, 2014, BIOMED SIGNAL PROCES, V11, P53, DOI 10.1016/j.bspc.2014.02.007
   Jiang B, 2016, J ELECTRON IMAGING, V25, DOI 10.1117/1.JEI.25.1.013022
   Jung H, 2015, IEEE I CONF COMP VIS, P2983, DOI 10.1109/ICCV.2015.341
   Kim KW, 2015, OPT ENG, V54, DOI 10.1117/1.OE.54.3.033103
   Krizhevsky A., 2009, TECHNICAL REPORT
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lee SJ, 2011, IEEE T INTELL TRANSP, V12, P254, DOI 10.1109/TITS.2010.2091503
   Li YQ, 2015, PATTERN RECOGN, V48, P3417, DOI 10.1016/j.patcog.2015.04.022
   Liu CJ, 2002, IEEE T IMAGE PROCESS, V11, P467, DOI 10.1109/TIP.2002.999679
   Lu Yufeng, 2007, 2007 1st International Conference on Bioinformatics and Biomedical Engineering, P581, DOI 10.1109/ICBBE.2007.152
   Ma JS, 2015, J CHEM INF MODEL, V55, P263, DOI 10.1021/ci500747n
   Mbouna RO, 2013, IEEE T INTELL TRANSP, V14, P1462, DOI 10.1109/TITS.2013.2262098
   Ngiam J, 2011, IEEE INT C MACH LEAR, V28, P689, DOI DOI 10.1145/2647868
   Patil RA, 2012, J ELECTRON IMAGING, V21, DOI 10.1117/1.JEI.21.4.043003
   Rau PS, 2005, P 19 INT C ENH SAF V
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Sadeghi H, 2015, J ELECTRON IMAGING, V24, DOI 10.1117/1.JEI.24.1.013005
   Sahayadhas A, 2012, SENSORS-BASEL, V12, P16937, DOI 10.3390/s121216937
   Song FY, 2014, PATTERN RECOGN, V47, P2825, DOI 10.1016/j.patcog.2014.03.024
   Srivastava N., 2012, ADV NEURAL INFORM PR, P2222, DOI DOI 10.1109/CVPR.2013.49
   Srivastava Nitish, 2012, INT C MACH LEARN WOR
   Sutskever I., 2011, P 28 INT C MACH LEAR, P1017
   Takei Y, 2005, IEEE SYS MAN CYBERN, P1765
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Wang MS, 2016, INT J AUTO TECH-KOR, V17, P165, DOI 10.1007/s12239-016-0016-y
   Wang RB, 2004, ITSC 2004: 7TH INTERNATIONAL IEEE CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS, PROCEEDINGS, P314, DOI 10.1109/ITSC.2004.1398917
   Wang RB, 2003, IEEE IV2003: INTELLIGENT VEHICLES SYMPOSIUM, PROCEEDINGS, P110, DOI 10.1109/IVS.2003.1212893
   Wang TS, 2005, Proceedings of 2005 IEEE International Workshop on VLSI Design and Video Technology, P373, DOI 10.1109/IWVDVT.2005.1504628
   Zhang W, 2015, PATTERN RECOGN, V48, P3191, DOI 10.1016/j.patcog.2015.04.012
   Zhang Y, 2015, OPTIK, V126, P4501, DOI 10.1016/j.ijleo.2015.08.185
   Zhao CH, 2014, J INTELL FUZZY SYST, V26, P91, DOI 10.3233/IFS-120717
   Zhao CH, 2013, IET INTELL TRANSP SY, V7, P36, DOI 10.1049/iet-its.2012.0005
NR 49
TC 5
Z9 5
U1 2
U2 26
PU IS&T & SPIE
PI BELLINGHAM
PA 1000 20TH ST, BELLINGHAM, WA 98225 USA
SN 1017-9909
EI 1560-229X
J9 J ELECTRON IMAGING
JI J. Electron. Imaging
PD SEP
PY 2016
VL 25
IS 5
AR 053024
DI 10.1117/1.JEI.25.5.053024
PG 11
WC Engineering, Electrical & Electronic; Optics; Imaging Science &
   Photographic Technology
SC Engineering; Optics; Imaging Science & Photographic Technology
GA EC5ZX
UT WOS:000388216900035
DA 2020-02-19
ER

PT J
AU Ma, XR
   Wang, HY
   Geng, J
AF Ma, Xiaorui
   Wang, Hongyu
   Geng, Jie
TI Spectral-Spatial Classification of Hyperspectral Image Based on Deep
   Auto-Encoder
SO IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE
   SENSING
LA English
DT Article
DE Deep auto-encoders (DAE); feature learning; hyperspectral image (HSI);
   supervised classification
ID ATTRIBUTE PROFILES; FRAMEWORK; KERNEL; SVM; REPRESENTATION; ALGORITHMS;
   SELECTION
AB Deep learning, which represents data by a hierarchical network, has proven to be efficient in computer vision. To investigate the effect of deep features in hyperspectral image (HSI) classification, this paper focuses on how to extract and utilize deep features in HSI classification framework. First, in order to extract spectral-spatial information, an improved deep network, spatial updated deep auto-encoder (SDAE), is proposed. SDAE, which is an improved deep auto-encoder (DAE), considers sample similarity by adding a regularization term in the energy function, and updates features by integrating contextual information. Second, in order to deal with the small training set using deep features, a collaborative representation-based classification is applied. Moreover, in order to suppress salt-and-pepper noise and smooth the result, we compute the residual of collaborative representation of all samples as a residual matrix, which can be effectively used in a graph-cut-based spatial regularization. The proposed method inherits the advantages of deep learning and has solutions to add spatial information of HSI in the learning network. Using collaborative representation-based classification with deep features makes the proposed classifier extremely robust under a small training set. Extensive experiments demonstrate that the proposed method provides encouraging results compared with some related techniques.
C1 [Ma, Xiaorui; Wang, Hongyu; Geng, Jie] Dalian Univ Technol, Sch Informat & Commun Engn, Dalian 116024, Peoples R China.
RP Wang, HY (reprint author), Dalian Univ Technol, Sch Informat & Commun Engn, Dalian 116024, Peoples R China.
EM xrui.ma@gmail.com; whyu@dlut.edu.cn; gengjie@mail.dlut.edu.cn
OI Geng, Jie/0000-0003-4858-823X
CR Arenas-Garcia J, 2013, IEEE SIGNAL PROC MAG, V30, P16, DOI 10.1109/MSP.2013.2250591
   Benediktsson JA, 2013, P IEEE, V101, P566, DOI 10.1109/JPROC.2012.2237076
   Bengio S., 2013, CORR
   Bengio Y., DEEP LEARNI IN PRESS
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bianchini M., 2013, HDB NEURAL INFORM PR
   Bioucas-Dias JM, 2013, IEEE GEOSC REM SEN M, V1, P6, DOI 10.1109/MGRS.2013.2244672
   Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60
   Bruzzone L, 2009, IEEE T GEOSCI REMOTE, V47, P3180, DOI 10.1109/TGRS.2009.2019636
   Camps-Valls G, 2006, IEEE GEOSCI REMOTE S, V3, P93, DOI 10.1109/LGRS.2005.857031
   Camps-Valls G, 2014, IEEE SIGNAL PROC MAG, V31, P45, DOI 10.1109/MSP.2013.2279179
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chen Y, 2013, IEEE T GEOSCI REMOTE, V51, P217, DOI 10.1109/TGRS.2012.2201730
   Chen YS, 2014, IEEE J-STARS, V7, P2094, DOI 10.1109/JSTARS.2014.2329330
   Dalla Mura M, 2010, IEEE T GEOSCI REMOTE, V48, P3747, DOI 10.1109/TGRS.2010.2048116
   Dean J., 2012, ADV NEURAL INFORM PR, V25, P1223
   Deng L., 2014, MSRTR201421
   Deng L, 2013, INT CONF ACOUST SPEE, P8604, DOI 10.1109/ICASSP.2013.6639345
   DEVILLIERS J, 1993, IEEE T NEURAL NETWOR, V4, P136, DOI 10.1109/72.182704
   Farag AA, 2005, IEEE T GEOSCI REMOTE, V43, P1617, DOI 10.1109/TGRS.2005.849059
   Fauvel M., 2007, THESIS
   Ghamisi P, 2015, IEEE T GEOSCI REMOTE, V53, P2335, DOI 10.1109/TGRS.2014.2358934
   Ghamisi P, 2014, IEEE T GEOSCI REMOTE, V52, P5771, DOI 10.1109/TGRS.2013.2292544
   Gu YF, 2013, IEEE J-STARS, V6, P1109, DOI 10.1109/JSTARS.2013.2243112
   Gu YF, 2012, IEEE T GEOSCI REMOTE, V50, P2852, DOI 10.1109/TGRS.2011.2176341
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hossain MA, 2011, INT GEOSCI REMOTE SE, P1720, DOI 10.1109/IGARSS.2011.6049567
   Huang X, 2009, INT J REMOTE SENS, V30, P3205, DOI 10.1080/01431160802559046
   HUGHES GF, 1968, IEEE T INFORM THEORY, V14, P55, DOI 10.1109/TIT.1968.1054102
   Jia XP, 2013, P IEEE, V101, P676, DOI 10.1109/JPROC.2012.2229082
   Krishnapuram B, 2005, IEEE T PATTERN ANAL, V27, P957, DOI 10.1109/TPAMI.2005.127
   Kuo BC, 2014, IEEE J-STARS, V7, P317, DOI 10.1109/JSTARS.2013.2262926
   Li J, 2013, IEEE T GEOSCI REMOTE, V51, P4816, DOI 10.1109/TGRS.2012.2230268
   Li J, 2013, IEEE T GEOSCI REMOTE, V51, P844, DOI 10.1109/TGRS.2012.2205263
   Li J, 2011, IEEE T GEOSCI REMOTE, V49, P3947, DOI 10.1109/TGRS.2011.2128330
   Li W, 2015, IEEE GEOSCI REMOTE S, V12, P48, DOI 10.1109/LGRS.2014.2325978
   Li W, 2015, IEEE GEOSCI REMOTE S, V12, P389, DOI 10.1109/LGRS.2014.2343956
   Li W, 2014, IEEE T GEOSCI REMOTE, V52, P477, DOI 10.1109/TGRS.2013.2241773
   Liangpei Zhang, 2013, IEEE Transactions on Geoscience and Remote Sensing, V51, P242, DOI 10.1109/TGRS.2012.2197860
   Melgani F, 2004, IEEE T GEOSCI REMOTE, V42, P1778, DOI 10.1109/TGRS.2004.831865
   Moser G, 2013, IEEE T GEOSCI REMOTE, V51, P2734, DOI 10.1109/TGRS.2012.2211882
   Ouyang WL, 2013, IEEE I CONF COMP VIS, P2056, DOI 10.1109/ICCV.2013.257
   Quesada-Barriuso P, 2014, IEEE J-STARS, V7, P1177, DOI 10.1109/JSTARS.2014.2308425
   Richards J. A., 2006, REMOTE SENSING DIGIT
   Romero A., 2014, P WORKSH HYP IM SIGN, P4951
   Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244
   Tan K, 2014, IEEE J-STARS, V7, P40, DOI 10.1109/JSTARS.2013.2265697
   Tarabalka Y, 2010, THESIS
   Tarabalka Y, 2010, IEEE GEOSCI REMOTE S, V7, P736, DOI 10.1109/LGRS.2010.2047711
   TONG X, 2013, IEEE J-STARS, V7, P3998
   Willett RM, 2014, IEEE SIGNAL PROC MAG, V31, P116, DOI 10.1109/MSP.2013.2279507
   Zhang LF, 2015, PATTERN RECOGN, V48, P3102, DOI 10.1016/j.patcog.2014.12.016
   Zhang LF, 2012, IEEE T GEOSCI REMOTE, V50, P879, DOI 10.1109/TGRS.2011.2162339
   Zhang L, 2011, IEEE I CONF COMP VIS, P471, DOI 10.1109/ICCV.2011.6126277
   Zhang QS, 2013, PROCEEDINGS OF 2013 IEEE INTERNATIONAL CONFERENCE ON GREY SYSTEMS AND INTELLIGENT SERVICES (GSIS), P16, DOI 10.1109/GSIS.2013.6714730
   Zhou Y., 2014, IEEE J-STARS, V8, P2351
NR 56
TC 81
Z9 87
U1 16
U2 89
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1939-1404
EI 2151-1535
J9 IEEE J-STARS
JI IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens.
PD SEP
PY 2016
VL 9
IS 9
BP 4073
EP 4085
DI 10.1109/JSTARS.2016.2517204
PG 13
WC Engineering, Electrical & Electronic; Geography, Physical; Remote
   Sensing; Imaging Science & Photographic Technology
SC Engineering; Physical Geography; Remote Sensing; Imaging Science &
   Photographic Technology
GA DY6NW
UT WOS:000385245000008
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Wang, L
   Zhang, BC
   Han, JG
   Shen, LL
   Qian, CS
AF Wang, Lei
   Zhang, Baochang
   Han, Jungong
   Shen, Linlin
   Qian, Cheng-shan
TI Robust object representation by boosting-like deep learning architecture
SO SIGNAL PROCESSING-IMAGE COMMUNICATION
LA English
DT Article
DE Boosting; Deep learning; Object representation; Synchronized feature
ID HUMAN ACTION RECOGNITION; MOTION
AB This paper presents a new deep learning architecture for robust object representation, aiming at efficiently combining the proposed synchronized multi-stage feature (SMF) and a boosting-like algorithm. The SMF structure can capture a variety of characteristics from the inputting object based on the fusion of the handcraft features and deep learned features. With the proposed boosting-like algorithm, we can obtain more convergence stability on training multi-layer network by using the boosted samples. We show the generalization of our object representation architecture by applying it to undertake various tasks, i.e. pedestrian detection and action recognition. Our approach achieves 15.89% and 3.85% reduction in the average miss rate compared with ACF and JointDeep on the largest Caltech dataset, and acquires competitive results on the MSRAction3D dataset. (C) 2016 Elsevier B.V. All rights reserved.
C1 [Wang, Lei; Zhang, Baochang] Beihang Univ, Sch Automat Sci & Elect Engn, Beijing, Peoples R China.
   [Han, Jungong] Northumbria Univ, Comp Sci & Digital Technol Dept, Newcastle, England.
   [Shen, Linlin] Shenzhen Univ, Comp Vis Inst, Sch Comp Sci & Software Engn, Shenzhen, Peoples R China.
   [Qian, Cheng-shan] Nanjing Univ Informat Sci & Technol, Sch Comp & Software, Nanjing, Jiangsu, Peoples R China.
RP Qian, CS (reprint author), Nanjing Univ Informat Sci & Technol, Sch Comp & Software, Nanjing, Jiangsu, Peoples R China.
EM bczhang@buaa.edu.cn; qianchengshan@163.com
FU Natural Science Foundation of ChinaNational Natural Science Foundation
   of China [61272052, 61272050, 61473086]; Program for New Century
   Excellent Talents University of Ministry of Education of ChinaProgram
   for New Century Excellent Talents in University (NCET); PAPD; CICAEET
   fund; Beijing Municipal Science & Technology Commission Cheng-shan
   QianBeijing Municipal Science & Technology Commission [Z161100001616005]
FX Baochang Zhang thanks for the support by Natural Science Foundation of
   China, under Contracts 61272052, 61272050 and 61473086, and by the
   Program for New Century Excellent Talents University of Ministry of
   Education of China. Thanks for the support of the PAPD and CICAEET fund,
   and Beijing Municipal Science & Technology Commission Z161100001616005
   Cheng-shan Qian is the corresponding author.
CR Bouvrie J., 2006, NOTES CONVOLUTIONAL
   Chen BJ, 2015, J MATH IMAGING VIS, V51, P124, DOI 10.1007/s10851-014-0511-6
   Chen C., 2015, SIGNAL IMAGE VIDEO P
   Chen C, 2015, IEEE WINT CONF APPL, P1092, DOI 10.1109/WACV.2015.150
   Chen C, 2015, IEEE T HUM-MACH SYST, V45, P51, DOI 10.1109/THMS.2014.2362520
   Chen C, 2016, J REAL-TIME IMAGE PR, V12, P155, DOI 10.1007/s11554-013-0370-1
   Chen YN, 2006, INT C PATT RECOG, P552
   Dollar P., 2009, BMVC, P2
   Dollar P, 2014, IEEE T PATTERN ANAL, V36, P1532, DOI 10.1109/TPAMI.2014.2300479
   Dollar P, 2012, IEEE T PATTERN ANAL, V34, P743, DOI 10.1109/TPAMI.2011.155
   Dollar Piotr, 2014, IEEE T PATTERN ANAL
   Gu B, 2015, IEEE T NEUR NET LEAR, V26, P1403, DOI 10.1109/TNNLS.2014.2342533
   Gu Bin, 2015, NEURAL NETW
   Han JG, 2013, IEEE T CYBERNETICS, V43, P1318, DOI 10.1109/TCYB.2013.2265378
   Hoiem D., 2008, P IEEE C COMP VIS PA, V80, P2137
   Jian MW, 2015, IEEE T CYBERNETICS, V45, P1575, DOI 10.1109/TCYB.2014.2356200
   Jones S, 2014, PROC CVPR IEEE, P604, DOI 10.1109/CVPR.2014.84
   Ke W., 2015, P 2015 IEEE INT C AC
   Lai ZH, 2014, IEEE T CIRC SYST VID, V24, P1651, DOI 10.1109/TCSVT.2014.2305495
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li J, 2015, IEEE T INF FOREN SEC, V10, P507, DOI 10.1109/TIFS.2014.2381872
   Li W, 2015, IEEE T GEOSCI REMOTE, V53, P3681, DOI 10.1109/TGRS.2014.2381602
   Li WB, 2010, 2010 THE 3RD INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND INDUSTRIAL APPLICATION (PACIIA2010), VOL I, P9, DOI 10.1109/CVPRW.2010.5543273
   Liu L., 2015, IEEE T CYBERN
   Liu XW, 2014, PROCEEDINGS OF THE TWENTY-EIGHTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1975
   Liu XW, 2013, IEEE T CYBERNETICS, V43, P557, DOI 10.1109/TSMCB.2012.2212243
   Ma Tinghuai, 2013, IEICE T INF SYST, VE98-D, P902
   Moreira M., 1995, NEURAL NETWORKS ADAP
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Ouyang WL, 2013, IEEE I CONF COMP VIS, P2056, DOI 10.1109/ICCV.2013.257
   Pan ZQ, 2015, IEEE T BROADCAST, V61, P166, DOI 10.1109/TBC.2015.2419824
   Sermanet P, 2013, PROC CVPR IEEE, P3626, DOI 10.1109/CVPR.2013.465
   Shao L, 2014, IEEE T CYBERNETICS, V44, P817, DOI 10.1109/TCYB.2013.2273174
   Shen LL, 2011, INT J PATTERN RECOGN, V25, P273, DOI 10.1142/S0218001411008555
   Tuzel O, 2008, IEEE T PATTERN ANAL, V30, P1713, DOI 10.1109/TPAMI.2008.75
   Vedaldi A, 2009, IEEE I CONF COMP VIS, P606, DOI 10.1109/ICCV.2009.5459183
   Vieira AW, 2014, PATTERN RECOGN LETT, V36, P221, DOI 10.1016/j.patrec.2013.07.011
   Viola P, 2005, INT J COMPUT VISION, V63, P153, DOI 10.1007/s11263-005-6644-8
   Watanabe Tomoki, 2010, IPSJ Transactions on Computer Vision and Applications, V2, P39, DOI 10.2197/ipsjtcva.2.39
   Wen XZ, 2015, INFORM SCIENCES, V295, P395, DOI 10.1016/j.ins.2014.10.040
   Xia L., 2012, IEEE COMP SOC C COMP, P20, DOI DOI 10.1109/CVPRW.2012.6239233
   Yang X, 2012, P 20 ACM INT C MULT, P1057, DOI DOI 10.1145/2393347.2396382
   Zhang DW, 2016, IEEE T NEUR NET LEAR, V27, P1163, DOI 10.1109/TNNLS.2015.2495161
   Zhu F, 2014, INT J COMPUT VISION, V109, P42, DOI 10.1007/s11263-014-0703-y
NR 44
TC 11
Z9 11
U1 0
U2 9
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0923-5965
EI 1879-2677
J9 SIGNAL PROCESS-IMAGE
JI Signal Process.-Image Commun.
PD SEP
PY 2016
VL 47
SI SI
BP 490
EP 499
DI 10.1016/j.image.2016.06.002
PG 10
WC Engineering, Electrical & Electronic
SC Engineering
GA DZ1MD
UT WOS:000385601600041
OA Green Accepted
DA 2020-02-19
ER

PT J
AU Zhao, MQ
   Wang, L
   Huang, JX
   Cai, CJ
   Xu, XM
AF Zhao, Mingquan
   Wang, Li
   Huang, Jiexiong
   Cai, Chengjia
   Xu, Xiangmin
TI A multi-scene deep learning model for image aesthetic evaluation
SO SIGNAL PROCESSING-IMAGE COMMUNICATION
LA English
DT Article
DE Deep learning; Image aesthetic; Multi-scene deep learning model;
   Pre-training
ID QUALITY
AB Aesthetic evaluation of images has attracted a lot of research interests recently. Previous work focused on extracting handcrafted image features or generic image descriptors to build statistical model for aesthetic evaluation. However, the effectiveness of these approaches is limited by researchers' understanding on the aesthetic rules. In this paper, we present a multi-scene deep learning model (MSDLM) to enable automatic aesthetic feature learning. This deep learning model achieves better results because it improves performance on some major problems, including limited data amount and categories, scenes dependent evaluation, unbalanced dataset, noise data etc. Major innovations are as follows. (1) We design a scene convolutional layer consist of multi-group descriptors in the network elaborately so that the model has a comprehensive learning capacity for image aesthetic. (2) We design a pre-training procedure to initialize our model. Through pre-training the multi-group descriptors discriminatively, our model can extract specific aesthetic features for various scenes, and reduce the impact of noise data when building the model. Experimental results show that our approach significantly outperforms existing methods on two benchmark datasets. (C) 2016 Elsevier B.V. All rights reserved.
C1 [Zhao, Mingquan; Wang, Li; Huang, Jiexiong; Cai, Chengjia; Xu, Xiangmin] South China Univ Technol, Sch Elect & Informat Engn, Guangzhou, Guangdong, Peoples R China.
RP Xu, XM (reprint author), South China Univ Technol, Sch Elect & Informat Engn, Guangzhou, Guangdong, Peoples R China.
EM xmxu@scut.edu.cn
FU Natural Science Foundation of Guangdong ProvinceNational Natural Science
   Foundation of Guangdong Province [2015A030313212]; Natural Science
   Foundation of China (NSFC)National Natural Science Foundation of China
   [61401161]; Science and Technology Planning project of Guangdong
   Province of China [2014B010111003, 2014B010111006]; State Scholarship
   Found of China [201506155081]; National Engineering Technology Research
   Center of Mobile Ultrasonic Detection [2013FU125X02]
FX This work was supported by the Natural Science Foundation of Guangdong
   Province #2015A030313212, Natural Science Foundation of China (NSFC)
   #61401161, and the Science and Technology Planning project of Guangdong
   Province of China #2014B010111003, #2014B010111006, and the State
   Scholarship Found of China #201506155081, and the National Engineering
   Technology Research Center of Mobile Ultrasonic Detection #2013FU125X02.
CR Datta R, 2006, LECT NOTES COMPUT SC, V3953, P288
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dong Z, 2015, NEUROCOMPUTING, V168, P308, DOI 10.1016/j.neucom.2015.05.095
   Guo LH, 2014, NEUROCOMPUTING, V143, P14, DOI 10.1016/j.neucom.2014.06.029
   Jarrett K, 2009, IEEE I CONF COMP VIS, P2146, DOI 10.1109/ICCV.2009.5459469
   Joshi D, 2011, IEEE SIGNAL PROC MAG, V28, P94, DOI 10.1109/MSP.2011.941851
   Ke Y., 2006, P IEEE COMP SOC C CO, V1, P419, DOI DOI 10.1109/CVPR.2006.303
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, P1097, DOI DOI 10.1145/3065386
   Lee H., 2009, P 26 ANN INT C MACH, P609, DOI DOI 10.1145/1553374.1553453
   Lu X, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P457, DOI 10.1145/2647868.2654927
   Marchesotti L, 2011, IEEE I CONF COMP VIS, P1784, DOI 10.1109/ICCV.2011.6126444
   Murray N, 2012, PROC CVPR IEEE, P2408, DOI 10.1109/CVPR.2012.6247954
   Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244
   Tang XO, 2013, IEEE T MULTIMEDIA, V15, P1930, DOI 10.1109/TMM.2013.2269899
   Tong HG, 2004, LECT NOTES COMPUT SC, V3331, P198
   Wang WN, 2015, SIGNAL PROCESS-IMAGE, V39, P499, DOI 10.1016/j.image.2015.07.006
   Wang WN, 2016, NEUROCOMPUTING, V172, P244, DOI 10.1016/j.neucom.2014.12.106
   Wang Weining, 2014, Journal of Computer Aided Design & Computer Graphics, V26, P1075
   Wang WN, 2014, SIGNAL PROCESS-IMAGE, V29, P424, DOI 10.1016/j.image.2014.01.004
   Yin W., 2012, P IEEE VIS COMM IM P, P1
   Zhe Dong, 2015, MultiMedia Modeling. 21st International Conference, MMM 2015. Proceedings: LNCS 8936, P524, DOI 10.1007/978-3-319-14442-9_57
NR 21
TC 20
Z9 25
U1 1
U2 18
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0923-5965
EI 1879-2677
J9 SIGNAL PROCESS-IMAGE
JI Signal Process.-Image Commun.
PD SEP
PY 2016
VL 47
SI SI
BP 511
EP 518
DI 10.1016/j.image.2016.05.009
PG 8
WC Engineering, Electrical & Electronic
SC Engineering
GA DZ1MD
UT WOS:000385601600043
DA 2020-02-19
ER

PT J
AU Mohanty, A
   Vaishnavi, P
   Jana, P
   Majumdar, A
   Ahmed, A
   Goswami, T
   Sahay, RR
AF Mohanty, Aparna
   Vaishnavi, Pratik
   Jana, Prerana
   Majumdar, Anubhab
   Ahmed, Alfaz
   Goswami, Trishita
   Sahay, Rajiv R.
TI Nrityabodha: Towards understanding Indian classical dance using a deep
   learning approach
SO SIGNAL PROCESSING-IMAGE COMMUNICATION
LA English
DT Article
DE Deep learning; Convolutional neural network; Gesture recognition; Body
   pose estimation; Kinect; Histogram-of-gradients
ID HAND POSE ESTIMATION; PICTORIAL STRUCTURES; RECOGNITION; TRACKING;
   MODELS; SYSTEM
AB Indian classical dance has existed since over 5000 years and is widely practised and performed all over the world. However, the semantic meaning of the dance gestures and body postures as well as the intricate steps accompanied by music and recital of poems is only understood fully by the connoisseur. The common masses who watch a concert rarely appreciate or understand the ideas conveyed by the dancer. Can machine learning algorithms aid a novice to understand the semantic intricacies being expertly conveyed by the dancer? In this work, we aim to address this highly challenging problem and propose deep learning based algorithms to identify body postures and hand gestures in order to comprehend the intended meaning of the dance performance. Specifically, we propose a convolutional neural network and validate its performance on standard datasets for poses and hand gestures as well as on constrained and real-world datasets of classical dance. We use transfer learning to show that the pre trained deep networks can reduce the time taken during training and also improve accuracy. Interestingly, we show with experiments performed using Kinect in constrained laboratory settings and data from Youtube that it is possible to identify body poses and hand gestures of the performer to understand the semantic meaning of the enacted dance piece. (C) 2016 Elsevier B.V. All rights reserved.
C1 [Mohanty, Aparna] Indian Inst Technol Kharagpur, Dept Elect Engn, Computat Vis Lab, Kharagpur, W Bengal, India.
   [Vaishnavi, Pratik] Sardar Vallabhbhai Natl Inst Technol, Dept Elect & Commun Engn, Surat, India.
   [Jana, Prerana; Majumdar, Anubhab; Ahmed, Alfaz; Goswami, Trishita] Indian Inst Engn Sci & Technol, Dept Comp Sci & Technol, Kolkata, India.
   [Sahay, Rajiv R.] Indian Inst Technol Kharagpur, Dept Elect Engn, Kharagpur, W Bengal, India.
RP Mohanty, A (reprint author), Indian Inst Technol Kharagpur, Dept Elect Engn, Computat Vis Lab, Kharagpur, W Bengal, India.
EM aparnamhnty@gmail.com
CR Agarwal A, 2006, IEEE T PATTERN ANAL, V28, P44, DOI 10.1109/TPAMI.2006.21
   Alon J, 2009, IEEE T PATTERN ANAL, V31, P1685, DOI 10.1109/TPAMI.2008.203
   Andriluka M, 2009, PROC CVPR IEEE, P1014, DOI 10.1109/CVPRW.2009.5206754
   Athitsos V, 2003, PROC CVPR IEEE, P432
   Barczak ALC, 2011, RES LETT INF MATH SC, V15, P12
   Bray M., 2004, 1 EUR C VIS MED PROD
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Dantone M, 2013, PROC CVPR IEEE, P3041, DOI 10.1109/CVPR.2013.391
   Delamarre Q, 2001, COMPUT VIS IMAGE UND, V81, P328, DOI 10.1006/cviu.2000.0892
   Eichner M, 2012, INT J COMPUT VISION, V99, P190, DOI 10.1007/s11263-012-0524-9
   Erol A, 2007, COMPUT VIS IMAGE UND, V108, P52, DOI 10.1016/j.cviu.2006.10.012
   Felzenszwalb Pedro, 2008, P IEEE C COMP VIS PA, V08, P1, DOI DOI 10.1109/CVPR.2008.4587597
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Felzenszwalb PF, 2005, INT J COMPUT VISION, V61, P55, DOI 10.1023/B:VISI.0000042934.15159.49
   Forsyth DA, 1997, PROC CVPR IEEE, P678, DOI 10.1109/CVPR.1997.609399
   Gupta A, 2009, IEEE T PATTERN ANAL, V31, P1775, DOI 10.1109/TPAMI.2009.83
   Hariharan D, 2011, LECT NOTES COMPUT SC, V6744, P186, DOI 10.1007/978-3-642-21786-9_32
   Hninn T., 2009, WORLD ACAD SCI ENG T, V50, P466
   Hua G, 2005, PROC CVPR IEEE, P747
   Johnson S, 2011, PROC CVPR IEEE, P1465, DOI 10.1109/CVPR.2011.5995318
   Just A, 2009, COMPUT VIS IMAGE UND, V113, P532, DOI 10.1016/j.cviu.2008.12.001
   Kapsouras I., 2013, P 6 BALK C INF, P71
   Kapsouras I, 2013, COMM COM INF SC, V383, P172
   Keskin C, 2012, LECT NOTES COMPUT SC, V7577, P852, DOI 10.1007/978-3-642-33783-3_61
   Krizhevsky A., 2009, THESIS, V1
   Krizhevsky A., 2012, ADV NEURAL INFORM PR
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y, 2004, CVPR
   Lee HK, 1999, IEEE T PATTERN ANAL, V21, P961, DOI 10.1109/34.799904
   Lei JN, 2012, UBICOMP'12: PROCEEDINGS OF THE 2012 ACM INTERNATIONAL CONFERENCE ON UBIQUITOUS COMPUTING, P208
   Licsar A, 2005, IMAGE VISION COMPUT, V23, P1102, DOI 10.1016/j.imavis.2005.07.016
   Mallik A., 2011, J COMPUTING CULTURAL, V4, P11
   Marcel Sebastien, 1999, P C HUM FACT COMP SY, P302
   Melax S., 2013, P GRAPH INT 2013 CAN, P63
   Mori G, 2002, LECT NOTES COMPUT SC, V2352, P666
   Ning H., 2008, P IEEE C COMP VIS PA, P1, DOI [DOI 10.1109/CVPR.2008.4587534, DOI 10.1109/ICPR.2008.4761237]
   Oberweger M, 2015, IEEE I CONF COMP VIS, P3316, DOI 10.1109/ICCV.2015.379
   Oikonomidis I., 2011, BMVC, V1, P3
   OROURKE J, 1980, IEEE T PATTERN ANAL, V2, P522, DOI 10.1109/TPAMI.1980.6447699
   Palm R B, 2012, THESIS
   Pisharady PK, 2013, INT J COMPUT VISION, V101, P403, DOI 10.1007/s11263-012-0560-5
   Pishchulin L, 2013, PROC CVPR IEEE, P588, DOI 10.1109/CVPR.2013.82
   Pishchulin L, 2012, PROC CVPR IEEE, P3178, DOI 10.1109/CVPR.2012.6248052
   Ren XF, 2005, IEEE I CONF COMP VIS, P824
   Rohrbach M, 2012, PROC CVPR IEEE, P1194, DOI 10.1109/CVPR.2012.6247801
   Rosales R, 2000, PROC CVPR IEEE, P721, DOI 10.1109/CVPR.2000.854946
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Samanta S., 2012, 2012 IEEE Workshop on Applications of Computer Vision (WACV), P265, DOI 10.1109/WACV.2012.6163050
   Sharp T, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P3633, DOI 10.1145/2702123.2702179
   Shotton J, 2013, COMMUN ACM, V56, P116, DOI 10.1145/2398356.2398381
   Sminchisescu C, 2007, IEEE T PATTERN ANAL, V29, P2030, DOI 10.1109/TPAMI.2007.1111
   Sridhar S, 2015, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2015.7298941
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Stenger B, 2006, IEEE T PATTERN ANAL, V28, P1372, DOI 10.1109/TPAMI.2006.189
   Subrahmanyam P., SWATHIS SANSKRITI SE
   Tang DH, 2014, PROC CVPR IEEE, P3786, DOI 10.1109/CVPR.2014.490
   Tian YD, 2012, LECT NOTES COMPUT SC, V7576, P256, DOI 10.1007/978-3-642-33715-4_19
   Tompson J, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2629500
   Triesch J, 1996, PROCEEDINGS OF THE SECOND INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, P170, DOI 10.1109/AFGR.1996.557260
   Triesch J, 2001, IEEE T PATTERN ANAL, V23, P1449, DOI 10.1109/34.977568
   Tzionas D, 2013, LECT NOTES COMPUT SC, V8142, P131, DOI 10.1007/978-3-642-40602-7_14
   Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412
   Wang F, 2013, PROC CVPR IEEE, P596, DOI 10.1109/CVPR.2013.83
   Xu C, 2013, IEEE I CONF COMP VIS, P3456, DOI 10.1109/ICCV.2013.429
   Yang Y., 2011, CVPR
   Yang YZ, 2015, PROC CVPR IEEE, P400, DOI 10.1109/CVPR.2015.7298637
   Yang Y, 2011, PROC CVPR IEEE, P1385, DOI 10.1109/CVPR.2011.5995741
NR 68
TC 4
Z9 4
U1 0
U2 12
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0923-5965
EI 1879-2677
J9 SIGNAL PROCESS-IMAGE
JI Signal Process.-Image Commun.
PD SEP
PY 2016
VL 47
SI SI
BP 529
EP 548
DI 10.1016/j.image.2016.05.019
PG 20
WC Engineering, Electrical & Electronic
SC Engineering
GA DZ1MD
UT WOS:000385601600045
DA 2020-02-19
ER

PT J
AU Lei, Z
   Yi, D
   Li, SZ
AF Lei, Zhen
   Yi, Dong
   Li, Stan Z.
TI Learning Stacked Image Descriptor for Face Recognition
SO IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY
LA English
DT Article
DE Deep discriminant face representation; face recognition; learning-based
   descriptor; stacked image descriptor (SID)
ID SPARSE REPRESENTATION; CLASSIFICATION; PATTERNS; HISTOGRAM; MODEL
AB Learning-based face descriptors have constantly improved the face recognition performance. Compared with the hand-crafted features, learning-based features are considered to be able to exploit information with better discriminative ability for specific tasks. Motivated by the recent success of deep learning, in this paper, we extend the original shallow face descriptors to deep discriminant face features by introducing a stacked image descriptor (SID). With deep structure, more complex facial information can be extracted and the discriminant and compactness of feature representation can be improved. The SID is learned in a forward optimization way, which is computational efficient compared with deep learning. Extensive experiments on various face databases are conducted to show that SID is able to achieve high face recognition performance with compact face representation, compared with other state-of-the-art descriptors.
C1 [Lei, Zhen; Yi, Dong; Li, Stan Z.] Chinese Acad Sci, Inst Automat, Ctr Biometr & Secur Res, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
RP Lei, Z (reprint author), Chinese Acad Sci, Inst Automat, Ctr Biometr & Secur Res, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
EM zlei@nlpr.ia.ac.cn; dyi@nlpr.ia.ac.cn; szli@nlpr.ia.ac.cn
FU Chinese National Natural Science FoundationNational Natural Science
   Foundation of China [61572501, 61375037, 61473291, 61572536]; National
   Science and Technology Support Program [2013BAK02B01]; Chinese Academy
   of SciencesChinese Academy of Sciences [KGZD-EW-102-2]; AuthenMetric
   Research and Development Funds
FX This work was supported in part by the Chinese National Natural Science
   Foundation under Project 61572501, Project 61375037, Project 61473291,
   and Project 61572536, in part by the National Science and Technology
   Support Program under Project 2013BAK02B01, in part by the Chinese
   Academy of Sciences under Project KGZD-EW-102-2, and in part by the
   AuthenMetric Research and Development Funds. This paper was recommended
   by Associate Editor Y. Fu.
CR Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228
   Cai X, 2012, P 20 ACM INT C MULT, P749
   Cao ZM, 2010, PROC CVPR IEEE, P2707, DOI 10.1109/CVPR.2010.5539992
   Chan T.-H., 2014, PCANET SIMPLE DEEP L
   Chen D, 2012, LECT NOTES COMPUT SC, V7574, P566, DOI 10.1007/978-3-642-33712-3_41
   Chen D, 2013, PROC CVPR IEEE, P3025, DOI 10.1109/CVPR.2013.389
   Chen M., 2012, P AS C SIGN SYST COM, P1
   Deng WH, 2012, IEEE T PATTERN ANAL, V34, P1864, DOI 10.1109/TPAMI.2012.30
   Ding C., 2014, MULTIDIRECTIONAL MUL
   Gao SH, 2010, LECT NOTES COMPUT SC, V6314, P1
   Gross R, 2010, IMAGE VISION COMPUT, V28, P807, DOI 10.1016/j.imavis.2009.08.002
   Guo Y., 2010, COMP VIS ACCV 2010, P185
   He KM, 2015, PROC CVPR IEEE, P5353, DOI 10.1109/CVPR.2015.7299173
   He R, 2014, IEEE T PATTERN ANAL, V36, P261, DOI 10.1109/TPAMI.2013.102
   He R, 2011, IEEE T PATTERN ANAL, V33, P1561, DOI 10.1109/TPAMI.2010.220
   Hu JL, 2014, PROC CVPR IEEE, P1875, DOI 10.1109/CVPR.2014.242
   Huang G. B., 2007, 0749 U MASS AMH
   Huang GB, 2012, PROC CVPR IEEE, P2518, DOI 10.1109/CVPR.2012.6247968
   Kan M, 2014, PROC CVPR IEEE, P1883, DOI 10.1109/CVPR.2014.243
   Lei Z, 2007, LECT NOTES COMPUT SC, V4642, P49
   Lei Z, 2014, IEEE T PATTERN ANAL, V36, P289, DOI 10.1109/TPAMI.2013.112
   Lei Z, 2012, PROC CVPR IEEE, P2512, DOI 10.1109/CVPR.2012.6247967
   Li AN, 2012, IEEE T IMAGE PROCESS, V21, P305, DOI 10.1109/TIP.2011.2160957
   Li HX, 2013, PROC CVPR IEEE, P3499, DOI 10.1109/CVPR.2013.449
   Li P, 2012, IEEE T PATTERN ANAL, V34, P144, DOI 10.1109/TPAMI.2011.104
   Li SX, 2012, LECT NOTES COMPUT SC, V7572, P102, DOI 10.1007/978-3-642-33718-5_8
   Li XX, 2013, IEEE T IMAGE PROCESS, V22, P1889, DOI 10.1109/TIP.2013.2237920
   Liao S., 2014, IEEE INT JOINT C BIO, P1, DOI DOI 10.1109/BTAS.2014.6996301
   Liu CJ, 2002, IEEE T IMAGE PROCESS, V11, P467, DOI 10.1109/TIP.2002.999679
   Maturana D, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P470, DOI 10.1109/FG.2011.5771444
   Maturana D, 2010, P 10 AS C COMP VIS, V6495, P618
   Mendez-Vazquez H., 2013, P ICB JUN, P1, DOI DOI 10.1109/ICB.2013.6612990
   Mousavi HS, 2014, IEEE IMAGE PROC, P4236, DOI 10.1109/ICIP.2014.7025860
   Vu NS, 2012, IEEE T IMAGE PROCESS, V21, P1352, DOI 10.1109/TIP.2011.2166974
   Phillips PJ, 2000, IEEE T PATTERN ANAL, V22, P1090, DOI 10.1109/34.879790
   Shao M., 2014, LOW RANK SPARSE MODE, P117
   Simonyan K., 2013, ADV NEURAL INFORM PR, P163
   Simonyan K, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.8
   Sun Y, 2013, IEEE I CONF COMP VIS, P1489, DOI 10.1109/ICCV.2013.188
   Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244
   Sun Yi, 2014, DEEP LEARNING FACE R
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Turk M. A., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P586, DOI 10.1109/CVPR.1991.139758
   ul Hussain S, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.99
   VASILESCU MAO, 2003, [No title captured], P93
   Wagner A, 2012, IEEE T PATTERN ANAL, V34, P372, DOI 10.1109/TPAMI.2011.112
   Wolf L, 2011, PROC CVPR IEEE, P529, DOI 10.1109/CVPR.2011.5995566
   Wright J, 2010, P IEEE, V98, P1031, DOI 10.1109/JPROC.2010.2044470
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Xie SF, 2010, IEEE T IMAGE PROCESS, V19, P1349, DOI 10.1109/TIP.2010.2041397
   Xiong XH, 2013, PROC CVPR IEEE, P532, DOI 10.1109/CVPR.2013.75
   Xu Y, 2011, IEEE T CIRC SYST VID, V21, P1255, DOI 10.1109/TCSVT.2011.2138790
   Yan SC, 2005, PROC CVPR IEEE, P526
   Yang M, 2014, INT J COMPUT VISION, V109, P209, DOI 10.1007/s11263-014-0722-8
   Yang M, 2013, PATTERN RECOGN, V46, P1865, DOI 10.1016/j.patcog.2012.06.022
   Yang M, 2011, PROC CVPR IEEE, P625, DOI 10.1109/CVPR.2011.5995393
   Zhang BH, 2007, IEEE T IMAGE PROCESS, V16, P57, DOI 10.1109/TIP.2006.884956
   Zhang HC, 2012, PATTERN RECOGN, V45, P1290, DOI 10.1016/j.patcog.2011.09.009
   Zhang L, 2011, IEEE I CONF COMP VIS, P471, DOI 10.1109/ICCV.2011.6126277
   Zhang WC, 2005, IEEE I CONF COMP VIS, P786
   Zhang YZ, 2013, IEEE I CONF COMP VIS, P2416, DOI 10.1109/ICCV.2013.300
   Zhu ZY, 2013, IEEE I CONF COMP VIS, P113, DOI 10.1109/ICCV.2013.21
NR 63
TC 20
Z9 20
U1 1
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1051-8215
EI 1558-2205
J9 IEEE T CIRC SYST VID
JI IEEE Trans. Circuits Syst. Video Technol.
PD SEP
PY 2016
VL 26
IS 9
BP 1685
EP 1696
DI 10.1109/TCSVT.2015.2473415
PG 12
WC Engineering, Electrical & Electronic
SC Engineering
GA DX0TP
UT WOS:000384078400009
DA 2020-02-19
ER

PT J
AU Mocanu, DC
   Mocanu, E
   Nguyen, PH
   Gibescu, M
   Liotta, A
AF Mocanu, Decebal Constantin
   Mocanu, Elena
   Nguyen, Phuong H.
   Gibescu, Madeleine
   Liotta, Antonio
TI A topological insight into restricted Boltzmann machines
SO MACHINE LEARNING
LA English
DT Article; Proceedings Paper
CT European Conference on Machine Learning and Principles and Practice of
   Knowledge Discovery in Databases (ECMLPKDD)
CY SEP 19-23, 2016
CL Riva del Garda, ITALY
SP Google, IBM, Deloitte, Siemens, Unicredit, Zalando, UNITN, UNIFI, ISTI CNR, ICAR CNR
DE Deep learning; Sparse restricted Boltzmann machines; Complex networks;
   Scale-free networks; Small-world networks
AB Restricted Boltzmann Machines (RBMs) and models derived from them have been successfully used as basic building blocks in deep artificial neural networks for automatic features extraction, unsupervised weights initialization, but also as density estimators. Thus, their generative and discriminative capabilities, but also their computational time are instrumental to a wide range of applications. Our main contribution is to look at RBMs from a topological perspective, bringing insights from network science. Firstly, here we show that RBMs and Gaussian RBMs (GRBMs) are bipartite graphs which naturally have a small-world topology. Secondly, we demonstrate both on synthetic and real-world datasets that by constraining RBMs and GRBMs to a scale-free topology (while still considering local neighborhoods and data distribution), we reduce the number of weights that need to be computed by a few orders of magnitude, at virtually no loss in generative performance. Thirdly, we show that, for a fixed number of weights, our proposed sparse models (which by design have a higher number of hidden neurons) achieve better generative capabilities than standard fully connected RBMs and GRBMs (which by design have a smaller number of hidden neurons), at no additional computational costs.
C1 [Mocanu, Decebal Constantin; Mocanu, Elena; Nguyen, Phuong H.; Gibescu, Madeleine; Liotta, Antonio] Eindhoven Univ Technol, Dept Elect Engn, Eindhoven, Netherlands.
RP Mocanu, DC (reprint author), Eindhoven Univ Technol, Dept Elect Engn, Eindhoven, Netherlands.
EM d.c.mocanu@tue.nl; e.mocanu@tue.nl; p.nguyen.hong@tue.nl;
   m.gibescu@tue.nl; a.liotta@tue.nl
RI Liotta, Antonio/G-9532-2014; Nguyen, Phuong H./C-6762-2011
OI Liotta, Antonio/0000-0002-2773-4421; Nguyen, Phuong
   H./0000-0003-1124-2710; Mocanu, Decebal Constantin/0000-0002-5636-7683;
   Mocanu, Elena/0000-0002-0856-579X
CR ACKLEY DH, 1985, COGNITIVE SCI, V9, P147
   Ammar Haitham Bou, 2013, Machine Learning and Knowledge Discovery in Databases. European Conference (ECML PKDD 2013). Proceedings: LNCS 8189, P449, DOI 10.1007/978-3-642-40991-2_29
   Ba J., 2014, ADV NEURAL INFORM PR, P2654
   Barabasi AL, 1999, SCIENCE, V286, P509, DOI 10.1126/science.286.5439.509
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Brugge K, 2013, MACH LEARN, V93, P53, DOI 10.1007/s10994-013-5390-3
   Carreira-Perpinan M. A., 2005, 10 INT WORKSH ART IN
   Clauset A, 2009, SIAM REV, V51, P661, DOI 10.1137/070710111
   Del Genio CI, 2011, PHYS REV LETT, V107, DOI 10.1103/PhysRevLett.107.178701
   Desjardins G., 2010, P 13 INT C ART INT S, V9, P145
   Dieleman S., 2012, P DEEP LEARN UNS FEA, P9
   Gehler Peter V, 2006, P 23 INT C MACH LEAR, P337, DOI DOI 10.1145/1143844.1143887
   Germain M., 2015, ICML, P881
   Hagberg A. A., 2008, 7 PYTH SCI C SCIPY20, V2008, P11
   HAKIMI SL, 1962, J SOC IND APPL MATH, V10, P496, DOI 10.1137/0110037
   Han S., 2015, ADV NEURAL INFORM PR, P1135
   Hinton G., 2012, LNCS, V7700, P599, DOI DOI 10.1007/978-3-642-35289-8_32
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Jones N, 2014, NATURE, V505, P146, DOI 10.1038/505146a
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kusy B, 2009, 2009 INTERNATIONAL CONFERENCE ON INFORMATION PROCESSING IN SENSOR NETWORKS (IPSN 2009), P109
   Larochelle H., 2011, ARTIF INTELL, P29
   Larochelle H., 2008, P 25 INT C MACH LEAR, P536, DOI [10.1145/1390156.1390224, DOI 10.1145/1390156.1390224]
   Latapy M, 2008, SOC NETWORKS, V30, P31, DOI 10.1016/j.socnet.2007.04.006
   Lee H., 2008, ADV NEURAL INFORM PR, V20, P873
   Luo H, 2011, AAAI
   Marlin B. M., 2010, P 13 INT C ART INT S, P509
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Mocanu DC, 2015, PATTERN RECOGN LETT, V66, P100, DOI 10.1016/j.patrec.2015.01.013
   Osogami T., 2014, ADV NEURAL INFORM PR, V27, P73
   Pessoa L, 2014, PHYS LIFE REV, V11, P400, DOI 10.1016/j.plrev.2014.03.005
   Ranzato M, 2008, ADV NEURAL INFORM PR, P1185
   Salakhutdinov R., 2007, P INT C MACH LEARN, V24, P791, DOI DOI 10.1145/1273496.1273596
   Salakhutdinov R., 2008, P 25 INT C MACH LEAR, P872, DOI [10.1145/1390156.1390266, DOI 10.1145/1390156.1390266]
   Smolensky P, 1986, PARALLEL DISTRIBUTED, P194
   Srivastava N., 2012, ADV NEURAL INFORM PR, P2222, DOI DOI 10.1109/CVPR.2013.49
   Strogatz SH, 2001, NATURE, V410, P268, DOI 10.1038/35065725
   Swersky K, 2012, NIPS, P3302
   Tieleman T., 2008, P 25 INT C MACH LEAR, P1064, DOI DOI 10.1145/1390156.1390290
   Tieleman T., 2009, P 26 ANN INT C MACH, P1033, DOI DOI 10.1145/1553374.1553506
   van der Hofstad R., 2016, RANDOM GRAPHS COMPLE, V1
   Wan C., 2015, 29 AAAI C ART INT
   Watts DJ, 1998, NATURE, V393, P440, DOI 10.1038/30918
   Welling M., 2005, ADV NEURAL INFORM PR, P1481
   Yosinski J., 2012, REPR LEARN WORKSH 29
   Zhou F, 2014, IEEE DATA MINING, P1115, DOI 10.1109/ICDM.2014.73
NR 47
TC 15
Z9 16
U1 0
U2 12
PU SPRINGER
PI DORDRECHT
PA VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN 0885-6125
EI 1573-0565
J9 MACH LEARN
JI Mach. Learn.
PD SEP
PY 2016
VL 104
IS 2-3
SI SI
BP 243
EP 270
DI 10.1007/s10994-016-5570-z
PG 28
WC Computer Science, Artificial Intelligence
SC Computer Science
GA DU2OY
UT WOS:000382051700005
OA Other Gold, Green Published
DA 2020-02-19
ER

PT J
AU Zhang, L
   Ma, WP
   Zhang, D
AF Zhang, Lu
   Ma, Wenping
   Zhang, Dan
TI Stacked Sparse Autoencoder in PolSAR Data Classification Using Local
   Spatial Information
SO IEEE GEOSCIENCE AND REMOTE SENSING LETTERS
LA English
DT Article
DE Deep learning; image classification; local spatial information;
   polarimetric synthetic aperture radar (PolSAR); sparse; stacked sparse
   autoencoder (SSAE)
ID LAND-COVER; NEURAL-NETWORK; SAR IMAGES
AB Terrain classification is an important topic in polarimetric synthetic aperture radar (PolSAR) image processing. Among various classification techniques, the stacked sparse autoencoder (SSAE) is a kind of deep learning method that can automatically learn useful features layer by layer in an unsupervised manner. However, the scattering measurements of individual pixels in PolSAR images are affected by the speckle; hence, the performance of pixel-based classification approaches would be poor. In this situation, a novel framework is proposed to learn robust features of PolSAR data. The local spatial information is introduced into SSAE to learn the deep spatial sparse features automatically for the first time. Furthermore, the influences of the neighbor pixels on the central pixel are controlled depending on the spatial distances from the neighbor pixels to the central pixel. Experimental results with fully PolSAR data indicate that the proposed method provides a competitive solution.
C1 [Zhang, Lu; Ma, Wenping; Zhang, Dan] Xidian Univ, Joint Int Res Lab Intelligent Percept & Computat, Key Lab Intelligent Percept & Image Understanding, Int Res Ctr Intelligent Percept & Computat,Minist, Xian 710071, Peoples R China.
RP Zhang, L (reprint author), Xidian Univ, Joint Int Res Lab Intelligent Percept & Computat, Key Lab Intelligent Percept & Image Understanding, Int Res Ctr Intelligent Percept & Computat,Minist, Xian 710071, Peoples R China.
EM oleand_er@126.com
FU National Basic Research Program (973 Program) of ChinaNational Basic
   Research Program of China [2013CB329402]; National Natural Science
   Foundation of ChinaNational Natural Science Foundation of China
   [61573267, 61473215, 61571342, 91438201, 91438103]; Fund for Foreign
   Scholars in University Research and Teaching Programs (the 111 Project)
   [B07048]; Program for Cheung Kong Scholars and Innovative Research Team
   in UniversityProgram for Changjiang Scholars & Innovative Research Team
   in University (PCSIRT) [IRT_15R53]
FX This work was supported in part by the National Basic Research Program
   (973 Program) of China under Grant 2013CB329402, by the National Natural
   Science Foundation of China under Grants 61573267, 61473215, and
   61571342, by the Fund for Foreign Scholars in University Research and
   Teaching Programs (the 111 Project) under Grant B07048, by the Major
   Research Plan of the National Natural Science Foundation of China under
   Grants 91438201 and 91438103, and by the Program for Cheung Kong
   Scholars and Innovative Research Team in University under Grant
   IRT_15R53.
CR Antropov O, 2014, IEEE T GEOSCI REMOTE, V52, P5256, DOI 10.1109/TGRS.2013.2287712
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Chen CT, 2003, IEEE T GEOSCI REMOTE, V41, P2089, DOI 10.1109/TGRS.2003.813494
   Fukuda S, 2001, INT GEOSCI REMOTE SE, P187, DOI 10.1109/IGARSS.2001.976097
   Gong H., 2009, P 5 INT C NAT COMP, V3, P565
   LEE JS, 1994, INT J REMOTE SENS, V15, P2299, DOI 10.1080/01431169408954244
   Mishra P, 2014, IEEE T GEOSCI REMOTE, V52, P2889, DOI 10.1109/TGRS.2013.2267548
   Ng Andrew Y., 2011, CS294A LECT NOTES, V72, P1, DOI DOI 10.1371/JOURNAL.PONE.0006098
   Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7
   Tzeng YC, 1998, IEEE T GEOSCI REMOTE, V36, P301, DOI 10.1109/36.655339
   Wu YH, 2008, IEEE GEOSCI REMOTE S, V5, P668, DOI 10.1109/LGRS.2008.2002263
NR 11
TC 42
Z9 42
U1 2
U2 46
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1545-598X
EI 1558-0571
J9 IEEE GEOSCI REMOTE S
JI IEEE Geosci. Remote Sens. Lett.
PD SEP
PY 2016
VL 13
IS 9
BP 1359
EP 1363
DI 10.1109/LGRS.2016.2586109
PG 5
WC Geochemistry & Geophysics; Engineering, Electrical & Electronic; Remote
   Sensing; Imaging Science & Photographic Technology
SC Geochemistry & Geophysics; Engineering; Remote Sensing; Imaging Science
   & Photographic Technology
GA DV1MA
UT WOS:000382684000031
DA 2020-02-19
ER

PT J
AU Azizpour, H
   Razavian, AS
   Sullivan, J
   Maki, A
   Carlsson, S
AF Azizpour, Hossein
   Razavian, Ali Sharif
   Sullivan, Josephine
   Maki, Atsuto
   Carlsson, Stefan
TI Factors of Transferability for a Generic ConvNet Representation
SO IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
LA English
DT Article
DE Convolutional neural networks; transfer learning; representation
   learning; deep learning; visual recognition
AB Evidence is mounting that Convolutional Networks (ConvNets) are the most effective representation learning method for visual recognition tasks. In the common scenario, a ConvNet is trained on a large labeled dataset (source) and the feed-forward units activation of the trained network, at a certain layer of the network, is used as a generic representation of an input image for a task with relatively smaller training set (target). Recent studies have shown this form of representation transfer to be suitable for a wide range of target visual recognition tasks. This paper introduces and investigates several factors affecting the transferability of such representations. It includes parameters for training of the source ConvNet such as its architecture, distribution of the training data, etc. and also the parameters of feature extraction such as layer of the trained ConvNet, dimensionality reduction, etc. Then, by optimizing these factors, we show that significant improvements can be achieved on various (17) visual recognition tasks. We further show that these visual recognition tasks can be categorically ordered based on their similarity to the source task such that a correlation between the performance of tasks and their similarity to the source task w.r.t. the proposed factors is observed.
C1 [Azizpour, Hossein; Razavian, Ali Sharif; Sullivan, Josephine; Maki, Atsuto; Carlsson, Stefan] Royal Inst Technol KTH, Comp Vis & Act Percept Lab, SE-10044 Stockholm, Sweden.
RP Azizpour, H (reprint author), Royal Inst Technol KTH, Comp Vis & Act Percept Lab, SE-10044 Stockholm, Sweden.
EM azizpour@csc.kth.se; razavian@csc.kth.se; sullivan@csc.kth.se;
   atsuto@csc.kth.se; stefanc@csc.kth.se
OI /0000-0001-5211-6388
FU Swedish Foundation for Strategic Research (SSF) within project VINST
FX The authors gratefully acknowledge the NVIDIA Corporation for their
   donation of the Tesla K40 GPUs used in this research. This work has been
   funded by the Swedish Foundation for Strategic Research (SSF) within the
   project VINST. The manuscript is an extended version of the CVPR
   Workshops DeepVision 2015 paper "From Generic to Specific Deep
   Representations for Visual Recognition".
CR Agrawal P, 2014, LECT NOTES COMPUT SC, V8695, P329, DOI 10.1007/978-3-319-10584-0_22
   [Anonymous], 2013, IMAGENET LARGE SCALE
   Arandjelovic R, 2011, IEEE I CONF COMP VIS, P375, DOI 10.1109/ICCV.2011.6126265
   Argyriou A, 2006, NIPS, V73, P41, DOI DOI 10.1007/S10994-007-5040-8
   Bengio Yoshua, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P437, DOI 10.1007/978-3-642-35289-8_26
   Bourdev L, 2011, IEEE I CONF COMP VIS, P1543, DOI 10.1109/ICCV.2011.6126413
   Chatfield K, 2014, ARXIV14053531CSCV
   Donahue J., 2014, P INT C MACH LEARN, P647
   Everingham M, 2012, PASCAL VISUAL OBJECT
   Farhadi A, 2009, PROC CVPR IEEE, P1778, DOI 10.1109/CVPRW.2009.5206772
   Fergus Rob, 2013, CORR
   FUKUSHIMA K, 1980, BIOL CYBERN, V36, P193, DOI 10.1007/BF00344251
   Gavves E, 2013, IEEE I CONF COMP VIS, P1713, DOI 10.1109/ICCV.2013.215
   Girshick R., 2015, P IEEE INT C COMP VI
   Girshick R, 2015, PROC CVPR IEEE, P437, DOI 10.1109/CVPR.2015.7298641
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Gutstein S, 2008, INT J ARTIF INTELL T, V17, P555, DOI 10.1142/S0218213008004059
   Jegou H, 2008, LECT NOTES COMPUT SC, V5302, P304, DOI 10.1007/978-3-540-88682-2_24
   Jegou H, 2012, LECT NOTES COMPUT SC, V7573, P774, DOI 10.1007/978-3-642-33709-3_55
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Koniusz P., 2013, TECH REP
   Krause J, 2015, PROC CVPR IEEE, P5546, DOI 10.1109/CVPR.2015.7299194
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lenc K., 2015, P BRIT MACH VIS C
   Li L.-J., 2010, ADV NEURAL INFORM PR, P1378
   Lin D, 2014, PROC CVPR IEEE, P3726, DOI 10.1109/CVPR.2014.476
   Ng JYH, 2015, PROC CVPR IEEE, P4694, DOI 10.1109/CVPR.2015.7299101
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47
   Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222
   Parkhi OM, 2012, PROC CVPR IEEE, P3498, DOI 10.1109/CVPR.2012.6248092
   Patterson G, 2012, PROC CVPR IEEE, P2751, DOI 10.1109/CVPR.2012.6247998
   Philbin J., 2008, P IEEE C COMP VIS PA, P1, DOI [DOI 10.1109/CVPR.2008.4587635, Dow Agrosciences LLC]
   Pratt LY., 1992, ADV NEURAL INFORM PR, P204
   Quattoni A, 2009, PROC CVPR IEEE, P413, DOI 10.1109/CVPRW.2009.5206537
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Sadeghi MA, 2011, PROC CVPR IEEE, P1745, DOI 10.1109/CVPR.2011.5995711
   Sermanet P., 2014, P INT C LEARN REPR
   Sharif Razavian A., 2015, P ICLR WORKSH
   Simonyan K., 2015, P ICLR
   Song Z, 2011, PROC CVPR IEEE, P1585, DOI 10.1109/CVPR.2011.5995330
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tolias G, 2013, IEEE I CONF COMP VIS, P1401, DOI 10.1109/ICCV.2013.177
   Toshev A, 2014, PROC CVPR IEEE, P1653, DOI 10.1109/CVPR.2014.214
   Tsagkatakis G., 2010, ECCV WORKSH, P29
   Welinder P., 2010, CNSTR2010001 CAL I T
   Xiao J., 2014, INT J COMPUT VISION, P1
   XIAO JX, 2010, PROC CVPR IEEE, P3485, DOI DOI 10.1109/CVPR.2010.5539970
   Yao BP, 2011, IEEE I CONF COMP VIS, P1331, DOI 10.1109/ICCV.2011.6126386
   Yosinski J., 2014, ARXIV14111792CSLG
   Zhang N, 2013, IEEE I CONF COMP VIS, P729, DOI 10.1109/ICCV.2013.96
   Zhang N, 2014, LECT NOTES COMPUT SC, V8689, P834, DOI 10.1007/978-3-319-10590-1_54
   Zhao WL, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.99
   Zhou B, 2014, ADV NEURAL INFORM PR, P487, DOI DOI 10.1162/153244303322533223
   Zhu XC, 2013, PLANT MOL BIOL REP, V31, P248, DOI 10.1007/s11105-012-0473-z
NR 56
TC 64
Z9 65
U1 1
U2 16
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 0162-8828
EI 1939-3539
J9 IEEE T PATTERN ANAL
JI IEEE Trans. Pattern Anal. Mach. Intell.
PD SEP
PY 2016
VL 38
IS 9
BP 1790
EP 1802
DI 10.1109/TPAMI.2015.2500224
PG 13
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
SC Computer Science; Engineering
GA DT4EK
UT WOS:000381432700006
PM 26584488
DA 2020-02-19
ER

PT J
AU Iliya, S
   Neri, F
AF Iliya, Sunday
   Neri, Ferrante
TI Towards Artificial Speech Therapy: A Neural System for Impaired Speech
   Segmentation
SO INTERNATIONAL JOURNAL OF NEURAL SYSTEMS
LA English
DT Article
DE Impaired speech; steady state; artificial neural network; support vector
   machine; compact differential evolution; metaheuristics
ID FRAME ROOF STRUCTURES; DIFFERENTIAL EVOLUTION; NETWORK; OPTIMIZATION;
   ALGORITHM; RECOGNITION; DESIGN; MODEL; BACKPROPAGATION; CLASSIFICATION
AB This paper presents a neural system-based technique for segmenting short impaired speech utterances into silent, unvoiced, and voiced sections. Moreover, the proposed technique identifies those points of the (voiced) speech where the spectrum becomes steady. The resulting technique thus aims at detecting that limited section of the speech which contains the information about the potential impairment of the speech. This section is of interest to the speech therapist as it corresponds to the possibly incorrect movements of speech organs (lower lip and tongue with respect to the vocal tract). Two segmentation models to detect and identify the various sections of the disordered (impaired) speech signals have been developed and compared. The first makes use of a combination of four artificial neural networks. The second is based on a support vector machine (SVM). The SVM has been trained by means of an ad hoc nested algorithm whose outer layer is a metaheuristic while the inner layer is a convex optimization algorithm. Several metaheuristics have been tested and compared leading to the conclusion that some variants of the compact differential evolution (CDE) algorithm appears to be well-suited to address this problem. Numerical results show that the SVM model with a radial basis function is capable of effective detection of the portion of speech that is of interest to a therapist. The best performance has been achieved when the system is trained by the nested algorithm whose outer layer is hybrid-population-based/CDE. A population-based approach displays the best performance for the isolation of silence/noise sections, and the detection of unvoiced sections. On the other hand, a compact approach appears to be clearly well-suited to detect the beginning of the steady state of the voiced signal. Both the proposed segmentation models display outperformed two modern segmentation techniques based on Gaussian mixture model and deep learning.
C1 [Iliya, Sunday; Neri, Ferrante] De Montfort Univ, Sch Comp Sci & Informat, Ctr Computat Intelligence, Leicester LE1 9BH, Leics, England.
   [Neri, Ferrante] Univ Jyvaskyla Jyvaskyla, Dept Math Informat Technol, Jyvaskyla, Finland.
RP Iliya, S (reprint author), De Montfort Univ, Sch Comp Sci & Informat, Ctr Computat Intelligence, Leicester LE1 9BH, Leics, England.
EM sundayiliyagoteng@yahoo.com; fneri@dmu.ac.uk
RI Neri, Ferrante/U-7036-2019
OI Neri, Ferrante/0000-0002-6100-6532
FU Higher Education Innovation Fund at De Montfort University, UK;
   Petroleum Technology Development Fund (PTDF) Scholarship, Nigeria
FX This work is supported by the Higher Education Innovation Fund at De
   Montfort University, UK. This research is supported by Petroleum
   Technology Development Fund (PTDF) Scholarship, Nigeria.
CR ADELI H, 1994, APPL MATH COMPUT, V62, P81, DOI 10.1016/0096-3003(94)90134-1
   ADELI H, 1993, INT J SUPERCOMPUT AP, V7, P155, DOI 10.1177/109434209300700206
   Adeli H., 1998, NEUROCOMPUTING DESIG
   Adeli H., 1995, MACHINE LEARNING NEU
   Ahlehagh H, 2012, IEEE ICC
   Ahmadlou M, 2010, INTEGR COMPUT-AID E, V17, P197, DOI 10.3233/ICA-2010-0345
   Al-khassaweneh M, 2010, INTEGR COMPUT-AID E, V17, P59, DOI 10.3233/ICA-2010-0329
   ALHASHEMY BAR, 1988, APPL ACOUST, V25, P169, DOI 10.1016/0003-682X(88)90092-8
   Baker JM, 2009, IEEE SIGNAL PROC MAG, V26, P75, DOI 10.1109/MSP.2009.932166
   BISHOP CM, 2000, [No title captured]
   Brest J, 2006, IEEE T EVOLUT COMPUT, V10, P646, DOI 10.1109/TEVC.2006.872133
   Brognaux S, 2016, IEEE-ACM T AUDIO SPE, V24, P5, DOI 10.1109/TASLP.2015.2456421
   Butcher JB, 2014, COMPUT-AIDED CIV INF, V29, P191, DOI 10.1111/mice.12039
   Celikoglu HB, 2013, COMPUT-AIDED CIV INF, V28, P273, DOI 10.1111/j.1467-8667.2012.00792.x
   Cheng JX, 2015, INTEGR COMPUT-AID E, V22, P103, DOI 10.3233/ICA-150481
   Chyzhyk D, 2015, INT J NEURAL SYST, V25, DOI 10.1142/S0129065715500070
   Coletta LFS, 2015, INTEGR COMPUT-AID E, V22, P229, DOI 10.3233/ICA-150485
   Cornelius P, 2011, J ASSIST TECHNOL, V5, P123, DOI 10.1108/17549451111173479
   Dai HZ, 2014, COMPUT-AIDED CIV INF, V29, P801, DOI 10.1111/mice.12117
   Das S, 2011, IEEE T EVOLUT COMPUT, V15, P4, DOI 10.1109/TEVC.2010.2059031
   Dash K., 2012, INT J ADV RES COMPUT, V2
   Deleforge A, 2015, INT J NEURAL SYST, V25, DOI 10.1142/S0129065714400036
   Deng HQ, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P176
   Dura-Bernal S, 2013, INT J NEURAL SYST, V23, DOI 10.1142/S0129065713500214
   Elizalde B, 2013, IEEE INT CON MULTI
   Fernandez JM, 2010, INTEGR COMPUT-AID E, V17, P243, DOI 10.3233/ICA-2010-0341
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Haykin S., 2008, NEURAL NETWORKS LEAR, P122
   HUNG SL, 1994, NEUROCOMPUTING, V6, P45, DOI 10.1016/0925-2312(94)90033-7
   HUNG SL, 1993, NEUROCOMPUTING, V5, P287, DOI 10.1016/0925-2312(93)90042-2
   Hussain A, 1999, Int J Neural Syst, V9, P467, DOI 10.1142/S0129065799000496
   Hussain A, 1999, Int J Neural Syst, V9, P461, DOI 10.1142/S0129065799000484
   Iacca Giovanni, 2012, Applications of Evolutionary Computation. Proceedings of EvoApplications 2012: EvoCOMNET, EvoCOMPLEX, EvoFIN, EvoGAMES, EvoHOT, EvoIASP, EvoNUM, EvoPAR, EvoRISK, EvoSTIM, and EvoSTOC, P285, DOI 10.1007/978-3-642-29178-4_29
   Iacca G, 2014, INT J NEURAL SYST, V24, DOI 10.1142/S0129065714500087
   Iacca G, 2011, 2011 IEEE WORKSHOP ON MEMETIC COMPUTING, P21
   Iacca G, 2012, J COMPUT SCI TECH-CH, V27, P1056, DOI 10.1007/s11390-012-1284-2
   Iacca G, 2011, EVOL INTELL, V4, P17, DOI 10.1007/s12065-010-0046-8
   Iliya S., 2014, IEEE INT S SIGN PROC
   Iliya S., 2014, IEEE S SERIES COMPUT
   Ionescu M, 2006, FUND INFORM, V71, P279
   Jarvelin A, 2006, INT J NEURAL SYST, V16, P241, DOI 10.1142/S0129065706000652
   Khalid M, 2014, COMPUT-AIDED CIV INF, V29, P221, DOI 10.1111/mice.12005
   King S, 2013, INT CONF ACOUST SPEE, P8096, DOI 10.1109/ICASSP.2013.6639242
   Kociecki M, 2015, ENG APPL ARTIF INTEL, V38, P168, DOI 10.1016/j.engappai.2014.10.012
   Kociecki M, 2014, ENG APPL ARTIF INTEL, V32, P218, DOI 10.1016/j.engappai.2014.01.010
   Lawrence R., 1975, THEORY APPL DIGITAL
   Lerner S. Z., 1991, International Journal of Neural Systems, V2, P55, DOI 10.1142/S0129065791000066
   Li DW, 2013, INTEGR COMPUT-AID E, V20, P201, DOI 10.3233/ICA-130428
   Liang JJ, 2006, IEEE T EVOLUT COMPUT, V10, P281, DOI 10.1109/TEVC.2005.857610
   Lim C. P., 2000, P IEEE C WEB INF SYS
   Lopez-Gordo MA, 2012, INT J NEURAL SYST, V22, DOI 10.1142/S0129065712500098
   Lostado R, 2015, INTEGR COMPUT-AID E, V22, P153, DOI 10.3233/ICA-150484
   Mininno E, 2011, IEEE T EVOLUT COMPUT, V15, P32, DOI 10.1109/TEVC.2010.2058120
   Neri F, 2010, ARTIF INTELL REV, V33, P61, DOI 10.1007/s10462-009-9137-2
   Neri F, 2008, STUD COMPUT INTELL, V153, P113
   Padmanabhan K., 2003, PRACTICAL APPROACH D
   PARK HS, 1995, COMPUT STRUCT, V57, P391, DOI 10.1016/0045-7949(95)00047-K
   Park SS, 2007, IEEE T AUDIO SPEECH, V15, P2202, DOI 10.1109/TASL.2007.903933
   Patel PB, 2010, INT J NEURAL SYST, V20, P87, DOI 10.1142/S0129065710002255
   Paulraj M. P., 2010, SIGN PROC ITS APPL C, P190
   Penaloza C, 2013, PROCD SOC BEHV, V94, P112, DOI 10.1016/j.sbspro.2013.09.054
   Penaloza C, 2015, APHASIOLOGY, V29, P724, DOI 10.1080/02687038.2014.982500
   Price KV, 2005, NAT COMP SER
   Qi FY, 2004, 2004 International Symposium on Chinese Spoken Language Processing, Proceedings, P77
   Radmard M., 2011, J SIGNAL INFORM PROC, V2, P336, DOI DOI 10.4236/JSIP.2011.24048.
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Sahidullah M, 2012, SPEECH COMMUN, V54, P543, DOI 10.1016/j.specom.2011.11.004
   Siddique N, 2014, INT J ARTIF INTELL T, V23, DOI 10.1142/S0218213014300014
   Siniscalchi SM, 2013, IEEE SIGNAL PROC LET, V20, P201, DOI 10.1109/LSP.2013.2237901
   Song W., 2015, CS224D U STANF
   Story BA, 2014, COMPUT-AIDED CIV INF, V29, P180, DOI 10.1111/mice.12040
   Sun J, 2013, IEEE T SYST MAN CY-S, V43, P801, DOI 10.1109/TSMCA.2012.2224338
   Vemulapalli S, 2014, INTEGR COMPUT-AID E, V21, P219, DOI 10.3233/ICA-140460
   Vlahogianni EI, 2013, COMPUT-AIDED CIV INF, V28, P420, DOI 10.1111/mice.12010
   Wang NM, 2015, ENG APPL ARTIF INTEL, V41, P249, DOI 10.1016/j.engappai.2015.01.018
   WILCOXON F, 1945, BIOMETRICS BULL, V1, P80, DOI 10.2307/3001968
   Yiu K K, 2002, Int J Neural Syst, V12, P381, DOI 10.1142/S0129065702001278
   Zaharie D., 2007, P INT MULT COMP SCI, P171
   Zeng XS, 2013, COMPUT-AIDED CIV INF, V28, P359, DOI 10.1111/mice.12000
   Zhang GX, 2014, INT J NEURAL SYST, V24, DOI 10.1142/S0129065714400061
   Zhou LR, 2013, COMPUT-AIDED CIV INF, V28, P210, DOI 10.1111/j.1467-8667.2012.00803.x
NR 81
TC 14
Z9 14
U1 0
U2 13
PU WORLD SCIENTIFIC PUBL CO PTE LTD
PI SINGAPORE
PA 5 TOH TUCK LINK, SINGAPORE 596224, SINGAPORE
SN 0129-0657
EI 1793-6462
J9 INT J NEURAL SYST
JI Int. J. Neural Syst.
PD SEP
PY 2016
VL 26
IS 6
AR 1650023
DI 10.1142/S0129065716500234
PG 16
WC Computer Science, Artificial Intelligence
SC Computer Science
GA DT2MN
UT WOS:000381314500002
PM 27354188
DA 2020-02-19
ER

PT J
AU Rebai, I
   BenAyed, Y
AF Rebai, Ilyes
   BenAyed, Yassine
TI Arabic speech synthesis and diacritic recognition
SO INTERNATIONAL JOURNAL OF SPEECH TECHNOLOGY
LA English
DT Article
DE Speech synthesis; Statistical parametric synthesis; Deep learning; Text
   diacritization; Non-uniform speech database
ID NEURAL-NETWORKS; RESTORATION
AB Text-to-speech system (TTS), known also as speech synthesizer, is one of the important technology in the last years due to the expanding field of applications. Several works on speech synthesizer have been made on English and French, whereas many other languages, including Arabic, have been recently taken into consideration. The area of Arabic speech synthesis has not sufficient progress and it is still in its first stage with a low speech quality. In fact, speech synthesis systems face several problems (e.g. speech quality, articulatory effect, etc.). Different methods were proposed to solve these issues, such as the use of large and different unit sizes. This method is mainly implemented with the concatenative approach to improve the speech quality and several works have proved its effectiveness. This paper presents an efficient Arabic TTS system based on statistical parametric approach and non-uniform units speech synthesis. Our system includes a diacritization engine. Modern Arabic text is written without mention the vowels, called also diacritic marks. Unfortunately, these marks are very important to define the right pronunciation of the text which explains the incorporation of the diacritization engine to our system. In this work, we propose a simple approach based on deep neural networks. Deep neural networks are trained to directly predict the diacritic marks and to predict the spectral and prosodic parameters. Furthermore, we propose a new simple stacked neural network approach to improve the accuracy of the acoustic models. Experimental results show that our diacritization system allows the generation of full diacritized text with high precision and our synthesis system produces high-quality speech.
C1 [Rebai, Ilyes; BenAyed, Yassine] Univ Sfax, Pole Technol, ISIMS, MIRACL,Multimedia Informat Syst & Adv Comp Lab, Route Tunis Km 10,BP 242, Sfax 3021, Tunisia.
RP Rebai, I (reprint author), Univ Sfax, Pole Technol, ISIMS, MIRACL,Multimedia Informat Syst & Adv Comp Lab, Route Tunis Km 10,BP 242, Sfax 3021, Tunisia.
EM rebai_ilyes@hotmail.fr
OI rebai, ilyes/0000-0003-4504-3146
CR Al-Said Ghadeer, 2009, Journal of Computer Sciences, V5, P207, DOI 10.3844/jcs.2009.207.213
   Alghamdi M, 2010, ARAB J SCI ENG, V35, P125
   Attia M., 2005, THESIS
   Badrashiny M., 2009, THESIS
   Ben Sassi S, 2001, ACS/IEEE INTERNATIONAL CONFERENCE ON COMPUTER SYSTEMS AND APPLICATIONS, PROCEEDINGS, P119, DOI 10.1109/AICCSA.2001.933962
   Chouireb F, 2008, SIGNAL IMAGE VIDEO P, V2, P73, DOI 10.1007/s11760-007-0038-z
   Ciresan D., 2011, COMPUTING RES REPOSI
   El-Imam YA, 2004, COMPUT SPEECH LANG, V18, P339, DOI 10.1016/S0885-2308(03)00035-4
   Elshafei M, 2002, INFORM SCIENCES, V140, P255, DOI 10.1016/S0020-0255(01)00175-X
   Elshafei M., 2006, P INT C MACH LEARN M, P128
   Fares T. S., 2007, INT J INTELLIGENT CO, V7, P49
   Fares TS, 2008, AIP CONF PROC, V1019, P93, DOI 10.1063/1.2953060
   Forti M, 2003, IEEE T CIRCUITS-I, V50, P1421, DOI 10.1109/TCSI.2003.818614
   Hamad Mazin, 2011, Proceedings of the 2011 IEEE Student Conference on Research and Development (SCOReD 2011), P409, DOI 10.1109/SCOReD.2011.6148774
   Harrat S., 2014, ISCA TUT RES WORKSH, P1
   Kantabutra V., 2006, INT C MACH LEARN MOD, P80
   Khalil K., 2013, P SAUD INT EL COMM P, P1
   Khorsheed M.S., 2012, J SOFTWARE ENG APPL, P124, DOI 10.4236/jsea.2012.512b024
   Kominek J., 2008, WORKSH SPOK LANG TEC, P1
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Martin K, 2013, P INT 2013, P2589
   Mnasri Z., 2010, SIGNAL PROCESSING IN, V4, P352
   Raghavendra E. V., 2010, NAT C COMM, P1
   Raitio T, 2011, IEEE T AUDIO SPEECH, V19, P153, DOI 10.1109/TASL.2010.2045239
   Rebai I., 2013, INT C ART INT SOUSS, P1
   Vinyals O, 2012, ADV NEURAL INFORM PR, V15, P2834
   Zen HG, 2013, INT CONF ACOUST SPEE, P7962, DOI 10.1109/ICASSP.2013.6639215
   Zitouni I, 2009, COMPUT SPEECH LANG, V23, P257, DOI 10.1016/j.csl.2008.06.001
NR 28
TC 0
Z9 0
U1 1
U2 3
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 1381-2416
EI 1572-8110
J9 INT J SPEECH TECHNOL
JI Int. J. Speech Technol.
PD SEP
PY 2016
VL 19
IS 3
BP 485
EP 494
DI 10.1007/s10772-016-9342-8
PG 10
WC Engineering, Electrical & Electronic
SC Engineering
GA DT0CV
UT WOS:000381151100005
DA 2020-02-19
ER

PT J
AU Qi, ZQ
   Wang, B
   Tian, YJ
   Zhang, P
AF Qi, Zhiquan
   Wang, Bo
   Tian, Yingjie
   Zhang, Peng
TI When Ensemble Learning Meets Deep Learning: a New Deep Support Vector
   Machine for Classification
SO KNOWLEDGE-BASED SYSTEMS
LA English
DT Article
DE Pattern recognition; Deep architectures; Support vector machine
ID MODEL
AB Recently, Deep Learning (DL) method has received a significant breakthrough in the data representation, whose success mainly depends on its deep structure. In this paper, we focus on the DL research based on Support Vector Machine (SVM), and first present an Ex-Adaboost learning strategy, and then propose a new Deep Support Vector Machine (called DeepSVM). Unlike other DL algorithms based on SVM, in each layer, Ex-Adaboost is applied to not only select SVMs with the minimal error rate and the highest diversity, but also to produce the weight for each feature. In this way, new training data is obtained. By stacking these SVMs into multiple layers following the same way, we finally acquire a new set of deep features that can greatly boost the classification performance. In the end, the training data represented by these new features is regarded as the input for a standard SVM classifier. In the experimental part, we offer these answers to the following questions: 1) is the deep structure of DeepSVM really useful for classification problem? 2) Does Ex-Adaboost work, and is it helpful for further improving on DeepSVM's performance with respect to the deep structure? 3) How much improvement in classification accuracy of DeepSVM, compared with other exist algorithms? (C) 2016 Elsevier B.V. All rights reserved.
C1 [Qi, Zhiquan; Wang, Bo; Tian, Yingjie] Chinese Acad Sci, Res Ctr Fictitious Econ & Data Sci, Beijing 100190, Peoples R China.
   [Qi, Zhiquan; Wang, Bo; Tian, Yingjie] Chinese Acad Sci, Key Lab Big Data Min & Knowledge Management, Beijing 100190, Peoples R China.
   [Zhang, Peng] Univ Technol Sydney, Sydney, NSW 2007, Australia.
RP Tian, YJ (reprint author), Chinese Acad Sci, Res Ctr Fictitious Econ & Data Sci, Beijing 100190, Peoples R China.; Tian, YJ (reprint author), Chinese Acad Sci, Key Lab Big Data Min & Knowledge Management, Beijing 100190, Peoples R China.
EM tyj@ucas.ac.cn
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61472390, 61402429, 11271361, 71331005, 91546201];
   CAS/SAFEA International Partnership Program for Creative Research Teams,
   Major International(Ragional) Joint Research Project [71110107026];
   Beijing Natural Science FoundationBeijing Natural Science Foundation
   [1162005]
FX This work has been partially supported by grants from National Natural
   Science Foundation of China (NO.61472390, NO.61402429, NO.11271361), key
   project of National Natural Science Foundation of China (NO.71331005,
   NO.91546201), the CAS/SAFEA International Partnership Program for
   Creative Research Teams, Major International(Ragional) Joint Research
   Project (NO.71110107026), and Beijing Natural Science Foundation (NO.
   1162005).
CR Abdullah A, 2009, 2009 INTERNATIONAL CONFERENCE OF SOFT COMPUTING AND PATTERN RECOGNITION, P301, DOI 10.1109/SoCPaR.2009.67
   Arnold L., 2011, ESANN
   Azamathulla HM, 2011, APPL SOFT COMPUT, V11, P2902, DOI 10.1016/j.asoc.2010.11.026
   Bengio Yoshua, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P437, DOI 10.1007/978-3-642-35289-8_26
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   BOURLARD H, 1988, BIOL CYBERN, V59, P291, DOI 10.1007/BF00332918
   Carrasco M, 2015, PATTERN RECOGN, V48, P1598, DOI 10.1016/j.patcog.2014.12.006
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Cho Y., 2009, ADV NEURAL INFORM PR, V22, P342, DOI DOI 10.1021/ed028p10
   Cho YM, 2010, NEURAL COMPUT, V22, P2678, DOI 10.1162/NECO_a_00018
   Ciresan DC, 2010, NEURAL COMPUT, V22, P3207, DOI 10.1162/NECO_a_00052
   Czibula G, 2014, APPL SOFT COMPUT, V18, P70, DOI 10.1016/j.asoc.2014.01.026
   Deng N. Y., 2009, SUPPORT VECTOR MACHI
   Ekici S, 2012, APPL SOFT COMPUT, V12, P1650, DOI 10.1016/j.asoc.2012.02.011
   Freund Y., 1999, Journal of Japanese Society for Artificial Intelligence, V14, P771
   FUKUSHIMA K, 1980, BIOL CYBERN, V36, P193, DOI 10.1007/BF00344251
   Goodfellow I., 2009, ADV NEURAL INFORM PR, V22, P646
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton G. E., 1994, ADV NEURAL INFORMATI, P3
   Hinton G. E., 2009, SCHOLARPEDIA, V4, P5947, DOI DOI 10.4249/scholarpedia.5947
   HINTON GE, 1989, ARTIF INTELL, V40, P185, DOI 10.1016/0004-3702(89)90049-0
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Japkowicz N, 2000, NEURAL COMPUT, V12, P531, DOI 10.1162/089976600300015691
   Jarrett K, 2009, IEEE I CONF COMP VIS, P2146, DOI 10.1109/ICCV.2009.5459469
   Kim S, 2015, NEURAL NETWORKS, V64, P19, DOI 10.1016/j.neunet.2014.09.007
   Le Cun B B, 1990, ADV NEURAL INFORM PR
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Murphy PM, 1992, UCI MACHINE LEARNING
   Raghavendra NS, 2014, APPL SOFT COMPUT, V19, P372, DOI 10.1016/j.asoc.2014.02.002
   Rumelhart D. E., 1988, LEARNING REPRESENTAT
   Sangwook Kim, 2013, Neural Information Processing. 20th International Conference, ICONIP 2013. Proceedings: LNCS 8226, P458, DOI 10.1007/978-3-642-42054-2_57
   Schwenk H., 1995, Advances in Neural Information Processing Systems 7, P991
   Simard PY, 2003, PROC INT CONF DOC, P958
   Socher R., 2011, ADV NEURAL INFORM PR, P801
   Stigler S. M., 1989, STAT SCI, V4, P73, DOI DOI 10.1214/SS/1177012580
   Tang Y., 2013, WORKSH CHALL REPR LE
   Tian YJ, 2014, IEEE T CYBERNETICS, V44, P1067, DOI 10.1109/TCYB.2013.2279167
   Vapnik, 1995, NATURE STAT LEARNING
   Vapnik V., 2006, ESTIMATION DEPENDENC
   Weston J., 2006, P 23 INT C MACH LEAR, P1009
   Wiering M, 2013, P INT WORKSH ADV REG
   Wiering M., 2013, P 25 BEN ART INT C B
NR 43
TC 18
Z9 18
U1 2
U2 74
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0950-7051
EI 1872-7409
J9 KNOWL-BASED SYST
JI Knowledge-Based Syst.
PD SEP 1
PY 2016
VL 107
BP 54
EP 60
DI 10.1016/j.knosys.2016.05.055
PG 7
WC Computer Science, Artificial Intelligence
SC Computer Science
GA DS2JN
UT WOS:000380595900005
DA 2020-02-19
ER

PT J
AU Deng, Y
   Bao, F
   Deng, XS
   Wang, RP
   Kong, YY
   Dai, QH
AF Deng, Yue
   Bao, Feng
   Deng, Xuesong
   Wang, Ruiping
   Kong, Youyong
   Dai, Qionghai
TI Deep and Structured Robust Information Theoretic Learning for Image
   Analysis
SO IEEE TRANSACTIONS ON IMAGE PROCESSING
LA English
DT Article
DE Data embedding; mutual information; deep learning; structured-sparse
   learning; image classification; brain MRI segmentation
ID FACE RECOGNITION; SEGMENTATION; MODEL; MRI; FRAMEWORK; SELECTION; SCENE
AB This paper presents a robust information theoretic (RIT) model to reduce the uncertainties, i.e., missing and noisy labels, in general discriminative data representation tasks. The fundamental pursuit of our model is to simultaneously learn a transformation function and a discriminative classifier that maximize the mutual information of data and their labels in the latent space. In this general paradigm, we, respectively, discuss three types of the RIT implementations with linear subspace embedding, deep transformation, and structured sparse learning. In practice, the RIT and deep RIT are exploited to solve the image categorization task whose performances will be verified on various benchmark data sets. The structured sparse RIT is further applied to a medical image analysis task for brain magnetic resonance image segmentation that allows group-level feature selections on the brain tissues.
C1 [Deng, Yue; Bao, Feng; Dai, Qionghai] Tsinghua Univ, Automat Dept, Tsinghua Natl Lab Informat Sci & Technol, Beijing 100084, Peoples R China.
   [Deng, Yue] Univ Calif San Francisco, San Francisco Med Ctr, San Francisco, CA 94158 USA.
   [Deng, Xuesong; Wang, Ruiping] Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
   [Kong, Youyong] Southeast Univ, Sch Comp Sci & Engn, Nanjing 210000, Jiangsu, Peoples R China.
RP Deng, Y (reprint author), Tsinghua Univ, Automat Dept, Tsinghua Natl Lab Informat Sci & Technol, Beijing 100084, Peoples R China.
EM yuedeng.thu@gmail.com; bf14@mails.tsinghua.edu.cn; dxuesong7@gmail.com;
   wangruiping@ict.ac.cn; kongyouyong@seu.edu.cn; qhdai@tsinghua.edu.cn
OI Bao, Feng/0000-0002-5721-9551
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61327902, 61120106003, 61379083]; National Science
   Foundation of Jiangsu Province, China [BK20150650]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61327902, Grant 61120106003, and Grant 61379083. The
   work of Y. Kong was supported by the National Science Foundation of
   Jiangsu Province, China, under Grant BK20150650. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Kiyoharu Aizawa. (Yue Deng and Feng Bao
   contributed equally to this work.)
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Ashburner J, 2005, NEUROIMAGE, V26, P839, DOI 10.1016/j.neuroimage.2005.02.018
   Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228
   BOHNING D, 1992, ANN I STAT MATH, V44, P197, DOI 10.1007/BF00048682
   Cai D., 2007, IEEE C COMP VIS ICCV, P1, DOI DOI 10.1109/ICCV.2007.4408856
   Cover T., 2006, ELEMENTS INFORM THEO
   Dahl GE, 2013, INT CONF ACOUST SPEE, P8609, DOI 10.1109/ICASSP.2013.6639346
   Dalal N, 2005, PROC CVPR IEEE, P886
   Deng Y, 2017, IEEE T CYBERNETICS, V47, P1434, DOI 10.1109/TCYB.2016.2547941
   Deng Y, 2017, IEEE T NEUR NET LEAR, V28, P653, DOI 10.1109/TNNLS.2016.2522401
   Deng Y, 2017, IEEE T FUZZY SYST, V25, P1006, DOI 10.1109/TFUZZ.2016.2574915
   Deng Y, 2015, IEEE T IND INFORM, V11, P467, DOI 10.1109/TII.2015.2404299
   Deng Y, 2014, IEEE T CYBERNETICS, V44, P1924, DOI 10.1109/TCYB.2014.2300192
   Deng Y, 2013, IEEE T NEUR NET LEAR, V24, P383, DOI 10.1109/TNNLS.2012.2235082
   Deng Y, 2012, IEEE J-STSP, V6, P566, DOI 10.1109/JSTSP.2012.2195472
   Deng Y, 2012, COMPUT VIS IMAGE UND, V116, P473, DOI 10.1016/j.cviu.2011.11.002
   Deng Y, 2011, IEEE T IMAGE PROCESS, V20, P2329, DOI 10.1109/TIP.2011.2109729
   Dogdas B, 2005, HUM BRAIN MAPP, V26, P273, DOI 10.1002/hbm.20159
   Donahue Jeff, 2013, DECAF DEEP CONVOLUTI
   Fei-Fei L, 2005, PROC CVPR IEEE, P524
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464
   Giraldo L. G. S., 2013, RATE DISTORTION AUTO
   Gomes R. G., 2010, ADV NEURAL INFORM PR, P775
   Grandvalet Y., 2005, P ADV NEUR INF PROC, V17, P1
   Greenspan H, 2006, IEEE T MED IMAGING, V25, P1233, DOI 10.1109/TMI.2006.880668
   Guyon Isabelle, 2003, J MACH LEARN RES, V3, P1157, DOI DOI 10.1162/153244303322753616
   Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814
   He LL, 2013, NEUROIMAGE, V64, P328, DOI 10.1016/j.neuroimage.2012.08.081
   Jenatton R., 2010, P 13 INT C ART INT S, P366
   Jenatton R, 2011, J MACH LEARN RES, V12, P2777
   Kong YY, 2015, IEEE SIGNAL PROC LET, V22, P573, DOI 10.1109/LSP.2014.2364612
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kwan RKS, 1999, IEEE T MED IMAGING, V18, P1085, DOI 10.1109/42.816072
   LeCun Yann, 1990, ADV NEURAL INFORM PR, P396, DOI DOI 10.1111/DSU.12130
   Lee H., 2007, ADV NEURAL INF PROCE, P801
   Lee KC, 2005, IEEE T PATTERN ANAL, V27, P684, DOI 10.1109/TPAMI.2005.92
   Liao S, 2009, IEEE T IMAGE PROCESS, V18, P1107, DOI 10.1109/TIP.2009.2015682
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mairal J, 2012, IEEE T PATTERN ANAL, V34, P791, DOI 10.1109/TPAMI.2011.156
   MOORE BC, 1981, IEEE T AUTOMAT CONTR, V26, P17, DOI 10.1109/TAC.1981.1102568
   Nene S. A., 1996, CUCS00696
   NEWTON MA, 1994, J R STAT SOC B, V56, P3
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Rohlfing T, 2012, IEEE T MED IMAGING, V31, P153, DOI 10.1109/TMI.2011.2163944
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sharma A, 2012, PROC CVPR IEEE, P2160, DOI 10.1109/CVPR.2012.6247923
   Si S, 2010, IEEE T KNOWL DATA EN, V22, P929, DOI 10.1109/TKDE.2009.126
   Torkkola K., 2003, Journal of Machine Learning Research, V3, P1415, DOI 10.1162/153244303322753742
   van Gemert JC, 2010, IEEE T PATTERN ANAL, V32, P1271, DOI 10.1109/TPAMI.2009.132
   Wang RP, 2011, IEEE T PATTERN ANAL, V33, P1776, DOI 10.1109/TPAMI.2011.39
   Yan SC, 2007, IEEE T PATTERN ANAL, V29, P40, DOI 10.1109/TPAMI.2007.250598
   Zhang TH, 2008, LECT NOTES COMPUT SC, V5302, P725, DOI 10.1007/978-3-540-88682-2_55
   Zhang YY, 2001, IEEE T MED IMAGING, V20, P45, DOI 10.1109/42.906424
   Zhou B, 2014, ADV NEURAL INFORM PR, P487, DOI DOI 10.1162/153244303322533223
NR 55
TC 11
Z9 11
U1 1
U2 39
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1057-7149
EI 1941-0042
J9 IEEE T IMAGE PROCESS
JI IEEE Trans. Image Process.
PD SEP
PY 2016
VL 25
IS 9
BP 4209
EP 4221
DI 10.1109/TIP.2016.2588330
PG 13
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
SC Computer Science; Engineering
GA DS1KX
UT WOS:000380355600005
PM 27392359
DA 2020-02-19
ER

PT J
AU Bharati, A
   Singh, R
   Vatsa, M
   Bowyer, KW
AF Bharati, Aparna
   Singh, Richa
   Vatsa, Mayank
   Bowyer, Kevin W.
TI Detecting Facial Retouching Using Supervised Deep Learning
SO IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY
LA English
DT Article
DE Image forensics; face recognition; face image retouching; face image
   alteration; biometric spoofing
AB Digitally altering, or retouching, face images is a common practice for images on social media, photo sharing websites, and even identification cards when the standards are not strictly enforced. This research demonstrates the effect of digital alterations on the performance of automatic face recognition, and also introduces an algorithm to classify face images as original or retouched with high accuracy. We first introduce two face image databases with unaltered and retouched images. Face recognition experiments performed on these databases show that when a retouched image is matched with its original image or an unaltered gallery image, the identification performance is considerably degraded, with a drop in matching accuracy of up to 25%. However, when images are retouched with the same style, the matching accuracy can be misleadingly high in comparison with matching original images. To detect retouching in face images, a novel supervised deep Boltzmann machine algorithm is proposed. It uses facial parts to learn discriminative features to classify face images as original or retouched. The proposed approach for classifying images as original or retouched yields an accuracy of over 87% on the data sets introduced in this paper and over 99% on three other makeup data sets used by previous researchers. This is a substantial increase in accuracy over the previous state-of-the-art algorithm, which has shown <50% accuracy in classifying original and retouched images from the ND-IIITD retouched faces database.
C1 [Bharati, Aparna; Singh, Richa; Vatsa, Mayank] Indraprastha Inst Informat Technol, Delhi 110020, India.
   [Bharati, Aparna; Bowyer, Kevin W.] Univ Notre Dame, Notre Dame, IN 46556 USA.
RP Bharati, A (reprint author), Indraprastha Inst Informat Technol, Delhi 110020, India.; Bharati, A (reprint author), Univ Notre Dame, Notre Dame, IN 46556 USA.
EM abharati@nd.edu; rsingh@iiitd.ac.in; mayank@iiitd.ac.in; kwb@nd.edu
RI Vatsa, Mayank/I-5050-2013; Singh, Richa/M-9961-2017
OI Vatsa, Mayank/0000-0001-5952-2274; Singh, Richa/0000-0003-4060-4573
CR Cheng C.F., 2013, EVID-BASED COMPL ALT, V2013, P1, DOI DOI 10.1103/PHYSREVLETT.110.225002
   CHEW HG, 2004, OPTIMIZATION CONTROL
   Dantcheva A., 2012, 2012 IEEE Fifth International Conference On Biometrics: Theory, Applications And Systems (BTAS 2012), P391, DOI 10.1109/BTAS.2012.6374605
   Dhamecha TI, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0099212
   Ferrara M, 2013, LECT NOTES COMPUT SC, V8156, P743
   Ferreira M, 2016, INT J ADV MANUF TECH, V85, P57, DOI 10.1007/s00170-014-6026-x
   Flynn PJ, 2003, LECT NOTES COMPUT SC, V2688, P44
   Kee E, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2629646
   Kee E, 2011, P NATL ACAD SCI USA, V108, P19907, DOI 10.1073/pnas.1110747108
   Klontz J. C., 2013, BTAS, P1
   Kose Neslihan, 2015, 2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG), P1, DOI 10.1109/FG.2015.7163104
   Larochelle H., 2008, P 25 INT C MACH LEAR, P536, DOI [10.1145/1390156.1390224, DOI 10.1145/1390156.1390224]
   Marcel S, 2014, ADV COMPUT VIS PATT, P1, DOI 10.1007/978-1-4471-6524-8
   SCHMAH T., 2008, ADV NEURAL INFORM PR, P1409
   Singh R, 2010, IEEE T INF FOREN SEC, V5, P441, DOI 10.1109/TIFS.2010.2054083
NR 15
TC 19
Z9 20
U1 1
U2 29
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1556-6013
EI 1556-6021
J9 IEEE T INF FOREN SEC
JI IEEE Trans. Inf. Forensic Secur.
PD SEP
PY 2016
VL 11
IS 9
BP 1903
EP 1913
DI 10.1109/TIFS.2016.2561898
PG 11
WC Computer Science, Theory & Methods; Engineering, Electrical & Electronic
SC Computer Science; Engineering
GA DS0ZY
UT WOS:000380326700002
DA 2020-02-19
ER

PT J
AU Papa, JP
   Scheirer, W
   Cox, DD
AF Papa, Joao Paulo
   Scheirer, Walter
   Cox, David Daniel
TI Fine-tuning Deep Belief Networks using Harmony Search
SO APPLIED SOFT COMPUTING
LA English
DT Article
DE Restricted Boltzmann Machines; Deep Belief Networks; Harmony Search;
   Meta-heuristics
ID RESTRICTED BOLTZMANN MACHINES; ALGORITHM
AB In this paper, we deal with the problem of Deep Belief Networks (DBNs) parameters fine-tuning by means of a fast meta-heuristic approach named Harmony Search (HS). Although such deep learning-based technique has been widely used in the last years, more detailed studies about how to set its parameters may not be observed in the literature. We have shown we can obtain more accurate results comparing HS against with several of its variants, a random search and two variants of the well-known Hyperopt library. The experimental results were carried out in two public datasets considering the task of binary image reconstruction, three DBN learning algorithms and three layers. (C) 2015 Elsevier B.V. All rights reserved.
C1 [Papa, Joao Paulo] UNESP Univ Estadual Paulista, Dept Comp, Bauru, Brazil.
   [Scheirer, Walter; Cox, David Daniel] Harvard Univ, Ctr Brain Sci, Cambridge, MA 02138 USA.
RP Papa, JP (reprint author), UNESP Univ Estadual Paulista, Dept Comp, Bauru, Brazil.
EM papa@fc.unesp.br; wscheirer@fas.harvard.edu; davidcox@fas.harvard.edu
RI Papa, Joao Paulo/C-4849-2012
OI Papa, Joao Paulo/0000-0002-6494-7514
FU FAPESPFundacao de Amparo a Pesquisa do Estado de Sao Paulo (FAPESP)
   [2013/20387-7, 2014/16250-9]; CNPqNational Council for Scientific and
   Technological Development (CNPq) [303182/2011-3, 470571/2013-6,
   306166/2014-3]
FX The authors are grateful to FAPESP grants #2013/20387-7 and
   #2014/16250-9, and CNPq grants #303182/2011-3, #470571/2013-6 and
   #306166/2014-3.
CR Ackley D.H., 1988, CONNECTIONIST MODELS, P285
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Berger J, 2014, PHILOS PSYCHOL, V27, P829, DOI 10.1080/09515089.2013.771241
   Brakel Philemon, 2012, Artificial Neural Networks and Machine Learning - ICANN 2012. 22nd International Conference on Artificial Neural Networks, P92, DOI 10.1007/978-3-642-33266-1_12
   Cho K.H., 2010, P 2010 INT JOINT C N, P1
   Fischer A, 2014, PATTERN RECOGN, V47, P25, DOI 10.1016/j.patcog.2013.05.025
   Fuqianga C., 2014, PUBL ASTRON SOC AUST, V31, P1
   Geem ZW, 2010, APPL MATH COMPUT, V217, P3881, DOI 10.1016/j.amc.2010.09.049
   Hinton G., 2012, NEURAL NETWORKS TRIC, V9, P599, DOI DOI 10.1007/978-3-642-35289-8_32
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Kulluk S, 2011, PROCEDIA COMPUT SCI, V3, DOI 10.1016/j.procs.2010.12.048
   Kuremoto T, 2012, COMM COM INF SC, V304, P17
   Larochelle H, 2012, J MACH LEARN RES, V13, P643
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Levy E, 2014, GECCO'14: PROCEEDINGS OF THE 2014 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P1143, DOI 10.1145/2576768.2598287
   Liu K, 2014, APPL MECH MATER, V556-562, P2085, DOI 10.4028/www.scientific.net/AMM.556-562.2085
   Mahdavi M, 2007, APPL MATH COMPUT, V188, P1567, DOI 10.1016/j.amc.2006.11.033
   Omran MGH, 2008, APPL MATH COMPUT, V198, P643, DOI 10.1016/j.amc.2007.09.004
   Pan QK, 2010, APPL MATH COMPUT, V216, P830, DOI 10.1016/j.amc.2010.01.088
   Papa J. P., 2015, P GEN EV COMP C GECC, P1449
   Papa JP, 2015, J COMPUT SCI-NETH, V9, P14, DOI 10.1016/j.jocs.2015.04.014
   Salakhutdinov R, 2012, NEURAL COMPUT, V24, P1967, DOI 10.1162/NECO_a_00311
   Sarikaya R, 2014, IEEE-ACM T AUDIO SPE, V22, P778, DOI 10.1109/TASLP.2014.2303296
   Tieleman T., 2008, P 25 INT C MACH LEAR, P1064, DOI DOI 10.1145/1390156.1390290
   Tieleman T., 2009, P 26 ANN INT C MACH, P1033, DOI DOI 10.1145/1553374.1553506
   Welling M., 2005, ADV NEURAL INFORM PR, P1481
   WILCOXON F, 1945, BIOMETRICS BULL, V1, P80, DOI 10.2307/3001968
   Yang XS, 2009, STUD COMPUT INTELL, V191, P1, DOI 10.1109/MILCOM.2009.5379772
   Yosinski J., 2012, REPR LEARN WORKSH IN, P1
   Zhang CX, 2014, PATTERN RECOGN LETT, V36, P161, DOI 10.1016/j.patrec.2013.10.009
   Zhou SS, 2010, IEEE IMAGE PROC, P1561, DOI 10.1109/ICIP.2010.5649922
   Zou DX, 2010, COMPUT IND ENG, V58, P307, DOI 10.1016/j.cie.2009.11.003
NR 34
TC 38
Z9 42
U1 3
U2 52
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 1568-4946
EI 1872-9681
J9 APPL SOFT COMPUT
JI Appl. Soft. Comput.
PD SEP
PY 2016
VL 46
BP 875
EP 885
DI 10.1016/j.asoc.2015.08.043
PG 11
WC Computer Science, Artificial Intelligence; Computer Science,
   Interdisciplinary Applications
SC Computer Science
GA DO7YU
UT WOS:000377999900063
OA Green Published
DA 2020-02-19
ER

PT J
AU Wei, YC
   Xia, W
   Lin, M
   Huang, JS
   Ni, BB
   Dong, J
   Zhao, Y
   Yan, SC
AF Wei, Yunchao
   Xia, Wei
   Lin, Min
   Huang, Junshi
   Ni, Bingbing
   Dong, Jian
   Zhao, Yao
   Yan, Shuicheng
TI HCP: A Flexible CNN Framework for Multi-Label Image Classification
SO IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
LA English
DT Article
DE Deep Learning; CNN; Multi-label Classification
AB Convolutional Neural Network (CNN) has demonstrated promising performance in single-label image classification tasks. However, how CNN best copes with multi-label images still remains an open problem, mainly due to the complex underlying object layouts and insufficient multi-label training images. In this work, we propose a flexible deep CNN infrastructure, called Hypotheses-CNN-Pooling (HCP), where an arbitrary number of object segment hypotheses are taken as the inputs, then a shared CNN is connected with each hypothesis, and finally the CNN output results from different hypotheses are aggregated with max pooling to produce the ultimate multi-label predictions. Some unique characteristics of this flexible deep CNN infrastructure include: 1) no ground-truth bounding box information is required for training; 2) the whole HCP infrastructure is robust to possibly noisy and/or redundant hypotheses; 3) the shared CNN is flexible and can be well pre-trained with a large-scale single-label image dataset, e.g., ImageNet; and 4) it may naturally output multi-label prediction results. Experimental results on Pascal VOC 2007 and VOC 2012 multi-label image datasets well demonstrate the superiority of the proposed HCP infrastructure over other state-of-the-arts. In particular, the mAP reaches 90.5% by HCP only and 93.2% after the fusion with our complementary result in [12] based on hand-crafted features on the VOC 2012 dataset.
C1 [Wei, Yunchao; Zhao, Yao] Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.
   [Wei, Yunchao; Zhao, Yao] Beijing Key Lab Adv Informat Sci & Network Techno, Beijing 100044, Peoples R China.
   [Wei, Yunchao; Xia, Wei; Lin, Min; Huang, Junshi; Dong, Jian; Zhao, Yao; Yan, Shuicheng] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 117548, Singapore.
   [Ni, Bingbing] Shanghai Jiao Tong Univ, Dept Elect Engn, Shanghai 200030, Peoples R China.
RP Wei, YC (reprint author), Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.; Wei, YC (reprint author), Beijing Key Lab Adv Informat Sci & Network Techno, Beijing 100044, Peoples R China.; Wei, YC (reprint author), Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 117548, Singapore.
EM wychao1987@gmail.com; weixiaee@gmail.com; mavenlin@gmail.com;
   junshi.huang@gmail.com; bingbing.ni@adsc.com.sg; djtcut@gmail.com;
   yzhao@bjtu.edu.cn; eleyans@nus.edu.sg
FU National Basic Research Program of ChinaNational Basic Research Program
   of China [2012CB316400]; Fundamental Scientific Research Project
   [K15JB00360]; National NSF of ChinaNational Natural Science Foundation
   of China [61210006, 61532005]
FX This work is supported in part by National Basic Research Program of
   China (No. 2012CB316400), Fundamental Scientific Research Project (No.
   K15JB00360), National NSF of China (61210006, 61532005).
CR Alexe B, 2012, IEEE T PATTERN ANAL, V34, P2189, DOI 10.1109/TPAMI.2012.28
   Arbelaez P, 2014, PROC CVPR IEEE, P328, DOI 10.1109/CVPR.2014.49
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Carreira J, 2012, IEEE T PATTERN ANAL, V34, P1312, DOI 10.1109/TPAMI.2011.231
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chatfield K., 2014, ARXIV14053531
   Chen Q, 2015, IEEE T PATTERN ANAL, V37, P13, DOI 10.1109/TPAMI.2014.2343217
   Chen Q, 2012, PROC CVPR IEEE, P3426, DOI 10.1109/CVPR.2012.6248083
   Cheng MM, 2014, PROC CVPR IEEE, P3286, DOI 10.1109/CVPR.2014.414
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Donahue J., 2013, ARXIV13101531
   Dong J, 2013, PROC CVPR IEEE, P827, DOI 10.1109/CVPR.2013.112
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Girshick R., 2013, ARXIV13112524
   Gong Y., 2014, ARXIV14031840
   Gong Yunchao, 2013, ARXIV13124894
   Griffin G., 2007, CALTECH 256 OBJECT C
   Harzallah H, 2009, IEEE I CONF COMP VIS, P237, DOI 10.1109/ICCV.2009.5459257
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI 10.1007/978-3-319-10578-9_23
   Jarrett K, 2009, IEEE I CONF COMP VIS, P2146, DOI 10.1109/ICCV.2009.5459469
   Jia Y., 2013, CAFFE OPEN SOURCE CO
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lazebnik S., 2006, P IEEE INT C COMP VI, V2, P2169, DOI DOI 10.1109/CVPR.2006.68
   LeCun Y, 2004, PROC CVPR IEEE, P97
   LeCun Yann, 1990, ADV NEURAL INFORM PR
   Lee H., 2009, P 26 ANN INT C MACH, P609, DOI DOI 10.1145/1553374.1553453
   Li FF, 2007, COMPUT VIS IMAGE UND, V106, P59, DOI 10.1016/j.cviu.2005.09.012
   Lin M., 2013, ARXIV13124400
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Oquab M., 2014, HAL01015140
   Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222
   Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11
   Razavian A. S., 2014, ARXIV14036382
   Sermanet P., 2013, ARXIV13126229
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Szegedy C., 2014, ARXIV14094842
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Wang JJ, 2010, PROC CVPR IEEE, P3360, DOI 10.1109/CVPR.2010.5540018
   Xia W, 2013, IEEE I CONF COMP VIS, P2176, DOI 10.1109/ICCV.2013.271
   Xia W, 2015, IEEE T CIRC SYST VID, V25, P582, DOI 10.1109/TCSVT.2014.2359134
   Xu ZW, 2015, PROC CVPR IEEE, P1798, DOI 10.1109/CVPR.2015.7298789
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
NR 44
TC 117
Z9 118
U1 17
U2 93
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 0162-8828
EI 1939-3539
J9 IEEE T PATTERN ANAL
JI IEEE Trans. Pattern Anal. Mach. Intell.
PD SEP
PY 2016
VL 38
IS 9
BP 1901
EP 1907
DI 10.1109/TPAMI.2015.2491929
PG 7
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
SC Computer Science; Engineering
GA DT4EK
UT WOS:000381432700014
PM 26513778
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Palangi, H
   Ward, R
   Deng, L
AF Palangi, Hamid
   Ward, Rabab
   Deng, Li
TI Distributed Compressive Sensing: A Deep Learning Approach
SO IEEE TRANSACTIONS ON SIGNAL PROCESSING
LA English
DT Article
DE Compressive sensing; deep learning; long short-term memory
ID SIMULTANEOUS SPARSE APPROXIMATION; RECOVERY; ALGORITHMS; NETWORKS; MODEL
AB Several recent studies on the compressed sensing problem with Multiple Measurement Vectors (MMVs) under the condition that the vectors in the different channels are jointly sparse have been recently carried. In this paper, this condition is relaxed. Instead, these sparse vectors are assumed to depend on each other but this dependency is assumed unknown. We capture this dependency by computing the conditional probability of each entry in each vector being non-zero, given the "residuals" of all previous vectors. To estimate these probabilities, we propose the use of the long short-term memory (LSTM), a data-driven model for sequence modeling that is deep in time. To learn the model parameters, we minimize a cross-entropy cost function. To reconstruct the sparse vectors at the decoder, we propose a greedy solver that uses the above model to estimate the conditional probabilities. By performing extensive experiments on two real world datasets, we show that the proposed method significantly outperforms the general MMV solver (the Simultaneous Orthogonal Matching Pursuit (SOMP)) and a number of the model-based Bayesian methods. The proposed method does not add any complexity to the general compressive sensing encoder. The trained model is used at the decoder only. As the proposed method is a data-driven method, it is only applicable when training data is available. In many applications however, training data is indeed available, e.g., in recorded images for which our method is successfully applied as to be reported in this paper.
C1 [Palangi, Hamid; Ward, Rabab] Univ British Columbia, Dept Elect & Comp Engn, Vancouver, BC V6T 1Z4, Canada.
   [Deng, Li] Microsoft Res, Redmond, WA 98052 USA.
RP Palangi, H (reprint author), Univ British Columbia, Dept Elect & Comp Engn, Vancouver, BC V6T 1Z4, Canada.
EM hamidp@ece.ubc.ca; rababw@ece.ubc.ca; deng@microsoft.com
RI Jeong, Yongwook/N-7413-2016
CR Angelosante D., 2009, P 16 INT C DIG SIGN, P1
   Baraniuk RG, 2007, IEEE SIGNAL PROC MAG, V24, P118, DOI 10.1109/MSP.2007.4286571
   Baraniuk RG, 2010, IEEE T INFORM THEORY, V56, P1982, DOI 10.1109/TIT.2010.2040894
   Bengio Y., 2013, ICASSP VANC MAY
   Candes EJ, 2006, COMMUN PUR APPL MATH, V59, P1207, DOI 10.1002/cpa.20124
   Candes EJ, 2011, APPL COMPUT HARMON A, V31, P59, DOI 10.1016/j.acha.2010.10.002
   Dahl GE, 2012, IEEE T AUDIO SPEECH, V20, P30, DOI 10.1109/TASL.2011.2134090
   Davies ME, 2012, IEEE T INFORM THEORY, V58, P1135, DOI 10.1109/TIT.2011.2173722
   DENG L, 1994, NEURAL NETWORKS, V7, P331, DOI 10.1016/0893-6080(94)90027-2
   Deng L, 2013, FOUND TRENDS SIGNAL, V7, pI, DOI 10.1561/2000000039
   Deng L, 2012, INT CONF ACOUST SPEE, P2133, DOI 10.1109/ICASSP.2012.6288333
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Duarte MF, 2011, IEEE T SIGNAL PROCES, V59, P4053, DOI 10.1109/TSP.2011.2161982
   Eldar YC, 2010, IEEE T INFORM THEORY, V56, P505, DOI 10.1109/TIT.2009.2034789
   ELMAN JL, 1990, COGNITIVE SCI, V14, P179, DOI 10.1016/0364-0213(90)90002-E
   Fang J., 2015, 2 DIMENSIONAL PATTER
   Fang J, 2014, INT CONF DIGIT SIG, P705, DOI 10.1109/ICDSP.2014.6900755
   Faul AC, 2002, ADV NEUR IN, V14, P383
   Gers FA, 2000, NEURAL COMPUT, V12, P2451, DOI 10.1162/089976600300015015
   Gers FA, 2003, J MACH LEARN RES, V3, P115, DOI 10.1162/153244303768966139
   Graves A., 2012, P REPR LEARN WORKSH
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Ji SH, 2008, IEEE T SIGNAL PROCES, V56, P2346, DOI 10.1109/TSP.2007.914345
   Ji SH, 2009, IEEE T SIGNAL PROCES, V57, P92, DOI 10.1109/TSP.2008.2005866
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Merhej D, 2011, IEEE T NEURAL NETWOR, V22, P1638, DOI 10.1109/TNN.2011.2164810
   Mesnil G., 2013, INTERSPEECH LYON AUG
   Mikolov T, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 1-2, P1045
   Mousavi A., 2015, DEEP LEARNING APPROA
   Nesterov Y., 1983, SOV MATH DOKL, V27, P372
   Palangi H., 2016, SIGNAL PROC IN PRESS
   Palangi H., 2013, P IEEE INT C AC SPEE
   Palangi H., 2013, P NIPS WORKSH DEEP L
   Palangi H, 2016, INT CONF ACOUST SPEE, P2692, DOI 10.1109/ICASSP.2016.7472166
   Palangi H, 2016, IEEE-ACM T AUDIO SPE, V24, P694, DOI 10.1109/TASLP.2016.2520371
   Palangi H, 2014, 2014 IEEE CHINA SUMMIT & INTERNATIONAL CONFERENCE ON SIGNAL AND INFORMATION PROCESSING (CHINASIP), P510, DOI 10.1109/ChinaSIP.2014.6889295
   Palangi H, 2013, INT CONF ACOUST SPEE, P3337, DOI 10.1109/ICASSP.2013.6638276
   ROBINSON AJ, 1994, IEEE T NEURAL NETWOR, V5, P298, DOI 10.1109/72.279192
   Sutskever I., 2013, P 30 INT C MACH LEAR, P1139
   Tipping ME, 2001, J MACH LEARN RES, V1, P211, DOI 10.1162/15324430152748236
   Tropp JA, 2006, SIGNAL PROCESS, V86, P572, DOI 10.1016/j.sigpro.2005.05.030
   Tropp JA, 2006, SIGNAL PROCESS, V86, P589, DOI 10.1016/j.sigpro.2005.05.031
   Vaswani N, 2010, IEEE T SIGNAL PROCES, V58, P4108, DOI 10.1109/TSP.2010.2048105
   Vincent P., P ICML, P1096
   Vincent P, 2011, NEURAL COMPUT, V23, P1661, DOI 10.1162/NECO_a_00142
   Wipf DP, 2007, IEEE T SIGNAL PROCES, V55, P3704, DOI 10.1109/TSP.2007.894265
   Yu D, 2015, SIGNALS COMMUN TECHN, P1, DOI 10.1007/978-1-4471-5779-3
   Zhang ZL, 2011, IEEE J-STSP, V5, P912, DOI 10.1109/JSTSP.2011.2159773
NR 50
TC 20
Z9 21
U1 6
U2 80
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1053-587X
EI 1941-0476
J9 IEEE T SIGNAL PROCES
JI IEEE Trans. Signal Process.
PD SEP 1
PY 2016
VL 64
IS 17
BP 4504
EP 4518
DI 10.1109/TSP.2016.2557301
PG 15
WC Engineering, Electrical & Electronic
SC Engineering
GA DT4FE
UT WOS:000381434700010
DA 2020-02-19
ER

PT J
AU Hao, X
   Zhang, GG
   Ma, S
AF Hao, Xing
   Zhang, Guigang
   Ma, Shang
TI Deep Learning
SO INTERNATIONAL JOURNAL OF SEMANTIC COMPUTING
LA English
DT Article
DE Deep learning; neural networks; training
ID NETWORKS
AB Deep learning is a branch of machine learning that tries to model high-level abstractions of data using multiple layers of neurons consisting of complex structures or non-liner transformations. With the increase of the amount of data and the power of computation, neural networks with more complex structures have attracted widespread attention and been applied to various fields. This paper provides an overview of deep learning in neural networks including popular architecture models and training algorithms.
C1 [Hao, Xing; Zhang, Guigang; Ma, Shang] Univ Calif Irvine, EECS, Irvine, CA 92617 USA.
   [Zhang, Guigang] Chinese Acad Sci, Inst Automat, Beijing 100190, Peoples R China.
RP Hao, X (reprint author), Univ Calif Irvine, EECS, Irvine, CA 92617 USA.
EM xingh2@uci.edu; guigang.zhang@ia.ac.cn; shangm@uci.edu
CR Adorf J., 2013, WEB SPEECH API
   Atlas L. E., 1988, P NEUR INF PROC SYST, P31
   Auli M., 2013, P 2013 C EMP METH NA, P1044
   Behnke S., 2003, HIERARCHICAL NEURAL, P2766
   Bengio Y, 2003, J MACH LEARN RES, V3, P1137, DOI 10.1162/153244303322533223
   Bruna J, 2013, IEEE T PATTERN ANAL, V35, P1872, DOI 10.1109/TPAMI.2012.230
   Chalasani Rakesh, 2013, ARXIV13013541
   Chellapilla K., 2006, P 10 INT WORKSH FRON
   Cho K., ARXIV1406
   Cires D. C., 2011, P 22 INT JOINT C ART, V22, P1237, DOI DOI 10.5591/978-1-57735-516-8/IJCAI11-210
   Courville A. C., 2011, AISTATS, V1, P5
   Deng  L., 2011, INTERSPEECH
   Deng L, 2013, INT CONF ACOUST SPEE, P3153, DOI 10.1109/ICASSP.2013.6638239
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Dunn A., 2007, PRO MICROSOFT SPEECH
   ELMAN JL, 1990, COGNITIVE SCI, V14, P179, DOI 10.1016/0364-0213(90)90002-E
   FUKUSHIMA K, 1980, BIOL CYBERN, V36, P193, DOI 10.1007/BF00344251
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Goffin V, 2005, INT CONF ACOUST SPEE, P1033
   GRAUPE D, 1989, 1989 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOLS 1-3, P1008, DOI 10.1109/ISCAS.1989.100522
   Graupe D., 1988, Proceedings of the 27th IEEE Conference on Decision and Control (IEEE Cat. No.88CH2531-2), P343, DOI 10.1109/CDC.1988.194325
   Graves A, 2009, IEEE T PATTERN ANAL, V31, P855, DOI 10.1109/TPAMI.2008.137
   Graves Alex, 2014, P 31 INT C MACH LEAR, P1764, DOI DOI 10.1145/1143844.1143891
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI 10.1007/978-3-319-10578-9_23
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Hinton GE, 2007, TRENDS COGN SCI, V11, P428, DOI 10.1016/j.tics.2007.09.004
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   HUANG Q, 1989, PROCEEDINGS OF THE 28TH IEEE CONFERENCE ON DECISION AND CONTROL, VOLS 1-3, P266, DOI 10.1109/CDC.1989.70115
   Jaeger H, 2001, 14834 GMD GERM NAT R, V148, P34
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Kang Kai, 2014, ARXIV14114464
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Koutnik J., 2014, P 31 INT C MACH LEAR, P1845
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li J., 2015, ARXIV150100777
   Lin M., 2013, ARXIV13124400
   Liu SY, 2014, ADV MECH ENG, DOI 10.1155/2014/868041
   Mikolov Tomas, 2010, NTFRSPEECH, V2, P3
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Ouyang W, 2014, ARXIV14093505
   Rumelhart D. E., 1998, COGNITIVE MODELING, V5, P1
   Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093
   Sermanet P., 2013, ARXIV13126229
   Shao J, 2015, PROC CVPR IEEE, P4657, DOI 10.1109/CVPR.2015.7299097
   Shujie L., 2014, ACL
   Simard PY, 2003, PROC INT CONF DOC, P958
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Simonyan K., 2014, ADV NEURAL INFORM PR, P568, DOI DOI 10.1109/ICCVW.2017.368
   Strigl D, 2010, EUROMICRO WORKSHOP P, P317, DOI 10.1109/PDP.2010.43
   Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244
   Sutskever I., 2011, P 28 INT C MACH LEAR, P1017
   Szegedy C., 2016, U. S. Patent, Patent No. [9,275,308, 9275308]
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Venugopalan S., ARXIV1412
   Vincent P, 2008, P 25 INT C MACH LEAR, P1096, DOI DOI 10.1145/1390156.1390294
   Zhou S., 2015, P 7 ACM INT C INT M
NR 58
TC 6
Z9 6
U1 1
U2 25
PU WORLD SCIENTIFIC PUBL CO PTE LTD
PI SINGAPORE
PA 5 TOH TUCK LINK, SINGAPORE 596224, SINGAPORE
SN 1793-351X
EI 1793-7108
J9 INT J SEMANT COMPUT
JI Int. J. Semant. Comput.
PD SEP
PY 2016
VL 10
IS 3
SI SI
BP 417
EP 439
DI 10.1142/S1793351X16500045
PG 23
WC Computer Science, Artificial Intelligence
SC Computer Science
GA EE5NW
UT WOS:000389655900008
DA 2020-02-19
ER

PT J
AU Li, QD
   Jin, ZP
   Wang, C
   Zeng, DD
AF Li, Qiudan
   Jin, Zhipeng
   Wang, Can
   Zeng, Daniel Dajun
TI Mining opinion summarizations using convolutional neural networks in
   Chinese microblogging systems
SO KNOWLEDGE-BASED SYSTEMS
LA English
DT Article
DE Chinese microblogging systems; Hot topics; Convolutional neural network;
   Opinion summarization; Maximal marginal relevance
ID SENTIMENT ANALYSIS
AB Chinese microblogging is an increasingly popular social media platform. Accurately summarizing representative opinions from microblogs can increase understanding of the semantics of opinions. The unique challenges of Chinese opinion summarization in microblogging systems are automatic learning of important features and selection of representative sentences. Deep-learning methods can automatically discover multiple levels of representations from raw data instead of requiring manual engineering. However, there have been very few systematic studies on sentiment analysis of Chinese hot topics using deep-learning methods. Based on the latest deep-learning research, in this paper, we propose a convolutional neural network (CNN)-based opinion summarization method for Chinese microblogging systems. The model first applies CNN to automatically mine useful features and perform sentiment analysis; then, by making good use of the obtained sentiment features, the semantic relationships among features are computed according to a hybrid ranking function; and finally, representative opinion sentences that are semantically related to the features are extracted using Maximal Marginal Relevance, which meets "relevant novelty" requirements. Experimental results on two real-world datasets verify the efficacy of the proposed model. (C) 2016 Elsevier B.V. All rights reserved.
C1 [Li, Qiudan; Jin, Zhipeng; Wang, Can; Zeng, Daniel Dajun] Chinese Acad Sci, Inst Automat, State Key Lab Management & Control Complex Syst, Beijing 100190, Peoples R China.
   [Jin, Zhipeng; Wang, Can; Zeng, Daniel Dajun] Univ Chinese Acad Sci, Beijing, Peoples R China.
   [Zeng, Daniel Dajun] Univ Arizona, Dept Management Informat Syst, Tucson, AZ 85721 USA.
RP Zeng, DD (reprint author), Chinese Acad Sci, Inst Automat, State Key Lab Management & Control Complex Syst, Beijing 100190, Peoples R China.; Zeng, DD (reprint author), Univ Chinese Acad Sci, Beijing, Peoples R China.; Zeng, DD (reprint author), Univ Arizona, Dept Management Informat Syst, Tucson, AZ 85721 USA.
EM qiudan.li@ia.ac.cn; jinzhipeng2013@ia.ac.cn; wangcan2015@ia.ac.cn;
   zeng@email.arizona.edu
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [91224008, 61172106, 61402123, 71402177]; Important
   National Science & Technology Specific Project [2013ZX10004218]
FX This research is supported in part by National Natural Science
   Foundation of China under Grant No. 91224008, 61172106, 61402123,
   71402177. The Important National Science & Technology Specific Project
   under Grant No. 2013ZX10004218.
CR Aletras N, 2014, PROCEEDINGS OF THE 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 2, P631
   Bai J., 2014, WEIBO USER DEV REPOR
   Barbosa L., 2010, P 23 INT C COMP LING, P36
   Brody S., 2011, P C EMP METH NAT LAN, P562
   Cano A.E. Basave, 2014, P ASS COMP LING ACL, P618
   Carbonell J., 1998, Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P335, DOI 10.1145/290941.291025
   Davidov D., 2010, P 23 INT C COMP LING, P241
   dos Santos C.N., 2014, P 25 INT C COMP LING, P23
   Fawcett T, 2006, PATTERN RECOGN LETT, V27, P861, DOI 10.1016/j.patrec.2005.10.010
   Hall M., 2009, SIGKDD EXPLORATIONS, V11, P10, DOI DOI 10.1145/1656274.1656278
   Hu M., 2004, P 10 ACM SIGKDD INT, V04, P168, DOI DOI 10.1145/1014052.1014073
   Hu Minqing, 2004, AAAI, P755
   Hu X., 2013, P 22 INT C WORLD WID, P607, DOI DOI 10.1145/2488388.2488442
   Jiang L, 2011, PROC OF THE 49TH ANN, V1, P151
   Jin Z., 2015, P 2015 IEEE INT C IN, P132
   Kalchbrenner N, 2014, PROCEEDINGS OF THE 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P655
   Kim Y., 2014, P 2014 C EMP METH NA, P1746, DOI DOI 10.3115/V1/D14-1181
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Li T., 2010, P SIAM INT C DAT MIN, P293
   [梁军 Liang Jun], 2014, [中文信息学报, Journal of Chinese Information Processing], V28, P155
   LU Y, 2015, P 24 INT C WORLD WID, P1211
   Meng X., 2012, P 18 ACM SIGKDD INT, P379, DOI DOI 10.1145/2339530.2339592
   Miao QL, 2010, J AM SOC INF SCI TEC, V61, P2288, DOI 10.1002/asi.21400
   Mihalcea R, 2004, P EMNLP, P404
   MIKOLOV T., 2013, ARXIV13013781
   Narock T, 2013, J AM SOC INF SCI TEC, V64, P416, DOI 10.1002/asi.22769
   O'Connor B., 2010, 4 INT AAAI C WEBL SO, P122
   Pang B, 2002, PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING, P79
   Ravi K, 2015, KNOWL-BASED SYST, V89, P14, DOI 10.1016/j.knosys.2015.06.015
   Serrano-Guerrero J, 2015, INFORM SCIENCES, V311, P18, DOI 10.1016/j.ins.2015.03.040
   Socher R., 2012, P 2012 JOINT C EMP M, P1201
   Socher R., 2013, P C EMP METH NAT LAN, V2013, P1631, DOI DOI 10.1371/JOURNAL.PONE.0073791
   Song S., 2012, P 2012 INT JOINT C N, P1
   Titov  Ivan, 2008, P 17 INT C WORLD WID, P111, DOI DOI 10.1145/1367497.1367513
   Yi Chang, 2013, P 6 ACM INT C WEB SE, P527
   Zhang CL, 2009, J AM SOC INF SCI TEC, V60, P2474, DOI 10.1002/asi.21206
   Zhang ZF, 2013, DECIS SUPPORT SYST, V54, P870, DOI 10.1016/j.dss.2012.09.012
   ZHAO J, 2012, [No title captured], P1528, DOI DOI 10.1145/2339530.2339772
   Zhou L, 2008, J AM SOC INF SCI TEC, V59, P98, DOI 10.1002/asi.20735
   Zhu LH, 2014, SIGMOD'14: PROCEEDINGS OF THE 2014 ACM SIGMOD INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P1531, DOI 10.1145/2588555.2593682
   ZHUANG L, 2006, P 15 ACM INT C INF K, P43, DOI DOI 10.1145/1183614.1183625
NR 41
TC 21
Z9 24
U1 4
U2 54
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0950-7051
EI 1872-7409
J9 KNOWL-BASED SYST
JI Knowledge-Based Syst.
PD SEP 1
PY 2016
VL 107
BP 289
EP 300
DI 10.1016/j.knosys.2016.06.017
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA DS2JN
UT WOS:000380595900022
DA 2020-02-19
ER

PT J
AU Liu, XQ
   Zhu, TS
AF Liu, Xiaoqian
   Zhu, Tingshao
TI Deep learning for constructing microblog behavior representation to
   identify social media user's personality
SO PEERJ COMPUTER SCIENCE
LA English
DT Article
DE Personality prediction; Social media behavior; Deep learning; Feature
   learning
AB Due to the rapid development of information technology, the Internet has gradually become a part of everyday life. People would like to communicate with friends to share their opinions on social networks. The diverse behavior on socials networks is an ideal reflection of users' personality traits. Existing behavior analysis methods for personality prediction mostly extract behavior attributes with heuristic analysis. Although they work fairly well, they are hard to extend and maintain. In this paper, we utilize a deep learning algorithm to build a feature learning model for personality prediction, which could perform an unsupervised extraction of the Linguistic Representation Feature Vector (LRFV) activity without supervision from text actively published on the Sina microblog. Compared with other feature extractsion methods, LRFV, as an abstract representation of microblog content, could describe a user's semantic information more objectively and comprehensively. In the experiments, the personality prediction model is built using a linear regression algorithm, and different attributes obtained through different feature extraction methods are taken as input of the prediction model, respectively. The results show that LRFV performs better in microblog behavior descriptions, and improves the performance of the personality prediction model.
C1 [Liu, Xiaoqian; Zhu, Tingshao] Chinese Acad Sci, Inst Psychol, Beijing, Peoples R China.
RP Liu, XQ; Zhu, TS (reprint author), Chinese Acad Sci, Inst Psychol, Beijing, Peoples R China.
EM liuxiao-qian@psych.ac.cn; tszhu@psych.ac.cn
FU Young Talent Research Fund [Y4CX103005]; National Basic Research Program
   of ChinaNational Basic Research Program of China [2014CB744600]; CAS
   Strategic Priority Research Program [XDA06030800]
FX The authors received support from the Young Talent Research Fund
   (Y4CX103005), National Basic Research Program of China (2014CB744600),
   and CAS Strategic Priority Research Program (XDA06030800). The funders
   had no role in study design, data collection and analysis, decision to
   publish, or preparation of the manuscript.
CR Adali S, 2014, SOC NETW ANAL MIN, V4, DOI 10.1007/s13278-014-0159-7
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   CAPRARA GV, 1993, PERS INDIV DIFFER, V15, P281, DOI 10.1016/0191-8869(93)90218-R
   Chen J, 2015, APPL SOFT COMPUT, V30, P663, DOI 10.1016/j.asoc.2015.01.007
   Ciresan D, 2012, PROC CVPR IEEE, P3642, DOI 10.1109/CVPR.2012.6248110
   Cohen J., 1988, STAT POWER ANAL BEHA, V2
   Costa P.T., 1992, REVISED NEO PERSONAL
   Dunteman G.E., 1989, SAGE U PAPER SERIES
   Funder DC, 2001, ANNU REV PSYCHOL, V52, P197, DOI 10.1146/annurev.psych.52.1.197
   Gao R., 2013, INT C BRAIN HLTH INF
   Golbeck J., 2011, Proceedings of the 2011 IEEE Third International Conference on Privacy, Security, Risk and Trust and IEEE Third International Conference on Social Computing (PASSAT/SocialCom 2011), P149, DOI 10.1109/PASSAT/SocialCom.2011.33
   Golbeck J., 2011, CHI 11 HUM FACT COMP, P253, DOI DOI 10.1145/1979742.1979614
   Huang Chin-Lan, 2012, CHINESE J PSYCHOL, V54, P185, DOI DOI 10.1007/978-3-319-09912-5_
   Huang GB, 2012, PROC CVPR IEEE, P2518, DOI 10.1109/CVPR.2012.6247968
   Huijie L, 2014, P IEEE INT C MULT EX
   Lima ACES, 2013, 2013 1ST BRICS COUNTRIES CONGRESS ON COMPUTATIONAL INTELLIGENCE AND 11TH BRAZILIAN CONGRESS ON COMPUTATIONAL INTELLIGENCE (BRICS-CCI & CBIC), P195, DOI 10.1109/BRICS-CCI-CBIC.2013.41
   Lin HJ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P507
   Loan CV, 1992, SIAM REV, V35, P142
   Mischel W., 2007, INTRO PERSONALITY IN
   Ortigosa A, 2014, J COMPUT SYST SCI, V80, P57, DOI 10.1016/j.jcss.2013.03.008
   Qiu L, 2012, J RES PERS, V46, P710, DOI 10.1016/j.jrp.2012.08.008
   Socher R, 2013, C EMP METH NAT LANG, DOI [10.1037/0022-3514.87.1.123, DOI 10.1037/0022-3514.87.1.123]
   Tausczik YR, 2010, J LANG SOC PSYCHOL, V29, P24, DOI 10.1177/0261927X09351676
   Vazire S, 2004, J PERS SOC PSYCHOL, V87, P123, DOI 10.1037/0022-3514.87.1.123
   Zhang XW, 2013, WORLD WIDE WEB, V16, P497, DOI 10.1007/s11280-012-0181-5
NR 26
TC 6
Z9 6
U1 2
U2 6
PU PEERJ INC
PI LONDON
PA 341-345 OLD ST, THIRD FLR, LONDON, EC1V 9LL, ENGLAND
SN 2376-5992
J9 PEERJ COMPUT SCI
JI PeerJ Comput. Sci.
PD SEP
PY 2016
AR e81
DI 10.7717/peerj-cs.81
PG 15
WC Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Computer Science, Theory & Methods
SC Computer Science
GA VD6TM
UT WOS:000437459800002
OA DOAJ Gold
DA 2020-02-19
ER

PT J
AU Tome, D
   Monti, F
   Baroffio, L
   Bondi, L
   Tagliasacchi, M
   Tubaro, S
AF Tome, D.
   Monti, F.
   Baroffio, L.
   Bondi, L.
   Tagliasacchi, M.
   Tubaro, S.
TI Deep Convolutional Neural Networks for pedestrian detection
SO SIGNAL PROCESSING-IMAGE COMMUNICATION
LA English
DT Article
DE Deep learning; Pedestrian detection; Convolutional Neural Networks;
   Optimization
AB Pedestrian detection is a popular research topic due to its paramount importance for a number of applications, especially in the fields of automotive, surveillance and robotics. Despite the significant improvements, pedestrian detection is still an open challenge that calls for more and more accurate algorithms. In the last few years, deep learning and in particular Convolutional Neural Networks emerged as the state of the art in terms of accuracy for a number of computer vision tasks such as image classification, object detection and segmentation, often outperforming the previous gold standards by a large margin. In this paper, we propose a pedestrian detection system based on deep learning, adapting a general-purpose convolutional network to the task at hand. By thoroughly analyzing and optimizing each step of the detection pipeline we propose an architecture that outperforms traditional methods, achieving a task accuracy close to that of state-of-the-art approaches, while requiring a low computational time. Finally, we tested the system on an NVIDIA Jetson TK1, a 192-core platform that is envisioned to be a forerunner computational brain of future self-driving cars. (C) 2016 Elsevier B.V. All rights reserved.
C1 [Tome, D.; Monti, F.; Baroffio, L.; Bondi, L.; Tagliasacchi, M.; Tubaro, S.] Politecn Milan, Dipartimento Elettron Informaz & Bioingn, Piazza Leonardo Da Vinci 32, I-20133 Milan, Italy.
RP Baroffio, L (reprint author), Politecn Milan, Dipartimento Elettron Informaz & Bioingn, Piazza Leonardo Da Vinci 32, I-20133 Milan, Italy.
EM denis.tome@mail.polimi.it; federico2.monti@mail.polimi.it;
   luca.baroffio@polimi.it; luca.bondi@polimi.it;
   marco.tagliasacchi@polimi.it; stefano.tubaro@polimi.it
RI ; Bondi, Luca/L-4871-2015
OI Monti, Federico/0000-0003-3736-891X; Bondi, Luca/0000-0003-3974-7542
FU European CommissionEuropean Commission Joint Research Centre [296676]
FX The project GreenEyes acknowledges the financial support of the Future
   and Emerging Technologies (FET) program within the Seventh Framework
   Program for Research of the European Commission, under FET-Open Grant
   number: 296676.
CR Alexe B, 2012, IEEE T PATTERN ANAL, V34, P2189, DOI 10.1109/TPAMI.2012.28
   Benenson R., 2014, COMPUTER VISION ROAD
   Dalal N, 2005, PROC CVPR IEEE, P886
   Deng J., 2009, CVPR
   Dollar P., 2009, P BRIT MACH VIS C, P1, DOI DOI 10.5244/C.23.91
   Dollar P, 2014, IEEE T PATTERN ANAL, V36, P1532, DOI 10.1109/TPAMI.2014.2300479
   Dollar P, 2012, IEEE T PATTERN ANAL, V34, P743, DOI 10.1109/TPAMI.2011.155
   Nguyen DT, 2016, PATTERN RECOGN, V51, P148, DOI 10.1016/j.patcog.2015.08.027
   Endres I, 2010, LECT NOTES COMPUT SC, V6315, P575, DOI 10.1007/978-3-642-15555-0_42
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504
   Girshick R, 2016, IEEE T PATTERN ANAL, V38, P142, DOI 10.1109/TPAMI.2015.2437384
   Hosang J. H., 2015, P IEEE C COMP VIS PA
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Karayev S., 2013, ARXIV13113715
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   LeCun Yann, 1995, HDB BRAIN THEORY NEU, P255, DOI DOI 10.1109/IJCNN.2004.1381049
   Nam W., 2014, P 28 ANN C NEUR INF
   Ouyang WL, 2013, IEEE I CONF COMP VIS, P2056, DOI 10.1109/ICCV.2013.257
   Ouyang WL, 2012, PROC CVPR IEEE, P3258, DOI 10.1109/CVPR.2012.6248062
   Paisitkriangkrai S., 2014, ARXIV14095209
   Polana R., 1993, Proceedings. 1993 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.93CH3309-2), P2, DOI 10.1109/CVPR.1993.341009
   Sermanet P, 2013, PROC CVPR IEEE, P3626, DOI 10.1109/CVPR.2013.465
   Stockman G., 2001, COMPUTER VISION
   Szegedy C., 2015, CVPR
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Tian Y., 2014, ARXIV14120069
   Uijlings J. R. R., 2013, INT J COMPUTER VISIO
   VIOLA P, 2003, P 9 IEEE INT C COMP, V2, P734, DOI DOI 10.1109/ICCV.2003.1238422
   Yang B, 2015, IEEE I CONF COMP VIS, P82, DOI 10.1109/ICCV.2015.18
   Zhang S., 2015, ARXIV150105759
NR 31
TC 48
Z9 51
U1 5
U2 91
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0923-5965
EI 1879-2677
J9 SIGNAL PROCESS-IMAGE
JI Signal Process.-Image Commun.
PD SEP
PY 2016
VL 47
SI SI
BP 482
EP 489
DI 10.1016/j.image.2016.05.007
PG 8
WC Engineering, Electrical & Electronic
SC Engineering
GA DZ1MD
UT WOS:000385601600040
DA 2020-02-19
ER

PT J
AU Lim, S
   Yang, JH
AF Lim, Sejoon
   Yang, Ji Hyun
TI Driver state estimation by convolutional neural network using multimodal
   sensor data
SO ELECTRONICS LETTERS
LA English
DT Article
AB A driver state estimation algorithm that uses multimodal vehicular and physiological sensor data is proposed. Deep learning is applied to the fused multimodal data rather than each modality being treated as a different feature. A convolutional neural network model is developed and the driver state estimation algorithm is implemented using Google TensorFlow. The results show that deep learning is a very promising approach for driver state estimation compared with previously studied algorithms, such as dynamic Bayesian networks.
C1 [Lim, Sejoon; Yang, Ji Hyun] Kookmin Univ, Coll Automot Engn, Seoul, South Korea.
RP Lim, S (reprint author), Kookmin Univ, Coll Automot Engn, Seoul, South Korea.
EM lim@kookmin.ac.kr
FU Technology Innovation Program - Ministry of Trade, Industry, and Energy
   [10047761]; Basic Science Research Program through the National Research
   Foundation of Korea (NRF); Ministry of Science, ICT, and Future Planning
   [2014R1A1A1002037]
FX This work was supported by the Technology Innovation Program (10047761,
   Development of connectivity-based personalized intelligent integration
   cockpit module with analysis function of drivers' condition(at least
   three type) and driving environment) funded by the Ministry of Trade,
   Industry, and Energy. The second author was partly supported by Basic
   Science Research Program through the National Research Foundation of
   Korea (NRF), funded by the Ministry of Science, ICT, and Future Planning
   (2014R1A1A1002037). The authors appreciate Prof. Woon Sung Lee for
   providing the experimental platform for collecting vehicle and eye
   tracking data using the SCANeR Simulator and faceLAB.
CR Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Martinez H. P., 2014, ACM INT C MULT INT I
   Singh S, 2015, 812115 DOT NAT HIGHW
   Yang J. H., 2015, INT C CONTR AUT SYST
   Yang JH, 2015, IEEE SYS MAN CYBERN, P1238, DOI 10.1109/SMC.2015.221
NR 5
TC 4
Z9 4
U1 1
U2 40
PU INST ENGINEERING TECHNOLOGY-IET
PI HERTFORD
PA MICHAEL FARADAY HOUSE SIX HILLS WAY STEVENAGE, HERTFORD SG1 2AY, ENGLAND
SN 0013-5194
EI 1350-911X
J9 ELECTRON LETT
JI Electron. Lett.
PD AUG 18
PY 2016
VL 52
IS 17
BP 1495
EP 1496
DI 10.1049/el.2016.1393
PG 2
WC Engineering, Electrical & Electronic
SC Engineering
GA DU9FF
UT WOS:000382521000040
DA 2020-02-19
ER

PT J
AU Cavalcante, RC
   Brasileiro, RC
   Souza, VLP
   Nobrega, JP
   Oliveira, ALI
AF Cavalcante, Rodolfo C.
   Brasileiro, Rodrigo C.
   Souza, Victor L. P.
   Nobrega, Jarley P.
   Oliveira, Adriano L. I.
TI Computational Intelligence and Financial Markets: A Survey and Future
   Directions
SO EXPERT SYSTEMS WITH APPLICATIONS
LA English
DT Article
DE Financial markets; Computational intelligence; Trading systems; Deep
   learning; Online learning
ID SUPPORT VECTOR REGRESSION; INDEPENDENT COMPONENT ANALYSIS; EMPIRICAL
   MODE DECOMPOSITION; PARTICLE SWARM OPTIMIZATION; ARTIFICIAL
   NEURAL-NETWORK; FEATURE-SELECTION METHODS; TIME-SERIES; STOCK-PRICE;
   PORTFOLIO CONSTRUCTION; GENETIC ALGORITHMS
AB Financial markets play an important role on the economical and social organization of modern society. In these kinds of markets, information is an invaluable asset. However, with the modernization of the financial transactions and the information systems, the large amount of information available for a trader can make prohibitive the analysis of a financial asset. In the last decades, many researchers have attempted to develop computational intelligent methods and algorithms to support the decision-making in different financial market segments. In the literature, there is a huge number of scientific papers that investigate the use of computational intelligence techniques to solve financial market problems. However, only few studies have focused on review the literature of this topic. Most of the existing review articles have a limited scope, either by focusing on a specific financial market application or by focusing on a family of machine learning algorithms. This paper presents a review of the application of several computational intelligent methods in several financial applications. This paper gives an overview of the most important primary studies published from 2009 to 2015, which cover techniques for preprocessing and clustering of financial data, for forecasting future market movements, for mining financial text information, among others. The main contributions of this paper are: (i) a comprehensive review of the literature of this field, (ii) the definition of a systematic procedure for guiding the task of building an intelligent trading system and (iii) a discussion about the main challenges and open problems in this scientific field. (C) 2016 Elsevier Ltd. All rights reserved.
C1 [Cavalcante, Rodolfo C.] Univ Fed Alagoas, Nucleo Ciencias Exatas, BR-57309005 Arapiraca, AL, Brazil.
   [Cavalcante, Rodolfo C.; Brasileiro, Rodrigo C.; Souza, Victor L. P.; Nobrega, Jarley P.; Oliveira, Adriano L. I.] Univ Fed Pernambuco, Ctr Informat, Recife, PE, Brazil.
RP Cavalcante, RC (reprint author), Univ Fed Alagoas, Nucleo Ciencias Exatas, BR-57309005 Arapiraca, AL, Brazil.
EM rodolfo.cavalcante@arapiraca.ufal.br; rcb5@cin.ufpe.br;
   vlfs@cin.ufpe.br; jpn@cin.ufpe.br; alio@cin.ufpe.br
FU National Institute of Science and Technology for Software Engineering
   (INES1) - CNPq; FACEPE [573964/2008-4, APQ-1037-1.03/08]; CNPq
   (Brazilian Council for Scientific and Technological Development)National
   Council for Scientific and Technological Development (CNPq)
   [484164/2013-9]
FX This work was supported by the National Institute of Science and
   Technology for Software Engineering (INES<SUP>1</SUP>), funded by CNPq
   and FACEPE, grants 573964/2008-4 and APQ-1037-1.03/08, and CNPq
   (Brazilian Council for Scientific and Technological Development), grant
   484164/2013-9.
CR Abdual-Salam M. E., 2010, 7 INT C INF SYST INF, P1
   Aghabozorgi S, 2014, EXPERT SYST APPL, V41, P1301, DOI 10.1016/j.eswa.2013.08.028
   Aguilar-Rivera R, 2015, EXPERT SYST APPL, V42, P7684, DOI 10.1016/j.eswa.2015.06.001
   Aldridge I, 2009, HIGH FREQUENCY TRADI, P459
   [Anonymous], 1964, KYKLOS, V17, P1
   Atsalakis GS, 2009, EXPERT SYST APPL, V36, P5932, DOI 10.1016/j.eswa.2008.07.006
   Ballings M, 2015, EXPERT SYST APPL, V42, P7046, DOI 10.1016/j.eswa.2015.05.013
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Brasileiro RC, 2013, 2013 IEEE CONGRESS ON EVOLUTIONARY COMPUTATION (CEC), P1810
   Brockwell P. J., 2009, TIME SERIES THEORY M
   Cavalcante R.C., 2015, INT JOINT C NEUR NET, P1
   Cavalcante RC, 2014, IEEE IJCNN, P1424, DOI 10.1109/IJCNN.2014.6889870
   Chande T. S., 1999, TECHNICAL ANAL DEV I
   Chapados N, 2001, IEEE T NEURAL NETWOR, V12, P890, DOI 10.1109/72.935098
   Chen WS, 2009, EXPERT SYST APPL, V36, P4075, DOI 10.1016/j.eswa.2008.03.020
   Chen X., 2010, J NANOMATER, V2010, P1, DOI DOI 10.1016/J.JALLC0M.2010.06.029.ISSN
   Cheng CH, 2014, ECON MODEL, V36, P136, DOI 10.1016/j.econmod.2013.09.033
   Chourmouziadis K, 2016, EXPERT SYST APPL, V43, P298, DOI 10.1016/j.eswa.2015.07.063
   Chuang CC, 2002, IEEE T NEURAL NETWOR, V13, P1322, DOI 10.1109/TNN.2002.804227
   Croce RM, 2009, J HOUS ECON, V18, P281, DOI 10.1016/j.jhe.2009.09.001
   Cryer JD, 2008, SPRINGER TEXTS STAT, P1
   D'Urso P, 2013, PHYSICA A, V392, P2114, DOI 10.1016/j.physa.2013.01.027
   Dai WS, 2012, EXPERT SYST APPL, V39, P4444, DOI 10.1016/j.eswa.2011.09.145
   de Carvalho FDT, 2013, FUZZY SET SYST, V215, P1, DOI 10.1016/j.fss.2012.09.011
   de Oliveira FA, 2011, IEEE SYS MAN CYBERN, P2151, DOI 10.1109/ICSMC.2011.6083990
   Dhar S, 2010, Proceedings of the 2010 International Conference on Communication and Computational Intelligence (INCOCCI), P597
   Ding X, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P2327
   Durenard E., 2013, PROFESSIONAL AUTOMAT
   Evans C, 2013, MATH COMPUT MODEL, V58, P1249, DOI 10.1016/j.mcm.2013.02.002
   FAMA EF, 1965, J BUS, V38, P34, DOI 10.1086/294743
   Gama J, 2014, ACM COMPUT SURV, V46, DOI 10.1145/2523813
   Gama J, 2012, PROG ARTIF INTELL, V1, P45, DOI 10.1007/s13748-011-0002-6
   Ghazali R, 2009, NEUROCOMPUTING, V72, P2359, DOI 10.1016/j.neucom.2008.12.005
   Gheyas IA, 2011, NEUROCOMPUTING, V74, P3855, DOI 10.1016/j.neucom.2011.08.005
   Grane A, 2014, J EMPIR FINANC, V26, P26, DOI 10.1016/j.jempfin.2014.01.005
   Grane A, 2010, COMPUT STAT DATA AN, V54, P2580, DOI 10.1016/j.csda.2009.12.010
   Groth SS, 2011, DECIS SUPPORT SYST, V50, P680, DOI 10.1016/j.dss.2010.08.019
   Guerard JB, 2015, INT J FORECASTING, V31, P550, DOI 10.1016/j.ijforecast.2014.10.003
   Gunduz H, 2015, EXPERT SYST APPL, V42, P9001, DOI 10.1016/j.eswa.2015.07.058
   Guo-qiang X., 2011, INT C CONTR AUT SYST, P1
   Hafezi R, 2015, APPL SOFT COMPUT, V29, P196, DOI 10.1016/j.asoc.2014.12.028
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hsieh TJ, 2012, NEUROCOMPUTING, V82, P196, DOI 10.1016/j.neucom.2011.11.020
   Hsu SH, 2009, EXPERT SYST APPL, V36, P7947, DOI 10.1016/j.eswa.2008.10.065
   Hu Y, 2015, EXPERT SYST APPL, V42, P212, DOI 10.1016/j.eswa.2014.07.059
   Huang Chao, 2012, 2012 8th International Conference on Natural Computation, P79, DOI 10.1109/ICNC.2012.6234569
   Huang CL, 2009, EXPERT SYST APPL, V36, P1529, DOI 10.1016/j.eswa.2007.11.062
   Huang CF, 2012, APPL SOFT COMPUT, V12, P807, DOI 10.1016/j.asoc.2011.10.009
   Irlicht L, 2014, ALGORITHMIC FINANC, V3, P173, DOI 10.3233/AF-140038
   Jasemi M, 2011, EXPERT SYST APPL, V38, P3884, DOI 10.1016/j.eswa.2010.09.049
   Jiangling Yin, 2011, Proceedings of the 2011 International Conference on System Science and Engineering (ICSSE), P394, DOI 10.1109/ICSSE.2011.5961935
   Kao LJ, 2013, NEUROCOMPUTING, V99, P534, DOI 10.1016/j.neucom.2012.06.037
   Kara Y, 2011, EXPERT SYST APPL, V38, P5311, DOI 10.1016/j.eswa.2010.10.027
   Kayal A, 2010, 2010 IEEE International Conference on Intelligent Computing and Intelligent Systems (ICIS 2010), P159, DOI 10.1109/ICICISYS.2010.5658495
   Kourentzes N, 2014, EXPERT SYST APPL, V41, P4235, DOI 10.1016/j.eswa.2013.12.011
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kumar D. A., 2013, Proceedings of the 2013 International Conference on Pattern Recognition, Informatics and Mobile Engineering (PRIME), P72, DOI 10.1109/ICPRIME.2013.6496450
   Kuremoto T, 2014, NEUROCOMPUTING, V137, P47, DOI 10.1016/j.neucom.2013.03.047
   Lam M, 2004, DECIS SUPPORT SYST, V37, P567, DOI 10.1016/S0167-9236(03)00088-5
   Langkvist M, 2014, PATTERN RECOGN LETT, V42, P11, DOI 10.1016/j.patrec.2014.01.008
   Lasfer A., 2013, 5 INT C MOD SIM APPL, P1
   Le Roux N, 2008, NEURAL COMPUT, V20, P1631, DOI 10.1162/neco.2008.04-07-510
   Lee H., 2009, P 26 ANN INT C MACH, P609, DOI DOI 10.1145/1553374.1553453
   Lee MC, 2009, EXPERT SYST APPL, V36, P10896, DOI 10.1016/j.eswa.2009.02.038
   Li XQ, 2009, EXPERT SYST APPL, V36, P7818, DOI 10.1016/j.eswa.2008.11.014
   Liang X, 2013, NEUROCOMPUTING, V115, P142, DOI 10.1016/j.neucom.2013.01.011
   Liang X, 2009, NEUROCOMPUTING, V72, P3055, DOI 10.1016/j.neucom.2009.03.015
   Liao SH, 2013, EXPERT SYST APPL, V40, P1542, DOI 10.1016/j.eswa.2012.08.075
   Lin CS, 2012, ECON MODEL, V29, P2583, DOI 10.1016/j.econmod.2012.07.018
   Lin FY, 2014, EXPERT SYST APPL, V41, P2472, DOI 10.1016/j.eswa.2013.09.047
   Liu FJ, 2012, NEUROCOMPUTING, V83, P12, DOI 10.1016/j.neucom.2011.09.033
   Lu CJ, 2011, EXPERT SYST APPL, V38, P15194, DOI 10.1016/j.eswa.2011.05.082
   Lu CJ, 2010, EXPERT SYST APPL, V37, P7056, DOI 10.1016/j.eswa.2010.03.012
   Lu CJ, 2009, DECIS SUPPORT SYST, V47, P115, DOI 10.1016/j.dss.2009.02.001
   Lui YH, 1998, J INT MONEY FINANC, V17, P535, DOI 10.1016/S0261-5606(98)00011-4
   Luo F., 2010, 8 WORLD C INT CONTR, P5048
   Mabu S, 2015, APPL SOFT COMPUT, V36, P357, DOI 10.1016/j.asoc.2015.07.020
   Mahdi A A, 2009, Proceedings of the 2009 First International Conference on Computational Intelligence, Modelling and Simulation. CSSim 2009 Information Getting Started, P104, DOI 10.1109/CSSim.2009.57
   Majhi B, 2015, NEUROCOMPUTING, V167, P502, DOI 10.1016/j.neucom.2015.04.044
   Majhi R, 2009, EXPERT SYST APPL, V36, P181, DOI 10.1016/j.eswa.2007.09.005
   Markowitz H, 1952, J FINANC, V7, P77, DOI 10.1111/j.1540-6261.1952.tb01525.x
   Martinez Leonardo C, 2009, Proceedings 2009 International Joint Conference on Neural Networks (IJCNN 2009 - Atlanta), P2006, DOI 10.1109/IJCNN.2009.5179050
   Murphy J. J., 1999, TECHNICAL ANAL FINAN
   Nassirtoussi AK, 2015, EXPERT SYST APPL, V42, P306, DOI 10.1016/j.eswa.2014.08.004
   Nayak RK, 2015, APPL SOFT COMPUT, V35, P670, DOI 10.1016/j.asoc.2015.06.040
   Neto A. C., 2010, 17 IEEE NPSS REAL TI, P1
   Nikfarjam A, 2010, INT CONF COMPUT AUTO, P256, DOI 10.1109/ICCAE.2010.5451705
   Oliveira ALI, 2006, NEUROCOMPUTING, V70, P79, DOI 10.1016/j.neucom.2006.05.008
   Palit A. K., 2006, ADV IND CONTROL
   Paranjape-Voditel P, 2013, APPL SOFT COMPUT, V13, P1055, DOI 10.1016/j.asoc.2012.09.012
   Patel J, 2015, EXPERT SYST APPL, V42, P2162, DOI 10.1016/j.eswa.2014.10.031
   Pinto JM, 2015, EXPERT SYST APPL, V42, P6699, DOI 10.1016/j.eswa.2015.04.056
   Pulido M, 2014, INFORM SCIENCES, V280, P188, DOI 10.1016/j.ins.2014.05.006
   Qiu X., 2015, P 2014 IEEE S COMP I, P21
   Qu JL, 2009, INT C MANAGE SCI ENG, P1442, DOI 10.1109/ICMSE.2009.5317982
   Rodriguez-Gonzalez A, 2011, EXPERT SYST APPL, V38, P11489, DOI 10.1016/j.eswa.2011.03.023
   Roshan W. D. S., 2011, 2011 IEEE 6th International Conference on Industrial and Information Systems (ICIIS 2011), P322, DOI 10.1109/ICIINFS.2011.6038088
   Ruiz E.J., 2012, P 5 ACM INT C WEB SE, P513, DOI DOI 10.1145/2124295.2124358
   Schumaker RP, 2012, DECIS SUPPORT SYST, V53, P458, DOI 10.1016/j.dss.2012.03.001
   Shahpazov V. L., 2013, SIGN PROC S SPS, P1
   Shen FR, 2015, NEUROCOMPUTING, V167, P243, DOI 10.1016/j.neucom.2015.04.071
   Si YW, 2013, ENG APPL ARTIF INTEL, V26, P2581, DOI 10.1016/j.engappai.2013.08.015
   SONI S, 2011, INT J COMPUTER SCI E, V2, P71
   Tay FEH, 2001, OMEGA-INT J MANAGE S, V29, P309, DOI 10.1016/S0305-0483(01)00026-3
   Teixeira LA, 2010, EXPERT SYST APPL, V37, P6885, DOI 10.1016/j.eswa.2010.03.033
   Ticknor JL, 2013, EXPERT SYST APPL, V40, P5501, DOI 10.1016/j.eswa.2013.04.013
   Tkac M, 2016, APPL SOFT COMPUT, V38, P788, DOI 10.1016/j.asoc.2015.09.040
   Tresp V., 2001, HDB NEURAL NETWORK S, P1
   Tsai CF, 2010, DECIS SUPPORT SYST, V50, P258, DOI 10.1016/j.dss.2010.08.028
   Tsinaslanidis PE, 2014, EXPERT SYST APPL, V41, P6848, DOI 10.1016/j.eswa.2014.04.028
   Tung WL, 2011, EXPERT SYST APPL, V38, P4668, DOI 10.1016/j.eswa.2010.07.116
   Vanstone B, 2012, MATH COMPUT SIMULAT, V86, P78, DOI 10.1016/j.matcom.2011.01.002
   Vanstone B, 2010, EXPERT SYST APPL, V37, P6602, DOI 10.1016/j.eswa.2010.02.124
   Vanstone B, 2009, EXPERT SYST APPL, V36, P6668, DOI 10.1016/j.eswa.2008.08.019
   Vincent P, 2008, P 25 INT C MACH LEAR, P1096, DOI DOI 10.1145/1390156.1390294
   Wang BH, 2012, NEUROCOMPUTING, V83, P136, DOI 10.1016/j.neucom.2011.12.013
   Wang JZ, 2011, EXPERT SYST APPL, V38, P14346, DOI 10.1016/j.eswa.2011.04.222
   Wang J, 2015, NEUROCOMPUTING, V156, P68, DOI 10.1016/j.neucom.2014.12.084
   Webb AR, 2003, STAT PATTERN RECOGNI
   Wu L, 2010, IEEE T POWER SYST, V25, P1519, DOI 10.1109/TPWRS.2009.2039948
   Yang HQ, 2009, NEUROCOMPUTING, V72, P2659, DOI 10.1016/j.neucom.2008.09.014
   Yoo P. D., 2007, COMP INT MOD CONTR A, V2, P835, DOI [10.1109/CIMCA.2005.1631572, DOI 10.1109/CIMCA.2005.1631572]
   Yoshihara A, 2014, LECT NOTES ARTIF INT, V8862, P759, DOI 10.1007/978-3-319-13560-1_60
   Yuan YB, 2013, MATH COMPUT MODEL, V57, P932, DOI 10.1016/j.mcm.2012.10.004
   Yuhong Li, 2010, Proceedings 2010 3rd International Symposium on Computational Intelligence and Design (ISCID 2010), P211, DOI 10.1109/ISCID.2010.70
   Yukun Bao, 2011, 2011 Fourth International Joint Conference on Computational Sciences and Optimization (CSO), P598, DOI 10.1109/CSO.2011.70
   Yunusoglu MG, 2013, EXPERT SYST APPL, V40, P908, DOI 10.1016/j.eswa.2012.05.047
   Zhang XL, 2013, IEEE T AUDIO SPEECH, V21, P697, DOI 10.1109/TASL.2012.2229986
   Zhou DD, 2009, BIOL MED PHYS BIOMED, P1, DOI 10.1007/978-0-387-77261-5_1
   Zhu BZ, 2013, OMEGA-INT J MANAGE S, V41, P517, DOI 10.1016/j.omega.2012.06.005
   Zliobaite I., 2012, ACM SIGKDD EXPLORATI, V14, P48, DOI DOI 10.1145/2408736.2408746
NR 132
TC 92
Z9 93
U1 28
U2 307
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0957-4174
EI 1873-6793
J9 EXPERT SYST APPL
JI Expert Syst. Appl.
PD AUG 15
PY 2016
VL 55
BP 194
EP 211
DI 10.1016/j.eswa.2016.02.006
PG 18
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic; Operations Research & Management Science
SC Computer Science; Engineering; Operations Research & Management Science
GA DK3IT
UT WOS:000374811000015
DA 2020-02-19
ER

PT J
AU Lee, HY
   Cho, JW
   Kim, M
   Park, HM
AF Lee, Ho-Yong
   Cho, Ji-Won
   Kim, Minook
   Park, Hyung-Min
TI DNN-Based Feature Enhancement Using DOA-Constrained ICA for Robust
   Speech Recognition
SO IEEE SIGNAL PROCESSING LETTERS
LA English
DT Article
DE Deep neural networks (DNNs); feature enhancement (FE); independent
   component analysis (ICA); robust speech recognition
ID SEPARATION; NOISE
AB The performance of automatic speech recognition (ASR) system is often degraded in adverse real-world environments. In recent times, deep learning has successfully emerged as a breakthrough for acoustic modeling in ASR; accordingly, deep neural network (DNN)-based speech feature enhancement (FE) approaches have attracted much attention owing to their powerful modeling capabilities. However, DNN-based approaches are unable to achieve remarkable performance improvements for speech with severe distortion in the test environments different from training environments. In this letter, we propose a DNN-based FE method where the DNN inputs include preenhanced spectral features computed from multichannel input signals to reconstruct noise-robust features. The preenhanced spectral features are obtained by direction-of-arrival (DOA)-constrained independent component analysis (DCICA) followed by Bayesian FE using a hidden-Markov-model prior, to exploit the capabilities of efficient online target speech extraction and efficient FE with prior information for robust ASR. In addition, noise spectral features computed from DCICA are included for further improvement. Therefore, the DNN is trained to reconstruct a clean spectral feature vector, from a sequence of corrupted input feature vectors in addition to the corresponding preenhanced and noise feature vectors. Experimental results demonstrate that the proposed method significantly improves recognition performance, even in mismatched noise conditions.
C1 [Lee, Ho-Yong; Cho, Ji-Won; Kim, Minook; Park, Hyung-Min] Sogang Univ, Dept Elect Engn, Seoul 04107, South Korea.
RP Lee, HY (reprint author), Sogang Univ, Dept Elect Engn, Seoul 04107, South Korea.
EM hy009191@gmail.com; jiwonn85@sogang.ac.kr; min8328@sogang.ac.kr;
   hpark@sogang.ac.kr
FU Basic Science Research Program through National Research Foundation of
   Korea - Ministry of Science, ICT, and future Planning
   [2014R1A2A2A01006581]
FX This work was supported by the Basic Science Research Program through
   the National Research Foundation of Korea funded by the Ministry of
   Science, ICT, and future Planning (2014R1A2A2A01006581). The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. Daniel Povey.
CR ALLEN JB, 1979, J ACOUST SOC AM, V65, P943, DOI 10.1121/1.382599
   Araki S, 2015, INT CONF ACOUST SPEE, P116, DOI 10.1109/ICASSP.2015.7177943
   Cho JW, 2016, SIGNAL PROCESS, V120, P200, DOI 10.1016/j.sigpro.2015.09.002
   Cho JW, 2013, IEEE SIGNAL PROC LET, V20, P1199, DOI 10.1109/LSP.2013.2283585
   Droppo J, 2008, SPRINGER HDB SPEECH, P653
   Feng  X., 2014, P ICASSP, P1778
   Garofolo J. S., 1993, DARPA TIMIT ACOUSTIC, V93, P27403, DOI DOI 10.6028/NIST.IR.4930
   Han K, 2015, IEEE-ACM T AUDIO SPE, V23, P982, DOI 10.1109/TASLP.2015.2416653
   Kim B, 2014, ELECTRON LETT, V50, P889, DOI 10.1049/el.2014.0416
   Kim M, 2015, SIGNAL PROCESS, V117, P126, DOI 10.1016/j.sigpro.2015.04.022
   Krueger A, 2010, IEEE T AUDIO SPEECH, V18, P1692, DOI 10.1109/TASL.2010.2049684
   Mohamed AR, 2012, IEEE T AUDIO SPEECH, V20, P14, DOI 10.1109/TASL.2011.2109382
   Narayanan A, 2014, IEEE-ACM T AUDIO SPE, V22, P826, DOI 10.1109/TASLP.2014.2305833
   Povey D., 2011, P IEEE WORKSH AUT SP
   Price P., 1988, ICASSP 88: 1988 International Conference on Acoustics, Speech, and Signal Processing (Cat. No.88CH2561-9), P651, DOI 10.1109/ICASSP.1988.196669
   Raj B, 1997, INT CONF ACOUST SPEE, P851, DOI 10.1109/ICASSP.1997.596069
   Segura JC, 2001, P EUROSPEECH 01, P221
   Seide F., 2011, P INTERSPEECH, P437
   Seltzer ML, 2013, INT CONF ACOUST SPEE, P7398, DOI 10.1109/ICASSP.2013.6639100
   VARGA A, 1993, SPEECH COMMUN, V12, P247, DOI 10.1016/0167-6393(93)90095-3
   Virtanen T., 2012, TECHNIQUES NOISE ROB
   Wang YX, 2013, IEEE T AUDIO SPEECH, V21, P1381, DOI 10.1109/TASL.2013.2250961
   Weninger Felix, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P4623, DOI 10.1109/ICASSP.2014.6854478
   Xu Y, 2015, IEEE-ACM T AUDIO SPE, V23, P7, DOI 10.1109/TASLP.2014.2364452
   Xu Y, 2014, IEEE SIGNAL PROC LET, V21, P65, DOI 10.1109/LSP.2013.2291240
   Yilmaz O, 2004, IEEE T SIGNAL PROCES, V52, P1830, DOI 10.1109/TSP.2004.828896
   Yulan Liu, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P5542, DOI 10.1109/ICASSP.2014.6854663
NR 27
TC 4
Z9 4
U1 1
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1070-9908
EI 1558-2361
J9 IEEE SIGNAL PROC LET
JI IEEE Signal Process. Lett.
PD AUG
PY 2016
VL 23
IS 8
AR UNSP 1091
DI 10.1109/LSP.2016.2583658
PG 5
WC Engineering, Electrical & Electronic
SC Engineering
GA EI1HH
UT WOS:000392227200001
DA 2020-02-19
ER

PT J
AU Wang, PC
   Li, WQ
   Gao, ZM
   Zhang, J
   Tang, C
   Ogunbona, PO
AF Wang, Pichao
   Li, Wanqing
   Gao, Zhimin
   Zhang, Jing
   Tang, Chang
   Ogunbona, Philip O.
TI Action Recognition From Depth Maps Using Deep Convolutional Neural
   Networks
SO IEEE TRANSACTIONS ON HUMAN-MACHINE SYSTEMS
LA English
DT Article
DE Action recognition; deep learning; depth maps; pseudocolor coding
ID FEATURES; ENSEMBLE
AB This paper proposes a new method, i.e., weighted hierarchical depth motion maps (WHDMM) + three-channel deep convolutional neural networks (3ConvNets), for human action recognition from depth maps on small training datasets. Three strategies are developed to leverage the capability of ConvNets in mining discriminative features for recognition. First, different viewpoints are mimicked by rotating the 3-D points of the captured depth maps. This not only synthesizes more data, but also makes the trained ConvNets view-tolerant. Second, WHDMMs at several temporal scales are constructed to encode the spatiotemporal motion patterns of actions into 2-D spatial structures. The 2-D spatial structures are further enhanced for recognition by converting the WHDMMs into pseudocolor images. Finally, the three ConvNets are initialized with the models obtained from ImageNet and fine-tuned independently on the color-coded WHDMMs constructed in three orthogonal planes. The proposed algorithm was evaluated on the MSRAction3D, MSRAction3DExt, UTKinect-Action, and MSRDailyActivity3D datasets using cross-subject protocols. In addition, the method was evaluated on the large dataset constructed from the above datasets. The proposed method achieved 2-9% better results on most of the individual datasets. Furthermore, the proposed method maintained its performance on the large dataset, whereas the performance of existing methods decreased with the increased number of actions.
C1 [Wang, Pichao; Li, Wanqing; Gao, Zhimin; Zhang, Jing; Ogunbona, Philip O.] Univ Wollongong, Adv Multimedia Res Lab, Wollongong, NSW 2522, Australia.
   [Tang, Chang] Tianjin Univ, Sch Elect Informat Engn, Tianjin 300072, Peoples R China.
RP Wang, PC; Li, WQ; Gao, ZM; Zhang, J; Ogunbona, PO (reprint author), Univ Wollongong, Adv Multimedia Res Lab, Wollongong, NSW 2522, Australia.; Tang, C (reprint author), Tianjin Univ, Sch Elect Informat Engn, Tianjin 300072, Peoples R China.
EM pw212@uowmail.edu.au; wanqing@uow.edu.au; zg126@uowmail.edu.au;
   jz960@uowmail.edu.au; tangchang@tju.edu.cn; philipo@uow.edu.au
OI Ogunbona, Philip O./0000-0003-4119-2873
CR Abidi BR, 2006, IEEE T SYST MAN CY C, V36, P784, DOI 10.1109/TSMCC.2005.855523
   Agrawal P, 2014, LECT NOTES COMPUT SC, V8695, P329, DOI 10.1007/978-3-319-10584-0_22
   Althloothi S, 2014, PATTERN RECOGN, V47, P1800, DOI 10.1016/j.patcog.2013.11.032
   Chaaraoui AA, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P91, DOI 10.1109/ICCVW.2013.19
   Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231
   HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Jia Yangqing, 2014, ARXIV14085093
   Johnson J, 2012, MOL BIOL CELL, V23, P754, DOI 10.1091/mbc.E11-09-0824
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li WQ, 2008, IEEE T CIRC SYST VID, V18, P1499, DOI 10.1109/TCSVT.2008.2005597
   Li WB, 2010, 2010 THE 3RD INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND INDUSTRIAL APPLICATION (PACIIA2010), VOL I, P9, DOI 10.1109/CVPRW.2010.5543273
   Lu CW, 2014, PROC CVPR IEEE, P772, DOI 10.1109/CVPR.2014.104
   Oreifej O, 2013, PROC CVPR IEEE, P716, DOI 10.1109/CVPR.2013.98
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Sermanet P., 2014, P INT C LEARN REPR, P1, DOI DOI 10.1016/J.VISRES.2006.11.009
   Simonyan K., 2014, ADV NEURAL INFORM PR, P568, DOI DOI 10.1109/ICCVW.2017.368
   Smisek J., 2011, P IEEE INT C COMP VI, P3
   Taylor GW, 2010, LECT NOTES COMPUT SC, V6316, P140, DOI 10.1007/978-3-642-15567-3_11
   Wang H., 2009, P BMVC
   Wang J, 2014, IEEE T PATTERN ANAL, V36, P914, DOI 10.1109/TPAMI.2013.198
   Wang J, 2012, PROC CVPR IEEE, P1290, DOI 10.1109/CVPR.2012.6247813
   Wang PC, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1119, DOI 10.1145/2733373.2806296
   Xia L., 2012, IEEE COMP SOC C COMP, P20, DOI DOI 10.1109/CVPRW.2012.6239233
   Xia L, 2013, PROC CVPR IEEE, P2834, DOI 10.1109/CVPR.2013.365
   Yang X, 2012, P 20 ACM INT C MULT, P1057, DOI DOI 10.1145/2393347.2396382
   Yang XD, 2014, PROC CVPR IEEE, P804, DOI 10.1109/CVPR.2014.108
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhu Y, 2013, IEEE COMPUT SOC CONF, P486, DOI 10.1109/CVPRW.2013.78
NR 30
TC 104
Z9 106
U1 9
U2 46
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 2168-2291
EI 2168-2305
J9 IEEE T HUM-MACH SYST
JI IEEE T. Hum.-Mach. Syst.
PD AUG
PY 2016
VL 46
IS 4
BP 498
EP 509
DI 10.1109/THMS.2015.2504550
PG 12
WC Computer Science, Artificial Intelligence; Computer Science, Cybernetics
SC Computer Science
GA DX3NL
UT WOS:000384279700002
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Wang, C
   Yang, HJ
   Meinel, C
AF Wang, Cheng
   Yang, Haojin
   Meinel, Christoph
TI A deep semantic framework for multimodal representation learning
SO MULTIMEDIA TOOLS AND APPLICATIONS
LA English
DT Article
DE Multimodal representation; Deep neural networks; Semantic feature;
   Cross-modal retrieval
ID IMAGE RETRIEVAL; FEATURES
AB Multimodal representation learning has gained increasing importance in various real-world multimedia applications. Most previous approaches focused on exploring inter-modal correlation by learning a common or intermediate space in a conventional way, e.g. Canonical Correlation Analysis (CCA). These works neglected the exploration of fusing multiple modalities at higher semantic level. In this paper, inspired by the success of deep networks in multimedia computing, we propose a novel unified deep neural framework for multimodal representation learning. To capture the high-level semantic correlations across modalities, we adopted deep learning feature as image representation and topic feature as text representation respectively. In joint model learning, a 5-layer neural network is designed and enforced with a supervised pre-training in the first 3 layers for intra-modal regularization. The extensive experiments on benchmark Wikipedia and MIR Flickr 25K datasets show that our approach achieves state-of-the-art results compare to both shallow and deep models in multimodal and cross-modal retrieval.
C1 [Wang, Cheng; Yang, Haojin; Meinel, Christoph] Univ Potsdam, Hasso Plattner Inst, Prof Dr Helmert Str 2-3, D-14482 Potsdam, Germany.
RP Wang, C (reprint author), Univ Potsdam, Hasso Plattner Inst, Prof Dr Helmert Str 2-3, D-14482 Potsdam, Germany.
EM cheng.wang@hpi.de; haojin.yang@hpi.de; christoph.meinel@hpi.de
CR [Anonymous], 2015 IEEE C COMP VIS, P2015
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Bosch A, 2007, IEEE I CONF COMP VIS, P1863
   Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734
   Chandrika P, 2010, P ACM INT C IM VID R, P342
   Pereira JC, 2014, IEEE T PATTERN ANAL, V36, P521, DOI 10.1109/TPAMI.2013.142
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   ESCALANTE HJ, 2008, P 1 ACM INT C MULT I, P172
   Feng FX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P7, DOI 10.1145/2647868.2654902
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Hoffman J., 2013, ARXIV13013224
   Huiskes M. J., 2008, P 1 ACM INT C MULT I, P39, DOI DOI 10.1145/1460096.1460104
   Jacob L., 2009, ADV NEURAL INFORM PR, P745
   Jaderberg M, 2014, LECT NOTES COMPUT SC, V8692, P512, DOI 10.1007/978-3-319-10593-2_34
   Jia Yangqing, 2014, ARXIV14085093
   Kang CC, 2015, IEEE T MULTIMEDIA, V17, P370, DOI 10.1109/TMM.2015.2390499
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   Lee H., 2009, P 26 ANN INT C MACH, P609, DOI DOI 10.1145/1553374.1553453
   Liao RJ, 2014, WSDM'14: PROCEEDINGS OF THE 7TH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P493, DOI 10.1145/2556195.2556238
   Lienhart Rainer, 2009, P ACM INT C IM VID R
   Liu D, 2013, PROC CVPR IEEE, P803, DOI 10.1109/CVPR.2013.109
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Manjunath BS, 2001, IEEE T CIRC SYST VID, V11, P703, DOI 10.1109/76.927424
   Mao X., 2013, P 21 ACM INT C MULT, P897
   Ngiam J, 2011, P 28 INT C MACH LEAR, P689
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Pereira JC, 2014, COMPUT VIS IMAGE UND, V124, P123, DOI 10.1016/j.cviu.2014.03.003
   Perronnin F., 2007, P IEEE COMP SOC C CO, P1, DOI DOI 10.1109/CVPR.2007.383266
   Pham TT, 2007, P 16 ACM C INF KNOWL, P439
   Rasiwasia N., 2010, P ACM MULT 2010, P251, DOI DOI 10.1145/1873951.1873987
   Sharma A, 2012, PROC CVPR IEEE, P2160, DOI 10.1109/CVPR.2012.6247923
   Shu XB, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P35, DOI 10.1145/2733373.2806216
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   Srivastava N., 2012, ADV NEURAL INFORM PR, P2222, DOI DOI 10.1109/CVPR.2013.49
   Thompson B., 2005, ENCY STAT BEHAV SCI
   Vedaldi A, 2010, P 18 ACM INT C MULT, P1469, DOI DOI 10.1145/1873951.1874249
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Wang KY, 2013, IEEE I CONF COMP VIS, P2088, DOI 10.1109/ICCV.2013.261
   Wang W, 2014, PROC VLDB ENDOW, V7, P649, DOI 10.14778/2732296.2732301
   Wang YF, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P307, DOI 10.1145/2647868.2654901
   Wu F, 2013, 27 AAAI C ART INT
   Wu Fei, 2013, P 21 ACM INT C MULT, P877
   Wu ZX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P167, DOI 10.1145/2647868.2654931
   Xie L, 2013, P ACM C INT C MULT R, P175
   Yu J, 2012, INT C PATT RECOG, P246
   Yu Z, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P395, DOI 10.1145/2600428.2609563
   Zhai XH, 2013, MULTIMEDIA SYST, V19, P395, DOI 10.1007/s00530-012-0297-6
   Zhai XH, 2012, INT CONF ACOUST SPEE, P2337, DOI 10.1109/ICASSP.2012.6288383
   Zhang Y., 2012, CONVEX FORMULATION L
   Zhou JL, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P415
NR 52
TC 19
Z9 19
U1 1
U2 11
PU SPRINGER
PI DORDRECHT
PA VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN 1380-7501
EI 1573-7721
J9 MULTIMED TOOLS APPL
JI Multimed. Tools Appl.
PD AUG
PY 2016
VL 75
IS 15
BP 9255
EP 9276
DI 10.1007/s11042-016-3380-8
PG 22
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic
SC Computer Science; Engineering
GA DU3LY
UT WOS:000382113500024
DA 2020-02-19
ER

PT J
AU Long, MS
   Wang, JM
   Cao, Y
   Sun, JG
   Yu, PS
AF Long, Mingsheng
   Wang, Jianmin
   Cao, Yue
   Sun, Jiaguang
   Yu, Philip S.
TI Deep Learning of Transferable Representation for Scalable Domain
   Adaptation
SO IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING
LA English
DT Article
DE Domain adaptation; deep learning; denoising autoencoder; neural network;
   two-sample test; multiple kernel learning
ID REGULARIZATION; KERNEL
AB Domain adaptation generalizes a learning model across source domain and target domain that are sampled from different distributions. It is widely applied to cross-domain data mining for reusing labeled information and mitigating labeling consumption. Recent studies reveal that deep neural networks can learn abstract feature representation, which can reduce, but not remove, the cross-domain discrepancy. To enhance the invariance of deep representation and make it more transferable across domains, we propose a unified deep adaptation framework for jointly learning transferable representation and classifier to enable scalable domain adaptation, by taking the advantages of both deep learning and optimal two-sample matching. The framework constitutes two inter-dependent paradigms, unsupervised pre-training for effective training of deep models using deep denoising autoencoders, and supervised fine-tuning for effective exploitation of discriminative information using deep neural networks, both learned by embedding the deep representations to reproducing kernel Hilbert spaces (RKHSs) and optimally matching different domain distributions. To enable scalable learning, we develop a linear-time algorithm using unbiased estimate that scales linearly to large samples. Extensive empirical results show that the proposed framework significantly outperforms state of the art methods on diverse adaptation tasks: sentiment polarity prediction, email spam filtering, newsgroup content categorization, and visual object recognition.
C1 [Long, Mingsheng; Wang, Jianmin; Cao, Yue; Sun, Jiaguang] Tsinghua Univ, Tsinghua Natl Lab Informat Sci & Techonolgy TNLis, Sch Software, Beijing, Peoples R China.
   [Yu, Philip S.] Tsinghua Univ, Inst Data Sci, Beijing, Peoples R China.
   [Yu, Philip S.] Univ Illinois, Chicago, IL 60607 USA.
RP Long, MS (reprint author), Tsinghua Univ, Tsinghua Natl Lab Informat Sci & Techonolgy TNLis, Sch Software, Beijing, Peoples R China.
EM mingsheng@tsinghua.edu.cn; jimwang@tsinghua.edu.cn;
   yue-cao14@mails.tsinghua.edu.cn; sunjg@tsinghua.edu.cn; psyu@uic.edu
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61325008, 61502265]; China Postdoctoral Science
   FoundationChina Postdoctoral Science Foundation [2015T80088]; National
   Science and Technology Supporting Program [2015BAH14F02]; NSFNational
   Science Foundation (NSF) [III-1526499]; Tsinghua National Laboratory
   (TNList) Special Fund for Big Data Science and Technology
FX This work was supported by the National Natural Science Foundation of
   China (61325008, 61502265), China Postdoctoral Science Foundation
   (2015T80088), National Science and Technology Supporting Program
   (2015BAH14F02), NSF through grant III-1526499, and Tsinghua National
   Laboratory (TNList) Special Fund for Big Data Science and Technology.
   Jianmin Wang is the corresponding author.
CR Ben-David S., 2007, ADV NEURAL INFORM PR, P137, DOI DOI 10.1007/S10994-009-5152-4
   Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4
   Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Blitzer J., 2007, ACL, V45, P440, DOI DOI 10.1109/IRPS.2011.5784441
   Calais Guerra P. H., 2011, P 17 ACM SIGKDD INT, P150, DOI [DOI 10.1145/2020408.2020438, 10.1145/2020408.2020438]
   Chen M, 2014, P 31 INT C MACH LEAR, P1476
   Chen M., 2011, ADV NEURAL INFORM PR, P2456
   Chen M., 2012, P 29 INT C MACH LEAR, P767, DOI DOI 10.1007/S11222-007-9033-Z
   Donahue J., 2014, P INT C MACH LEARN, P647
   Duan LX, 2012, IEEE T NEUR NET LEAR, V23, P504, DOI 10.1109/TNNLS.2011.2178556
   Duan LX, 2012, IEEE T PATTERN ANAL, V34, P465, DOI 10.1109/TPAMI.2011.114
   Ganin Y., 2015, INT C MACH LEARN, P1180
   GLOROT Xavier, 2011, P 28 INT C MACH LEAR, V28, P513, DOI DOI 10.1177/1753193411430810
   Gong BQ, 2012, PROC CVPR IEEE, P2066, DOI 10.1109/CVPR.2012.6247911
   Goodfellow I. J., 2013, ARXIV13084214
   Gretton A., 2012, ADV NEURAL INFORM PR, P1214
   Gretton A, 2012, J MACH LEARN RES, V13, P723
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hoffman Judy, 2014, ADV NEURAL INFORM PR, V27, P3536
   Huang J., 2006, ADV NEURAL INFORM PR, P601
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Larochelle H, 2009, J MACH LEARN RES, V10, P1
   Long M., 2015, INT C MACH LEARN, V37, P97
   Long MS, 2015, IEEE T KNOWL DATA EN, V27, P1519, DOI 10.1109/TKDE.2014.2373376
   Long MS, 2013, IEEE I CONF COMP VIS, P2200, DOI 10.1109/ICCV.2013.274
   Long MS, 2014, IEEE T KNOWL DATA EN, V26, P1076, DOI 10.1109/TKDE.2013.111
   Maaten Laurens, 2013, P 30 INT C MACH LEAR, V28, P410
   Mansour Y., 2009, P COLT, P19
   Ngiam J, 2011, IEEE INT C MACH LEAR, V28, P689, DOI DOI 10.1145/2647868
   Pan SJ, 2011, IEEE T NEURAL NETWOR, V22, P199, DOI 10.1109/TNN.2010.2091281
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Pan SJ, 2010, P 19 INT C WORLD WID, P751, DOI DOI 10.1145/1772690.1772767
   Rumelhart D.E., 1986, PARALLEL DISTRIBUTED, V1, P318, DOI DOI 10.1016/B978-1-4832-1446-7.50035-2
   Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16
   Sejdinovic D, 2013, ANN STAT, V41, P2263, DOI 10.1214/13-AOS1140
   Socher R., 2011, P 28 INT C MACH LEAR, P129, DOI DOI 10.1007/978-3-540-87479-9
   Sriperumbudur B., 2009, ADV NEURAL INFORM PR, V22, P1750
   Tang J., 2012, P 18 ACM SIGKDD INT, P1285, DOI DOI 10.1145/2339530.2339730
   Tzeng E., 2014, ARXIV14123474
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Wang X., 2014, P ADV NEUR INF PROC, P1898
   YOSINSKI J, 2014, ADV NEURAL INFORM PR, P3320, DOI DOI 10.1109/IJCNN.2016.7727519
   Zaremba W., 2013, ADV NEURAL INFORM PR, P755
   Zhang K., 2013, P 30 INT C MACH LEAR, P1425
   Zhang L, 2013, PROCEEDINGS OF 2013 CHINA INTERNATIONAL CONFERENCE ON INSURANCE AND RISK MANAGEMENT, P819
   Zhou JT, 2014, PROCEEDINGS OF THE TWENTY-EIGHTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2213
   Zhuang FZ, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P4119
NR 48
TC 36
Z9 36
U1 5
U2 34
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1041-4347
EI 1558-2191
J9 IEEE T KNOWL DATA EN
JI IEEE Trans. Knowl. Data Eng.
PD AUG 1
PY 2016
VL 28
IS 8
BP 2027
EP 2040
DI 10.1109/TKDE.2016.2554549
PG 14
WC Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Engineering, Electrical & Electronic
SC Computer Science; Engineering
GA DR8BA
UT WOS:000380122200007
DA 2020-02-19
ER

PT J
AU Wu, D
   Pigou, L
   Kindermans, PJ
   Le, NDH
   Shao, L
   Dambre, J
   Odobez, JM
AF Wu, Di
   Pigou, Lionel
   Kindermans, Pieter-Jan
   Nam Do-Hoang Le
   Shao, Ling
   Dambre, Joni
   Odobez, Jean-Marc
TI Deep Dynamic Neural Networks for Multimodal Gesture Segmentation and
   Recognition
SO IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
LA English
DT Article
DE Deep learning; convolutional neural networks; deep belief networks;
   hidden Markov models; gesture recognition
AB This paper describes a novel method called Deep Dynamic Neural Networks (DDNN) for multimodal gesture recognition. A semi-supervised hierarchical dynamic framework based on a Hidden Markov Model (HMM) is proposed for simultaneous gesture segmentation and recognition where skeleton joint information, depth and RGB images, are the multimodal input observations. Unlike most traditional approaches that rely on the construction of complex handcrafted features, our approach learns high-level spatiotemporal representations using deep neural networks suited to the input modality: a Gaussian-Bernouilli Deep Belief Network (DBN) to handle skeletal dynamics, and a 3D Convolutional Neural Network (3DCNN) to manage and fuse batches of depth and RGB images. This is achieved through the modeling and learning of the emission probabilities of the HMM required to infer the gesture sequence. This purely data driven approach achieves a Jaccard index score of 0.81 in the ChaLearn LAP gesture spotting challenge. The performance is on par with a variety of state-of-the-art hand-tuned feature-based approaches and other learning-based methods, therefore opening the door to the use of deep learning techniques in order to further explore multimodal time series data.
C1 [Wu, Di] IDIAP, Percept & Act Understanding, Martigny, Valais, Switzerland.
   [Pigou, Lionel; Dambre, Joni] Univ Ghent, ELIS, Ghent, Oost Vlaanderen, Belgium.
   [Kindermans, Pieter-Jan] TU Berlin, Machine Learning Grp, Berlin, Germany.
   [Nam Do-Hoang Le; Odobez, Jean-Marc] IDIAP Res Inst, Comp Vis, Martigny, Valais, Switzerland.
   [Shao, Ling] Northumbria Univ, Dept Comp Sci & Digital Technol, Newcastle Upon Tyne, Tyne & Wear, England.
RP Wu, D (reprint author), IDIAP, Percept & Act Understanding, Martigny, Valais, Switzerland.
EM dwu@idiap.ch; lionel.pigou@ugent.be; p.kindermans@tu-berlin.de;
   nam.le@idiap.ch; ling.shao@ieee.org; joni.dambre@ugent.be;
   odobez@idiap.ch
RI Shao, Ling/D-3535-2011; Dambre, Joni/C-2926-2013
OI Dambre, Joni/0000-0002-9373-1210
CR [Anonymous], 2013, P ACM CHAL MULT MOD
   Baccouche M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.124
   Bastien F., 2012, P DEEP LEARN UNS FEA
   Bishop CM, 2006, PATTERN RECOGNITION
   Bourlard H., 1994, CONNECTIONIST SPEECH
   Camgoz N. C., 2014, COMP VIS ECCV 2014 W, P579
   Chang J. Y., 2014, COMP VIS ECCV 2014 W, P503
   Chaudhry R, 2013, IEEE COMPUT SOC CONF, P471, DOI 10.1109/CVPRW.2013.153
   Chen G., 2014, P EUR C COMP VIS WOR, V1, P608
   Ciresan D, 2012, PROC CVPR IEEE, P3642, DOI 10.1109/CVPR.2012.6248110
   Dollar P., 2005, Proceedings. 2nd Joint IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance (VS-PETS) (IEEE Cat. No. 05EX1178), P65
   Dosovitskiy A., 2014, ADV NEURAL INFORM PR, V27, P766
   Escalera S., 2014, COMP VIS ECCV 2014 W, P459
   Evangelidis GD, 2015, LECT NOTES COMPUT SC, V8925, P595, DOI 10.1007/978-3-319-16178-5_42
   Fothergill Simon, 2012, P SIGCHI C HUM FACT, P1737, DOI DOI 10.1145/2207676.2208303
   Graves A, 2009, IEEE T PATTERN ANAL, V31, P855, DOI 10.1109/TPAMI.2008.137
   Gupta S, 2014, LECT NOTES COMPUT SC, V8695, P345, DOI 10.1007/978-3-319-10584-0_23
   Guyon I., 2012, COMP VIS PATT REC WO, P1, DOI DOI 10.1109/CVPRW.2012.6239178
   Han J., 2013, IEEE TRANS SYST MAN, V43, P1317
   Hannun A., 2014, ARXIV14125567
   Hinton G. E, 2012, ARXIV12070580
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Jarrett K, 2009, IEEE I CONF COMP VIS, P2146, DOI 10.1109/ICCV.2009.5459469
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Kindermans P.-J, 2012, ADV NEURAL INF PROCE, P719
   Klaser A., 2008, P BRIT MACH VIS C, P275
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543
   Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7
   Le Q. V., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3361, DOI 10.1109/CVPR.2011.5995496
   Liang B., 2014, P COMP VIS WORKSH, P623
   Liu L, 2014, PATTERN RECOGN, V47, P3819, DOI 10.1016/j.patcog.2014.07.006
   Marszalek M., 2009, P IEEE C COMP VIS PA, P2929, DOI DOI 10.1109/CVPR.2009.5206557
   Mnih Volodymyr, 2013, ARXIV13125602
   Mohamed AR, 2012, IEEE T AUDIO SPEECH, V20, P14, DOI 10.1109/TASL.2011.2109382
   Monnier C, 2015, LECT NOTES COMPUT SC, V8925, P491, DOI 10.1007/978-3-319-16178-5_34
   Morris A, 2001, SPEECH COMMUN, V34, P25, DOI 10.1016/S0167-6393(00)00044-3
   Muller M., 2006, P 2006 ACM SIGGRAPH, V2, P137
   Nandakumar K, 2013, ICMI'13: PROCEEDINGS OF THE 2013 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P475, DOI 10.1145/2522848.2532593
   Neverova N, 2014, WORKSH EUR C COMP VI, P474, DOI DOI 10.1007/978-3-319-16178-533
   Neverova N., 2014, ARXIV150100102
   Neverova N., 2013, P 15 IEEE INT C COMP, P474
   Ngiam J, 2011, IEEE INT C MACH LEAR, V28, P689, DOI DOI 10.1145/2647868
   Nowozin S, 2012, ACTION POINTS REPRES
   Ofli F, 2014, J VIS COMMUN IMAGE R, V25, P24, DOI 10.1016/j.jvcir.2013.04.007
   Peng X, 2015, P EUR C COMP VIS WOR, P518
   Pigou L., 2014, P EUR C COMP VIS PAT, P1
   Renals S, 1994, IEEE T SPEECH AUDI P, V2, P161, DOI 10.1109/89.260359
   Salakhutdinov Ruslan, 2009, THESIS
   Schmidhuber J., 2014, ARXIV14047828
   Scovanner P., 2007, P 15 INT C MULT, P357, DOI DOI 10.1145/1291233.1291311
   Shao L, 2014, IEEE T CYBERNETICS, V44, P817, DOI 10.1109/TCYB.2013.2273174
   Shotton J, 2011, PROC CVPR IEEE, P1297, DOI 10.1109/CVPR.2011.5995316
   Socher R., 2012, ADV NEURAL INFORM PR, V3, P665, DOI DOI 10.1002/2014GB005021
   Sutskever I., 2013, P 30 INT C MACH LEAR, P1139
   Taylor GW, 2010, LECT NOTES COMPUT SC, V6316, P140, DOI 10.1007/978-3-642-15567-3_11
   Wang H, 2013, INT J COMPUT VISION, V103, P60, DOI 10.1007/s11263-012-0594-8
   Wang H, 2009, J OPTOELECTRON BIOME, V1, P1
   Wang J, 2012, PROC CVPR IEEE, P1290, DOI 10.1109/CVPR.2012.6247813
   Wang S., 2006, IEEE C COMP VIS PATT, V2, P1521, DOI DOI 10.1109/CVPR.2006.132
   Willems G, 2008, LECT NOTES COMPUT SC, V5303, P650, DOI 10.1007/978-3-540-88688-4_48
   Wu D., 2014, COMP VIS ECCV 2014 W, P552
   Wu D, 2014, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2014.98
   Wu D, 2013, IEEE T CIRC SYST VID, V23, P236, DOI 10.1109/TCSVT.2012.2203731
   Wu JX, 2013, ICMI'13: PROCEEDINGS OF THE 2013 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P453
   Yao A, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.67
   Yu D., 2012, AUTOMATIC SPEECH REC
NR 67
TC 111
Z9 115
U1 7
U2 91
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 0162-8828
EI 1939-3539
J9 IEEE T PATTERN ANAL
JI IEEE Trans. Pattern Anal. Mach. Intell.
PD AUG
PY 2016
VL 38
IS 8
SI SI
BP 1583
EP 1597
DI 10.1109/TPAMI.2016.2537340
PG 15
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
SC Computer Science; Engineering
GA DR5EO
UT WOS:000379926200008
PM 26955020
OA Green Accepted
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Zhao, WZ
   Du, SH
AF Zhao, Wenzhi
   Du, Shihong
TI Spectral-Spatial Feature Extraction for Hyperspectral Image
   Classification: A Dimension Reduction and Deep Learning Approach
SO IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING
LA English
DT Article
DE Balanced local discriminant embedding (BLDE); convolutional neural
   network (CNN); deep learning (DL); dimension reduction (DR); feature
   extraction
ID REPRESENTATIONS; MULTISCALE
AB In this paper, we propose a spectral-spatial feature based classification (SSFC) framework that jointly uses dimension reduction and deep learning techniques for spectral and spatial feature extraction, respectively. In this framework, a balanced local discriminant embedding algorithm is proposed for spectral feature extraction from high-dimensional hyperspectral data sets. In the meantime, convolutional neural network is utilized to automatically find spatial-related features at high levels. Then, the fusion feature is extracted by stacking spectral and spatial features together. Finally, the multiple-feature-based classifier is trained for image classification. Experimental results on well-known hyperspectral data sets show that the proposed SSFC method outperforms other commonly used methods for hyperspectral image classification.
C1 [Zhao, Wenzhi; Du, Shihong] Peking Univ, Inst Remote Sensing & GIS, Beijing 100871, Peoples R China.
RP Du, SH (reprint author), Peking Univ, Inst Remote Sensing & GIS, Beijing 100871, Peoples R China.
EM kdxiaozhi@gmail.com; dshgis@hotmail.com
RI Du, Shihong/C-9132-2019
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [41471315]; Weng Hongwu Scientific Research
   Foundation of Peking University, China [WHW201505]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 41471315 and the Weng Hongwu Scientific
   Research Foundation of Peking University, China under Grant WHW201505.
   (Corresponding author: Shihong Du.)
CR Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153
   Chang C.-I., 2007, HYPERSPECTRAL DATA E
   Chen HT, 2005, PROC CVPR IEEE, P846
   Chen SG, 2011, IEEE GEOSCI REMOTE S, V8, P369, DOI 10.1109/LGRS.2010.2076407
   Chen XY, 2014, IEEE GEOSCI REMOTE S, V11, P1797, DOI 10.1109/LGRS.2014.2309695
   Chen YS, 2014, IEEE J-STARS, V7, P2094, DOI 10.1109/JSTARS.2014.2329330
   Dalla Mura M, 2011, IEEE GEOSCI REMOTE S, V8, P542, DOI 10.1109/LGRS.2010.2091253
   Dare P, 2001, ISPRS J PHOTOGRAMM, V56, P13, DOI 10.1016/S0924-2716(01)00031-4
   Donoho D. L., 2000, P AMS MATH CHALL LEC, P1
   Fang Y, 2014, IEEE GEOSCI REMOTE S, V11, P1712, DOI 10.1109/LGRS.2014.2306689
   Fauvel M, 2012, PATTERN RECOGN, V45, P381, DOI 10.1016/j.patcog.2011.03.035
   Fauvel M, 2008, IEEE T GEOSCI REMOTE, V46, P3804, DOI 10.1109/TGRS.2008.922034
   Gui J, 2012, PATTERN RECOGN, V45, P2884, DOI 10.1016/j.patcog.2012.02.005
   Hasanlou M, 2012, IEEE GEOSCI REMOTE S, V9, P1046, DOI 10.1109/LGRS.2012.2189547
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Kang XD, 2014, IEEE T GEOSCI REMOTE, V52, P3742, DOI 10.1109/TGRS.2013.2275613
   Kuo BC, 2004, IEEE T GEOSCI REMOTE, V42, P1096, DOI 10.1109/TGRS.2004.825578
   Lawrence S., 1997, NEURAL NETWORKS IEEE, V8, P98, DOI DOI 10.1109/72.554195
   LEE C, 1993, IEEE T GEOSCI REMOTE, V31, P792, DOI 10.1109/36.239901
   Li W, 2012, IEEE T GEOSCI REMOTE, V50, P1185, DOI 10.1109/TGRS.2011.2165957
   Lunga D, 2014, IEEE SIGNAL PROC MAG, V31, P55, DOI 10.1109/MSP.2013.2279894
   Luo H., 2013, P IEEE INT C CYBCONF, P156
   Plaza A, 2005, IEEE T GEOSCI REMOTE, V43, P466, DOI 10.1109/TGRS.2004.841417
   Plaza A, 2002, IEEE T GEOSCI REMOTE, V40, P2025, DOI 10.1109/TGRS.2002.802494
   Shi Q, 2013, IEEE T GEOSCI REMOTE, V51, P4800, DOI 10.1109/TGRS.2012.2230445
   Song BQ, 2014, IEEE T GEOSCI REMOTE, V52, P5122, DOI 10.1109/TGRS.2013.2286953
   Sugiyama M, 2007, J MACH LEARN RES, V8, P1027
   Yan SC, 2007, IEEE T PATTERN ANAL, V29, P40, DOI 10.1109/TPAMI.2007.250598
   Yin JH, 2012, IEEE T IND INFORM, V8, P935, DOI 10.1109/TII.2012.2205397
   Yue J, 2015, REMOTE SENS LETT, V6, P468, DOI 10.1080/2150704X.2015.1047045
   Zhang LF, 2012, IEEE T GEOSCI REMOTE, V50, P879, DOI 10.1109/TGRS.2011.2162339
   Zhao WZ, 2016, ISPRS J PHOTOGRAMM, V113, P155, DOI 10.1016/j.isprsjprs.2016.01.004
   Zhao WZ, 2015, INT J REMOTE SENS, V36, P3368, DOI 10.1080/2150704X.2015.1062157
   Zhou YC, 2015, IEEE T GEOSCI REMOTE, V53, P1082, DOI 10.1109/TGRS.2014.2333539
NR 34
TC 250
Z9 273
U1 42
U2 299
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 0196-2892
EI 1558-0644
J9 IEEE T GEOSCI REMOTE
JI IEEE Trans. Geosci. Remote Sensing
PD AUG
PY 2016
VL 54
IS 8
BP 4544
EP 4554
DI 10.1109/TGRS.2016.2543748
PG 11
WC Geochemistry & Geophysics; Engineering, Electrical & Electronic; Remote
   Sensing; Imaging Science & Photographic Technology
SC Geochemistry & Geophysics; Engineering; Remote Sensing; Imaging Science
   & Photographic Technology
GA DT4FD
UT WOS:000381434600015
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Chen, SZ
   Wang, HP
   Xu, F
   Jin, YQ
AF Chen, Sizhe
   Wang, Haipeng
   Xu, Feng
   Jin, Ya-Qiu
TI Target Classification Using the Deep Convolutional Networks for SAR
   Images
SO IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING
LA English
DT Article
DE Automatic target recognition (ATR); deep convolutional networks
   (ConvNets); deep learning; synthetic aperture radar (SAR)
ID NEURAL-NETWORKS; RECOGNITION; REPRESENTATION; PERFORMANCE
AB The algorithm of synthetic aperture radar automatic target recognition (SAR-ATR) is generally composed of the extraction of a set of features that transform the raw input into a representation, followed by a trainable classifier. The feature extractor is often hand designed with domain knowledge and can significantly impact the classification accuracy. By automatically learning hierarchies of features from massive training data, deep convolutional networks (ConvNets) recently have obtained state-of-the-art results in many computer vision and speech recognition tasks. However, when ConvNets was directly applied to SAR-ATR, it yielded severe overfitting due to limited training images. To reduce the number of free parameters, we present a new all-convolutional networks (A-ConvNets), which only consists of sparsely connected layers, without fully connected layers being used. Experimental results on the Moving and Stationary Target Acquisition and Recognition (MSTAR) benchmark data set illustrate that A-ConvNets can achieve an average accuracy of 99% on classification of ten-class targets and is significantly superior to the traditional ConvNets on the classification of target configuration and version variants.
C1 [Chen, Sizhe; Wang, Haipeng; Xu, Feng; Jin, Ya-Qiu] Fudan Univ, Minist Educ, Key Lab Informat Sci Electromagnet Waves, Shanghai 200433, Peoples R China.
RP Wang, HP (reprint author), Fudan Univ, Minist Educ, Key Lab Informat Sci Electromagnet Waves, Shanghai 200433, Peoples R China.
EM hpwang@fudan.edu.cn; fengxu@fudan.edu.cn
RI XU, Feng/A-4582-2010; Wang, haipeng/R-3809-2016
OI XU, Feng/0000-0002-7015-1467; Wang, haipeng/0000-0003-1912-7143
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61571132, 61571134, 61471127, 61331020]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61571132, Grant 61571134, 61471127, and Grant
   61331020.
CR Abdel-Hamid O, 2014, IEEE-ACM T AUDIO SPE, V22, P1533, DOI 10.1109/TASLP.2014.2339736
   Bengio Yoshua, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P437, DOI 10.1007/978-3-642-35289-8_26
   Bishop CM, 2006, PATTERN RECOGNITION
   Bouvrie J., 2006, NOTES CONVOLUTIONAL, P38
   Ciresan D, 2012, PROC CVPR IEEE, P3642, DOI 10.1109/CVPR.2012.6248110
   Cui Y, 2011, IEEE GEOSCI REMOTE S, V8, P641, DOI 10.1109/LGRS.2010.2098434
   Dong GG, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2421440
   Dong GG, 2014, IEEE SIGNAL PROC LET, V21, P952, DOI 10.1109/LSP.2014.2321565
   Dudgeon D. E., 1993, Lincoln Laboratory Journal, V6, P3
   FUKUSHIMA K, 1980, BIOL CYBERN, V36, P193, DOI 10.1007/BF00344251
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hirose A., 2013, COMPLEX VALUED NEURA
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Keydel ER, 1996, P SOC PHOTO-OPT INS, V2757, P228, DOI 10.1117/12.242059
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   LeCun Y, 1998, LECT NOTES COMPUT SC, V1524, P9
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y, 2010, IEEE INT SYMP CIRC S, P253, DOI 10.1109/ISCAS.2010.5537907
   Novak L. M., 1997, Lincoln Laboratory Journal, V10, P187
   Novak LM, 2000, RADAR CONF, P836, DOI 10.1109/RADAR.2000.851944
   O'Sullivan JA, 2001, IEEE T AERO ELEC SYS, V37, P91, DOI 10.1109/7.913670
   Olson CF, 1997, IEEE T IMAGE PROCESS, V6, P103, DOI 10.1109/83.552100
   Park JI, 2014, IEEE T AERO ELEC SYS, V50, P1092, DOI 10.1109/TAES.2013.120378
   Ross T. D., 1999, P SOC PHOTO-OPT INS, V3721, P566
   Sandirasegaram N. M., 2005, 2005154 DRDC
   Singh R, 2002, P SOC PHOTO-OPT INS, V4727, P265, DOI 10.1117/12.478684
   Srinivas U, 2014, IEEE T AERO ELEC SYS, V50, P591, DOI 10.1109/TAES.2013.120340
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244
   Sun YJ, 2007, IEEE T AERO ELEC SYS, V43, P112, DOI 10.1109/TAES.2007.357120
   Xavier Glorot, 2011, P 14 INT C ART INT S, P315, DOI DOI 10.1177/1753193410395357
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhao Q, 2001, IEEE T AERO ELEC SYS, V37, P643, DOI 10.1109/7.937475
   Zhao Q, 2000, OPT ENG, V39, P1230, DOI 10.1117/1.602495
NR 35
TC 250
Z9 282
U1 42
U2 202
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 0196-2892
EI 1558-0644
J9 IEEE T GEOSCI REMOTE
JI IEEE Trans. Geosci. Remote Sensing
PD AUG
PY 2016
VL 54
IS 8
BP 4806
EP 4817
DI 10.1109/TGRS.2016.2551720
PG 12
WC Geochemistry & Geophysics; Engineering, Electrical & Electronic; Remote
   Sensing; Imaging Science & Photographic Technology
SC Geochemistry & Geophysics; Engineering; Remote Sensing; Imaging Science
   & Photographic Technology
GA DT4FD
UT WOS:000381434600036
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Mansoor, A
   Cerrolaza, JJ
   Idrees, R
   Biggs, E
   Alsharid, MA
   Avery, RA
   Linguraru, MG
AF Mansoor, Awais
   Cerrolaza, Juan J.
   Idrees, Rabia
   Biggs, Elijah
   Alsharid, Mohammad A.
   Avery, Robert A.
   Linguraru, Marius George
TI Deep Learning Guided Partitioned Shape Model for Anterior Visual Pathway
   Segmentation
SO IEEE TRANSACTIONS ON MEDICAL IMAGING
LA English
DT Article
DE Anterior visual pathway; intensity normalization; MRI; partitioned
   statistical model; shape model; sparse learning
ID IMAGE; CT; ALGORITHM; NERVES; MR
AB Analysis of cranial nerve systems, such as the anterior visual pathway (AVP), from MRI sequences is challenging due to their thin long architecture, structural variations along the path, and low contrast with adjacent anatomic structures. Segmentation of a pathologic AVP (e.g., with low-grade gliomas) poses additional challenges. In this work, we propose a fully automated partitioned shape model segmentation mechanism for AVP steered by multiple MRI sequences and deep learning features. Employing deep learning feature representation, this framework presents a joint partitioned statistical shape model able to deal with healthy and pathological AVP. The deep learning assistance is particularly useful in the poor contrast regions, such as optic tracts and pathological areas. Our main contributions are: 1) a fast and robust shape localization method using conditional space deep learning, 2) a volumetric multiscale curvelet transform-based intensity normalization method for robust statistical model, and 3) optimally partitioned statistical shape and appearance models based on regional shape variations for greater local flexibility. Our method was evaluated on MRI sequences obtained from 165 pediatric subjects. A mean Dice similarity coefficient of 0.779 was obtained for the segmentation of the entire AVP (optic nerve only = 0.791) using the leave-one-out validation. Results demonstrated that the proposed localized shape and sparse appearance-based learning approach significantly outperforms current state-of-the-art segmentation approaches and is as robust as the manual segmentation.
C1 [Mansoor, Awais; Cerrolaza, Juan J.; Biggs, Elijah; Avery, Robert A.; Linguraru, Marius George] Childrens Natl Hlth Syst, Washington, DC 20010 USA.
   [Idrees, Rabia] George Washington Univ, Sch Med & Hlth Sci, Washington, DC 20052 USA.
   [Alsharid, Mohammad A.] Khalifa Univ, Dept Comp Sci, Sharjah, U Arab Emirates.
RP Mansoor, A (reprint author), Childrens Natl Hlth Syst, Washington, DC 20010 USA.
EM awais.mansoor@gmail.com
FU Gilbert Family Neurofibromatosis Institute
FX This project was supported in part by a grant from the Gilbert Family
   Neurofibromatosis Institute and a philanthropic gift from the Government
   of Abu Dhabi to Children's National Health System.
CR Bekes G, 2008, MED PHYS, V35, P735, DOI 10.1118/1.2826557
   Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153
   Carpenter MB, 1983, HUMAN NEUROANATOMY
   Casio FA, 2008, MED IMAGE ANAL, V12, P469, DOI 10.1016/j.media.2008.02.001
   Chan J., 2007, OPTIC NERVE DISORDER
   Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669
   Cody DD, 2002, RADIOGRAPHICS, V22, P1255, DOI 10.1148/radiographics.22.5.g02se041255
   COOTES TF, 1995, COMPUT VIS IMAGE UND, V61, P38, DOI 10.1006/cviu.1995.1004
   Dolz J, 2015, I S BIOMED IMAGING, P1102, DOI 10.1109/ISBI.2015.7164064
   Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Ji J, 2013, PEDIATR RADIOL, V43, P1336, DOI 10.1007/s00247-013-2694-1
   Larochelle H., 2007, P 24 INT C MACH LEAR, P473, DOI DOI 10.1145/1273496.1273556
   Linguraru MG, 2012, MED IMAGE ANAL, V16, P904, DOI 10.1016/j.media.2012.02.001
   Liu JM, 2009, IEEE T MED IMAGING, V28, P571, DOI 10.1109/TMI.2008.2007820
   Lorensen W. E., 1987, COMPUTER GRAPHICS, V21, P163, DOI DOI 10.1145/37402.37422
   Mansoor A., 2015, P NF C CHILDR TUM FD
   Mansoor A., 2015, CLIN IMAGE BASED PRO, P109
   Mansoor A, 2015, RADIOGRAPHICS, V35, P1056, DOI 10.1148/rg.2015140232
   Mansoor A, 2014, IEEE T MED IMAGING, V33, P2293, DOI 10.1109/TMI.2014.2337057
   Mirzaalian H, 2015, LECT NOTES COMPUT SC, V9349, P12, DOI 10.1007/978-3-319-24553-9_2
   Noble JH, 2011, MED IMAGE ANAL, V15, P877, DOI 10.1016/j.media.2011.05.001
   Panda S., 2014, P SPIE MED IMAG
   Panda S., 2013, INT SOC MAGN RESON M
   Philipsen RHHM, 2015, IEEE T MED IMAGING, V34, P1965, DOI 10.1109/TMI.2015.2418031
   Plis SM, 2014, FRONT NEUROSCI-SWITZ, V8, DOI 10.3389/fnins.2014.00229
   Sheth S, 2009, RADIOGRAPHICS, V29, P1045, DOI 10.1148/rg.294085743
   Starck JL, 2003, IEEE T IMAGE PROCESS, V12, P706, DOI 10.1109/TIP.2003.813140
   Starck JL, 2002, IEEE T IMAGE PROCESS, V11, P670, DOI 10.1109/TIP.2002.1014998
   Toth R, 2011, MED IMAGE ANAL, V15, P214, DOI 10.1016/j.media.2010.09.002
   van Ginneken B, 2002, IEEE T MED IMAGING, V21, P924, DOI 10.1109/TMI.2002.803121
   Villamizar M, 2006, INT C PATT RECOG, P81
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Xue Yang, 2014, Clinical Image-Based Procedures. Translational Research in Medical Imaging. Third International Workshop, CLIP 2014 Held in Conjunction with MICCAI 2014. Revised Selected Papers: LNCS 8680, P109, DOI 10.1007/978-3-319-13909-8_14
   Zhang WL, 2015, NEUROIMAGE, V108, P214, DOI 10.1016/j.neuroimage.2014.12.061
   Zhao M., 2004, P AS C COMP VIS, P1074
   Zheng YF, 2008, IEEE T MED IMAGING, V27, P1668, DOI 10.1109/TMI.2008.2004421
   Zhu ZT, 2014, 2014 INTERNATIONAL CONFERENCE ON SECURITY, PATTERN ANALYSIS, AND CYBERNETICS (SPAC), P279, DOI 10.1109/SPAC.2014.6982699
NR 38
TC 7
Z9 9
U1 1
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 0278-0062
EI 1558-254X
J9 IEEE T MED IMAGING
JI IEEE Trans. Med. Imaging
PD AUG
PY 2016
VL 35
IS 8
BP 1856
EP 1865
DI 10.1109/TMI.2016.2535222
PG 10
WC Computer Science, Interdisciplinary Applications; Engineering,
   Biomedical; Engineering, Electrical & Electronic; Imaging Science &
   Photographic Technology; Radiology, Nuclear Medicine & Medical Imaging
SC Computer Science; Engineering; Imaging Science & Photographic
   Technology; Radiology, Nuclear Medicine & Medical Imaging
GA DT4FR
UT WOS:000381436000007
PM 26930677
DA 2020-02-19
ER

PT J
AU Neverova, N
   Wolf, C
   Taylor, G
   Nebout, F
AF Neverova, Natalia
   Wolf, Christian
   Taylor, Graham
   Nebout, Florian
TI ModDrop: Adaptive Multi-Modal Gesture Recognition
SO IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
LA English
DT Article
DE Gesture recognition; convolutional neural networks; multi-modal
   learning; deep learning
ID POSE; MODELS
AB We present a method for gesture detection and localisation based on multi-scale and multi-modal deep learning. Each visual modality captures spatial information at a particular spatial scale (such as motion of the upper body or a hand), and the whole system operates at three temporal scales. Key to our technique is a training strategy which exploits: i) careful initialization of individual modalities; and ii) gradual fusion involving random dropping of separate channels (dubbed ModDrop) for learning cross-modality correlations while preserving uniqueness of each modality-specific representation. We present experiments on the ChaLearn 2014 Looking at People Challenge gesture recognition track, in which we placed first out of 17 teams. Fusing multiple modalities at several spatial and temporal scales leads to a significant increase in recognition rates, allowing the model to compensate for errors of the individual classifiers as well as noise in the separate channels. Futhermore, the proposed ModDrop training technique ensures robustness of the classifier to missing signals in one or several channels to produce meaningful predictions from any number of available modalities. In addition, we demonstrate the applicability of the proposed fusion scheme to modalities of arbitrary nature by experiments on the same dataset augmented with audio.
C1 [Neverova, Natalia; Wolf, Christian] INSA Lyon, LIRIS, UMR5205, F-69621 Villeurbanne, France.
   [Taylor, Graham] Univ Guelph, Sch Engn, Guelph, ON N1G 2W1, Canada.
   [Nebout, Florian] Awabot, Villeurbanne, Rhone Alpes, France.
RP Neverova, N (reprint author), INSA Lyon, LIRIS, UMR5205, F-69621 Villeurbanne, France.
EM natalia.neverova@liris.cnrs.fr; christian.wolf@liris.cnrs.fr;
   gwtaylor@uoguelph.ca; florian.nebout@awabot.com
CR Alexandre LA, 2001, PATTERN RECOGN LETT, V22, P1283, DOI 10.1016/S0167-8655(01)00073-3
   Baccouche M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.124
   Bach F. R., 2004, P 21 INT C MACH LEAR, P6, DOI DOI 10.1145/1015330.1015424
   Baldi P, 2014, ARTIF INTELL, V210, P78, DOI 10.1016/j.artint.2014.02.004
   Bergstra J., 2010, P PYTH SCI COMP C SC, P3
   Camgoz N. C., 2014, COMP VIS ECCV 2014 W, P579
   Chang J. Y., 2014, COMP VIS ECCV 2014 W, P503
   Chen B., 2010, P NEUR INF PROC SYST
   Chen G., 2014, P EUR C COMP VIS WOR, V1, P608
   Chen X, 2013, ICMI'13: PROCEEDINGS OF THE 2013 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P467, DOI 10.1145/2522848.2532591
   Chen XJ, 2014, PROC CVPR IEEE, P1979, DOI 10.1109/CVPR.2014.254
   Couprie C., 2014, P INT C LEARN REPR
   Deng L, 2013, INT CONF ACOUST SPEE, P8604, DOI 10.1109/ICASSP.2013.6639345
   Escalera S., 2014, COMP VIS ECCV 2014 W, P459
   Escalera S, 2013, ICMI'13: PROCEEDINGS OF THE 2013 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P445, DOI 10.1145/2522848.2532595
   Evangelidis GD, 2015, LECT NOTES COMPUT SC, V8925, P595, DOI 10.1007/978-3-319-16178-5_42
   Fang GL, 2007, IEEE T SYST MAN CY A, V37, P1, DOI 10.1109/TSMCA.2006.886347
   Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231
   Gehler P, 2009, IEEE I CONF COMP VIS, P221, DOI 10.1109/ICCV.2009.5459169
   Geurts P, 2006, MACH LEARN, V63, P3, DOI 10.1007/s10994-006-6226-1
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Goodfellow I., 2013, ARXIV13024389V4
   Hernandez-Vela A, 2014, PATTERN RECOGN LETT, V50, P112, DOI 10.1016/j.patrec.2013.09.009
   Hinton G. E, 2012, ARXIV12070580
   Jain A., 2014, P AS C COMP VIS ACCV, p302~315
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Kahou SE, 2013, ICMI'13: PROCEEDINGS OF THE 2013 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P543, DOI 10.1145/2522848.2531745
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Keskin C., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P1228, DOI 10.1109/ICCVW.2011.6130391
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Le Q. V., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3361, DOI 10.1109/CVPR.2011.5995496
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee A, 2001, P EUR C SPEECH COMM, P1691
   Lehmann E. L., 1999, ELEMENTS LARGE SAMPL, P631
   Monnier C, 2015, LECT NOTES COMPUT SC, V8925, P491, DOI 10.1007/978-3-319-16178-5_34
   Nandakumar K., 2013, P 15 ACM INT C MULT, P475, DOI DOI 10.1145/2522848.2532593
   Natarajan P, 2012, PROC CVPR IEEE, P1298, DOI 10.1109/CVPR.2012.6247814
   Neverova N, 2014, WORKSH EUR C COMP VI, P474, DOI DOI 10.1007/978-3-319-16178-533
   Neverova N, 2015, LECT NOTES COMPUT SC, V9005, P687, DOI 10.1007/978-3-319-16811-1_45
   Neverova N, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P484, DOI 10.1109/ICCVW.2013.69
   Ngiam J, 2011, IEEE INT C MACH LEAR, V28, P689, DOI DOI 10.1145/2647868
   Oikonomidis I, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.101
   Peng X, 2015, P EUR C COMP VIS WOR, P518
   Pigou Lionel, 2014, WORKSH EUR C COMP VI, P572
   Qian C, 2014, PROC CVPR IEEE, P1106, DOI 10.1109/CVPR.2014.145
   Ranzato MA, 2007, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2007.383157
   Sermanet P., 2014, P INT C LEARN REPR
   Shotton J, 2011, PROC CVPR IEEE, P1297, DOI 10.1109/CVPR.2011.5995316
   Simonyan K., 2014, ARXIV14062199V1
   Srivastava N., 2013, NIPS, P2231
   Sung JY, 2012, IEEE INT CONF ROBOT, P842, DOI 10.1109/ICRA.2012.6224591
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Tang DH, 2014, PROC CVPR IEEE, P3786, DOI 10.1109/CVPR.2014.490
   Tang DH, 2013, IEEE I CONF COMP VIS, P3224, DOI 10.1109/ICCV.2013.400
   Tompson J, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2629500
   Wang F, 2013, PROC CVPR IEEE, P596, DOI 10.1109/CVPR.2013.83
   Wang H, 2013, INT J COMPUT VISION, V103, P60, DOI 10.1007/s11263-012-0594-8
   Wang J, 2012, PROC CVPR IEEE, P1290, DOI 10.1109/CVPR.2012.6247813
   Wang Sida, 2013, P 30 INT C MACH LEAR, P118
   Wu D., 2014, COMP VIS ECCV 2014 W, P552
   Wu ZX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P167, DOI 10.1145/2647868.2654931
   Ye GN, 2012, PROC CVPR IEEE, P3021, DOI 10.1109/CVPR.2012.6248032
   Zanfir M, 2013, IEEE I CONF COMP VIS, P2752, DOI 10.1109/ICCV.2013.342
   Zhou F, 2013, IEEE T PATTERN ANAL, V35, P582, DOI 10.1109/TPAMI.2012.137
NR 64
TC 61
Z9 61
U1 3
U2 35
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 0162-8828
EI 1939-3539
J9 IEEE T PATTERN ANAL
JI IEEE Trans. Pattern Anal. Mach. Intell.
PD AUG
PY 2016
VL 38
IS 8
SI SI
BP 1692
EP 1706
DI 10.1109/TPAMI.2015.2461544
PG 15
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
SC Computer Science; Engineering
GA DR5EO
UT WOS:000379926200016
DA 2020-02-19
ER

PT J
AU Ong, BT
   Sugiura, K
   Zettsu, K
AF Ong, Bun Theang
   Sugiura, Komei
   Zettsu, Koji
TI Dynamically pre-trained deep recurrent neural networks using
   environmental monitoring data for predicting PM2.5
SO NEURAL COMPUTING & APPLICATIONS
LA English
DT Article
DE Time series prediction; Deep learning; Pre-training; Recurrent neural
   networks; Elastic net; Fine particulate matter; Environmental sensor
   data
ID COMPETITION
AB Fine particulate matter (PM2.5) has a considerable impact on human health, the environment and climate change. It is estimated that with better predictions, US$9 billion can be saved over a 10-year period in the USA (State of the science fact sheet air quality http://www.noaa.gov/factsheets/new, 2012). Therefore, it is crucial to keep developing models and systems that can accurately predict the concentration of major air pollutants. In this paper, our target is to predict concentration in Japan using environmental monitoring data obtained from physical sensors with improved accuracy over the currently employed prediction models. To do so, we propose a deep recurrent neural network (DRNN) that is enhanced with a novel pre-training method using auto-encoder especially designed for time series prediction. Additionally, sensors selection is performed within DRNN without harming the accuracy of the predictions by taking advantage of the sparsity found in the network. The numerical experiments show that DRNN with our proposed pre-training method is superior than when using a canonical and a state-of-the-art auto-encoder training method when applied to time series prediction. The experiments confirm that when compared against the prediction system VENUS (National Institute for Environmental Studies. Visual Atmospheric Environment Utility System.http://envgis5.nies.go.jp/osenyosoku/, 2014),, our technique improves the accuracy of concentration level predictions that are being reported in Japan.
C1 [Ong, Bun Theang; Sugiura, Komei; Zettsu, Koji] Natl Inst Informat & Commun Technol, Univ Commun Res Inst, Informat Serv Platform Lab, 3-5 Hikaridai,Seika Cho, Kyoto 6190289, Japan.
RP Ong, BT (reprint author), Natl Inst Informat & Commun Technol, Univ Commun Res Inst, Informat Serv Platform Lab, 3-5 Hikaridai,Seika Cho, Kyoto 6190289, Japan.
EM ong_bt@nict.go.jp; komei.sugiura@nict.go.jp; zettsu@nict.go.jp
CR Air Quality Expert Group, 2012, TECHNICAL REPORT
   Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153
   Bengio Y., 2009, P 26 ANN INT C MACH, P41, DOI DOI 10.1145/1553374.1553380
   Bengio Y, 2013, INT CONF ACOUST SPEE, P8624, DOI 10.1109/ICASSP.2013.6639349
   Bergen S, 2013, ENV HLTH PERSPECT, V121, P1025
   BOURLARD H, 1988, BIOL CYBERN, V59, P291, DOI 10.1007/BF00332918
   Box G. E. P., 1976, TIME SERIES ANAL FOR
   Budde M, 2013, P 12 INT C MOB UB MU, P1, DOI DOI 10.1145/2541831.2541859
   Cheng HB, 2006, LECT NOTES ARTIF INT, V3918, P765
   Crone SF, 2011, INT J FORECASTING, V27, P635, DOI 10.1016/j.ijforecast.2011.04.001
   Deng L, 2013, INT CONF ACOUST SPEE, P8599, DOI 10.1109/ICASSP.2013.6639344
   Dergham M., 2011, ADV MAT RES, V324, P489
   Feng XY, 2012, J ENVIRON SCI-CHINA, V24, P665, DOI 10.1016/S1001-0742(11)60807-3
   Graves A., 2008, NIPS, P545
   Hagan M. T., 1996, NEURAL NETWORK DESIG
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   HOERL AE, 1970, TECHNOMETRICS, V12, P55
   Kumar PR, 2007, EUR J OPER RES, V180, P1, DOI 10.1016/j.ejor.2006.08.043
   Kuremoto T, 2014, NEUROCOMPUTING, V137, P47, DOI 10.1016/j.neucom.2013.03.047
   LeCun Y., 1998, NEURAL NETWORKS TRIC
   Lendasse A, 2007, NEUROCOMPUTING, V70, P2325, DOI 10.1016/j.neucom.2007.02.013
   McKeen S, 2007, J GEOPHYS RES-ATMOS, V112, DOI 10.1029/2006JD007608
   MinHan Kim, 2009, 2009 ICROS-SICE International Joint Conference. ICCAS-SICE 2009, P1688
   National Institute for Environmental Studies, 2014, VIS ATM ENV UT SYST
   Ong BT, 2014, IEEE INT CONF BIG DA, P760, DOI 10.1109/BigData.2014.7004302
   Pascanu R, 2014, P 2014 INT C LEARN R
   Ranzato M., 2006, ADV NEURAL INFORM PR, V19, P1137
   Ranzato M. A., 2007, ADV NEURAL INFORM PR, p1185 
   Rifai S, 2011, P 28 INT C MACH LEAR
   Takemura T, 2000, J GEOPHYS RES-ATMOS, V105, P17853, DOI 10.1029/2000JD900265
   Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267
   Tino P, 2004, IEEE T NEURAL NETWOR, V15, P6, DOI 10.1109/TNN.2003.820839
   Ugalde HMR, 2015, NEURAL COMPUT APPL, V26, P171, DOI 10.1007/s00521-014-1716-8
   Vincent P, 2008, P 25 INT C MACH LEAR, P1096, DOI DOI 10.1145/1390156.1390294
   Wakamatsu S, 2013, ASIAN J ATMOS ENVIRO, V7, P177, DOI 10.5572/ajae.2013.7.4.177
   Whitle P., 1951, HYPOTHESIS TESTING T
   World Health Organization, 2013, HLTH EFF PART MAT
   Young SR, 2012, 2012 11TH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA 2012), VOL 2, P54, DOI 10.1109/ICMLA.2012.140
   Zheng Y, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P1436
   Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x
NR 41
TC 54
Z9 57
U1 7
U2 64
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 0941-0643
EI 1433-3058
J9 NEURAL COMPUT APPL
JI Neural Comput. Appl.
PD AUG
PY 2016
VL 27
IS 6
BP 1553
EP 1566
DI 10.1007/s00521-015-1955-3
PG 14
WC Computer Science, Artificial Intelligence
SC Computer Science
GA DQ3CE
UT WOS:000379079600009
OA Other Gold, Green Published
DA 2020-02-19
ER

PT J
AU Li, Q
   Li, K
   You, X
   Bu, SH
   Liu, ZB
AF Li, Qin
   Li, Ke
   You, Xiong
   Bu, Shuhui
   Liu, Zhenbao
TI Place recognition based on deep feature and adaptive weighting of
   similarity matrix
SO NEUROCOMPUTING
LA English
DT Article
DE Convolutional Neural Networks (CNNs); Image description matrix;
   Similarity matrix; Place recognition
ID LOCALIZATION; EFFICIENT; SPACE; SHIFT
AB Effective features and similarity measures are two key points to achieve good performance in place recognition. In this paper we propose an image similarity measurement method based on deep learning and similarity matrix analyzing, which can be used for place recognition and infrastructure-free navigation. In order to obtain high representative feature, Convolutional Neural Networks (CNNs) are adopted to extract hierarchical information of objects in the image. In the method, the image is divided into patches, then the similarity matrix is constructed according to the patch similarities. The overall image similarity is determined by a proposed adaptive weighting scheme based on analyzing the data difference in the similarity matrix. Experimental results show that the proposed method is more robust than the existing methods, and it can effectively distinguish the different place images with similar-looking and the same place images with local changes. Furthermore, the proposed method has the capability to effectively solve the loop closure detection in Simultaneous Locations and Mapping (SLAM). (C) 2016 Published by Elsevier B.V.
C1 [Li, Qin; Li, Ke; You, Xiong] Zhengzhou Inst Surveying & Mapping, Zhengzhou, Peoples R China.
   [Bu, Shuhui; Liu, Zhenbao] Northwestern Polytech Univ, Zhengzhou, Peoples R China.
RP Li, K (reprint author), Zhengzhou Inst Surveying & Mapping, Zhengzhou, Peoples R China.
EM leequer120419@163.com; Like19771223@163.com; youarexion@163.com;
   bushuhui@nwpu.edu.cn; liuzhenbao@nwpu.edu.cn
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [41201390, 61202185, 61573284]; Project of Science
   and Technology Innovation of Henan Province [142101510005]; Fundamental
   Research Funds for the Central UniversitiesFundamental Research Funds
   for the Central Universities [310201401-(JCQ01009, JCQ01012)]; Open
   Projects Program of National Laboratory of Pattern Recognition (NLPR)
FX Our research is partially supported by grants from National Natural
   Science Foundation of China (41201390, 61202185, and 61573284), the
   Project of Science and Technology Innovation of Henan Province
   (142101510005), the Fundamental Research Funds for the Central
   Universities (310201401-(JCQ01009, JCQ01012)), and the Open Projects
   Program of National Laboratory of Pattern Recognition (NLPR).
CR Abdel-hamid O., 2013, P INTERSPEECH, P3366
   Achanta R, 2010, TECHNICAL REPORT
   Badino H, 2012, IEEE INT CONF ROBOT, P1635, DOI 10.1109/ICRA.2012.6224716
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1016/j.cviu.2007.09.014
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Ciresan DC, 2011, PROC INT CONF DOC, P1135, DOI 10.1109/ICDAR.2011.229
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Cummins M. J., 2010, P 27 INT C MACH LEAR, P3
   Cummins M, 2008, INT J ROBOT RES, V27, P647, DOI 10.1177/0278364908090961
   Cummins Mark, 2009, ROBOTICS SCI SYSTEMS, V5
   Douze M, 2009, P ACM INT C IM VID R, P19
   Dudek G., 2000, Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No.00CH37065), P1030, DOI 10.1109/ROBOT.2000.844735
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   Fu-cheng You, 2010, 2010 INT C MULT TECH, P1
   Fukumoto K, 2015, INT J INTELL TRANSP, V13, P63, DOI 10.1007/s13177-014-0086-z
   FUKUSHIMA K, 1980, BIOL CYBERN, V36, P193, DOI 10.1007/BF00344251
   HAN J, 2014, IEEE T CIRCUITS SYST, V25, P1309
   Han JW, 2015, IEEE T CYBERNETICS, V45, P1692, DOI 10.1109/TCYB.2014.2358647
   Han JW, 2015, IEEE T GEOSCI REMOTE, V53, P3325, DOI 10.1109/TGRS.2014.2374218
   Han JW, 2014, ISPRS J PHOTOGRAMM, V89, P37, DOI 10.1016/j.isprsjprs.2013.12.011
   Han JW, 2013, IEEE T CIRC SYST VID, V23, P2009, DOI 10.1109/TCSVT.2013.2242594
   HUA GC, 2015, J REF J ADV COMPUT I, V19, P11
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li Zhengqin, 2009, T PAMI, V31, P2209
   Liu J, 2013, ARXIV13045168
   Liu M, 2014, IEEE T ROBOT, V30, P310, DOI 10.1109/TRO.2013.2272250
   Lowe D., 1999, P INT C COMP VIS, V2, P1150, DOI [10.1109/ICCV.1999.790410, DOI 10.1109/ICCV.1999.790410]
   Ming-Yu Liu, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2097, DOI 10.1109/CVPR.2011.5995323
   Nowak E, 2006, LECT NOTES COMPUT SC, V3954, P490
   Pass G., 1996, Proceeding. Third IEEE Workshop on Applications of Computer Vision. WACV'96 (Cat. No.96TB100084), P96, DOI 10.1109/ACV.1996.572008
   Ren XF, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P10
   Roth P. M., 2008, ICGTR0108 GRAZ U TEC
   Roy K., 2013, INT J SCI RES, V2, P538
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Sikirid Ivan, 2013, ARXIV13100316
   Smith M, 2009, INT J ROBOT RES, V28, P595, DOI 10.1177/0278364909103911
   Valgren C, 2010, ROBOT AUTON SYST, V58, P149, DOI 10.1016/j.robot.2009.09.010
   Vedaldi A., 2015, MATCONVNET CONVOLUTI
   Wang JM, 2013, 2013 INTERNATIONAL CONFERENCE ON COMPUTER AND ROBOT VISION (CRV), P91, DOI 10.1109/CRV.2013.27
NR 40
TC 8
Z9 8
U1 0
U2 43
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0925-2312
EI 1872-8286
J9 NEUROCOMPUTING
JI Neurocomputing
PD JUL 26
PY 2016
VL 199
BP 114
EP 127
DI 10.1016/j.neucom.2016.03.029
PG 14
WC Computer Science, Artificial Intelligence
SC Computer Science
GA DN7BN
UT WOS:000377230400011
DA 2020-02-19
ER

PT J
AU Zhang, CY
   Chen, CLP
   Chen, DW
   Ng, KT
AF Zhang, Chun-Yang
   Chen, C. L. Philip
   Chen, Dewang
   Ng, Kin Tek
TI MapReduce based distributed learning algorithm for Restricted Boltzmann
   Machine
SO NEUROCOMPUTING
LA English
DT Article; Proceedings Paper
CT 11th International Symposium on Neural Networks (ISNN)
CY NOV 28-DEC 01, 2014
CL PEOPLES R CHINA
SP Chinese Univ Hong Kong, Univ Macau, European Neural Network Soc, Int Neural Network Soc, IEEE Computat Intelligence Soc, Asia Pacific Neural Network Assembly
DE Deep learning; Restricted Boltzmann Machine; Big data; MapReduce
AB Deep learning is recently regarded as the closest artificial intelligence model to human brain. It is about learning multiple levels of representation and abstraction that help to make sense of data such as images, sound, and text. One deep model often consists of a hierarchical architecture that has the capability to model super non-linear and stochastic problems. Restricted Boltzmann Machine (RBM) is the main constructing block of current deep networks, as most of deep architectures are built with it. Based on MapReduce framework and Hadoop distributed file system, this paper proposes a distributed algorithm for training the RBM model. Its implementation and performance are evaluated on Big Data platform-Hadoop. The main contribution of the new learning algorithm is that it solves the scalability problem that limits the development of deep learning. The intelligence growing process of human brain requires learning from Big Data. The distributed learning mechanism for RBM makes it possible to abstract sophisticated and informative features from Big Data to achieve high-level intelligence. The evaluations of the proposed learning algorithm are carried out on image inpainting and classification problems based on the BAS dataset and MNIST hand-written digits dataset. (C) 2016 Elsevier B.V. All rights reserved.
C1 [Zhang, Chun-Yang; Chen, Dewang] Fuzhou Univ, Coll Math & Comp Sci, Fuzhou, Peoples R China.
   [Zhang, Chun-Yang; Chen, C. L. Philip; Ng, Kin Tek] Univ Macau, Fac Sci & Technol, Dept Comp & Informat Sci, Macau, Peoples R China.
RP Chen, CLP (reprint author), Univ Macau, Fac Sci & Technol, Dept Comp & Informat Sci, Macau, Peoples R China.
EM cyzhangfst@gmail.com; philip.chen@ieee.org; dwchen@fzu.edu.cn;
   joker315029@hotmail.com
RI Chen, C. L. Philip/O-2657-2016
OI Chen, C. L. Philip/0000-0001-5451-7230
CR Bai J, 2015, NEUROCOMPUTING, V165, P280, DOI 10.1016/j.neucom.2015.03.017
   Bekkerman R., 2012, SCALING MACHINE LEAR
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Chen CLP, 2015, IEEE T FUZZY SYST, V23, P2163, DOI 10.1109/TFUZZ.2015.2406889
   Chen CLP, 2014, INFORM SCIENCES, V275, P314, DOI 10.1016/j.ins.2014.01.015
   Cho K. H., 2013, INT JOINT C NEUR NET, P1, DOI [10.1109/IJCNN.2013.6706831, DOI 10.1109/IJCNN.2013.6706831]
   Dean J, 2004, USENIX Association Proceedings of the Sixth Symposium on Operating Systems Design and Implementation (OSDE '04), P137
   Fischer A, 2014, PATTERN RECOGN, V47, P25, DOI 10.1016/j.patcog.2013.05.025
   Hinton G, 2005, AISTATS, V10, P33
   Hinton G., 2010003 UTML TR
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Leng B, 2015, NEUROCOMPUTING, V151, P593, DOI 10.1016/j.neucom.2014.06.084
   Palit I, 2012, IEEE T KNOWL DATA EN, V24, P1904, DOI 10.1109/TKDE.2011.208
   Salakhutdinov R., 2007, P INT C MACH LEARN, V24, P791, DOI DOI 10.1145/1273496.1273596
   Salakhutdinov R, 2013, IEEE T PATTERN ANAL, V35, P1958, DOI 10.1109/TPAMI.2012.269
   Salakhutdinov R, 2012, NEURAL COMPUT, V24, P1967, DOI 10.1162/NECO_a_00311
   Siniscalchi SM, 2013, NEUROCOMPUTING, V106, P148, DOI 10.1016/j.neucom.2012.11.008
   Sutskever I., 2007, J MACH LEARN RES, P548
   Taylor G.W., 2007, ADV NEURAL INFORM PR, V19, P1345
   Wang N., CORR
   Yu WC, 2015, NEUROCOMPUTING, V149, P308, DOI 10.1016/j.neucom.2014.03.077
   Zhang CY, 2014, IEEE SYS MAN CYBERN, P4037, DOI 10.1109/SMC.2014.6974564
   Zhou SS, 2013, NEUROCOMPUTING, V120, P536, DOI 10.1016/j.neucom.2013.04.017
NR 26
TC 14
Z9 15
U1 2
U2 47
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0925-2312
EI 1872-8286
J9 NEUROCOMPUTING
JI Neurocomputing
PD JUL 19
PY 2016
VL 198
SI SI
BP 4
EP 11
DI 10.1016/j.neucom.2015.09.129
PG 8
WC Computer Science, Artificial Intelligence
SC Computer Science
GA DN7BM
UT WOS:000377230300002
DA 2020-02-19
ER

PT J
AU Unger, M
   Bar, A
   Shapira, B
   Rokach, L
AF Unger, Moshe
   Bar, Ariel
   Shapira, Bracha
   Rokach, Lior
TI Towards latent context-aware recommendation systems
SO KNOWLEDGE-BASED SYSTEMS
LA English
DT Article
DE Recommendation; Recommender systems; Context-aware recommender systems;
   Context; Matrix factorization; Deep learning
AB The emergence and penetration of smart mobile devices has given rise to the development of context aware systems that utilize sensors to collect available data about users in order to improve various user services. Recently, the use of context-aware recommender systems (CARS) aimed at recommending items to users has expanded, particularly those that consider user context. Adding context to recommendation systems is challenging, because the addition of various environmental contexts to the recommendation process results in the expansion of its dimensionality, and thus increases sparsity. Therefore, existing CARS tend to incorporate a small set of pre-defined explicit contexts which do not necessary represent user context or reflect the optimal set of features for the recommendation process. We suggest a novel approach centered on representing environmental features as low dimensional unsupervised latent contexts. We extract data from a rich set of mobile sensors in order to infer unexplored user contexts in an unsupervised manner. The latent contexts are hidden context patterns modeled as numeric vectors which are efficiently extracted from raw sensor data. The latent contexts are automatically learned for each user utilizing unsupervised deep learning techniques and PCA on the data collected from the user's mobile phone. Integrating the data extracted from high dimensional sensors into a new latent context-aware recommendation algorithm results in up to a 20% increase in recommendation accuracy. (C) 2016 Elsevier B.V. All rights reserved.
C1 [Unger, Moshe] Ben Gurion Univ Negev, POB 84105, IL-84105 Beer Sheva, Israel.
   [Unger, Moshe] BGU, Telekom Innovat Labs, POB 84105, Beer Sheva, Israel.
RP Unger, M (reprint author), Ben Gurion Univ Negev, POB 84105, IL-84105 Beer Sheva, Israel.; Unger, M (reprint author), BGU, Telekom Innovat Labs, POB 84105, Beer Sheva, Israel.
EM mosheun@post.bgu.ac.il
RI Rokach, Lior/F-8247-2010
OI Rosh, Orna/0000-0002-1642-4575
CR Adomavicius G, 2011, RECOMMENDER SYSTEMS HANDBOOK, P217, DOI 10.1007/978-0-387-85820-3_7
   Aharony N, 2011, PERVASIVE MOB COMPUT, V7, P643, DOI 10.1016/j.pmcj.2011.09.004
   Armentano Marcelo G., 2013, Human-Computer Interaction. Users and Contexts of Use. 15th International Conference, HCI International 2013. Proceedings: LNCS 8006, P107, DOI 10.1007/978-3-642-39265-8_12
   Badrul S., 2000, P 2 ACM C EL COMM
   Baltrunas L, 2009, P 3 ACM C REC SYST
   Baltrunas L., 2011, P 5 ACM C REC SYST
   Baltrunas L, 2014, USER MODEL USER-ADAP, V24, P7, DOI 10.1007/s11257-012-9137-9
   Billsus D, 2002, COMMUN ACM, V45, P34
   Bobadilla J, 2013, KNOWL-BASED SYST, V46, P109, DOI 10.1016/j.knosys.2013.03.012
   Codina V., 2013, P 3 WORKSH CONT AW R
   Consolvo S., 2008, P SIGCHI C HUM FACT
   Foggia P., 2014, ADV VID SIGN BAS SUR
   Guo L, 2015, SOFT COMPUT, V19, P1351, DOI 10.1007/s00500-014-1347-0
   Guo WM, 2014, APPL MECH MATER, V668-669, P1237, DOI 10.4028/www.scientific.net/AMM.668-669.1237
   Hariri Negar, 2012, P 6 ACM C REC SYST
   Hazan E., 2011, ADV NEURAL INF PROCE, P1233
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Huang H., 2012, USING CONTEXT AWARE
   Ji K, 2015, KNOWL-BASED SYST, V83, P42, DOI 10.1016/j.knosys.2015.03.008
   Jolliffe I, 2002, PRINCIPAL COMPONENT
   Karatzoglou A., 2010, P 4 ACM C REC SYST
   Konig I., 2013, P 2013 ACM C PERV UB
   Koren Y, 2009, COMPUTER, V42, P30, DOI 10.1109/MC.2009.263
   Krizhevsky A, 2012, ADV NEURAL INF PROCE
   Kwon Y, 2014, EXPERT SYST APPL, V41, P6067, DOI 10.1016/j.eswa.2014.04.037
   Lane N. D., 2015, P 16 INT WORKSH MOB
   Lee WP, 2014, KNOWL-BASED SYST, V56, P167, DOI 10.1016/j.knosys.2013.11.007
   Li F., 2013, WORKSH ACT CONT AW S
   Low Y, 2014, ARXIV14082041
   Panniello U, 2009, P 3 ACM C REC SYST
   Rodriguez JJ, 2006, IEEE T PATTERN ANAL, V28, P1619, DOI 10.1109/TPAMI.2006.211
   Savage N. S., 2012, IM FEELING LOCO LOCA
   Shani G, 2011, RECOMMENDER SYSTEMS HANDBOOK, P257, DOI 10.1007/978-0-387-85820-3_8
   Sitkrongwong P., 2013, WEB INT WI INT AG TE, V1
   Sun F., 2013, COMP SCI ENG CSE 201
   Trabelsi D, 2013, IEEE T AUTOM SCI ENG, V10, P829, DOI 10.1109/TASE.2013.2256349
   Unger, 2014, P 2014 ACM INT JOINT
   Pereira ALV, 2015, KNOWL-BASED SYST, V82, P11, DOI 10.1016/j.knosys.2015.02.016
   Wang H, 2015, P 21 ACM SIGKDD INT
   Xu Z., NEUROCOMPUTING, V155
   Zhou P, 2015, KNOWL-BASED SYST, V88, P97, DOI 10.1016/j.knosys.2015.08.003
NR 41
TC 25
Z9 28
U1 1
U2 48
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0950-7051
EI 1872-7409
J9 KNOWL-BASED SYST
JI Knowledge-Based Syst.
PD JUL 15
PY 2016
VL 104
BP 165
EP 178
DI 10.1016/j.knosys.2016.04.020
PG 14
WC Computer Science, Artificial Intelligence
SC Computer Science
GA DO4EA
UT WOS:000377733400014
DA 2020-02-19
ER

PT J
AU Jiao, ZC
   Gao, XB
   Wang, Y
   Li, J
AF Jiao, Zhicheng
   Gao, Xinbo
   Wang, Ying
   Li, Jie
TI A deep feature based framework for breast masses classification
SO NEUROCOMPUTING
LA English
DT Article
DE Deep learning; Convolutional neural network; Breast mass classification;
   Computer-aided diagnosis; Feature visualization
ID COMPUTER-AIDED DIAGNOSIS; NEURAL-NETWORK; MAMMOGRAMS
AB Characteristic classification of mass plays a role of vital importance in diagnosis of breast cancer. The existing computer aided diagnosis (CAD) methods used to benefit a lot from low-level or middle-level features which are not that good at the simulation of real diagnostic processes, adding difficulties in improving the classification performance. In this paper, we design a deep feature based framework for breast mass classification task. It mainly contains a convolutional neural network (CNN) and a decision mechanism. Combining intensity information and deep features automatically extracted by the trained CNN from the original image, our proposed method could better simulate the diagnostic procedure operated by doctors and achieved state-of-art performance. In this framework, doctors' global and local impressions left by mass images were represented by deep features extracted from two different layers called high-level and middle-level features. Meanwhile, the original images were regarded as detailed descriptions of the breast mass. Then, classifiers based on features above were used in combination to predict classes of test images. And outcomes of classifiers based on different features were analyzed jointly to determine the types of test images. With the help of two kinds of feature visualization methods, deep features extracted from different layers illustrate effective in classification performance and diagnosis simulation. In addition, our method was applied to DDSM dataset and achieved high accuracy under two objective evaluation measures. (C) 2016 Elsevier B.V. All rights reserved.
C1 [Jiao, Zhicheng; Gao, Xinbo; Wang, Ying; Li, Jie] Xidian Univ, Sch Elect Engn, Lab Video & Image Proc Syst, Xian, Peoples R China.
RP Gao, XB (reprint author), Xidian Univ, Sch Elect Engn, Lab Video & Image Proc Syst, Xian, Peoples R China.
EM xbgao@mail.xidian.edu.cn
CR [Anonymous], 2004, DIG DAT SCREEN MAMM
   Avni U, 2011, IEEE T MED IMAGING, V30, P733, DOI 10.1109/TMI.2010.2095026
   Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153
   Bengio Y., 2013, HDB NEURAL INFORM PR, V49
   Beura S, 2015, NEUROCOMPUTING, V154, P1, DOI 10.1016/j.neucom.2014.12.032
   Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16
   Cadieu CF, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003963
   CHAN HP, 1990, INVEST RADIOL, V25, P1102, DOI 10.1097/00004424-199010000-00006
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Cheng E, 2010, I S BIOMED IMAGING, P197, DOI 10.1109/ISBI.2010.5490381
   Cire D. C., 2012, ADV NEURAL INFORM PR, V2, P2843
   Ciresan DC, 2013, LECT NOTES COMPUT SC, V8150, P411, DOI 10.1007/978-3-642-40763-5_51
   Coates A., 2011, INT C ART INT STAT, V15, P215, DOI 10.1177/1753193410390845
   Cristianini N, 2002, J INTELL INF SYST, V18, P127, DOI 10.1023/A:1013625426931
   Csurka G., 2004, WORKSH STAT LEARN CO, V1, P1, DOI DOI 10.1234/12345678
   Cun Le, 1990, ADV NEURAL INF PROCE
   Dahl GE, 2013, INT CONF ACOUST SPEE, P8609, DOI 10.1109/ICASSP.2013.6639346
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Doi K, 2007, COMPUT MED IMAG GRAP, V31, P198, DOI 10.1016/j.compmedimag.2007.02.002
   Donahue J., 2013, ARXIV13101531
   Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231
   Fergus R, 2005, IEEE I CONF COMP VIS, P1816
   FUKUSHIMA K, 1980, BIOL CYBERN, V36, P193, DOI 10.1007/BF00344251
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Helmstaedter M, 2013, NATURE, V500, P168, DOI 10.1038/nature12346
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Howlader NA, 2011, SEER CANC STAT REV 1
   HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837
   Jain V., 2009, ADV NEURAL INFORM PR, P769
   Jemal A, 2008, CA-CANCER J CLIN, V58, P71, DOI 10.3322/CA.2007.0010
   Jiang YL, 1999, ACAD RADIOL, V6, P22, DOI 10.1016/S1076-6332(99)80058-0
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   LeCun Y, 2010, IEEE INT SYMP CIRC S, P253, DOI 10.1109/ISCAS.2010.5537907
   Mavroforakis ME, 2006, ARTIF INTELL MED, V37, P145, DOI 10.1016/j.artmed.2006.03.002
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Ng A, 2011, CS294A LECT NOTES, V72
   Ramirez-Villegas JF, 2012, NEUROCOMPUTING, V77, P82, DOI 10.1016/j.neucom.2011.08.015
   Rangayyan RM, 1997, IEEE T MED IMAGING, V16, P799, DOI 10.1109/42.650876
   Rojas-Dominguez A, 2009, COMPUT BIOL MED, V39, P678, DOI 10.1016/j.compbiomed.2009.05.002
   Roth H., 2015, IEEE T MED IMAGING
   Salakhutdinov R, 2009, ARTIF INTELL, P448, DOI DOI 10.1109/CVPRW.2009.5206577
   Sutskever I, 2009, ADV NEURAL INFORM PR, P1601
   Timp S, 2007, IEEE T MED IMAGING, V26, P945, DOI 10.1109/TMI.2007.897392
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   van Ginneken B, 2001, IEEE T MED IMAGING, V20, P1228, DOI 10.1109/42.974918
   Vedaldi A., 2014, ARXIV1412A564
   Vedaldi A, 2010, P 18 ACM INT C MULT, P1469, DOI DOI 10.1145/1873951.1874249
   Verma B, 2010, EXPERT SYST APPL, V37, P3344, DOI 10.1016/j.eswa.2009.10.016
   Verma B, 2009, PATTERN RECOGN, V42, P1845, DOI 10.1016/j.patcog.2009.02.009
   Wang DF, 2009, NEUROCOMPUTING, V72, P3296, DOI 10.1016/j.neucom.2009.02.015
   Wang S, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON NANOSCALE ARCHITECTURE, P1
   Wang Y, 2014, NEUROCOMPUTING, V144, P107, DOI 10.1016/j.neucom.2013.11.050
   Wang Y, 2011, PATTERN RECOGN, V44, P1903, DOI 10.1016/j.patcog.2010.08.002
   Xavier Glorot, 2011, P 14 INT C ART INT S, P315, DOI DOI 10.1177/1753193410395357
   Xie W., 2015, NEUROCOMPUTING
   Yoon S, 2009, PATTERN RECOGN LETT, V30, P1489, DOI 10.1016/j.patrec.2009.06.012
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zeiler MD, 2010, PROC CVPR IEEE, P2528, DOI 10.1109/CVPR.2010.5539957
   Zhang WL, 2015, NEUROIMAGE, V108, P214, DOI 10.1016/j.neuroimage.2014.12.061
NR 61
TC 61
Z9 65
U1 5
U2 87
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0925-2312
EI 1872-8286
J9 NEUROCOMPUTING
JI Neurocomputing
PD JUL 12
PY 2016
VL 197
BP 221
EP 231
DI 10.1016/j.neucom.2016.02.060
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA DM9OD
UT WOS:000376694700020
DA 2020-02-19
ER

PT J
AU Jaureguiberry, X
   Vincent, E
   Richard, G
AF Jaureguiberry, Xabier
   Vincent, Emmanuel
   Richard, Gael
TI Fusion Methods for Speech Enhancement and Audio Source Separation
SO IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING
LA English
DT Article
DE Audio source separation; fusion; aggregation; ensemble; deep neural
   networks (DNNs); deep learning; variational Bayes; model averaging;
   non-negative matrix factorization (NMF); speech enhancement; singing
   voice extraction
ID NONNEGATIVE MATRIX FACTORIZATION; BLIND SEPARATION; MODEL; MUSIC
AB A wide variety of audio source separation techniques exist and can already tackle many challenging industrial issues. However, in contrast with other application domains, fusion principles were rarely investigated in audio source separation despite their demonstrated potential in classification tasks. In this paper, we propose a general fusion framework which takes advantage of the diversity of existing separation techniques in order to improve separation quality. We obtain new source estimates by summing the individual estimates given by different separation techniques weighted by a set of fusion coefficients. We investigate three alternative fusion methods which are based on standard nonlinear optimization, Bayesian model averaging, or deep neural networks. Experiments conducted for both speech enhancement and singing voice extraction demonstrate that all the proposed methods outperform traditional model selection. The use of deep neural networks for the estimation of time-varying coefficients notably leads to large quality improvements, up to 3 dB in terms of signal-to-distortion ratio compared to model selection.
C1 [Jaureguiberry, Xabier] Zenly, F-75002 Paris, France.
   [Vincent, Emmanuel] INRIA, F-54600 Villers Les Nancy, France.
   [Richard, Gael] Telecom ParisTech, F-75014 Paris, France.
RP Jaureguiberry, X (reprint author), Zenly, F-75002 Paris, France.
EM xabierj@gmail.com; emmanuel.vincent@inria.fr;
   gael.richard@telecom-paristech.fr
FU Research Program EDi-Son3D - ANR, the French State agency for
   researchFrench National Research Agency (ANR) [ANR-13-CORD-0008-01]
FX This work was supported in part by the Research Program EDi-Son3D
   (ANR-13-CORD-0008-01) funded by ANR, the French State agency for
   research.
CR Adiloglu K., 2012, RT0428 INRIA
   Appriou A, 2001, INT J INTELL SYST, V16, P1107
   Bastien F., 2012, P DEEP LEARN UNS FEA, P1
   Beal MJ, 2003, BAYESIAN STATISTICS 7, P453
   Bergstra J., 2010, P 9 PYTH SCI C, P1
   Bertin N, 2007, INT CONF ACOUST SPEE, P65
   Bertsekas DP, 1999, NONLINEAR PROGRAMMIN
   Bishop CM, 2006, PATTERN RECOGNITION
   Blei D. M., 2010, P 27 INT C MACH LEAR, P439
   Burred J. J., 2006, P AUD ENG SOC CONV, P1
   Cemgil Ali Taylan, 2009, Comput Intell Neurosci, P785152, DOI 10.1155/2009/785152
   Chandna S, 2014, 2014 IEEE WORKSHOP ON STATISTICAL SIGNAL PROCESSING (SSP), P424, DOI 10.1109/SSP.2014.6884666
   Cichocki A., 2009, NONNEGATIVE MATRIX T
   Coleman TF, 1996, SIAM J OPTIMIZ, V6, P418, DOI 10.1137/0806023
   Comon P, 2010, HANDBOOK OF BLIND SOURCE SEPARATION: INDEPENDENT COMPONENT ANALYSIS AND APPLICATIONS, P1
   CORDUNEANU A, 2001, ARTIF INTELL, V2001, P27
   Duffner S, 2007, LECT NOTES COMPUT SC, V4668, P249
   Durrieu JL, 2011, IEEE J-STSP, V5, P1180, DOI 10.1109/JSTSP.2011.2158801
   Durrieu JL, 2010, IEEE T AUDIO SPEECH, V18, P564, DOI 10.1109/TASL.2010.2041114
   Ellis DPW, 2006, INT CONF ACOUST SPEE, P957
   Fevotte C, 2009, NEURAL COMPUT, V21, P793, DOI 10.1162/neco.2008.04-08-771
   Freund Y., 1995, COMPUTATIONAL LEARNI, V904, P23, DOI DOI 10.1007/3-540-59119-2_166
   Geiger J. T., 2013, P 2 CHIME WORKSH JUN, P25
   Grais Emad M., 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P3734, DOI 10.1109/ICASSP.2014.6854299
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Hoeting JA, 1999, STAT SCI, V14, P382
   Huang PS, 2012, INT CONF ACOUST SPEE, P57, DOI 10.1109/ICASSP.2012.6287816
   Jaureguiberry X., 2013, P IEEE INT WORKSH MA, P1
   Jaureguiberry X., 2014, P INTERSPEECH, P4
   Jaureguiberry X, 2014, 2014 IEEE WORKSHOP ON STATISTICAL SIGNAL PROCESSING (SSP), P33, DOI 10.1109/SSP.2014.6884568
   Jourjine A, 2000, INT CONF ACOUST SPEE, P2985, DOI 10.1109/ICASSP.2000.861162
   Katahira K, 2008, J PHYS CONF SER, V95, DOI 10.1088/1742-6596/95/1/012015
   Kittler J, 1998, IEEE T PATTERN ANAL, V20, P226, DOI 10.1109/34.667881
   Lee DD, 2001, ADV NEUR IN, V13, P556
   Liutkus A, 2014, IEEE T SIGNAL PROCES, V62, P4298, DOI 10.1109/TSP.2014.2332434
   Moritz N, 2013, P 2 INT WORKSH MACH, P1
   Mysore GJ, 2010, LECT NOTES COMPUT SC, V6365, P140, DOI 10.1007/978-3-642-15995-4_18
   O'Grady PD, 2005, INT J IMAG SYST TECH, V15, P18, DOI 10.1002/ima.20035
   Rabinovich A., 2006, P IEEE C COMP VIS PA, P1130
   Rafii Z., 2012, ISMIR, P583
   Raj B., 2011, P INTERSPEECH, V2011, P1217
   Ravanelli M, 2015, MMCOMMONS'15: PROCEEDINGS OF THE 2015 WORKSHOP ON COMMUNITY-ORGANIZED MULTIMODAL MINING: OPPORTUNITIES FOR NOVEL SOLUTIONS, P19, DOI 10.1145/2814815.2814816
   ROSENTHAL DF, 1998, COMPUTATIONAL AUDITO
   Roux J. L., 2013, P IEEE WORKSH APPL S, P1
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Tan VYF, 2013, IEEE T PATTERN ANAL, V35, P1592, DOI 10.1109/TPAMI.2012.240
   Vincent E, 2006, IEEE T AUDIO SPEECH, V14, P91, DOI 10.1109/TSA.2005.860342
   Vincent E, 2007, SIGNAL PROCESS, V87, P1933, DOI 10.1016/j.sigpro.2007.01.016
   Vincent E, 2006, IEEE T AUDIO SPEECH, V14, P1462, DOI 10.1109/TSA.2005.858005
   Vincent E, 2014, IEEE SIGNAL PROC MAG, V31, P107, DOI 10.1109/MSP.2013.2297440
   Vincent E, 2013, 2013 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING (ASRU), P162, DOI 10.1109/ASRU.2013.6707723
   Vincent E, 2013, INT CONF ACOUST SPEE, P126, DOI 10.1109/ICASSP.2013.6637622
   Virtanen T, 2008, INT CONF ACOUST SPEE, P1825, DOI 10.1109/ICASSP.2008.4517987
   Weninger F, 2014, 2014 IEEE GLOBAL CONFERENCE ON SIGNAL AND INFORMATION PROCESSING (GLOBALSIP), P577, DOI 10.1109/GlobalSIP.2014.7032183
   Yilmaz O, 2004, IEEE T SIGNAL PROCES, V52, P1830, DOI 10.1109/TSP.2004.828896
   Yu D., 2011, U.S. Patent, Patent No. [13/304643, 13304643]
   Zeiler MD, 2013, INT CONF ACOUST SPEE, P3517, DOI 10.1109/ICASSP.2013.6638312
NR 57
TC 10
Z9 10
U1 1
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 2329-9290
EI 2329-9304
J9 IEEE-ACM T AUDIO SPE
JI IEEE-ACM Trans. Audio Speech Lang.
PD JUL
PY 2016
VL 24
IS 7
BP 1266
EP 1279
DI 10.1109/TASLP.2016.2553441
PG 14
WC Acoustics; Engineering, Electrical & Electronic
SC Acoustics; Engineering
GA EK4AZ
UT WOS:000393870800010
DA 2020-02-19
ER

PT J
AU Kavi, R
   Kulathumani, V
   Rohit, F
   Kecojevic, V
AF Kavi, Rahul
   Kulathumani, Vinod
   Rohit, Fnu
   Kecojevic, Vlad
TI Multiview fusion for activity recognition using deep neural networks
SO JOURNAL OF ELECTRONIC IMAGING
LA English
DT Article
DE recurrent neural networks; long short term memory; driver distraction;
   multicamera systems; camera network
ID DATASETS
AB Convolutional neural networks (ConvNets) coupled with long short term memory (LSTM) networks have been recently shown to be effective for video classification as they combine the automatic feature extraction capabilities of a neural network with additional memory in the temporal domain. This paper shows how multiview fusion can be applied to such a ConvNet LSTM architecture. Two different fusion techniques are presented. The system is first evaluated in the context of a driver activity recognition system using data collected in a multicamera driving simulator. These results show significant improvement in accuracy with multiview fusion and also show that deep learning performs better than a traditional approach using spatiotemporal features even without requiring any background subtraction. The system is also validated on another publicly available multiview action recognition dataset that has 12 action classes and 8 camera views. (C) 2016 SPIE and IS&T
C1 [Kavi, Rahul; Kulathumani, Vinod; Rohit, Fnu] West Virginia Univ, Dept Comp Sci & Elect Engn, 109 Res Way, Morgantown, WV 26506 USA.
   [Kecojevic, Vlad] West Virginia Univ, Dept Min Engn, 395 Evansdale Dr, Morgantown, WV 26506 USA.
RP Kulathumani, V (reprint author), West Virginia Univ, Dept Comp Sci & Elect Engn, 109 Res Way, Morgantown, WV 26506 USA.
EM vinod.kulathumani@mail.wvu.edu
FU Alpha Foundation for the Improvement of Mine Safety; Health, Inc.
FX This study was partially sponsored by the Alpha Foundation for the
   Improvement of Mine Safety and Health, Inc. The views, opinions, and
   recommendations expressed herein are solely those of the authors and do
   not imply any endorsement by the Alpha Foundation, its directors or
   staff. Its financial contribution is gratefully acknowledged.
CR Bergstra J., 2010, P 9 PYTH SCI C, P1
   Dalal N, 2005, PROC CVPR IEEE, P886
   Davis JW, 2001, IEEE WORKSHOP ON DETECTION AND RECOGNITION OF EVENTS IN VIDEO, PROCEEDINGS, P39, DOI 10.1109/EVENT.2001.938864
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Du Y, 2016, IEEE T IMAGE PROCESS, V25, P3010, DOI 10.1109/TIP.2016.2552404
   Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714
   Dubuisson S, 2016, MACH VISION APPL, V27, P23, DOI 10.1007/s00138-015-0713-y
   Edwards M, 2016, COMPUT VIS IMAGE UND, V144, P73, DOI 10.1016/j.cviu.2015.10.010
   Fan JL, 2010, IEEE T NEURAL NETWOR, V21, P1610, DOI 10.1109/TNN.2010.2066286
   Fang HQ, 2013, LECT NOTES ELECTR EN, V254, P341, DOI 10.1007/978-3-642-38524-7_37
   Feng JG, 2015, FRONT INFORM TECH EL, V16, P917, DOI 10.1631/FITEE.1500080
   Holte M. B., 2011, P 2011 JOINT ACM WOR, P47, DOI DOI 10.1145/2072572.2072588
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kavi R, 2013, J SENS ACTUAR NETW, V2, P486, DOI 10.3390/jsan2030486
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kulathumani V., 2011, WVU MULTIVIEW ACTION
   Kulathumani V., 2015, INTEGRATED SURFACE M
   Kushwaha AKS, 2015, J ELECTRON IMAGING, V24, DOI 10.1117/1.JEI.24.5.051005
   Poppe R, 2010, IMAGE VISION COMPUT, V28, P976, DOI 10.1016/j.imavis.2009.11.014
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Sermanet P., 2013, ARXIV13126229
   Shahroudy A., 2016, IEEE C COMP VIS PATT
   Shao L, 2012, PATTERN RECOGN LETT, V33, P438, DOI 10.1016/j.patrec.2011.05.015
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Singh B, 2016, IEEE C COMP VIS PATT
   Soutner D., 2013, TEXT SPEECH DIALOGUE, P105
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   SUNDERMEYER M, 2012, INTERSPEECH
   Tieleman T., 2012, COURSERA NEURAL NETW
   Weinland D, 2011, COMPUT VIS IMAGE UND, V115, P224, DOI 10.1016/j.cviu.2010.10.002
NR 31
TC 10
Z9 10
U1 1
U2 17
PU IS&T & SPIE
PI BELLINGHAM
PA 1000 20TH ST, BELLINGHAM, WA 98225 USA
SN 1017-9909
EI 1560-229X
J9 J ELECTRON IMAGING
JI J. Electron. Imaging
PD JUL
PY 2016
VL 25
IS 4
AR 043010
DI 10.1117/1.JEI.25.4.043010
PG 8
WC Engineering, Electrical & Electronic; Optics; Imaging Science &
   Photographic Technology
SC Engineering; Optics; Imaging Science & Photographic Technology
GA EC0JP
UT WOS:000387787000023
DA 2020-02-19
ER

PT J
AU Jiao, LC
   Liu, F
AF Jiao, Licheng
   Liu, Fang
TI Wishart Deep Stacking Network for Fast POLSAR Image Classification
SO IEEE TRANSACTIONS ON IMAGE PROCESSING
LA English
DT Article
DE Deep stacking network (DSN); POLSAR image classification; Wishart
   network (WN); Wishart deep stacking network (W-DSN)
ID DECOMPOSITION; INFORMATION
AB Inspired by the popular deep learning architecture, deep stacking network (DSN), a specific deep model for polarimetric synthetic aperture radar (POLSAR) image classification is proposed in this paper, which is named Wishart DSN (W-DSN). First of all, a fast implementation of Wishart distance is achieved by a special linear transformation, which speeds up the classification of POLSAR image and makes it possible to use this polarimetric information in the following neural network (NN). Then, a single-hidden-layer NN based on the fast Wishart distance is defined for POLSAR image classification, which is named Wishart network (WN) and improves the classification accuracy. Finally, a multi-layer NN is formed by stacking WNs, which is in fact the proposed deep learning architecture W-DSN for POLSAR image classification and improves the classification accuracy further. In addition, the structure of WN can be expanded in a straightforward way by adding hidden units if necessary, as well as the structure of the W-DSN. As a preliminary exploration on formulating specific deep learning architecture for POLSAR image classification, the proposed methods may establish a simple but clever connection between POLSAR image interpretation and deep learning. The experiment results tested on real POLSAR image show that the fast implementation of Wishart distance is very efficient (a POLSAR image with 768 000 pixels can be classified in 0.53 s), and both the single-hidden-layer architecture WN and the deep learning architecture W-DSN for POLSAR image classification perform well and work efficiently.
C1 [Jiao, Licheng; Liu, Fang] Xidian Univ, Key Lab Intelligent Percept & Image Understanding, Sch Elect Engn, Int Res Ctr Intelligent Percept & Computat,Minist, Xian 710071, Peoples R China.
RP Jiao, LC (reprint author), Xidian Univ, Key Lab Intelligent Percept & Image Understanding, Sch Elect Engn, Int Res Ctr Intelligent Percept & Computat,Minist, Xian 710071, Peoples R China.
EM lchjiao@mail.xidian.edu.cn; fayliu77@163.com
FU National Basic Research Program (973 Program) of ChinaNational Basic
   Research Program of China [2013CB329402]; National Natural Science
   Foundation of ChinaNational Natural Science Foundation of China
   [61271302, 61272282, 61572383, 61573267]
FX This work was supported in part by the National Basic Research Program
   (973 Program) of China under Grant 2013CB329402 and in part by the
   National Natural Science Foundation of China under Grant 61271302, Grant
   61272282, Grant 61572383, and Grant 61573267. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. David Clausi.
CR Ainsworth TL, 2009, ISPRS J PHOTOGRAMM, V64, P464, DOI 10.1016/j.isprsjprs.2008.12.008
   Attema E., 1998, ESA PUBL SP, V1225, P59
   Cao F, 2007, IEEE T GEOSCI REMOTE, V45, P3454, DOI 10.1109/TGRS.2007.907601
   Chen CT, 2003, IEEE T GEOSCI REMOTE, V41, P2089, DOI 10.1109/TGRS.2003.813494
   Chen L. C., 2014, 14127062 ARXIV
   Cloude SR, 1997, IEEE T GEOSCI REMOTE, V35, P68, DOI 10.1109/36.551935
   Collobert R, 2011, J MACH LEARN RES, V12, P2493
   Dahl GE, 2012, IEEE T AUDIO SPEECH, V20, P30, DOI 10.1109/TASL.2011.2134090
   Deng L., 2011, P INTERSPEECH, P2285
   Deng L, 2013, INT CONF ACOUST SPEE, P3153, DOI 10.1109/ICASSP.2013.6638239
   Deng L, 2012, INT CONF ACOUST SPEE, P2133, DOI 10.1109/ICASSP.2012.6288333
   Du LJ, 1996, INT GEOSCI REMOTE SE, P439, DOI 10.1109/IGARSS.1996.516366
   Fukuda S, 2001, INT GEOSCI REMOTE SE, P187, DOI 10.1109/IGARSS.2001.976097
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   GOODMAN NR, 1963, ANN MATH STAT, V34, P152, DOI 10.1214/aoms/1177704250
   Guo YH, 2015, INT GEOSCI REMOTE SE, P1841, DOI 10.1109/IGARSS.2015.7326150
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lardeux C, 2009, IEEE T GEOSCI REMOTE, V47, P4143, DOI 10.1109/TGRS.2009.2023908
   Lee JS, 1999, IEEE T GEOSCI REMOTE, V37, P2249, DOI 10.1109/36.789621
   LEE JS, 1994, INT GEOSCI REMOTE SE, P2179, DOI 10.1109/IGARSS.1994.399685
   Lee JS, 2004, IEEE T GEOSCI REMOTE, V42, P722, DOI 10.1109/TGRS.2003.819883
   LEE JS, 1994, INT J REMOTE SENS, V15, P2299, DOI 10.1080/01431169408954244
   Li J., 2015, SPARSE DEEP STACKING
   Liu F, 2016, IEEE T GEOSCI REMOTE, V54, P3292, DOI 10.1109/TGRS.2016.2514504
   Liu M, 2014, IEEE T GEOSCI REMOTE, V52, P7483, DOI 10.1109/TGRS.2014.2310451
   Lv Q, 2015, J SENSORS, DOI 10.1155/2015/538063
   Pinheiro P. H. O., 2013, RECURRENT CONVOLUTIO
   POTTIER E, 1993, P SOC PHOTO-OPT INS, V1748, P72, DOI 10.1117/12.140635
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shang F, 2013, INT GEOSCI REMOTE SE, P3175, DOI 10.1109/IGARSS.2013.6723501
   Simonyan K., 2014, VERY DEEP CONVOLUTIO
   Singh G, 2013, IEEE T GEOSCI REMOTE, V51, P3014, DOI 10.1109/TGRS.2012.2212446
   Tzeng YC, 1998, IEEE T GEOSCI REMOTE, V36, P301, DOI 10.1109/36.655339
   Ulaby F. T., 1990, RADAR POLARIMETRY GE, V1, P376
   Wang CL, 2014, IET RADAR SONAR NAV, V8, P957, DOI 10.1049/iet-rsn.2014.0076
   Wang W., 2006, P 10 IEEE SING INT C, P1
   Yu P, 2012, IEEE T GEOSCI REMOTE, V50, P1302, DOI 10.1109/TGRS.2011.2164085
   Zhou GY, 2010, IEEE NATL RADAR CONF, P491, DOI 10.1109/RADAR.2010.5494572
NR 38
TC 34
Z9 34
U1 2
U2 25
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1057-7149
EI 1941-0042
J9 IEEE T IMAGE PROCESS
JI IEEE Trans. Image Process.
PD JUL
PY 2016
VL 25
IS 7
DI 10.1109/TIP.2016.2567069
PG 14
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
SC Computer Science; Engineering
GA DW5QW
UT WOS:000383702600001
PM 28113713
DA 2020-02-19
ER

PT J
AU Castro, E
   Hjelm, RD
   Plis, SM
   Dinh, L
   Turner, JA
   Calhoun, VD
AF Castro, Eduardo
   Hjelm, R. Devon
   Plis, Sergey M.
   Dinh, Laurent
   Turner, Jessica A.
   Calhoun, Vince D.
TI Deep Independence Network Analysis of Structural Brain Imaging:
   Application to Schizophrenia
SO IEEE TRANSACTIONS ON MEDICAL IMAGING
LA English
DT Article
DE Deep learning; NICE; nonlinear ICA; schizophrenia; structural MRl
ID VOXEL-BASED MORPHOMETRY; GRAY-MATTER DIFFERENCES; COMPONENT ANALYSIS;
   1ST-EPISODE SCHIZOPHRENIA; LIKELIHOOD ESTIMATION; METAANALYSIS; VOLUME;
   MRI; INFORMATION; SEPARATION
AB Linear independent component analysis (ICA) is a standard signal processing technique that has been extensively used on neuroimaging data to detect brain networks with coherent brain activity (functional MRI) or covarying structural patterns (structural MRI). However, its formulation assumes that the measured brain signals are generated by a linear mixture of the underlying brain networks and this assumption limits its ability to detect the inherent nonlinear nature of brain interactions. In this paper, we introduce nonlinear independent component estimation (NICE) to structural MRI data to detect abnormal patterns of gray matter concentration in schizophrenia patients. For this biomedical application, we further addressed the issue of model regularization of nonlinear ICA by performing dimensionality reduction prior to NICE, together with an appropriate control of the complexity of the model and the usage of a proper approximation of the probability distribution functions of the estimated components. We show that our results are consistent with previous findings in the literature, but we also demonstrate that the incorporation of nonlinear associations in the data enables the detection of spatial patterns that are not identified by linear ICA. Specifically, we show networks including basal ganglia, cerebellum and thalamus that show significant differences in patients versus controls, some of which show distinct nonlinear patterns.
C1 [Castro, Eduardo; Hjelm, R. Devon; Plis, Sergey M.; Calhoun, Vince D.] Univ New Mexico, Mind Res Network, Albuquerque, NM 87106 USA.
   [Dinh, Laurent] Univ Montreal, Dept Comp Sci & Operat Res, Montreal, PQ H3C 3J7, Canada.
   [Turner, Jessica A.] Georgia State Univ, Dept Psychol, Atlanta, GA 30303 USA.
RP Castro, E (reprint author), Univ New Mexico, Mind Res Network, Albuquerque, NM 87106 USA.
EM ecastrow@gmail.com
RI Turner, Jessica A/H-7282-2015
OI Turner, Jessica A/0000-0003-0076-8434; Castro,
   Eduardo/0000-0002-7788-9069
FU NIHUnited States Department of Health & Human ServicesNational
   Institutes of Health (NIH) - USA [R01EB005846, P20GM103472]
FX This work was funded by NIH Grants R01EB005846 and P20GM103472. Asterisk
   indicates corresponding author.
CR Allen E. A., 2011, MIND RES NETW MED IM
   Almeida LB, 2004, J MACH LEARN RES, V4, P1297
   Ashburner J, 2000, NEUROIMAGE, V11, P805, DOI 10.1006/nimg.2000.0582
   Bangalore SS, 2009, NEUROREPORT, V20, P729, DOI 10.1097/WNR.0b013e32832ae501
   Beckmann CF, 2001, NEUROIMAGE, V13, pS76
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bora E, 2011, SCHIZOPHR RES, V127, P46, DOI 10.1016/j.schres.2010.12.020
   Canessa N, 2013, J NEUROSCI, V33, P14307, DOI 10.1523/JNEUROSCI.0497-13.2013
   Caprihan A, 2011, BRAIN CONNECT, V1, P133, DOI 10.1089/brain.2011.0015
   Castro E., 2015, P 2015 C DES CIRC IN, P1, DOI DOI 10.1109/DCIS.2015.7388591)
   Cronenwett WJ, 2010, CURR TOP BEHAV NEURO, V4, P509, DOI 10.1007/7854_2010_55
   Dinh L., 2014, CORR
   Fischer Asja, 2012, Progress in Pattern Recognition, Image Analysis, ComputerVision, and Applications. Proceedings 17th Iberoamerican Congress, CIARP 2012, P14, DOI 10.1007/978-3-642-33275-3_2
   Fjell AM, 2013, NEUROBIOL AGING, V34, P2239, DOI 10.1016/j.neurobiolaging.2013.04.006
   Friston KJ, 2000, PHILOS T R SOC B, V355, P215, DOI 10.1098/rstb.2000.0560
   Glahn DC, 2008, BIOL PSYCHIAT, V64, P774, DOI 10.1016/j.biopsych.2008.03.031
   Gupta CN, 2015, SCHIZOPHRENIA BULL, V41, P1133, DOI 10.1093/schbul/sbu177
   Harmeling S, 2003, NEURAL COMPUT, V15, P1089, DOI 10.1162/089976603765202677
   Hjelm RD, 2014, NEUROIMAGE, V96, P245, DOI 10.1016/j.neuroimage.2014.03.048
   Honea R, 2005, AM J PSYCHIAT, V162, P2233, DOI 10.1176/appi.ajp.162.12.2233
   Huang P, 2015, SCI REP-UK, V5, DOI 10.1038/srep14505
   Hyvarinen A, 1999, NEURAL NETWORKS, V12, P429, DOI 10.1016/S0893-6080(98)00140-3
   Ilin A, 2004, LECT NOTES COMPUT SC, V3195, P766
   Kingma D.P., 2014, INT C LEARN REPR BAN
   Koch K, 2014, BRIT J PSYCHIAT, V205, P204, DOI 10.1192/bjp.bp.113.138099
   Leung M, 2011, SCHIZOPHRENIA BULL, V37, P199, DOI 10.1093/schbul/sbp099
   Meda SA, 2008, SCHIZOPHR RES, V101, P95, DOI 10.1016/j.schres.2008.02.007
   Rubin A., 2012, STAT EVIDENCE BASED
   Singer A, 2008, APPL COMPUT HARMON A, V25, P226, DOI 10.1016/j.acha.2007.11.001
   Suzuki M, 2002, SCHIZOPHR RES, V55, P41, DOI 10.1016/S0920-9964(01)00224-9
   Tieleman T., 2012, COURSERA NEURAL NETW
   Vita A, 2012, TRANSL PSYCHIAT, V2, DOI 10.1038/tp.2012.116
   Xu L, 2009, HUM BRAIN MAPP, V30, P711, DOI 10.1002/hbm.20540
   Yang HH, 1998, SIGNAL PROCESS, V64, P291, DOI 10.1016/S0165-1684(97)00196-5
NR 34
TC 9
Z9 9
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 0278-0062
EI 1558-254X
J9 IEEE T MED IMAGING
JI IEEE Trans. Med. Imaging
PD JUL
PY 2016
VL 35
IS 7
BP 1729
EP 1740
DI 10.1109/TMI.2016.2527717
PG 12
WC Computer Science, Interdisciplinary Applications; Engineering,
   Biomedical; Engineering, Electrical & Electronic; Imaging Science &
   Photographic Technology; Radiology, Nuclear Medicine & Medical Imaging
SC Computer Science; Engineering; Imaging Science & Photographic
   Technology; Radiology, Nuclear Medicine & Medical Imaging
GA DR5GN
UT WOS:000379931600014
PM 26891483
OA Green Accepted
DA 2020-02-19
ER

PT J
AU Gao, W
   Zhou, ZH
AF Gao, Wei
   Zhou, Zhi-Hua
TI Dropout Rademacher complexity of deep neural networks
SO SCIENCE CHINA-INFORMATION SCIENCES
LA English
DT Article
DE artificial intelligence; machine learning; deep learning; dropout;
   Rademacher complexity
ID BOUNDS
AB Great successes of deep neural networks have been witnessed in various real applications. Many algorithmic and implementation techniques have been developed; however, theoretical understanding of many aspects of deep neural networks is far from clear. A particular interesting issue is the usefulness of dropout, which was motivated from the intuition of preventing complex co-adaptation of feature detectors. In this paper, we study the Rademacher complexity of different types of dropouts, and our theoretical results disclose that for shallow neural networks (with one or none hidden layer) dropout is able to reduce the Rademacher complexity in polynomial, whereas for deep neural networks it can amazingly lead to an exponential reduction.
C1 [Gao, Wei; Zhou, Zhi-Hua] Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing 210023, Jiangsu, Peoples R China.
   [Gao, Wei; Zhou, Zhi-Hua] Nanjing Univ, Collaborat Innovat Ctr Novel Software Technol & I, Nanjing 210023, Jiangsu, Peoples R China.
RP Zhou, ZH (reprint author), Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing 210023, Jiangsu, Peoples R China.; Zhou, ZH (reprint author), Nanjing Univ, Collaborat Innovat Ctr Novel Software Technol & I, Nanjing 210023, Jiangsu, Peoples R China.
EM zhouzh@lamda.nju.edu.cn
FU National Basic Research Program of ChinaNational Basic Research Program
   of China [2014CB340501]; National Natural Science Foundation of
   ChinaNational Natural Science Foundation of China [61333014]; Jiangsu
   Science Foundation [BK20150586]
FX This work was supported by National Basic Research Program of China
   (Grant No. 2014CB340501), National Natural Science Foundation of China
   (Grant No. 61333014) and Jiangsu Science Foundation (Grant No.
   BK20150586).
CR Amari S, 1997, IEEE T NEURAL NETWOR, V8, P985, DOI 10.1109/72.623200
   Anthony Martin, 2009, NEURAL NETWORK LEARN
   Ba J., 2013, ADV NEURAL INFORM PR, P3084
   Baldi P, 2014, ARTIF INTELL, V210, P78, DOI 10.1016/j.artint.2014.02.004
   Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690
   Bartlett PL, 1998, IEEE T INFORM THEORY, V44, P525, DOI 10.1109/18.661502
   Bo LF, 2011, PROC CVPR IEEE, P1729, DOI 10.1109/CVPR.2011.5995719
   Ciresan D, 2012, PROC CVPR IEEE, P3642, DOI 10.1109/CVPR.2012.6248110
   Ciresan DC, 2010, NEURAL COMPUT, V22, P3207, DOI 10.1162/NECO_a_00052
   Coates A., 2013, P 30 INT C MACH LEAR, p1337~1345
   Cortes C., 2010, P 27 INT C MACH LEAR, P247
   Dahl GE, 2012, IEEE T AUDIO SPEECH, V20, P30, DOI 10.1109/TASL.2011.2134090
   Dahl GE, 2013, INT CONF ACOUST SPEE, P8609, DOI 10.1109/ICASSP.2013.6639346
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton G. E, 2012, ARXIV12070580
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Kakade S M, 2008, ADV NEURAL INFORM PR, V24, P351
   Karpinski M, 1997, J COMPUT SYST SCI, V54, P169, DOI 10.1006/jcss.1997.1477
   Koltchinskii V, 2002, ANN STAT, V30, P1
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   Ledoux M., 2002, PROBABILITY BANACH S
   Liu YJ, 2014, SCI CHINA INFORM SCI, V57, DOI 10.1007/s11432-012-4779-0
   Maurer A, 2006, J MACH LEARN RES, V7, P117
   McAllester David, 2013, ARXIV13072118
   McDiarmid C., 1989, SURVEYS COMBINATORIC, V141, P148, DOI DOI 10.1017/CBO9781107359949.008
   Meir R., 2003, J MACHINE LEARNING R, V4, P839
   Mobahi H., 2009, P 26 ANN INT C MACH, P737
   Nair V, 2010, ICML, V27, P807, DOI DOI 10.0RG/PAPERS/432.PDF
   Neal RM, 1996, LECT NOTES STAT
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Wager S., 2013, ADV NEURAL INFORM PR, V26, P351
   Wan L., 2013, P 30 INT C MACH LEAR, P1058
   Wang Sida, 2013, P 30 INT C MACH LEAR, P118
   Weigend A. S., 1991, ADV NEURAL INFORMATI, P875
   Zou B, 2013, SCI CHINA INFORM SCI, V56, DOI 10.1007/s11432-011-4412-7
NR 35
TC 17
Z9 21
U1 7
U2 21
PU SCIENCE PRESS
PI BEIJING
PA 16 DONGHUANGCHENGGEN NORTH ST, BEIJING 100717, PEOPLES R CHINA
SN 1674-733X
EI 1869-1919
J9 SCI CHINA INFORM SCI
JI Sci. China-Inf. Sci.
PD JUL
PY 2016
VL 59
IS 7
AR 072104
DI 10.1007/s11432-015-5470-z
PG 12
WC Computer Science, Information Systems; Engineering, Electrical &
   Electronic
SC Computer Science; Engineering
GA DS3LR
UT WOS:000380685000014
DA 2020-02-19
ER

PT J
AU Xu, L
   Jiang, CX
   Ren, Y
   Chen, HH
AF Xu, Lei
   Jiang, Chunxiao
   Ren, Yong
   Chen, Hsiao-Hwa
TI Microblog Dimensionality Reduction-A Deep Learning Approach
SO IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING
LA English
DT Article
DE Microblog mining; dimension reduction; text representation; semantic
   relatedness; deep autoencoder
AB Exploring potentially useful information from huge amount of textual data produced by microblogging services has attracted much attention in recent years. An important preprocessing step of microblog text mining is to convert natural language texts into proper numerical representations. Due to the short-length characteristics of microblog texts, using term frequency vectors to represent microblog texts will cause "sparse data" problem. Finding proper representations of microblog texts is a challenging issue. In this paper, we apply deep networks to map the high-dimensional representations of microblog texts to low-dimensional representations. To improve the result of dimensionality reduction, we take advantage of the semantic similarity derived from two types of microblog-specific information, namely the retweet relationship and hashtags. Two types of approaches, including modifying training data and modifying the training objective of deep networks, are proposed to make use of microblog-specific information. Experiment results show that the deep models perform better than traditional dimensionality reduction methods such as latent semantic analysis and latent Dirichlet allocation topic model, and the use of microblog-specific information can help to learn better representations.
C1 [Xu, Lei] Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
   [Jiang, Chunxiao; Ren, Yong] Tsinghua Univ, Dept Elect Engn, Beijing 100084, Peoples R China.
   [Chen, Hsiao-Hwa] Natl Cheng Kung Univ, Dept Engn Sci, Tainan 70101, Taiwan.
RP Xu, L (reprint author), Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
EM l-xu08@mails.tsinghua.edu.cn; chx.jiang@gmail.com; reny@tsinghua.edu.cn;
   hshwchen@ieee.org
RI Jiang, Chunxiao/W-3904-2017
OI Jiang, Chunxiao/0000-0002-3703-121X
FU NSFC ChinaNational Natural Science Foundation of China [61371079,
   61271267, 91338203]; China Postdoctoral Science FoundationChina
   Postdoctoral Science Foundation
FX This work was supported by the NSFC China under projects 61371079,
   61271267 and 91338203, and a China Postdoctoral Science Foundation
   funded project.
CR Achtert E, 2012, PROC INT CONF DATA, P1285, DOI 10.1109/ICDE.2012.128
   Amigo E, 2009, INFORM RETRIEVAL, V12, P461, DOI 10.1007/s10791-008-9066-8
   Bengio Yoshua, 2012, CORR
   Blei D. M., 2001, J MACH LEARN RES, V3, P993
   Chakraborty D., 2004, P INT C COMM DEV INT, P454
   Collobert R., 2008, P 25 ICML, V25, P160, DOI [10.1145/1390156.1390177, DOI 10.1145/1390156.1390177]
   Deng L, 2014, APSIPA TRANS SIGNAL, V3, DOI 10.1017/atsip.2013.9
   Diao Q., 2012, P 50 ANN M ASS COMP, P536
   Erhan D, 2010, J MACH LEARN RES, V11, P625
   Grant C. E., 2011, P TEXT RETR C, P1
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   HOLMSTROM L, 1992, IEEE T NEURAL NETWOR, V3, P24, DOI 10.1109/72.105415
   Hu X., 2011, P 20 ACM INT C INF K, P2465
   Jin O., 2011, P 20 ACM INT C INF K, P775, DOI DOI 10.1145/2063576.2063689
   Kaijun Wang, 2009, Data Science Journal, V8, P88, DOI 10.2481/dsj.007-020
   Landauer TK, 1998, DISCOURSE PROCESS, V25, P259, DOI 10.1080/01638539809545028
   Le Q. V., 2014, P 31 INT C MACH LEAR, P1188, DOI DOI 10.1007/978-1-4614-3223-4
   Maas A.L., 2011, P 49 ANN M ASS COMP, P142
   Masci J., 2013, CORR
   Masci J, 2014, IEEE T PATTERN ANAL, V36, P824, DOI 10.1109/TPAMI.2013.225
   Mikolov T., 2013, CORR
   Min M.R., 2010, P 27 INT C MACH LEAR, P791
   Norouzi M., 2011, INT C MACH LEARN, P353
   Ramage D., 2010, P 4 INT AAAI C WEBL, V10, P130
   Ranzato M. A., 2008, P 25 INT C MACH LEAR, P792, DOI DOI 10.1145/1390156.1390256
   Salakhutdinov R, 2009, INT J APPROX REASON, V50, P969, DOI 10.1016/j.ijar.2008.11.006
   SALTON G, 1975, COMMUN ACM, V18, P613, DOI 10.1145/361219.361220
   Tang JL, 2012, FRONT COMPUT SCI-CHI, V6, P88, DOI 10.1007/s11704-011-1167-7
   Teh Y.W., 2004, NIPS, P1385
   Turian J, 2010, ACL 2010: 48TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, P384
   Wang XJ, 2013, ADV INTEL SYS RES, V64, P502
   Xia Yan, 2013, Journal of Networks, V8, P917, DOI 10.4304/jnw.8.4.917-923
   Yan T, 2010, THIRD INTERNATIONAL SYMPOSIUM ON COMPUTER SCIENCE AND COMPUTATIONAL TECHNOLOGY (ISCSCT 2010), P338
   Yu D, 2011, IEEE SIGNAL PROC MAG, V28, P145, DOI 10.1109/MSP.2010.939038
   Zhou SS, 2013, NEUROCOMPUTING, V120, P536, DOI 10.1016/j.neucom.2013.04.017
NR 36
TC 7
Z9 8
U1 1
U2 35
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1041-4347
EI 1558-2191
J9 IEEE T KNOWL DATA EN
JI IEEE Trans. Knowl. Data Eng.
PD JUL 1
PY 2016
VL 28
IS 7
BP 1779
EP 1789
DI 10.1109/TKDE.2016.2540639
PG 11
WC Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Engineering, Electrical & Electronic
SC Computer Science; Engineering
GA DR7ZF
UT WOS:000380117500013
DA 2020-02-19
ER

PT J
AU Gao, LG
   Chen, PY
   Yu, SM
AF Gao, Ligang
   Chen, Pai-Yu
   Yu, Shimeng
TI Demonstration of Convolution Kernel Operation on Resistive Cross-Point
   Array
SO IEEE ELECTRON DEVICE LETTERS
LA English
DT Article
DE Convolution kernel; neuromorphic computing; cross-point array; resistive
   memory
ID DEVICES; SYNAPSES
AB Convolution is the key operation in the convolutional neural network, one of the most popular deep learning algorithms. The implementation of the convolution kernel on the resistive cross-point array is different than the implementation of the matrix-vector multiplication in prior works. In this letter, we propose a dimensional reduction of 2-D kernel matrix into 1-D column vector, i.e., a column of the array, and enable the parallel readout of multiple 2-D kernels simultaneously. As a proof-of-concept demonstration, we use the Prewitt kernels to detect both horizontal and vertical edges of the 20 x 20 pixels of black-and-white MNIST handwritten digits. The experiments were performed on the fabricated 12 x 12 resistive cross-point array based on the Pt/HfOx/TiN structure. The experimental results of the Prewitt kernel operation perfectly matches the simulation results, indicating the feasibility of the proposed implementation methodology of the convolution kernel on resistive cross-point array.
C1 [Gao, Ligang; Chen, Pai-Yu; Yu, Shimeng] Arizona State Univ, Sch Elect Comp & Energy Engn, Tempe, AZ 85287 USA.
RP Yu, SM (reprint author), Arizona State Univ, Sch Elect Comp & Energy Engn, Tempe, AZ 85287 USA.
EM shimengy@asu.edu
FU  [NSF-CCF-1552687]
FX This work was supported by the NSF-CCF-1552687. The review of this
   letter was arranged by Editor B. Govoreanu. (Corresponding author:
   Shimeng Yu.)
CR Eryilmaz S. B., 2013, P IEEE INT EL DEV M, DOI DOI 10.1109/IEDM.2013.6724691
   Fang RC, 2015, IEEE ELECTR DEVICE L, V36, P567, DOI 10.1109/LED.2015.2420665
   Gao B, 2014, ACS NANO, V8, P6998, DOI 10.1021/nn501824r
   Gao LG, 2015, IEEE ELECTR DEVICE L, V36, P1157, DOI 10.1109/LED.2015.2481819
   Gao LG, 2015, NANOTECHNOLOGY, V26, DOI 10.1088/0957-4484/26/45/455204
   Garbin D, 2015, IEEE T ELECTRON DEV, V62, P2494, DOI 10.1109/TED.2015.2440102
   Hu M, 2014, IEEE T NEUR NET LEAR, V25, P1864, DOI 10.1109/TNNLS.2013.2296777
   Kim KH, 2012, NANO LETT, V12, P389, DOI 10.1021/nl203687n
   Kim S., 2015, IEDM, DOI [10.1109/IEDM.2015.7409716, 10.1109/iedm.2015.7409716, DOI 10.1109/IEDM.2015.7409716]
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Park S, 2015, SCI REP-UK, V5, DOI 10.1038/srep10123
   Prezioso M, 2015, NATURE, V521, P61, DOI 10.1038/nature14441
   Querlioz D, 2015, P IEEE, V103, P1398, DOI 10.1109/JPROC.2015.2437616
   Suri M., 2015, NEUR NETW IJCNN 2015, P1
   Wang I., 2014, IEEE INT EL DEV M, P665, DOI [10.1109/IEDM.2014.7047127(2014), DOI 10.1109/IEDM.2014.7047127]
   Wong HSP, 2012, P IEEE, V100, P1951, DOI 10.1109/JPROC.2012.2190369
   Yang JJS, 2013, NAT NANOTECHNOL, V8, P13, DOI [10.1038/NNANO.2012.240, 10.1038/nnano.2012.240]
NR 18
TC 34
Z9 34
U1 2
U2 26
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 0741-3106
EI 1558-0563
J9 IEEE ELECTR DEVICE L
JI IEEE Electron Device Lett.
PD JUL
PY 2016
VL 37
IS 7
BP 870
EP 873
DI 10.1109/LED.2016.2573140
PG 4
WC Engineering, Electrical & Electronic
SC Engineering
GA DR5JZ
UT WOS:000379940600013
DA 2020-02-19
ER

PT J
AU Sun, YN
   Mao, H
   Guo, Q
   Yi, Z
AF Sun, Yanan
   Mao, Hua
   Guo, Quan
   Yi, Zhang
TI Learning a good representation with unsymmetrical auto-encoder
SO NEURAL COMPUTING & APPLICATIONS
LA English
DT Article
DE Auto-encoder; Neural networks; Feature learning; Deep learning;
   Unsupervised learning
AB Auto-encoders play a fundamental role in unsupervised feature learning and learning initial parameters of deep architectures for supervised tasks. For given input samples, robust features are used to generate robust representations from two perspectives: (1) invariant to small variation of samples and (2) reconstruction by decoders with minimal error. Traditional auto-encoders with different regularization terms have symmetrical numbers of encoder and decoder layers, and sometimes parameters. We investigate the relation between the number of layers and propose an unsymmetrical structure, i.e., an unsymmetrical auto-encoder (UAE), to learn more effective features. We present empirical results of feature learning using the UAE and state-of-the-art auto-encoders for classification tasks with a range of datasets. We also analyze the gradient vanishing problem mathematically and provide suggestions for the appropriate number of layers to use in UAEs with a logistic activation function. In our experiments, UAEs demonstrated superior performance with the same configuration compared to other autoencoders.
C1 [Sun, Yanan; Mao, Hua; Guo, Quan; Yi, Zhang] Sichuan Univ, Coll Comp Sci, Machine Intelligence Lab, Chengdu 610065, Peoples R China.
RP Yi, Z (reprint author), Sichuan Univ, Coll Comp Sci, Machine Intelligence Lab, Chengdu 610065, Peoples R China.
EM zhangyi@scu.edu.cn
FU National Science Foundation of ChinaNational Natural Science Foundation
   of China [61432012]
FX This work was supported by the National Science Foundation of China
   under Grant 61432012.
CR BALDI P, 1989, NEURAL NETWORKS, V2, P53, DOI 10.1016/0893-6080(89)90014-2
   Baldi P, 1991, NEURAL COMPUT, V3, P526, DOI 10.1162/neco.1991.3.4.526
   Baum EB, 1989, NEURAL COMPUT, V1, P151, DOI 10.1162/neco.1989.1.1.151
   Bengio Y, 2012, UNSUPERVISED TRANSF, V7, P19
   Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Doya K., 1992, LEARNING RTRL, V3, P17
   Erhan D, 2009, TECHNICAL REPORT
   Goodfellow I., 2009, ADV NEURAL INFORM PR, V22, P646
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   HINTON GE, 1987, LECT NOTES COMPUT SC, V258, P1
   Jarrett K, 2009, IEEE I CONF COMP VIS, P2146, DOI 10.1109/ICCV.2009.5459469
   Krizhevsky A, 2009, LEARNING MULTIPLE LA
   Lee H., 2009, P 26 ANN INT C MACH, P609, DOI DOI 10.1145/1553374.1553453
   Liou CY, 2008, NEUROCOMPUTING, V71, P3150, DOI 10.1016/j.neucom.2008.04.030
   Liou CY, 2014, NEUROCOMPUTING, V139, P84, DOI 10.1016/j.neucom.2013.09.055
   Moody J., 1995, ADV NEURAL INFORM PR, V4, P950
   Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7
   Pineda F. J., 1988, Journal of Complexity, V4, P216, DOI 10.1016/0885-064X(88)90021-0
   Ranzato M, 2008, ADV NEURAL INFORM PR, P1185
   Salah R, 2011, P 28 INT C MACH LEAR, P833, DOI 10.1016/j.wasman.2010.10.006
   Scholkopf B., 2007, ADV NEURAL INFORM PR, V19, P1137, DOI DOI 10.7551/mitpress/7503.003.0147
   Schwartz DB, 1990, NEURAL COMPUT, V2, P374, DOI 10.1162/neco.1990.2.3.374
   Tishby N., 1989, IJCNN: International Joint Conference on Neural Networks (Cat. No.89CH2765-6), P403, DOI 10.1109/IJCNN.1989.118274
   Vincent P, 2008, P 25 INT C MACH LEAR, P1096, DOI DOI 10.1145/1390156.1390294
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
NR 26
TC 3
Z9 4
U1 0
U2 10
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 0941-0643
EI 1433-3058
J9 NEURAL COMPUT APPL
JI Neural Comput. Appl.
PD JUL
PY 2016
VL 27
IS 5
SI SI
BP 1361
EP 1367
DI 10.1007/s00521-015-1939-3
PG 7
WC Computer Science, Artificial Intelligence
SC Computer Science
GA DP0BO
UT WOS:000378152800021
OA Green Accepted
DA 2020-02-19
ER

PT J
AU Paisitkriangkrai, S
   Sherrah, J
   Janney, P
   van den Hengel, A
AF Paisitkriangkrai, Sakrapee
   Sherrah, Jamie
   Janney, Pranam
   van den Hengel, Anton
TI Semantic Labeling of Aerial and Satellite Imagery
SO IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE
   SENSING
LA English
DT Article
DE Aerial imagery; conditional random fields; convolutional neural
   networks; deep learning; satellite imagery and remote sensing; semantic
   labeling
ID ENERGY MINIMIZATION; REAL-TIME; CLASSIFICATION; WATER
AB Inspired by the recent success of deep convolutional neural networks (CNNs) and feature aggregation in the field of computer vision and machine learning, we propose an effective approach to semantic pixel labeling of aerial and satellite imagery using both CNN features and hand-crafted features. Both CNN and hand-crafted features are applied to dense image patches to produce per-pixel class probabilities. Conditional random fields (CRFs) are applied as a postprocessing step. The CRF infers a labeling that smooths regions while respecting the edges present in the imagery. The combination of these factors leads to a semantic labeling framework which outperforms all existing algorithms on the International Society of Photogrammetry and Remote Sensing (IS-PRS) two-dimensional Semantic Labeling Challenge dataset. We advance state-of-the-art results by improving the overall accuracy to 88% on the ISPRS Semantic Labeling Contest. In this paper, we also explore the possibility of applying the proposed framework to other types of data. Our experimental results demonstrate the generalization capability of our approach and its ability to produce accurate results.
C1 [Paisitkriangkrai, Sakrapee; van den Hengel, Anton] Univ Adelaide, Australian Ctr Visual Technol, Adelaide, SA 5000, Australia.
   [Sherrah, Jamie; Janney, Pranam] Def Sci & Technol Grp, Dept Def, Edinburgh, SA 5111, Australia.
RP Janney, P (reprint author), Def Sci & Technol Grp, Dept Def, Edinburgh, SA 5111, Australia.
EM sakrapee.paisitkriangkrai@adelaide.edu.au;
   jamie.sherrah@dsto.defence.gov.au; pranam.janney@dsto.defence.gov.au;
   anton.vandenHengel@adelaide.edu.au
FU Australian Research CouncilAustralian Research Council [LP130100156]
FX This work was supported in part by the Australian Research Council
   Linkage Project LP130100156. (Corresponding author: Pranam Janney)
CR [Anonymous], 2015, ISPRS 2D SEMANTIC LA
   [Anonymous], 2015, IEEE GRSS DATA FUSIO
   [Anonymous], 2013, THESIS
   Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114
   Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60
   Bruzzone L, 2014, REMOTE SENS DIGIT IM, V18, P127, DOI 10.1007/978-94-007-7969-3_9
   Chen YS, 2014, IEEE J-STARS, V7, P2094, DOI 10.1109/JSTARS.2014.2329330
   Dalal N, 2005, PROC CVPR IEEE, P886
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Farabet C., 2012, P INT C MACH LEARN, P319
   Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Firat O, 2014, INT C PATT RECOG, P3708, DOI 10.1109/ICPR.2014.637
   Foody GM, 2013, IEEE J-STARS, V6, P1305, DOI 10.1109/JSTARS.2013.2250257
   Gerke M., 2015, TRP2015 U TWENT
   Girshick R, 2014, P IEEE C COMP VIS PA, P1
   Huang X, 2015, IEEE J-STARS, V8, P2097, DOI 10.1109/JSTARS.2015.2420713
   Iovan C, 2008, IEEE J-STARS, V1, P206, DOI 10.1109/JSTARS.2008.2007514
   Karantzalos K, 2015, IEEE J-STARS, V8, P4665, DOI 10.1109/JSTARS.2015.2461556
   Kluckner S, 2010, INT ARCH PHOTOGRAMM, V38, P233
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kussul N, 2015, INT GEOSCI REMOTE SE, P165, DOI 10.1109/IGARSS.2015.7325725
   Ladicky L., 2010, 11 EUR C COMP VIS CR
   Lagrange A, 2015, INT GEOSCI REMOTE SE, P4173, DOI 10.1109/IGARSS.2015.7326745
   Lin AYM, 2011, IEEE J-STARS, V4, P870, DOI 10.1109/JSTARS.2011.2143696
   Liu K, 2014, INT J COMPUT VISION, V106, P342, DOI 10.1007/s11263-013-0634-z
   Makantasis K, 2015, INT GEOSCI REMOTE SE, P4959, DOI 10.1109/IGARSS.2015.7326945
   Meng LF, 2012, IEEE J-STARS, V5, P146, DOI 10.1109/JSTARS.2011.2179639
   Mnih V, 2010, LECT NOTES COMPUT SC, V6316, P210, DOI 10.1007/978-3-642-15567-3_16
   Niemeyer J, 2011, LECT NOTES COMPUT SC, V6952, P233, DOI 10.1007/978-3-642-24393-6_20
   Paisitkriangkrai Sakrapee, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P36, DOI 10.1109/CVPRW.2015.7301381
   Paisitkriangkrai S, 2016, IEEE T PATTERN ANAL, V38, P1243, DOI 10.1109/TPAMI.2015.2474388
   Porway J, 2008, PROC CVPR IEEE, P141
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Rifkin R, 2004, J MACH LEARN RES, V5, P101
   Sherrah J., 2016, CORR
   Shotton J, 2009, INT J COMPUT VISION, V81, P2, DOI 10.1007/s11263-007-0109-1
   Soergel U, 2008, IEEE J-STARS, V1, P147, DOI 10.1109/JSTARS.2008.2001156
   Thuy Thi Nguyen, 2007, 2007 IEEE International Conference on Research, Innovation and Vision for the Future, P87
   Tuia D, 2015, ISPRS J PHOTOGRAMM, V105, P272, DOI 10.1016/j.isprsjprs.2015.01.006
   Vakalopoulou M, 2015, INT GEOSCI REMOTE SE, P1873, DOI 10.1109/IGARSS.2015.7326158
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Walk S, 2010, PROC CVPR IEEE, P1030, DOI 10.1109/CVPR.2010.5540102
   Yokoya N, 2015, IEEE J-STARS, V8, P2053, DOI 10.1109/JSTARS.2015.2404578
   Zarea A, 2016, IEEE J-STARS, V9, P1864, DOI 10.1109/JSTARS.2015.2470547
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang L, 2015, IEEE J-STARS, V8, P4895, DOI 10.1109/JSTARS.2015.2467377
   Zhao R, 2014, IEEE J-STARS, V7, P1227, DOI 10.1109/JSTARS.2014.2311995
   Zou Q, 2015, IEEE GEOSCI REMOTE S, V12, P2321, DOI 10.1109/LGRS.2015.2475299
   Zou W. Y., 2014, CORR
NR 50
TC 47
Z9 49
U1 2
U2 38
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1939-1404
EI 2151-1535
J9 IEEE J-STARS
JI IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens.
PD JUL
PY 2016
VL 9
IS 7
SI SI
BP 2868
EP 2881
DI 10.1109/JSTARS.2016.2582921
PG 14
WC Engineering, Electrical & Electronic; Geography, Physical; Remote
   Sensing; Imaging Science & Photographic Technology
SC Engineering; Physical Geography; Remote Sensing; Imaging Science &
   Photographic Technology
GA DY2EI
UT WOS:000384905500004
DA 2020-02-19
ER

PT J
AU Liu, D
   Wang, ZW
   Wen, BH
   Yang, JC
   Han, W
   Huang, TS
AF Liu, Ding
   Wang, Zhaowen
   Wen, Bihan
   Yang, Jianchao
   Han, Wei
   Huang, Thomas S.
TI Robust Single Image Super-Resolution via Deep Networks With Sparse Prior
SO IEEE TRANSACTIONS ON IMAGE PROCESSING
LA English
DT Article
DE Image super-resolution; deep neural networks; sparse coding
ID ALGORITHM; LIMITS
AB Single image super-resolution (SR) is an ill-posed problem, which tries to recover a high-resolution image from its low-resolution observation. To regularize the solution of the problem, previous methods have focused on designing good priors for natural images, such as sparse representation, or directly learning the priors from a large data set with models, such as deep neural networks. In this paper, we argue that domain expertise from the conventional sparse coding model can be combined with the key ingredients of deep learning to achieve further improved results. We demonstrate that a sparse coding model particularly designed for SR can be incarnated as a neural network with the merit of end-to-end optimization over training data. The network has a cascaded structure, which boosts the SR performance for both fixed and incremental scaling factors. The proposed training and testing schemes can be extended for robust handling of images with additional degradation, such as noise and blurring. A subjective assessment is conducted and analyzed in order to thoroughly evaluate various SR techniques. Our proposed model is tested on a wide range of images, and it significantly outperforms the existing state-of-the-art methods for various scaling factors both quantitatively and perceptually.
C1 [Liu, Ding; Han, Wei; Huang, Thomas S.] Univ Illinois, Beckman Inst, Dept Elect & Comp Engn, Urbana, IL 61801 USA.
   [Wang, Zhaowen] Adobe Syst Inc, San Jose, CA 95110 USA.
   [Wen, Bihan] Univ Illinois, Dept Elect & Comp Engn, Coordinated Sci Lab, Urbana, IL 61801 USA.
   [Yang, Jianchao] Snapchat Inc, Venice, CA 90291 USA.
RP Liu, D (reprint author), Univ Illinois, Beckman Inst, Dept Elect & Comp Engn, Urbana, IL 61801 USA.
EM dingliu2@illinois.edu; zhawang@adobe.com; bwen3@illinois.edu;
   jianchao.yang@snapchat.com; weihan3@illinois.edu; t-huang1@illinois.edu
RI Wen, Bihan/B-3123-2017; Wen, Bihan/M-2343-2019
OI Wen, Bihan/0000-0002-6874-6453; Wen, Bihan/0000-0002-6874-6453; Liu,
   Ding/0000-0002-0931-1345
CR Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   Aly HA, 2005, IEEE T IMAGE PROCESS, V14, P1647, DOI 10.1109/TIP.2005.851684
   Baker S, 2002, IEEE T PATTERN ANAL, V24, P1167, DOI 10.1109/TPAMI.2002.1033210
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   BRADLEY RA, 1952, BIOMETRIKA, V39, P324, DOI 10.1093/biomet/39.3-4.324
   Buades A, 2005, PROC CVPR IEEE, P60
   Chang SY, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P119, DOI 10.1145/2783258.2783296
   Cui Z, 2014, LECT NOTES COMPUT SC, V8693, P49, DOI 10.1007/978-3-319-10602-1_4
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Daubechies I, 2004, COMMUN PUR APPL MATH, V57, P1413, DOI 10.1002/cpa.20042
   Deng C, 2016, IEEE T NEUR NET LEAR, V27, P2472, DOI 10.1109/TNNLS.2015.2468069
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Dong WS, 2011, IEEE I CONF COMP VIS, P1259, DOI 10.1109/ICCV.2011.6126377
   Fattal R, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276496, 10.1145/1239451.1239546]
   Freedman G, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1944846.1944852
   Gao XB, 2012, IEEE T IMAGE PROCESS, V21, P3194, DOI 10.1109/TIP.2012.2190080
   Glasner D, 2009, IEEE I CONF COMP VIS, P349, DOI 10.1109/ICCV.2009.5459271
   Gregor K., 2010, P 27 INT C MACH LEAR, P399
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Kavukcuoglu K., 2010, FAST INFERENCE SPARS
   Kim KI, 2010, IEEE T PATTERN ANAL, V32, P1127, DOI 10.1109/TPAMI.2010.25
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   Le QV, 2013, INT CONF ACOUST SPEE, P8595, DOI 10.1109/ICASSP.2013.6639343
   Lin M., 2013, NETWORK NETWORK
   Lin ZC, 2004, IEEE T PATTERN ANAL, V26, P83, DOI 10.1109/TPAMI.2004.1261081
   Lu XQ, 2012, PROC CVPR IEEE, P1648, DOI 10.1109/CVPR.2012.6247858
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Nair V, 2010, ICML, V27, P807, DOI DOI 10.0RG/PAPERS/432.PDF
   Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222
   Osendorfer C, 2014, LECT NOTES COMPUT SC, V8836, P250, DOI 10.1007/978-3-319-12643-2_31
   Ravishankar S, 2015, IEEE J-STSP, V9, P625, DOI 10.1109/JSTSP.2015.2417131
   Rozell CJ, 2008, NEURAL COMPUT, V20, P2526, DOI 10.1162/neco.2008.03-07-486
   Timofte R, 2015, LECT NOTES COMPUT SC, V9006, P111, DOI 10.1007/978-3-319-16817-3_8
   Timofte R, 2013, IEEE I CONF COMP VIS, P1920, DOI 10.1109/ICCV.2013.241
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z., 2015, SPARSE CODING ITS AP
   Wang ZY, 2015, IEEE T IMAGE PROCESS, V24, P4359, DOI 10.1109/TIP.2015.2462113
   Wang ZF, 2015, PROCEEDINGS OF 2015 INTERNATIONAL CONFERENCE ON FLUID POWER AND MECHATRONICS - FPM 2015, P370, DOI 10.1109/FPM.2015.7337142
   Wang ZH, 2016, PROCEEDINGS OF THE 4TH INTERNATIONAL WORKSHOP ON ENERGY HARVESTING AND ENERGY-NEUTRAL SENSING SYSTEMS (ENSSYS'16), P1, DOI 10.1145/2996884.2996885
   WEN BH, 2015, P IEEE ICIP, P118
   Wen BH, 2015, INT J COMPUT VISION, V114, P137, DOI 10.1007/s11263-014-0761-1
   Yang CY, 2014, LECT NOTES COMPUT SC, V8692, P372, DOI 10.1007/978-3-319-10593-2_25
   Yang JC, 2013, PROC CVPR IEEE, P1059, DOI 10.1109/CVPR.2013.141
   Yang JC, 2012, IEEE T IMAGE PROCESS, V21, P3467, DOI 10.1109/TIP.2012.2192127
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Zeyde R, 2012, CURVES SURFACES, P711, DOI [DOI 10.1007/978-3-642-27413-8_47, 10.1007/978-3-642-27413-8_47]
   Zhang KB, 2012, IEEE T IMAGE PROCESS, V21, P4544, DOI 10.1109/TIP.2012.2208977
   Zhangyang Wang, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P1, DOI 10.1109/CVPRW.2015.7301266
NR 50
TC 100
Z9 105
U1 11
U2 63
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1057-7149
EI 1941-0042
J9 IEEE T IMAGE PROCESS
JI IEEE Trans. Image Process.
PD JUL
PY 2016
VL 25
IS 7
BP 3194
EP 3207
DI 10.1109/TIP.2016.2564643
PG 14
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
SC Computer Science; Engineering
GA DX2WR
UT WOS:000384234000001
PM 27168598
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Giryes, R
   Sapiro, G
   Bronstein, AM
AF Giryes, Raja
   Sapiro, Guillermo
   Bronstein, Alex M.
TI Deep Neural Networks with Random Gaussian Weights: A Universal
   Classification Strategy?
SO IEEE TRANSACTIONS ON SIGNAL PROCESSING
LA English
DT Article
DE Artificial neural networks; computation theory; deep learning; learning
   systems
ID SIGNAL RECOVERY; SPARSE; PROJECTIONS
AB Three important properties of a classification machinery are i) the system preserves the core information of the input data; ii) the training examples convey information about unseen data; and iii) the system is able to treat differently points from different classes. In this paper, we show that these fundamental properties are satisfied by the architecture of deep neural networks. We formally prove that these networks with random Gaussian weights perform a distance-preserving embedding of the data, with a special treatment for in-class and out-of-class data. Similar points at the input of the network are likely to have a similar output. The theoretical analysis of deep networks here presented exploits tools used in the compressed sensing and dictionary learning literature, thereby making a formal connection between these important topics. The derived results allow drawing conclusions on the metric learning properties of the network and their relation to its structure, as well as providing bounds on the required size of the training set such that the training examples would represent faithfully the unseen data. The results are validated with state-of-the-art trained networks.
C1 [Giryes, Raja; Bronstein, Alex M.] Tel Aviv Univ, Fac Engn, Sch Elect Engn, IL-69978 Ramat Aviv, Israel.
   [Sapiro, Guillermo] Duke Univ, Dept Elect & Comp Engn, Durham, NC 27708 USA.
RP Giryes, R; Bronstein, AM (reprint author), Tel Aviv Univ, Fac Engn, Sch Elect Engn, IL-69978 Ramat Aviv, Israel.; Sapiro, G (reprint author), Duke Univ, Dept Elect & Comp Engn, Durham, NC 27708 USA.
EM raja@tauex.tau.ac.il; guillermo.sapiro@duke.edu; bron@eng.tau.ac.il
OI Giryes, Raja/0000-0002-2830-0297
FU NSFNational Science Foundation (NSF); ONROffice of Naval Research; NGA;
   NSSEFF; ARO; ERC StG [335491]
FX Work partially supported by NSF, ONR, NGA, NSSEFF, and ARO. The work of
   A. M. Bronstein is supported by ERC StG 335491 (RAPID).
CR Ai A, 2014, LINEAR ALGEBRA APPL, V441, P222, DOI 10.1016/j.laa.2013.04.002
   Amelunxen D, 2014, INF INFERENCE, V3, P224, DOI 10.1093/imaiai/iau005
   Andoni A, 2006, ANN IEEE SYMP FOUND, P459
   Anselmi F., 2015, THEORET COM IN PRESS
   Arora S., 2014, P 31 INT C MACH LEAR, P584
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bruna J., 2014, P 31 INT C MACH LEAR, P307
   Bruna J., 2013, P ICLR WORKSH JAN
   Bruna J, 2013, IEEE T PATTERN ANAL, V35, P1872, DOI 10.1109/TPAMI.2012.230
   Candes EJ, 2006, IEEE T INFORM THEORY, V52, P5406, DOI 10.1109/TIT.2006.885507
   Candes EJ, 2013, COMMUN PUR APPL MATH, V66, P1241, DOI 10.1002/cpa.21432
   Chandrasekaran V, 2012, FOUND COMPUT MATH, V12, P805, DOI 10.1007/s10208-012-9135-7
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76
   Choromanska A, 2015, JMLR WORKSH CONF PRO, V38, P192
   Cox D, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P8, DOI 10.1109/FG.2011.5771385
   Cybenko G., 1989, Mathematics of Control, Signals, and Systems, V2, P303, DOI 10.1007/BF02551274
   Dauphin Y. N., 2014, ADV NEURAL INFORM PR, P2933
   Duarte-Carvajalino JM, 2009, IEEE T IMAGE PROCESS, V18, P1395, DOI 10.1109/TIP.2009.2022459
   Elad M, 2007, IEEE T SIGNAL PROCES, V55, P5695, DOI 10.1109/TSP.2007.900760
   Elhamifar E, 2013, IEEE I CONF COMP VIS, P209, DOI 10.1109/ICCV.2013.33
   Elhamifar E, 2013, IEEE T PATTERN ANAL, V35, P2765, DOI 10.1109/TPAMI.2013.57
   Gribonval R, 2015, IEEE T INFORM THEORY, V61, P6298, DOI 10.1109/TIT.2015.2472522
   GROMOV M, 1999, METRIC STRUCTURES RI
   Haupt J, 2010, IEEE T INFORM THEORY, V56, P5862, DOI 10.1109/TIT.2010.2070191
   Hegde C, 2015, IEEE T SIGNAL PROCES, V63, P6109, DOI 10.1109/TSP.2015.2452228
   HORNIK K, 1989, NEURAL NETWORKS, V2, P359, DOI 10.1016/0893-6080(89)90020-8
   Huang GB, 2011, INT J MACH LEARN CYB, V2, P107, DOI 10.1007/s13042-011-0019-y
   Huang J., 2015, P ADV NEUR INF PROC, P1333
   Huang JJ, 2015, IEEE I CONF COMP VIS, P4139, DOI 10.1109/ICCV.2015.471
   Johnson W. B., 1984, CONT MATH, V26, P189, DOI DOI 10.1090/CONM/026/737400
   Klartag B, 2005, J FUNCT ANAL, V225, P229, DOI 10.1016/j.jfa.2004.10.009
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   Ledoux M., 1991, PROBABILITY BANACH S
   Lu Z., 2014, SCALE KERNEL METHODS
   Mahendran A, 2015, PROC CVPR IEEE, P5188, DOI 10.1109/CVPR.2015.7299155
   Mairal J., 2014, ADV NEURAL INFORM PR, V27, P2627
   Mendelson S, 2008, CONSTR APPROX, V28, P277, DOI 10.1007/s00365-007-9005-8
   Montufar G. F., 2014, ADV NEURAL INFORM PR, V27, P2924
   Montufar GF, 2015, SIAM J DISCRETE MATH, V29, P321, DOI 10.1137/140957081
   Nair V, 2010, ICML, V27, P807, DOI DOI 10.0RG/PAPERS/432.PDF
   Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11
   Pinto N, 2009, PLOS COMPUT BIOL, V5, DOI 10.1371/journal.pcbi.1000579
   Plan Y., 2014, HIGH DIMENSIONAL EST
   Plan Y, 2014, DISCRETE COMPUT GEOM, V51, P438, DOI 10.1007/s00454-013-9561-6
   Plan Y, 2013, IEEE T INFORM THEORY, V59, P482, DOI 10.1109/TIT.2012.2207945
   Qiu Q, 2015, J MACH LEARN RES, V16, P187
   Rauhut H, 2012, APPL COMPUT HARMON A, V32, P242, DOI 10.1016/j.acha.2011.05.001
   Rudelson M., 2010, P INT C MATH
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Saligrama V, 2012, IEEE T INFORM THEORY, V58, P6023, DOI 10.1109/TIT.2012.2204164
   Saxe A., 2011, P 28 INT C MACH LEAR, P1089
   Saxe A., 2014, P INT C LEARN REPR I
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Simonyan K., 2015, P INT C LEARN REPR I
   Vedaldi A., P ACM INT C MULT, P689
   Wolf L, 2004, J MACH LEARN RES, V4, P913, DOI 10.1162/1532443041827934
   Yamaguchi O, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P318, DOI 10.1109/AFGR.1998.670968
NR 57
TC 26
Z9 27
U1 0
U2 28
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1053-587X
EI 1941-0476
J9 IEEE T SIGNAL PROCES
JI IEEE Trans. Signal Process.
PD JUL 1
PY 2016
VL 64
IS 13
BP 3444
EP 3457
DI 10.1109/TSP.2016.2546221
PG 14
WC Engineering, Electrical & Electronic
SC Engineering
GA DN5MX
UT WOS:000377114200013
OA Bronze
DA 2020-02-19
ER

PT J
AU Kim, S
   Park, B
   Song, BS
   Yang, S
AF Kim, Soowoong
   Park, Bogun
   Song, Bong Seop
   Yang, Seungjoon
TI Deep belief network based statistical feature learning for fingerprint
   liveness detection
SO PATTERN RECOGNITION LETTERS
LA English
DT Article
DE Fingerprint liveness detection; Fingerprint anti-spoofing; Deep belief
   network; Deep learning; Statistical feature extraction; Livdet2013
AB Fingerprint recognition systems are vulnerable to impersonation by fake or spoof fingerprints. Fingerprint liveness detection is a step to ensure whether a scanned fingerprint is live or fake prior to a recognition step. This paper presents a fingerprint liveness detection method based on a deep belief network (DBN). A DBN with multiple layers of restricted Boltzmann machine is used to learn features from a set of live and fake fingerprints and also to detect the liveness. The proposed method is a systematic application of a deep learning technique, and does not require specific domain expertise regarding fake fingerprints or recognition systems. The proposed method provides accurate detection of the liveness with various sensor datasets collected for the international fingerprint liveness detection competition. (C) 2016 Elsevier B.V. All rights reserved.
C1 [Kim, Soowoong; Yang, Seungjoon] Ulsan Natl Inst Sci & Technol, Sch Elect & Comp Engn, Ulsan 689798, South Korea.
   [Park, Bogun; Song, Bong Seop] Suprema Inc, 16F Parkview Off Tower Jeongja Dong, Songnam 463863, Gyeonggi, South Korea.
RP Yang, S (reprint author), Ulsan Natl Inst Sci & Technol, Sch Elect & Comp Engn, Ulsan 689798, South Korea.
EM syang@unist.ac.kr
FU Suprema Inc.
FX This work is supported by Suprema Inc.
CR Al-Ajlan A., 2013, INT WORKSH BIOM FOR, P1
   Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Delac K, 2004, PROCEEDINGS ELMAR-2004: 46TH INTERNATIONAL SYMPOSIUM ELECTRONICS IN MARINE, P184
   Ghiani L., 2013, P IEEE 6 INT C BIOM, P1
   Ghiani L, 2013, INT CONF BIOMETR, DOI 10.1109/ICB.2013.6613027
   Ghiani L, 2012, INT C PATT RECOG, P537
   Gottschlich C., 2014, IEEE INT JOINT C BIO, P1, DOI DOI 10.1109/BTAS.2014.6996224
   Hamal P., 2010, ISMIR, P339
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Jia J, 2007, LECT NOTES COMPUT SC, V4642, P309
   Johnson-Laird PN, 2013, PSYCHOL LEARN MOTIV, V59, P1, DOI 10.1016/B978-0-12-407187-2.00001-0
   Lee H., 2008, ADV NEURAL INFORM PR, V20, P873
   Liu P, 2014, PROC CVPR IEEE, P1805, DOI 10.1109/CVPR.2014.233
   Maltoni D, 2009, HDB FINGERPRINT RECO
   Marasco E, 2015, ACM COMPUT SURV, V47, DOI 10.1145/2617756
   Memon S.A., 2012, THESIS BRUNEL U
   Mohamed A.R., 2010, P INTERSPEECH, P2846
   Nikam SB, 2008, INT J BIOMETRICS, V1, P141, DOI 10.1504/IJBM.2008.020141
   Nixon KA, 2005, P SOC PHOTO-OPT INS, V5779, P214, DOI 10.1117/12.606643
   Nogueira RF, 2014, 2014 IEEE WORKSHOP ON BIOMETRIC MEASUREMENTS AND SYSTEMS FOR SECURITY AND MEDICAL APPLICATIONS (BIOMS) PROCEEDINGS, P22, DOI 10.1109/BIOMS.2014.6951531
   Parthasaradhi STV, 2005, IEEE T SYST MAN CY C, V35, P335, DOI 10.1109/TSMCC.2005.848192
   Reddy PV, 2008, IEEE T BIOMED CIRC S, V2, P328, DOI 10.1109/TBCAS.2008.2003432
   Sahasrabudhe M., 2014, P 2014 IND C COMP VI, P2
   Sazal M. M. R., 2014, EL INF COMM TECHN EI, P1
   Sousedik C., 2014, P IEEE INT JOINT C B, P1
   Srivastava N., 2013, THESIS U TORONTO
   YU D, 2011, INTERSPEECH, V237
NR 29
TC 21
Z9 22
U1 2
U2 38
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0167-8655
EI 1872-7344
J9 PATTERN RECOGN LETT
JI Pattern Recognit. Lett.
PD JUL 1
PY 2016
VL 77
BP 58
EP 65
DI 10.1016/j.patrec.2016.03.015
PG 8
WC Computer Science, Artificial Intelligence
SC Computer Science
GA DM7VP
UT WOS:000376568400009
DA 2020-02-19
ER

PT J
AU Yu, YT
   Li, J
   Guan, HY
   Wang, C
AF Yu, Yongtao
   Li, Jonathan
   Guan, Haiyan
   Wang, Cheng
TI Automated Detection of Three-Dimensional Cars in Mobile Laser Scanning
   Point Clouds Using DBM-Hough-Forests
SO IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING
LA English
DT Article
DE Car detection; deep learning; Hough forest; mobile laser scanning (MLS);
   point cloud; visibility estimation
ID AIRBORNE LIDAR DATA; OBJECT DETECTION; VEHICLE EXTRACTION
AB This paper presents an automated algorithm for rapidly and effectively detecting cars directly from large-volume 3-D point clouds. Rather than using low-order descriptors, a multilayer feature generation model is created to obtain high-order feature representations for 3-D local patches through deep learning techniques. To handle cars with different levels of incompleteness caused by data acquisition ways and occlusions, a hierarchical visibility estimation model is developed to augment Hough voting. Considering scale and orientation variations in the azimuth direction, a set of multiscale Hough forests is constructed to rotationally cast votes to estimate cars' centroids. Quantitative assessments show that the proposed algorithm achieves average completeness, correctness, quality, and F-1-measure of 0.94, 0.96, 0.90, and 0.95, respectively, in detecting 3-D cars. Comparative studies also demonstrate that the proposed algorithm outperforms the other four existing algorithms in accurately and completely detecting 3-D cars from large-scale 3-D point clouds.
C1 [Yu, Yongtao] Xiamen Univ, Fujian Key Lab Sensing & Comp Smart Cities, Xiamen 361005, Peoples R China.
   [Yu, Yongtao] Huaiyin Inst Technol, Fac Comp & Software Engn, Huaian 223003, Peoples R China.
   [Li, Jonathan; Wang, Cheng] Xiamen Univ, Fujian Key Lab Sensing & Comp Smart Cities, Sch Informat Sci & Engn, Xiamen 361005, Peoples R China.
   [Li, Jonathan] Univ Waterloo, Dept Geog & Environm Management, Waterloo, ON N2L 3G1, Canada.
   [Guan, Haiyan] Nanjing Univ Informat Sci & Technol, Coll Geog & Remote Sensing, Nanjing 210044, Jiangsu, Peoples R China.
RP Yu, YT (reprint author), Xiamen Univ, Fujian Key Lab Sensing & Comp Smart Cities, Xiamen 361005, Peoples R China.; Yu, YT (reprint author), Huaiyin Inst Technol, Fac Comp & Software Engn, Huaian 223003, Peoples R China.
EM allennessy.yu@gmail.com; junli@uwaterloo.ca
RI Wang, Cheng/A-9472-2012
OI Wang, Cheng/0000-0001-6075-796X
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [41471379]; Priority Academic Program Development
   and Collaborative Innovation Center of Atmospheric Environment and
   Equipment Technology
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 41471379 and in part by the Priority
   Academic Program Development and the Collaborative Innovation Center of
   Atmospheric Environment and Equipment Technology.
CR Bai X, 2014, IEEE T GEOSCI REMOTE, V52, P6508, DOI 10.1109/TGRS.2013.2296782
   Borcs A, 2015, IEEE T GEOSCI REMOTE, V53, P1475, DOI 10.1109/TGRS.2014.2344438
   Borcs A., 2012, ISPRS ANN PHOTOGRAMM, VI-3, P93
   Carneiro G, 2013, IEEE T PATTERN ANAL, V35, P2592, DOI 10.1109/TPAMI.2013.96
   Chen B, 2013, IEEE T PATTERN ANAL, V35, P1887, DOI 10.1109/TPAMI.2013.19
   Chen ZY, 2016, IEEE T GEOSCI REMOTE, V54, P103, DOI 10.1109/TGRS.2015.2451002
   Descombes X, 2009, J MATH IMAGING VIS, V33, P347, DOI 10.1007/s10851-008-0117-y
   Fortin B, 2012, IEEE T VEH TECHNOL, V61, P3838, DOI 10.1109/TVT.2012.2211630
   Gall J, 2011, IEEE T PATTERN ANAL, V33, P2188, DOI 10.1109/TPAMI.2011.70
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Kortgen M., 2003, P CENTR EUR SEM COMP, V3, P5
   Lei Z, 2012, IEEE T GEOSCI REMOTE, V50, P1206, DOI 10.1109/TGRS.2011.2166966
   Patterson A, 2008, LECT NOTES COMPUT SC, V5305, P553, DOI 10.1007/978-3-540-88693-8_41
   Rusu R., 2009, IEEE INT C ROB AUT I, P3212, DOI DOI 10.1109/R0B0T.2009.5152473
   Rusu RB, 2008, 2008 IEEE/RSJ INTERNATIONAL CONFERENCE ON ROBOTS AND INTELLIGENT SYSTEMS, VOLS 1-3, CONFERENCE PROCEEDINGS, P3384, DOI 10.1109/IROS.2008.4650967
   Salakhutdinov R, 2013, IEEE T PATTERN ANAL, V35, P1958, DOI 10.1109/TPAMI.2012.269
   Salakhutdinov R, 2012, NEURAL COMPUT, V24, P1967, DOI 10.1162/NECO_a_00311
   Tang JX, 2015, IEEE T GEOSCI REMOTE, V53, P1174, DOI 10.1109/TGRS.2014.2335751
   Tieleman T., 2008, P 25 INT C MACH LEAR, P1064, DOI DOI 10.1145/1390156.1390290
   Varela-Gonzalez M, 2014, MEASUREMENT, V53, P215, DOI 10.1016/j.measurement.2014.03.033
   Velizhev A., 2012, ISPRS ANN PHOTOGRAMM, V3, P179
   Wang HY, 2015, IEEE J-STARS, V8, P1570, DOI 10.1109/JSTARS.2015.2394803
   Wang HY, 2014, IEEE GEOSCI REMOTE S, V11, P1807, DOI 10.1109/LGRS.2014.2309965
   Williams K, 2013, REMOTE SENS-BASEL, V5, P4652, DOI 10.3390/rs5094652
   Yan WY, 2014, IEEE T GEOSCI REMOTE, V52, P7658, DOI 10.1109/TGRS.2014.2316195
   Yang BS, 2014, ISPRS J PHOTOGRAMM, V95, P109, DOI 10.1016/j.isprsjprs.2014.05.012
   Yao W, 2011, IEEE GEOSCI REMOTE S, V8, P607, DOI 10.1109/LGRS.2010.2097239
   Yao W, 2011, ISPRS J PHOTOGRAMM, V66, P260, DOI 10.1016/j.isprsjprs.2010.10.005
   Yao W, 2010, PATTERN RECOGN LETT, V31, P1100, DOI 10.1016/j.patrec.2010.02.006
   Yu YT, 2015, IEEE T INTELL TRANSP, V16, P2167, DOI 10.1109/TITS.2015.2399492
   Yu YT, 2015, IEEE GEOSCI REMOTE S, V12, P492, DOI 10.1109/LGRS.2014.2347347
   Yu YT, 2015, IEEE T GEOSCI REMOTE, V53, P1374, DOI 10.1109/TGRS.2014.2338915
   Zhang JX, 2014, REMOTE SENS-BASEL, V6, P8405, DOI 10.3390/rs6098405
NR 33
TC 16
Z9 17
U1 7
U2 46
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 0196-2892
EI 1558-0644
J9 IEEE T GEOSCI REMOTE
JI IEEE Trans. Geosci. Remote Sensing
PD JUL
PY 2016
VL 54
IS 7
BP 4130
EP 4142
DI 10.1109/TGRS.2016.2537830
PG 13
WC Geochemistry & Geophysics; Engineering, Electrical & Electronic; Remote
   Sensing; Imaging Science & Photographic Technology
SC Geochemistry & Geophysics; Engineering; Remote Sensing; Imaging Science
   & Photographic Technology
GA DO0OO
UT WOS:000377478400031
DA 2020-02-19
ER

PT J
AU Zuo, Z
   Shuai, B
   Wang, G
   Liu, X
   Wang, XX
   Wang, B
   Chen, YS
AF Zuo, Zhen
   Shuai, Bing
   Wang, Gang
   Liu, Xiao
   Wang, Xingxing
   Wang, Bing
   Chen, Yushi
TI Learning Contextual Dependence With Convolutional Hierarchical Recurrent
   Neural Networks
SO IEEE TRANSACTIONS ON IMAGE PROCESSING
LA English
DT Article
DE Deep learning; image classification; recurrent neural networks;
   convolutional neural networks
AB Deep convolutional neural networks (CNNs) have shown their great success on image classification. CNNs mainly consist of convolutional and pooling layers, both of which are performed on local image areas without considering the dependence among different image regions. However, such dependence is very important for generating explicit image representation. In contrast, recurrent neural networks (RNNs) are well known for their ability of encoding contextual information in sequential data, and they only require a limited number of network parameters. Thus, we proposed the hierarchical RNNs (HRNNs) to encode the contextual dependence in image representation. In HRNNs, each RNN layer focuses on modeling spatial dependence among image regions from the same scale but different locations. While the cross RNN scale connections target on modeling scale dependencies among regions from the same location but different scales. Specifically, we propose two RNN models: 1) hierarchical simple recurrent network (HSRN), which is fast and has low computational cost and 2) hierarchical long-short term memory recurrent network, which performs better than HSRN with the price of higher computational cost. In this paper, we integrate CNNs with HRNNs, and develop end-to-end convolutional hierarchical RNNs (C-HRNNs) for image classification. C-HRNNs not only utilize the discriminative representation power of CNNs, but also utilize the contextual dependence learning ability of our HRNNs. On four of the most challenging object/ scene image classification benchmarks, our C-HRNNs achieve the state-of-the-art results on Places 205, SUN 397, and MIT indoor, and the competitive results on ILSVRC 2012.
C1 [Zuo, Zhen; Shuai, Bing; Wang, Gang; Liu, Xiao; Wang, Xingxing; Wang, Bing] Nanyang Technol Univ, Singapore 639798, Singapore.
   [Chen, Yushi] Harbin Inst Technol, Harbin, Peoples R China.
RP Zuo, Z; Shuai, B; Wang, G; Liu, X; Wang, XX; Wang, B (reprint author), Nanyang Technol Univ, Singapore 639798, Singapore.; Chen, YS (reprint author), Harbin Inst Technol, Harbin, Peoples R China.
EM zzuo1@ntu.edu.sg; bshuai001@ntu.edu.sg; wanggang@ntu.edu.sg;
   liux0072@ntu.edu.sg; wangxx@ntu.edu.sg; wang0775@ntu.edu.sg;
   chenyushi@hit.edu.cn
FU National Research Foundation, Prime Ministers Office, Singapore, through
   the Rapid-Rich Object Search Laboratory under its Interactive and
   Digital Media (IDM) Futures Funding Initiative; Singapore Ministry of
   Education within the Tier 2 Research Project [ARC28/14]; Singapore
   Agency for Science, Technology and Research within the Science and
   Engineering Research Council [PSF1321202099]
FX This work was supported in part by the National Research Foundation,
   Prime Ministers Office, Singapore, through the Rapid-Rich Object Search
   Laboratory under its Interactive and Digital Media (IDM) Futures Funding
   Initiative and administered by the IDM Programme Office, in part by the
   Singapore Ministry of Education within the Tier 2 Research Project under
   Grant ARC28/14, and in part by the Singapore Agency for Science,
   Technology and Research within the Science and Engineering Research
   Council under Grant PSF1321202099. The associate editor coordinating the
   review of this manuscript and approving it for publication was Prof.
   Dacheng Tao.
CR Bengio Y., 2013, ADV NEURAL INFORM PR, V26, P899
   Boulanger-lewandowski N., 2012, P 29 INT C MACH LEAR, P1159
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76
   Chen X, 2014, LEARNING RECURRENT V
   Dalal N, 2005, PROC CVPR IEEE, P886
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Doersch Carl, 2013, ADV NEURAL INFORM PR, P494
   Donahue J., 2014, P INT C MACH LEARN, P647
   Donahue J., 2014, LONG TERM RECURRENT
   Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714
   Eigen D., 2013, UNDERSTANDING DEEP A
   ELMAN JL, 1990, COGNITIVE SCI, V14, P179, DOI 10.1016/0364-0213(90)90002-E
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Gong YC, 2014, LECT NOTES COMPUT SC, V8695, P392, DOI 10.1007/978-3-319-10584-0_26
   Graves A., 2009, ADV NEURAL INFORM PR, V21, P545
   Graves Alex, 2013, GENERATING SEQUENCES
   Graves Alex, 2014, P 31 INT C MACH LEAR, P1764, DOI DOI 10.1145/1143844.1143891
   Gregor K., 2015, DRAW RECURRENT NEURA
   He KM, 2015, IEEE T PATTERN ANAL, V37, P1904, DOI 10.1109/TPAMI.2015.2389824
   Hihi S., 1995, ADV NEURAL INFORM PR, P493
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Jaeger H, 2002, 159 GMD GERM NAT RES, P1
   Jia Y, 2014, CAFFE CONVOLUTIONAL
   Juneja M, 2013, PROC CVPR IEEE, P923, DOI 10.1109/CVPR.2013.124
   Karpathy A., 2014, DEEP VISUAL SEMANTIC
   Koutnik  J., 2014, JMLR WORKSHOP C P, P1863
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lapin M, 2014, PROC CVPR IEEE, P1434, DOI 10.1109/CVPR.2014.186
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   LeCun Yann, 1990, ADV NEURAL INFORM PR, P396, DOI DOI 10.1111/DSU.12130
   Lee H., 2009, P 26 ANN INT C MACH, P609, DOI DOI 10.1145/1553374.1553453
   Li L.-J., 2010, ADV NEURAL INFORM PR, P1378
   Li QN, 2013, PROC CVPR IEEE, P851, DOI 10.1109/CVPR.2013.115
   Lin D, 2014, PROC CVPR IEEE, P3726, DOI 10.1109/CVPR.2014.476
   Lin R., 2015, P 2015 C EMP METH NA, P899
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mao J., 2014, DEEP CAPTIONING MULT
   Mikolov T., 2012, THESIS BRNO U TECHNO
   Mnih V., 2014, ADV NEURAL INFORM PR, P2204, DOI DOI 10.1017/S037346330300239X
   Nair V, 2009, ADV NEURAL INF PROCE, V22, P1339
   Ng J.Y.-H., 2015, SHORT SNIPPETS DEEP
   Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222
   Ouyang W, 2014, DEEPID NET MULTISTAG
   Pinheiro P. O., 2014, P ICML, P1
   Quattoni A, 2009, PROC CVPR IEEE, P413, DOI 10.1109/CVPRW.2009.5206537
   Rolfe J. T., 2013, DISCRIMINATIVE RECUR
   Sanchez J, 2013, INT J COMPUT VISION, V105, P222, DOI 10.1007/s11263-013-0636-x
   Sermanet P, 2013, OVERFEAT INTEGRATED
   Simonyan K., 2014, VERY DEEP CONVOLUTIO
   Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244
   Sutskever I, 2009, ADV NEURAL INFORM PR, P1601
   Sutskever I., 2011, P 28 INT C MACH LEAR, P1017
   Sutskever I., 2013, THESIS U TORONTO TOR
   Szegedy C., 2014, GOING DEEPER CONVOLU
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Toshev A, 2014, PROC CVPR IEEE, P1653, DOI 10.1109/CVPR.2014.214
   Venugopalan Subhashini, 2014, TRANSLATING VIDEOS N
   Visin F., 2015, RENET RECURRENT NEUR
   Wang JJ, 2010, PROC CVPR IEEE, P3360, DOI 10.1109/CVPR.2010.5540018
   Wang Xinggang, 2013, P 30 INT C MACH LEAR, P846
   XIAO JX, 2010, PROC CVPR IEEE, P3485, DOI DOI 10.1109/CVPR.2010.5539970
   Yan X, 2014, LECT NOTES COMPUT SC, V8692, P215, DOI 10.1007/978-3-319-10593-2_15
   Yang M, 2011, IEEE I CONF COMP VIS, P543, DOI 10.1109/ICCV.2011.6126286
   Yu J, 2014, IEEE T CYBERNETICS, V44, P2431, DOI 10.1109/TCYB.2014.2307862
   Yu J, 2012, IEEE T IMAGE PROCESS, V21, P3262, DOI 10.1109/TIP.2012.2190083
   Zeiler M. D., 2013, VISUALIZING UNDERSTA
   Zhang J., 2014, INT J DISTRIB SENS N, V2014, P1, DOI DOI 10.1371/J0URNAL.P0NE.0110734
   Zhen Zuo, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P18, DOI 10.1109/CVPRW.2015.7301268
   Zhou B, 2014, ADV NEURAL INFORM PR, P487, DOI DOI 10.1162/153244303322533223
NR 71
TC 28
Z9 29
U1 2
U2 37
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1057-7149
EI 1941-0042
J9 IEEE T IMAGE PROCESS
JI IEEE Trans. Image Process.
PD JUL
PY 2016
VL 25
IS 7
BP 2983
EP 2996
DI 10.1109/TIP.2016.2548241
PG 14
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
SC Computer Science; Engineering
GA DM1EH
UT WOS:000376087700004
PM 28113173
DA 2020-02-19
ER

PT J
AU Karakida, R
   Okada, M
   Amari, S
AF Karakida, Ryo
   Okada, Masato
   Amari, Shun-ichi
TI Dynamical analysis of contrastive divergence learning: Restricted
   Boltzmann machines with Gaussian visible units
SO NEURAL NETWORKS
LA English
DT Article
DE Deep learning; Restricted Boltzmann machine; Contrastive divergence;
   Component analysis; Stability of learning algorithms
ID MINOR COMPONENTS; PRINCIPAL
AB The restricted Boltzmann machine (RBM) is an essential constituent of deep learning, but it is hard to train by using maximum likelihood (ML) learning, which minimizes the Kullback-Leibler (KL) divergence. Instead, contrastive divergence (CD) learning has been developed as an approximation of ML learning and widely used in practice. To clarify the performance of CD learning, in this paper, we analytically derive the fixed points where ML and CDn learning rules converge in two types of RBMs: one with Gaussian visible and Gaussian hidden units and the other with Gaussian visible and Bernoulli hidden units. In addition, we analyze the stability of the fixed points. As a result, we find that the stable points of CDn learning rule coincide with those of ML learning rule in a Gaussian-Gaussian RBM. We also reveal that larger principal components of the input data are extracted at the stable points. Moreover, in a Gaussian-Bernoulli RBM, we find that both ML and CDn learning can extract independent components at one of stable points. Our analysis demonstrates that the same feature components as those extracted by ML learning are extracted simply by performing CD1 learning. Expanding this study should elucidate the specific solutions obtained by CD learning in other types of RBMs or in deep networks. (C) 2016 Elsevier Ltd. All rights reserved.
C1 [Karakida, Ryo; Okada, Masato] Univ Tokyo, Dept Complex Sci & Engn, 5-1-5 Kashiwanoha, Kashiwa, Chiba 2778561, Japan.
   [Okada, Masato; Amari, Shun-ichi] RIKEN Brain Sci Inst, 2-1 Hirosawa, Wako, Saitama 3510198, Japan.
RP Karakida, R (reprint author), Univ Tokyo, Dept Complex Sci & Engn, 5-1-5 Kashiwanoha, Kashiwa, Chiba 2778561, Japan.
EM karakida@mns.k.u-tokyo.ac.jp; okada@k.u-tokyo.ac.jp;
   amari@brain.riken.jp
FU Japan Society for the Promotion of Science (JSPS)Ministry of Education,
   Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for
   the Promotion of Science [26 . 8282]
FX This work was supported by a Grant-in-Aid for JSPS Fellows from the
   Japan Society for the Promotion of Science (JSPS) (Grant No. 26 . 8282).
CR Akaho Shotaro, 2008, Proceedings of the 2008 International Conference on Information Theory and Statistical Learning, P3
   Amari S, 1997, NEURAL NETWORKS, V10, P1345, DOI 10.1016/S0893-6080(97)00039-7
   AMARI SI, 1977, BIOL CYBERN, V26, P175, DOI 10.1007/BF00365229
   BALDI P, 1989, NEURAL NETWORKS, V2, P53, DOI 10.1016/0893-6080(89)90014-2
   Bengio Y, 2009, NEURAL COMPUT, V21, P1601, DOI 10.1162/neco.2008.11-07-647
   Chen TP, 2001, NEURAL NETWORKS, V14, P1377, DOI 10.1016/S0893-6080(01)00116-2
   Chen TP, 1998, IEEE T NEURAL NETWOR, V9, P58, DOI 10.1109/72.655030
   Dahl G., 2010, ADV NEURAL INFORM PR, P469
   Hinton G, 2005, AISTATS, V10, P33
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton G. E., 2010, 2010003 U TOR
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hyvarinen A, 2001, INDEPENDENT COMPONENT ANALYSIS: PRINCIPLES AND PRACTICE, P71
   Karakida R., 2014, DEEP LEARN REP UNPUB
   Lee H., 2008, ADV NEURAL INFORM PR, V20, P873
   Nair V, 2010, ICML, V27, P807, DOI DOI 10.0RG/PAPERS/432.PDF
   OJA E, 1992, NEURAL NETWORKS, V5, P927, DOI 10.1016/S0893-6080(05)80089-9
   Oja E, 1997, NEUROCOMPUTING, V17, P25, DOI 10.1016/S0925-2312(97)00045-3
   Oja E., 1989, International Journal of Neural Systems, V1, P61, DOI 10.1142/S0129065789000475
   Saxe Andrew M, 2014, INT C LEARN REPR
   Smolensky P, 1986, PARALLEL DISTRIBUTED, P194
   Sutskever I., 2010, INT C ART INT STAT, P789
   Tipping ME, 1999, J ROY STAT SOC B, V61, P611, DOI 10.1111/1467-9868.00196
   Wang N, 2014, ARXIV14015900
   Williams C., 2002, EDIINFRR0120 U ED
   WILLIAMS RJ, 1985, 8501 ICS U CAL
   YAN WY, 1994, IEEE T NEURAL NETWOR, V5, P674, DOI 10.1109/72.317720
   Yuille AL, 2005, ADV NEURAL INFORM PR, P1593
NR 28
TC 11
Z9 14
U1 0
U2 18
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0893-6080
EI 1879-2782
J9 NEURAL NETWORKS
JI Neural Netw.
PD JUL
PY 2016
VL 79
BP 78
EP 87
DI 10.1016/j.neunet.2016.03.013
PG 10
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA DM3GH
UT WOS:000376234300006
PM 27131468
DA 2020-02-19
ER

PT J
AU Hong, CQ
   Chen, XH
   Wang, XD
   Tang, CH
AF Hong, Chaoqun
   Chen, Xuhui
   Wang, Xiaodong
   Tang, Chaohui
TI Hypergraph regularized autoencoder for image-based 3D human pose
   recovery
SO SIGNAL PROCESSING
LA English
DT Article
DE Human pose recovery; Deep learning; Manifold regularization; Hypergraph;
   Patch alignment framework
ID RECOGNITION; FEATURES
AB Image-based human pose recovery is usually conducted by retrieving relevant poses with image features. However, semantic gap exists for current feature extractors, which limits recovery performance. In this paper, we propose a novel feature extractor with deep learning. It is based on denoising autoencoder and improves traditional methods by adopting locality preserved restriction. To impose this restriction, we introduce manifold regularization with hypergraph Laplacian. Hypergraph Laplacian matrix is constructed with patch alignment framework. In this way, an automatic feature extractor for silhouettes is achieved. Experimental results on two datasets show that the recovery error has been reduced by 10% to 20%, which demonstrates the effectiveness of the proposed method. (C) 2015 Elsevier B.V. All rights reserved.
C1 [Hong, Chaoqun; Chen, Xuhui; Wang, Xiaodong; Tang, Chaohui] Xiamen Univ Technol, Sch Comp & Informat Engn, Xiamen 361024, Fujian, Peoples R China.
RP Hong, CQ (reprint author), Ligong Rd 600, Xiamen 361024, Fujian, Peoples R China.
EM cqhong@xmut.edu.cn; xhchen@xmut.edu.cn; xdwangjsj@xmut.edu.cn;
   chhtang@xmut.edu.cn
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61202145]; Natural Science Foundation of Fujian
   Province of ChinaNatural Science Foundation of Fujian Province
   [2014J01256]
FX This work is supported by the National Natural Science Foundation of
   China (61202145) and the Natural Science Foundation of Fujian Province
   of China (2014J01256).
CR Agarwal A, 2006, IEEE T PATTERN ANAL, V28, P44, DOI 10.1109/TPAMI.2006.21
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Brand M., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1237, DOI 10.1109/ICCV.1999.790422
   Chen C, 2011, COMPUT VIS IMAGE UND, V115, P290, DOI 10.1016/j.cviu.2010.11.007
   Chen M, 2014, P 31 INT C MACH LEAR, P1476
   Dalal N, 2005, PROC CVPR IEEE, P886
   Gong C, 2014, IEEE T CYBERNETICS, V44, P882, DOI 10.1109/TCYB.2013.2274516
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hong CQ, 2015, IEEE T IND ELECTRON, V62, P3742, DOI 10.1109/TIE.2014.2378735
   Howe NR, 2000, ADV NEUR IN, V12, P820
   Karasuyama M., 2013, ADV NEURAL INFORM PR, P1547
   Lee H., 2009, P 26 ANN INT C MACH, P609, DOI DOI 10.1145/1553374.1553453
   Liu L., 2013, ACM INT C MULT, P997
   Mori G, 2005, IEEE T PATTERN ANAL, V27, P1832, DOI 10.1109/TPAMI.2005.220
   Mori G, 2002, LECT NOTES COMPUT SC, V2352, P666
   Rosales R, 2000, PROC CVPR IEEE, P721, DOI 10.1109/CVPR.2000.854946
   Scovanner P., 2007, P 15 INT C MULT, P357, DOI DOI 10.1145/1291233.1291311
   Shakhnarovich G, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P750
   Shen J, 2014, IEEE T IMAGE PROCESS, V23, P4786, DOI 10.1109/TIP.2014.2358082
   Sigal L, 2010, INT J COMPUT VISION, V87, P4, DOI 10.1007/s11263-009-0273-6
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Tao D., 2015, IEEE T CYBERN
   Tao DP, 2013, IEEE T CIRC SYST VID, V23, P1675, DOI 10.1109/TCSVT.2013.2255413
   Tao DP, 2013, IEEE T MULTIMEDIA, V15, P833, DOI 10.1109/TMM.2013.2238909
   Toshev A, 2014, IEEE C COMP VIS PATT
   Yang M, 2006, INT C PATT RECOG, P958
   Yang Y, 2009, P ACM MM INT C MULT, P175, DOI DOI 10.1145/1631272.1631298
   Yu J., 2015, NEUROCOMPUTING
   Yu J., 2013, MODERN MACHINE LEARN
   Yu J, 2015, IEEE T CYBERNETICS, V45, P767, DOI 10.1109/TCYB.2014.2336697
   Yu J, 2014, PATTERN RECOGN, V47, P3512, DOI 10.1016/j.patcog.2014.05.002
   Yu J, 2014, IEEE T CYBERNETICS, V44, P2431, DOI 10.1109/TCYB.2014.2307862
   Yu J, 2014, IEEE T IMAGE PROCESS, V23, P2019, DOI 10.1109/TIP.2014.2311377
   Yu J, 2014, IEEE T MULTIMEDIA, V16, P159, DOI 10.1109/TMM.2013.2284755
   Yuan Y, 2015, IEEE T NEUR NET LEAR, V26, P2222, DOI 10.1109/TNNLS.2014.2359471
   Zha Z. J., 2015, ACM T INTEL SYST TEC, V6
   Zha ZJ, 2013, IEEE T CIRC SYST VID, V23, P856, DOI 10.1109/TCSVT.2012.2226526
   Zhang TH, 2009, IEEE T KNOWL DATA EN, V21, P1299, DOI 10.1109/TKDE.2008.212
   ZHOU D, 2007, P ADV NEUR INF PROC, V19, P1601
NR 40
TC 26
Z9 26
U1 1
U2 20
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0165-1684
EI 1872-7557
J9 SIGNAL PROCESS
JI Signal Process.
PD JUL
PY 2016
VL 124
SI SI
BP 132
EP 140
DI 10.1016/j.sigpro.2015.10.004
PG 9
WC Engineering, Electrical & Electronic
SC Engineering
GA DI5KP
UT WOS:000373538100014
DA 2020-02-19
ER

PT J
AU Shi, J
   Zhou, SC
   Liu, X
   Zhang, Q
   Lu, MH
   Wang, TF
AF Shi, Jun
   Zhou, Shichong
   Liu, Xiao
   Zhang, Qi
   Lu, Minhua
   Wang, Tianfu
TI Stacked deep polynomial network based representation learning for tumor
   classification with small ultrasound image dataset
SO NEUROCOMPUTING
LA English
DT Article
DE Stacked deep polynomial network; Deep learning; Ultrasound image; Tumor
   classification; Texture feature; Small dataset
ID FEATURE-EXTRACTION; NEURAL-NETWORKS; BREAST-TUMOR; TEXTURE;
   QUANTIFICATION; HETEROGENEITY; DIAGNOSIS; MODELS
AB Ultrasound imaging has been widely used for tumor detection and diagnosis. In ultrasound based computer-aided diagnosis, feature representation is a crucial step. In recent years, deep learning (DL) has achieved great success in feature representation learning. However, it generally suffers from the small sample size problem. Since the medical datasets usually have small training samples, texture features are still very commonly used for small ultrasound image datasets. Compared with the commonly used DL algorithms, the newly proposed deep polynomial network (DPN) algorithm not only shows superior performance on large scale data, but also has the potential to learn effective feature representation from a relatively small dataset. In this work, a stacked DPN (S-DPN) algorithm is proposed to further improve the representation performance of the original DPN, and S-DPN is then applied to the task of texture feature learning for ultrasound based tumor classification with small dataset. The task tumor classification is performed on two image dataset, namely the breast B-mode ultrasound dataset and prostate ultrasound elastography dataset. In both cases, experimental results show that S-DPN achieves the best performance with classification accuracies of 92.40 +/- 1.1% and 90.28 +/- 2.78% on breast and prostate ultrasound datasets, respectively. This level of accuracy is significantly superior to all other compared algorithms in this work, including stacked auto-encoder and deep belief network. It suggests that S-DPN can be a strong candidate for the texture feature representation learning on small ultrasound datasets. (C) 2016 Elsevier B.V. All rights reserved.
C1 [Shi, Jun; Liu, Xiao; Zhang, Qi] Shanghai Univ, Inst Biomed Engn, Sch Commun & Informat Engn, Shanghai, Peoples R China.
   [Zhou, Shichong] Fudan Univ, Shanghai Canc Ctr, Dept Ultrasound, Shanghai 200433, Peoples R China.
   [Zhou, Shichong] Fudan Univ, Shanghai Med Coll, Dept Oncol, Shanghai 200433, Peoples R China.
   [Lu, Minhua; Wang, Tianfu] Shenzhen Univ, Natl Reg Key Technol Engn Lab Med Ultrasound, Guangdong Key Lab Biomed Measurements & Ultrasoun, Dept Biomed Engn,Sch Med, Shenzhen, Peoples R China.
RP Lu, MH (reprint author), Shenzhen Univ, Sch Med, Dept Biomed Engn, Nanhai Ave 3688, Shenzhen 518060, Guangdong, Peoples R China.
EM luminhua@szu.edu.cn
OI Wang, Tianfu/0000-0002-1248-1214
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61471231, 61471243, 61401267]; Innovation Program
   of Shanghai Municipal Education CommissionInnovation Program of Shanghai
   Municipal Education Commission [13YZ016]; Guangdong Key Laboratory for
   Biomedical Measurements and Ultrasound Imaging; Shenzhen Project
   [SGLH20131-010163759789, JCYJ20150731160834611]
FX This work is supported by the National Natural Science Foundation of
   China (61471231, 61471243, 61401267), the Innovation Program of Shanghai
   Municipal Education Commission (13YZ016), Guangdong Key Laboratory for
   Biomedical Measurements and Ultrasound Imaging, and the Shenzhen Project
   (SGLH20131-010163759789, JCYJ20150731160834611). The authors are
   grateful to Dr. Yehua Cai in the Huashan Hospital of Fudan University
   for providing the prostate ultrasound dataset.
CR Acharya UR, 2012, ULTRASOUND MED BIOL, V38, P899, DOI 10.1016/j.ultrasmedbio.2012.01.015
   Al-Kadi OS, 2015, MED IMAGE ANAL, V21, P59, DOI 10.1016/j.media.2014.12.004
   Arel I, 2010, IEEE COMPUT INTELL M, V5, P13, DOI 10.1109/MCI.2010.938364
   Bengio Y., 2007, P ADV NEUR INF PROC
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Brosch T., 2013, P MED IM COMP COMP A, V16, P633
   Carneiro G, 2013, IEEE T PATTERN ANAL, V35, P2592, DOI 10.1109/TPAMI.2013.96
   Carneiro G, 2012, IEEE T IMAGE PROCESS, V21, P968, DOI 10.1109/TIP.2011.2169273
   Chang H, 2013, IEEE I CONF COMP VIS, P169, DOI 10.1109/ICCV.2013.28
   CHEN S, 1989, INT J CONTROL, V50, P1873, DOI 10.1080/00207178908953472
   Cheng HD, 2010, PATTERN RECOGN, V43, P299, DOI 10.1016/j.patcog.2009.05.012
   Ciresan Dan C., 2012, NIPS, P2852
   Fasel I, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P1493, DOI 10.1109/ICPR.2010.369
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hutchinson B, 2013, IEEE T PATTERN ANAL, V35, P1944, DOI 10.1109/TPAMI.2012.268
   Li DC, 2011, ARTIF INTELL MED, V52, P45, DOI 10.1016/j.artmed.2011.02.001
   Liao S, 2013, LECT NOTES COMPUT SC, V8150, P254, DOI 10.1007/978-3-642-40763-5_32
   Liu SQ, 2015, IEEE T BIO-MED ENG, V62, P1132, DOI 10.1109/TBME.2014.2372011
   Livni R., 2013, ARXIV13047045
   Lu CA, 2007, IEEE T INF TECHNOL B, V11, P338, DOI 10.1109/TITB.2006.889702
   Menchon-Lara RM, 2015, NEUROCOMPUTING, V151, P161, DOI 10.1016/j.neucom.2014.09.066
   Moon WK, 2011, ULTRASOUND MED BIOL, V37, P539, DOI 10.1016/j.ultrasmedbio.2011.01.006
   Prasoon A, 2013, LECT NOTES COMPUT SC, V8150, P246, DOI 10.1007/978-3-642-40763-5_31
   Roth HR, 2014, LECT NOTES COMPUT SC, V8673, P520, DOI 10.1007/978-3-319-10404-1_65
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Sehgal CM, 2006, J MAMMARY GLAND BIOL, V11, P113, DOI 10.1007/s10911-006-9018-0
   Shin HC, 2013, IEEE T PATTERN ANAL, V35, P1930, DOI 10.1109/TPAMI.2012.277
   Suk HI, 2014, NEUROIMAGE, V101, P569, DOI 10.1016/j.neuroimage.2014.06.077
   Vapnik V. N., 1998, STAT LEARNING THEORY
   Wu KZ, 2014, OPTIK, V125, P4057, DOI 10.1016/j.ijleo.2014.01.114
   Yang HC, 2008, ULTRASONIC IMAGING, V30, P228, DOI 10.1177/016173460803000404
   Yu D, 2011, IEEE SIGNAL PROC MAG, V28, P145, DOI 10.1109/MSP.2010.939038
   Zhang Q, 2015, ULTRASOUND MED BIOL, V41, P588, DOI 10.1016/j.ultrasmedbio.2014.09.003
   Zhang Q, 2010, ULTRASOUND MED BIOL, V36, P111, DOI 10.1016/j.ultrasmedbio.2009.06.1097
   Zhang WL, 2015, NEUROIMAGE, V108, P214, DOI 10.1016/j.neuroimage.2014.12.061
   Zhou SC, 2013, BIOMED SIGNAL PROCES, V8, P688, DOI 10.1016/j.bspc.2013.06.011
NR 37
TC 64
Z9 64
U1 10
U2 89
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0925-2312
EI 1872-8286
J9 NEUROCOMPUTING
JI Neurocomputing
PD JUN 19
PY 2016
VL 194
BP 87
EP 94
DI 10.1016/j.neucom.2016.01.074
PG 8
WC Computer Science, Artificial Intelligence
SC Computer Science
GA DM7NU
UT WOS:000376548100009
DA 2020-02-19
ER

PT J
AU Zhang, W
   Liu, K
   Zhang, WD
   Zhang, YM
   Gu, JS
AF Zhang, Wei
   Liu, Kan
   Zhang, Weidong
   Zhang, Youmei
   Gu, Jason
TI Deep Neural Networks for wireless localization in indoor and outdoor
   environments
SO NEUROCOMPUTING
LA English
DT Article
DE Wireless positioning; Deep Neural Networks (DNNs); Hidden Markov model
   (HMM); Deep Learning; Stacked Denoising Autoencoder (SDA)
AB In this paper, we propose a wireless positioning method based on Deep Learning. To deal with the variant and unpredictable wireless signals, the positioning is casted in a four-layer Deep Neural Network (DNN) structure pre-trained by Stacked Denoising Autoencoder (SDA) that is capable of learning reliable features from a large set of noisy samples and avoids hand-engineering. Also, to maintain the temporal coherence, a Hidden Markov Model (HMM)-based fine localizer is introduced to smooth the initial positioning estimate obtained by the DNN-based coarse localizer. The data required for the experiments is collected from the real world in different periods to meet the actual environment. Experimental results indicate that the proposed system leads to substantial improvement on localization accuracy in coping with the turbulent wireless signals. (C) 2016 Elsevier B.V. All rights reserved.
C1 [Zhang, Wei; Liu, Kan; Zhang, Weidong; Zhang, Youmei] Shandong Univ, Sch Control Sci & Engn, Jinan, Peoples R China.
   [Gu, Jason] Dalhousie Univ, Dept Elect & Comp Engn, Halifax, NS B3H 3J5, Canada.
RP Liu, K (reprint author), Shandong Univ, Sch Control Sci & Engn, Jinan, Peoples R China.
EM sakuraxiafan@gmail.com
FU NSFCNational Natural Science Foundation of China [61203253, 61573222,
   61233014]; Major Research Program of Shandong Province
   [2015ZDXX0801A02]; Outstanding Young Scientist Award of Shandong
   Province [BS2013DX023]; Open Program of Jiangsu Key Laboratory of 3D
   Printing Equipment and Manufacturing [3DL201502]; Key Lab of ICSP MOE
   China
FX This work was supported by the NSFC Grant nos. 61203253, 61573222 and
   61233014, Major Research Program of Shandong Province 2015ZDXX0801A02,
   Research Found of Outstanding Young Scientist Award of Shandong Province
   BS2013DX023, Open Program of Jiangsu Key Laboratory of 3D Printing
   Equipment and Manufacturing 3DL201502, and Program of Key Lab of ICSP
   MOE China.
CR Bahl P., 2000, Proceedings IEEE INFOCOM 2000. Conference on Computer Communications. Nineteenth Annual Joint Conference of the IEEE Computer and Communications Societies (Cat. No.00CH37064), P775, DOI 10.1109/INFCOM.2000.832252
   Battiti R., 2002, P AINS 2002 UCLA
   Chang N, 2010, IEEE T CONSUM ELECTR, V56, P1860, DOI 10.1109/TCE.2010.5606338
   Chao-Lin Wu, 2004, 2004 IEEE International Conference on Networking, Sensing and Control (IEEE Cat. No.04EX761), P1026
   Chen ZQ, 2014, NEUROCOMPUTING, V123, P354, DOI 10.1016/j.neucom.2013.07.032
   Cheng L, 2011, IEEE T CONSUM ELECTR, V57, P1099, DOI 10.1109/TCE.2011.6018861
   Ciurana M, 2006, P 4 ACM INT WORKSH M, P121, DOI DOI 10.1145/1164783.1164806
   Dahl GE, 2012, IEEE T AUDIO SPEECH, V20, P30, DOI 10.1109/TASL.2011.2134090
   de Sa AO, 2016, NEUROCOMPUTING, V172, P322, DOI 10.1016/j.neucom.2015.03.099
   Deng L., 2012, APSIPA T SIGNAL INF
   Derr K, 2008, C IND ELECT APPL, P308, DOI 10.1109/ICIEA.2008.4582530
   Ding GM, 2013, 2013 IEEE WIRELESS COMMUNICATIONS AND NETWORKING CONFERENCE (WCNC), P2317
   FENG C, 2009, 2009 3 IEEE INT, P261
   Feng C, 2012, IEEE T MOBILE COMPUT, V11, P1983, DOI 10.1109/TMC.2011.216
   Fox D, 2003, IEEE PERVAS COMPUT, V2, P24, DOI 10.1109/MPRV.2003.1228524
   Guvenc I., 2003, GSPX INT SIGN PROC C, P91
   Hadsell R, 2009, J FIELD ROBOT, V26, P120, DOI 10.1002/rob.20276
   Hightower J, 2004, LECT NOTES COMPUT SC, V3205, P88
   Hinton GE, ARXIV12070580
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kusy B, 2009, 2009 INTERNATIONAL CONFERENCE ON INFORMATION PROCESSING IN SENSOR NETWORKS (IPSN 2009), P109
   Le QV, 2013, INT CONF ACOUST SPEE, P8595, DOI 10.1109/ICASSP.2013.6639343
   Li YM, 2013, NEUROCOMPUTING, V104, P170, DOI 10.1016/j.neucom.2012.10.011
   Lim CH, 2007, IEEE T CONSUM ELECTR, V53, P618, DOI 10.1109/TCE.2007.381737
   Mohamed AR, 2012, IEEE T AUDIO SPEECH, V20, P14, DOI 10.1109/TASL.2011.2109382
   Nikitaki S, 2010, CONF REC ASILOMAR C, P236, DOI 10.1109/ACSSC.2010.5757507
   Pan S.J., 2008, P WORKSH TRANSF LEAR
   Paul Anindya S, 2008, 2008 IEEE/ION Position, Location and Navigation Symposium - PLANS 2008, P646, DOI 10.1109/PLANS.2008.4569985
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Sermanet P, 2008, 2008 IEEE/RSJ INTERNATIONAL CONFERENCE ON ROBOTS AND INTELLIGENT SYSTEMS, VOLS 1-3, CONFERENCE PROCEEDINGS, P2525, DOI 10.1109/IROS.2008.4651203
   Vincent P, 2008, P 25 INT C MACH LEAR, P1096, DOI DOI 10.1145/1390156.1390294
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Yang Z, 2012, MOBICOM 12: PROCEEDINGS OF THE 18TH ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING, P269
   YANG ZX, 2016, NEUROCOMPUTING A, V174, P121, DOI DOI 10.1016/J.NEUC0M.2015.05.12
   Zhang BL, 2010, IEEE T CONSUM ELECTR, V56, P2208, DOI 10.1109/TCE.2010.5681092
   Zhang HT, 2014, 2014 IEEE INTERNATIONAL CONFERENCE ON INFORMATION AND AUTOMATION (ICIA), P225, DOI 10.1109/ICInfA.2014.6932657
   Zhang W, 2014, 2014 IEEE INTERNATIONAL CONFERENCE ON INFORMATION AND AUTOMATION (ICIA), P1176, DOI 10.1109/ICInfA.2014.6932827
   Zhang W, 2015, PATTERN RECOGN, V48, P3191, DOI 10.1016/j.patcog.2015.04.012
NR 39
TC 60
Z9 61
U1 2
U2 54
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0925-2312
EI 1872-8286
J9 NEUROCOMPUTING
JI Neurocomputing
PD JUN 19
PY 2016
VL 194
BP 279
EP 287
DI 10.1016/j.neucom.2016.02.055
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA DM7NU
UT WOS:000376548100028
DA 2020-02-19
ER

PT J
AU Hu, H
   Pang, L
   Shi, ZZ
AF Hu, Hong
   Pang, Liang
   Shi, Zhongzhi
TI Image matting in the perception granular deep learning
SO KNOWLEDGE-BASED SYSTEMS
LA English
DT Article
DE Image matting; Deep learning; Granular computing; Graph embedding;
   Granular deep learning
ID INFORMATION GRANULATION; GENERAL FRAMEWORK; FUZZY; ALGORITHM
AB In the past decade, proposed by Geoffrey Hinton, deep learning has been proved its powerful ability in processing data from lower level to higher level and gradually composes more and more semantic concepts by unsupervised feature learning for single modalities (e.g., text, images or audio). Usually a multi scale pyramid structure is applied in a layered deep learning neural network. But how to design a multi scale pyramid structure is still an open problem. At the same time, granular computing (GrC) has been an active topic of research in machine learning and computer vision. In this paper, inspired by the original insight of granular computing proposed by Zadeh, a generalized image-matting approach is defined in the framework of a novel Granular Deep Learning(GDL), in which the information similarity, proximity and functionality are very important for feature learning. We show that layered deep learning can be formally represented as a framework of a granular system defined by fuzzy logic. In this way, the pyramids or hierarchical structure of a layered deep learning neural network can be easily designed in such a granular system, i.e., the convolution pyramids or hierarchical convolutional factor analysis in the deep learning can be viewed as special cases of granular computing. The experiments show the effectiveness of our approach in the task of foreground and background separating. (C) 2016 Elsevier B.V. All rights reserved.
C1 [Hu, Hong; Pang, Liang; Shi, Zhongzhi] Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing, Peoples R China.
RP Hu, H (reprint author), Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing, Peoples R China.
EM huhong@ict.ac.cn; pangliang@qq.com; shizz@ics.ict.ac.cn
FU National Program on Key Basic Research Project (973 Program)National
   Basic Research Program of China [2013CB329502]; National Natural Science
   Foundation of ChinaNational Natural Science Foundation of China
   [61072085, 61035003]
FX This work is partially supported by the National Program on Key Basic
   Research Project (973 Program) (No. 2013CB329502), the National Natural
   Science Foundation of China (No. 61072085,61035003).
CR Bargiela A, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON GRANULAR COMPUTING, P806, DOI 10.1109/GRC.2006.1635922
   Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153
   Cao X., 2015, ARXIVORGPDF150405487
   CASTRO JL, 1995, IEEE T SYST MAN CYB, V25, P629, DOI 10.1109/21.370193
   Chen B, 2013, IEEE T PATTERN ANAL, V35, P1887, DOI 10.1109/TPAMI.2013.19
   Deng L, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 3 AND 4, P1692
   Dugas C., 2002, CIRANO WORKING PAPER, P472
   Erhan D, 2010, J MACH LEARN RES, V11, P625
   Farbman Z., 2011, ACM T GRAPHIC, V30, P61
   Fujita H, 2016, KNOWL-BASED SYST, V91, P1, DOI 10.1016/j.knosys.2015.10.026
   Fung G., 2001, KDD-2001. Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P77
   Glorot X., 2010, J MACH LEARN RES, V15
   Haykin, 2008, NEURAL NETWORKS COMP
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   HIROTA K, 1994, IEEE T FUZZY SYST, V2, P151, DOI 10.1109/91.277963
   Hirotao W. P. K., 1994, FUZZY SETS SYST, V68, P157
   Hong H. U., SCI CHINA, V51
   Hu H, 2014, EXPERT SYST APPL, V41, P2729, DOI 10.1016/j.eswa.2013.11.006
   Hu J, 2016, KNOWL-BASED SYST, V91, P179, DOI 10.1016/j.knosys.2015.10.006
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Levin A, 2008, IEEE T PATTERN ANAL, V30, P228, DOI 10.1109/TPAMI.2007.1177
   Li HX, 2000, IEEE T NEURAL NETWOR, V11, P356, DOI 10.1109/72.839006
   LIM YW, 1990, PATTERN RECOGN, V23, P935
   LIN CT, 1991, IEEE T COMPUT, V40, P1320, DOI 10.1109/12.106218
   Lin T. Y., 1999, COMPUTING WORDS INFO, P183
   Lin T.Y., 2007, NEIGHBORHOOD SYSTEMS
   Lin Tsau Young, 2012, COMPUT COMPLEX, P1404
   Lin TY, 1998, ROUGH SETS KNOWLEDGE, V1, P107
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Mountcastle VB, 1997, BRAIN, V120, P701, DOI 10.1093/brain/120.4.701
   Nair V, 2010, ICML, V27, P807, DOI DOI 10.0RG/PAPERS/432.PDF
   Nandedkar A. V., 2013, INTERACTIVE COLOUR V
   Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4
   Pedrycz A, 2012, FUZZY SET SYST, V203, P17, DOI 10.1016/j.fss.2012.03.009
   Pedrycz W., 1998, INTRO FUZZY SETS ANA
   Pedrycz W, 2006, IEEE T NEURAL NETWOR, V17, P636, DOI 10.1109/TNN.2006.873285
   Rifai S, 2011, LECT NOTES ARTIF INT, V6912, P645, DOI 10.1007/978-3-642-23783-6_41
   Salah R, 2011, P 28 INT C MACH LEAR, P833, DOI 10.1016/j.wasman.2010.10.006
   Salehi S, 2015, KNOWL-BASED SYST, V80, P78, DOI 10.1016/j.knosys.2015.02.018
   Tamura S, 1997, IEEE T NEURAL NETWOR, V8, P251, DOI 10.1109/72.557662
   Velmurugan T, 2014, APPL SOFT COMPUT, V19, P134, DOI 10.1016/j.asoc.2014.02.011
   Wang L., 2011, COLOR IMAGE SEGMENTA
   Wei D, 2012, NEUROCOMPUTING, V93, P115, DOI 10.1016/j.neucom.2012.03.016
   Yan SC, 2005, PROC CVPR IEEE, P830
   Yan SC, 2007, IEEE T PATTERN ANAL, V29, P40, DOI 10.1109/TPAMI.2007.250598
   Yao JT, 2013, IEEE T CYBERNETICS, V43, P1977, DOI 10.1109/TSMCC.2012.2236648
   Yao Y. Y., 2013, EMERGING PARADIGMS M, V13, P307
   Yao YY, 2015, KNOWL-BASED SYST, V80, P67, DOI 10.1016/j.knosys.2015.01.004
   Yao YY, 2001, INT J INTELL SYST, V16, P87, DOI 10.1002/1098-111X(200101)16:1<87::AID-INT7>3.0.CO;2-S
   Yao YY, 1998, INFORM SCIENCES, V111, P239, DOI 10.1016/S0020-0255(98)10006-3
   Yao YY, 2000, PROCEEDINGS OF THE FIFTH JOINT CONFERENCE ON INFORMATION SCIENCES, VOLS 1 AND 2, P186
   Yao YY, 2001, P INT COMP SOFTW APP, P638, DOI 10.1109/CMPSAC.2001.960680
   Yao YY, 1999, ADV SOFT COMPUTING E, P539, DOI 10.1007/978-1-4471-0819-1_40
   Zadeh LA, 1997, FUZZY SET SYST, V90, P111, DOI 10.1016/S0165-0114(97)00077-8
   Zhang J., 2014, P 20 ACM SIGKDD INT, P353
   Zhang L, 2004, FUND INFORM, V59, P287
   Zhang L., 2005, ICNN B 05 INT C, V1, pxiv
   Zhang XY, 2014, KNOWL-BASED SYST, V71, P146, DOI 10.1016/j.knosys.2014.07.022
NR 58
TC 4
Z9 4
U1 1
U2 57
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0950-7051
EI 1872-7409
J9 KNOWL-BASED SYST
JI Knowledge-Based Syst.
PD JUN 15
PY 2016
VL 102
BP 51
EP 63
DI 10.1016/j.knosys.2016.03.018
PG 13
WC Computer Science, Artificial Intelligence
SC Computer Science
GA DM7OJ
UT WOS:000376549600004
DA 2020-02-19
ER

PT J
AU Guo, Q
   Jia, J
   Shen, GY
   Zhang, L
   Cai, LH
   Yi, Z
AF Guo, Quan
   Jia, Jia
   Shen, Guangyao
   Zhang, Lei
   Cai, Lianhong
   Yi, Zhang
TI Learning robust uniform features for cross-media social data by using
   cross autoencoders
SO KNOWLEDGE-BASED SYSTEMS
LA English
DT Article
DE Cross-media; Social data; Cross modality; Deep learning; Autoencoder;
   Convolutional network
AB Cross-media analysis exploits social data with different modalities from multiple sources simultaneously and synergistically to discover knowledge and better understand the world. There are two levels of cross media social data. One is the element, which is made up of text, images, voice, or any combinations of modalities. Elements from the same data source can have different modalities. The other level of cross media social data is the new notion of aggregative subject (AS) a collection of time-series social elements sharing the same semantics (i.e., a collection of tweets, photos, blogs, and news of emergency events). While traditional feature learning methods focus on dealing with single modality data or data fused across multiple modalities, in this study, we systematically analyze the problem of feature learning for cross-media social data at the previously mentioned two levels. The general purpose is to obtain a robust and uniform representation from the social data in time-series and across different modalities. We propose a novel unsupervised method for cross-modality element-level feature learning called cross autoencoder (CAE). CAE can capture the cross-modality correlations in element samples. Furthermore, we extend it to the AS using the convolutional neural network (CNN), namely convolutional cross autoencoder (CCAE). We use CAEs as filters in the CCAE to handle cross-modality elements and the CNN framework to handle the time sequence and reduce the impact of outliers in AS. We finally apply the proposed method to classification tasks to evaluate the quality of the generated representations against several real-world social media datasets. In terms of accuracy, CAE gets 7.33% and 14.31% overall incremental rates on two element-level datasets. CCAE gets 11.2% and 60.5% overall incremental rates on two AS-level datasets. Experimental results show that the proposed CAE and CCAE work well with all tested classifiers and perform better than several other baseline feature learning methods. (C) 2016 Elsevier B.V. All rights reserved.
C1 [Guo, Quan; Zhang, Lei; Yi, Zhang] Sichuan Univ, Coll Comp Sci, Chengdu 610065, Peoples R China.
   [Jia, Jia; Shen, Guangyao; Cai, Lianhong] Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
RP Zhang, L (reprint author), Sichuan Univ, Coll Comp Sci, Chengdu 610065, Peoples R China.
EM guoquanscu@gmail.com; jjia@mail.tsinghua.edu.cn; thusgy2012@gmail.com;
   leizhang@scu.edu.cn; clh-dcs@tsinghua.edu.cn; zhangyi@scu.edu.cn
FU National Basic Research Program (973 Program) of ChinaNational Basic
   Research Program of China [2011CB302201, 2012CB316401]; Key Program of
   National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61432012, U1435213]; National Natural Science
   Foundation of ChinaNational Natural Science Foundation of China
   [61370023]
FX This work is supported by the National Basic Research Program (973
   Program) of China under Grant No. 2011CB302201 and Grant No.
   2012CB316401, the Key Program of National Natural Science Foundation of
   China under Grant No. 61432012 and Grant No. U1435213, and also National
   Natural Science Foundation of China under Grant No. 61370023.
CR Bengio Y., 2013, IEEE T PATTERN ANAL, V35
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411
   ESCALANTE HJ, 2008, P 1 ACM INT C MULT I, P172
   Fellbaum Christiane, 1998, WORDNET
   Feng FX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P7, DOI 10.1145/2647868.2654902
   Gao R, 2013, LECT NOTES ARTIF INT, V8211, P359, DOI 10.1007/978-3-319-02753-1_36
   Harris ZS, 1954, WORD, V10, P146, DOI 10.1080/00437956.1954.11659520
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hinton GE, 2007, TRENDS COGN SCI, V11, P428, DOI 10.1016/j.tics.2007.09.004
   Jia J, 2012, P 20 ACM INT C MULT, P857
   Jolliffe I., 2005, PRINCIPAL COMPONENT
   Kamvar S. D., 2011, P 4 ACM INT C WEB SE, P117
   Kamyshanska H, 2015, IEEE T PATTERN ANAL, V37, P1261, DOI 10.1109/TPAMI.2014.2362140
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lachenbruch PA, 1975, DISCRIMINANT ANAL
   LeCun Y., 1995, HDB BRAIN THEORY NEU, P3361
   Leng J., 2016, KNOWL BASED SYST
   Leutenegger S, 2011, IEEE I CONF COMP VIS, P2548, DOI 10.1109/ICCV.2011.6126542
   Liaw A., 2002, R NEWS, V23, P18, DOI DOI 10.1177/154405910408300516
   Lowe D., 1999, P INT C COMP VIS, V2, P1150, DOI [10.1109/ICCV.1999.790410, DOI 10.1109/ICCV.1999.790410]
   Machajdik J, 2010, P INT C MULTIMEDIA, P83, DOI DOI 10.1145/1873951.1873965
   Mao X., 2013, P 21 ACM INT C MULT, P897
   Ngiam J, 2011, IEEE INT C MACH LEAR, V28, P689, DOI DOI 10.1145/2647868
   Pennebaker JW, 2007, DEV PSYCHOMETRIC PRO
   Pham TT, 2007, P 16 ACM C INF KNOWL, P439
   Qu H, 2013, IEEE T CYBERNETICS, V43, P995, DOI 10.1109/TSMCB.2012.2221695
   Salah R, 2011, P 28 INT C MACH LEAR, P833, DOI 10.1016/j.wasman.2010.10.006
   Seung HS, 1998, ADV NEUR IN, V10, P654
   Srivastava N., 2012, ADV NEURAL INFORM PR, V25, P2231
   Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244
   Svetnik V, 2003, J CHEM INF COMP SCI, V43, P1947, DOI 10.1021/ci034160g
   Vidal J. C., 2016, KNOW BASED SYST
   Vincent P, 2008, P 25 INT C MACH LEAR, P1096, DOI DOI 10.1145/1390156.1390294
   Wang XH, 2013, IEEE IMAGE PROC, P3230, DOI 10.1109/ICIP.2013.6738665
   Wang XH, 2013, VISUAL COMPUT, V29, P1121, DOI 10.1007/s00371-012-0755-3
   Xie Lexing, 2013, P 21 ACM INT C MULT, P967, DOI DOI 10.1145/2502081.2502113
   Yang Y., 2014, P AAAI, V14, P1, DOI DOI 10.1016/J.NAN0EN.2014.12.039
   Zhao J., 2012, P 18 ACM SIGKDD INT, P1528, DOI DOI 10.1145/2339530.2339772
   Zhou GY, 2016, KNOWL-BASED SYST, V93, P75, DOI 10.1016/j.knosys.2015.11.002
   Zhou P, 2015, KNOWL-BASED SYST, V88, P97, DOI 10.1016/j.knosys.2015.08.003
   Zhu Ren, 2014, MultiMedia Modeling. 20th Anniversary International Conference, MMM 2014. Proceedings: LNCS 8325, P327, DOI 10.1007/978-3-319-04114-8_28
NR 44
TC 18
Z9 18
U1 1
U2 28
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0950-7051
EI 1872-7409
J9 KNOWL-BASED SYST
JI Knowledge-Based Syst.
PD JUN 15
PY 2016
VL 102
BP 64
EP 75
DI 10.1016/j.knosys.2016.03.028
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA DM7OJ
UT WOS:000376549600005
DA 2020-02-19
ER

PT J
AU Cheng, XY
   Chen, X
   Mallat, S
AF Cheng, Xiuyuan
   Chen, Xu
   Mallat, Stephane
TI Deep Haar scattering networks
SO INFORMATION AND INFERENCE-A JOURNAL OF THE IMA
LA English
DT Article
DE deep learning; neural network; scattering transform; Haar wavelet;
   classification; images; graphs
ID LEARNING ALGORITHM; FIELD
AB An orthogonal Haar scattering transform is a deep network computed with a hierarchy of additions, subtractions and absolute values over pairs of coefficients. Unsupervised learning optimizes Haar pairs to obtain sparse representations of training data with an algorithm of polynomial complexity. For signals defined on a graph, a Haar scattering is computed by cascading orthogonal Haar wavelet transforms on the graph, with Haar wavelets having connected supports. It defines a representation which is invariant to local displacements of signal values on the graph. When the graph connectivity is unknown, unsupervised Haar learning can provide a consistent estimation of connected wavelet supports. Classification results are given on image data bases, defined on regular grids or graphs, with a connectivity which may be known or unknown.
C1 [Cheng, Xiuyuan] Yale Univ, Program Appl Math, New Haven, CT 06520 USA.
   [Chen, Xu] Princeton Univ, Dept Elect Engn, Princeton, NJ 08544 USA.
   [Mallat, Stephane] Ecole Normale Super, Dept Comp Sci, Paris, France.
RP Cheng, XY (reprint author), Yale Univ, Program Appl Math, New Haven, CT 06520 USA.
EM xiuyuan.cheng@yale.edu; xuchen@princeton.edu; stephane.mallat@ens.fr
FU ERCEuropean Research Council (ERC) [320959]
FX This work was supported by the ERC grant InvariantClass 320959.
CR Ankenman J. I., 2014, THESIS
   Arora S., 2013, ARXIV13106343
   Bengio Y., 2013, ARXIV13061091
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bruna J., 2014, C P INT C LEARN REPR
   Bruna J, 2013, IEEE T PATTERN ANAL, V35, P1872, DOI 10.1109/TPAMI.2012.230
   CHEN S, 1991, IEEE T NEURAL NETWOR, V2, P302, DOI 10.1109/72.80341
   Chen X., 2014, ADV NEURAL INFORM PR, P1709
   Coifman C., 1992, WAVELETS THEIR APPL, P153
   Coifman RR, 2006, APPL COMPUT HARMON A, V21, P53, DOI 10.1016/j.acha.2006.04.004
   Coifman RR, 2011, APPL NUMER HARMON AN, P161, DOI 10.1007/978-0-8176-8095-4_9
   Deng L., 2011, P INTERSPEECH, P2285
   EDMONDS J, 1965, CANADIAN J MATH, V17, P449, DOI 10.4153/CJM-1965-045-4
   Erhan D., 2009, J MACHINE LEARNING R, V5, P153
   GABOW HN, 1976, J ACM, V23, P221, DOI 10.1145/321941.321942
   Gavish M, 2010, P 27 INT C MACH LEAR, P367
   Goodfellow I. J., 2013, ARXIV13024389
   He K., 2015, ARXIV150201852
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton G. E, 2012, ARXIV12070580
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Jia YQ, 2012, PROC CVPR IEEE, P3370, DOI 10.1109/CVPR.2012.6248076
   Jones PW, 2011, P NATL ACAD SCI USA, V108, P15679, DOI 10.1073/pnas.1107769108
   Labusch K, 2008, IEEE T NEURAL NETWOR, V19, P1985, DOI 10.1109/TNN.2008.2005830
   Le Q., 2013, ICML P 30 INT C MACH, P224
   Le Q.V., 2010, ADV NEURAL INFORM PR, V23, P1279
   LeCun Y, 2010, IEEE INT SYMP CIRC S, P253, DOI 10.1109/ISCAS.2010.5537907
   Lee C.-Y., 2014, ARXIV14095185
   Lin T., 2014, P 31 INT C MACH LEAR, P1323
   Mallat S., 2013, ARXIV13065532
   Mallat S, 2016, PHILOS T R SOC A, V374, DOI 10.1098/rsta.2015.0203
   Mallat S, 2012, COMMUN PUR APPL MATH, V65, P1331, DOI 10.1002/cpa.21413
   Maurey B., 1991, GEOM FUNCT ANAL, V1, P188
   Oyallon E., 2014, ARXIV14128659
   Pisier G., 1985, SPRINGER LECT NOTES, V1206, P167
   Preis R, 1999, LECT NOTES COMPUT SC, V1563, P259
   Raif R., 2013, P ADV NEUR INF PROC, P998
   Roux N. L., 2008, ADV NEURAL INF PROCE, P841
   Salah R, 2011, P 28 INT C MACH LEAR, P833, DOI 10.1016/j.wasman.2010.10.006
   Salakhutdinov R, 2015, ANNU REV STAT APPL, V2, P361, DOI 10.1146/annurev-statistics-010814-020120
   Shuman DI, 2013, IEEE SIGNAL PROC MAG, V30, P83, DOI 10.1109/MSP.2012.2235192
   Szlam A. D., 2005, OPTICS PHOTONICS 200
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
NR 44
TC 2
Z9 2
U1 0
U2 1
PU OXFORD UNIV PRESS
PI OXFORD
PA GREAT CLARENDON ST, OXFORD OX2 6DP, ENGLAND
SN 2049-8764
EI 2049-8772
J9 INF INFERENCE
JI Inf. Inference
PD JUN
PY 2016
VL 5
IS 2
SI SI
BP 105
EP 133
DI 10.1093/imaiai/iaw007
PG 29
WC Computer Science, Artificial Intelligence
SC Computer Science
GA EH6BW
UT WOS:000391858500002
DA 2020-02-19
ER

PT J
AU Galanti, T
   Wolf, L
   Hazan, T
AF Galanti, Tomer
   Wolf, Lior
   Hazan, Tamir
TI A theoretical framework for deep transfer learning
SO INFORMATION AND INFERENCE-A JOURNAL OF THE IMA
LA English
DT Article
DE transfer learning; PAC learning; PAC-Bayesian; deep learning
ID MODEL
AB We generalize the notion of PAC learning to include transfer learning. In our framework, the linkage between the source and the target tasks is a result of having the sample distribution of all classes drawn from the same distribution of distributions, and by restricting all source and a target concepts to belong to the same hypothesis subclass. We have two models: an adversary model and a randomized model. In the adversary model, we show that for binary classification, conventional PAC-learning is equivalent to the new notion of PAC-transfer and to transfer generalization of the VC-dimension. For regression, we show that PAC-transferability may exist even in the absence of PAC-learning. In both adversary and randomized models, we provide PAC-Bayesian and VC-style generalization bounds to transfer learning. In the randomized model, we provide bounds specifically derived for Deep Learning. A wide discussion on the tradeoffs between the different involved parameters in the bounds is provided. We demonstrate both cases in which transfer does not reduce the sample size ('trivial transfer') and cases in which the sample size is reduced ('non-trivial transfer').
C1 [Galanti, Tomer; Wolf, Lior] Tel Aviv Univ, Sch Comp Sci, Tel Aviv, Israel.
   [Hazan, Tamir] Technion, Fac Ind Engn & Management, Haifa, Israel.
RP Wolf, L (reprint author), Tel Aviv Univ, Sch Comp Sci, Tel Aviv, Israel.
EM tomer22g@gmail.com; wolf@cs.tau.ac.il; tamir.hazan@gmail.com
FU GIF, the German-Israeli Foundation for Scientific Research and
   DevelopmentGerman-Israeli Foundation for Scientific Research and
   Development
FX This research was partly supported by a Grant from the GIF, the
   German-Israeli Foundation for Scientific Research and Development.
CR Ando RK, 2005, J MACH LEARN RES, V6, P1817
   Baxter J, 2000, J ARTIF INTELL RES, V12, P149, DOI 10.1613/jair.731
   Ben-David S., 2010, J MACH LEARN RES PRO, P129
   Ben-David S., 2007, ADV NEURAL INFORM PR, P137, DOI DOI 10.1007/S10994-009-5152-4
   Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4
   BLUMER A, 1989, J ACM, V36, P929, DOI 10.1145/76359.76371
   Bousquet O, 2002, J MACH LEARN RES, V2, P499, DOI 10.1162/153244302760200704
   Cortes C, 2008, LECT NOTES ARTIF INT, V5254, P38, DOI 10.1007/978-3-540-87987-9_8
   Crammer K, 2008, J MACH LEARN RES, V9, P1757
   Donahue J., 2013, CORR
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Hardt M., 2015, ARXIV E PRINTS
   Kakade S. M., 2008, LECT NOTES
   Kifer D., 2004, P 30 INT C VER LARG, V30, P180, DOI DOI 10.1016/B978-012088469-8/50019-X
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kuzborskij I., 2013, P 30 INT C MACH LEAR, P942
   Kuzborskij I, 2013, PROC CVPR IEEE, P3358, DOI 10.1109/CVPR.2013.431
   Li FF, 2006, IEEE T PATTERN ANAL, V28, P594, DOI 10.1109/TPAMI.2006.79
   Li X., 2007, INT C ART INT STAT, P275
   Mansour Y., 2009, COLT 2009 22 C LEARN
   Mansour Yishay, 2009, ADV NEURAL INFORM PR, P1041
   McAllester D., 2013, CORR
   McAllester D. A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P230, DOI 10.1145/279943.279989
   Mukherjee S., 2002, ADV COMPUTATIONAL MA
   Orabona F, 2009, IEEE INT CONF ROBOT, P439
   Pentina A., 2014, P INT C MACH LEARN, P991
   Poggio T., 2015, INF INFERENCE J IMA, V11, P2635
   Razavian A., 2014, CORR
   Rumelhart D. E., 1988, NEUROCOMPUTING FDN R, V323, P696, DOI DOI 10.1002/1520-6696(198907)25:3<239::AID-JHBS2300250311>3.0.CO;2-A
   Sauer N., 1972, Journal of Combinatorial Theory, Series A, V13, P145, DOI 10.1016/0097-3165(72)90019-2
   Shalev-Shwartz S., 2010, JMLR
   Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN
   Sukhbaatar Sainbayar, 2014, CORR
   Taigman Y, 2015, PROC CVPR IEEE, P2746, DOI 10.1109/CVPR.2015.7298891
   Taigman Yaniv, 2014, C COMP VIS PATT REC
   Tommasi T, 2014, IEEE T PATTERN ANAL, V36, P928, DOI 10.1109/TPAMI.2013.197
   Tommasi T, 2010, PROC CVPR IEEE, P3081, DOI 10.1109/CVPR.2010.5540064
   VALIANT LG, 1984, COMMUN ACM, V27, P1134, DOI 10.1145/1968.1972
   Vapnik V. N., 1998, STAT LEARNING THEORY
   VAPNIK VN, 1971, THEOR PROBAB APPL+, V16, P264, DOI 10.1137/1116025
   Vapnik VN, 1995, NATURE STAT LEARNING
   Yang J., 2007, P 15 INT C MULT, P188, DOI DOI 10.1145/1291233.1291276
   Yang Y., 2014, CORR
   Yosinski J., 2014, CORR
NR 44
TC 3
Z9 3
U1 0
U2 1
PU OXFORD UNIV PRESS
PI OXFORD
PA GREAT CLARENDON ST, OXFORD OX2 6DP, ENGLAND
SN 2049-8764
EI 2049-8772
J9 INF INFERENCE
JI Inf. Inference
PD JUN
PY 2016
VL 5
IS 2
SI SI
BP 159
EP 209
DI 10.1093/imaiai/iaw008
PG 51
WC Computer Science, Artificial Intelligence
SC Computer Science
GA EH6BW
UT WOS:000391858500004
DA 2020-02-19
ER

PT J
AU Alain, G
   Bengio, Y
   Yao, L
   Yosinski, J
   Thibodeau-Laufer, E
   Zhang, SZ
   Vincent, P
AF Alain, Guillaume
   Bengio, Yoshua
   Yao, Li
   Yosinski, Jason
   Thibodeau-Laufer, Eric
   Zhang, Saizheng
   Vincent, Pascal
TI GSNs: generative stochastic networks
SO INFORMATION AND INFERENCE-A JOURNAL OF THE IMA
LA English
DT Article
DE deep learning; auto-encoders; generative models
ID GRADIENT
AB We introduce a novel training principle for generative probabilistic models that is an alternative to maximum likelihood. The proposed Generative Stochastic Networks (GSNs) framework generalizes Denoising Auto-Encoders (DAEs), and is based on learning the transition operator of a Markov chain whose stationary distribution estimates the data distribution. The transition distribution is a conditional distribution that generally involves a small move, so it has fewer dominant modes and is unimodal in the limit of small moves. This simplifies the learning problem, making it less like density estimation and more akin to supervised function approximation, with gradients that can be obtained by backprop. The theorems provided here provide a probabilistic interpretation for DAEs and generalize them; seen in the context of this framework, auto-encoders that learn with injected noise are a special case of GSNs and can be interpreted as generative models. The theorems also provide an interesting justification for dependency networks and generalized pseudolikelihood, and define an appropriate joint distribution and sampling mechanism, even when the conditionals are not consistent. GSNs can be used with missing inputs and can be used to sample subsets of variables given the others. Experiments validating these theoretical results are conducted on both synthetic datasets and image datasets. The experiments employ a particular architecture that mimics the Deep Boltzmann Machine Gibbs sampler, but that allows training to proceed with backprop through a recurrent neural network with noise injected inside and without the need for layerwise pretraining.
C1 [Alain, Guillaume; Bengio, Yoshua; Yao, Li; Thibodeau-Laufer, Eric; Zhang, Saizheng; Vincent, Pascal] Univ Montreal, Dept Comp Sci & Operat Res, Montreal, PQ H3C 3J7, Canada.
   [Yosinski, Jason] Cornell Univ, Dept Comp Sci, Ithaca, NY 14853 USA.
RP Alain, G (reprint author), Univ Montreal, Dept Comp Sci & Operat Res, Montreal, PQ H3C 3J7, Canada.
EM guillaume.alain@umontreal.ca
FU NSERCNatural Sciences and Engineering Research Council of Canada;
   CIFARCanadian Institute for Advanced Research (CIFAR); NASANational
   Aeronautics & Space Administration (NASA); Canada Research ChairsCanada
   Research Chairs; Compute Canada
FX The authors received funding from NSERC, CIFAR (Y.B. is a CIFAR Senior
   Fellow), NASA (J.Y. is a Space Technology Research Fellow) and the
   Canada Research Chairs and Compute Canada.
CR Alain G., 2013, INT C LEARN REPR ICL
   Anselmi F., 2016, INF INFEREN IN PRESS
   Behnke S., 2001, International Journal of Computational Intelligence and Applications, V1, P427, DOI 10.1142/S1469026801000342
   Bengio Y., 2013, ARXIV13116184 U MONT
   Bengio Y., 2014, TECHNICAL REPORT
   Bengio Y., 2009, LEARNING DEEP ARCHIT
   Bengio Y., 2007, NIPS 2006
   Bengio Y., 2013, ICML 13
   Bengio Y., 2013, NIPS 2013
   Bengio Y., 2013, TECHNICAL REPORT
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bergstra J., 2010, P PYTH SCI COMP C SC
   Bordes A., 2013, MACHINE LEARNING LEA
   Bornschein J., 2014, TECHNICAL REPORT
   Breuleux O, 2011, NEURAL COMPUT, V23, P2058, DOI 10.1162/NECO_a_00158
   Cheng X., 2016, INF INFEREN IN PRESS
   Cho GE, 2001, LINEAR ALGEBRA APPL, V335, P137, DOI 10.1016/S0024-3795(01)00320-2
   Cho K, 2013, NEURAL COMPUT, V25, P805, DOI 10.1162/NECO_a_00397
   Collobert R., 2008, ICML 2008
   Dahl G. E., 2010, NIPS 2010
   DAYAN P, 1995, NEURAL COMPUT, V7, P889, DOI 10.1162/neco.1995.7.5.889
   Deng L., 2010, INTERSPEECH 2010
   Dinh L., 2015, ARXIV14108516
   Galanti T., 2016, INF INFEREN IN PRESS
   Goodfellow I., 2015, DEEP LEARNING
   Goodfellow I. J., 2013, NIPS 2013
   Goodfellow IJ, 2014, ADV NEURAL INFORM PR, P2672, DOI DOI 10.1017/CB09781139058452
   Gregor K., 2014, INT C MACH LEARN ICM
   Gutmann M., 2010, AISTATS 2010
   Heckerman D, 2001, J MACH LEARN RES, V1, P49, DOI 10.1162/153244301753344614
   Hinton G, 2012, TECHNICAL REPORT
   Hinton G. E., 2000, 2000004 GCNU TR U CO
   Hinton G. E., 1999, ICANN 1999
   HINTON GE, 1995, SCIENCE, V268, P1158, DOI 10.1126/science.7761831
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hyvarinen A, 2006, NEURAL COMPUT, V18, P2283, DOI 10.1162/neco.2006.18.10.2283
   Kingma D. P, 2013, TECHNICAL REPORT
   Kingma Diederik P, 2014, P INT C LEARN REPR I
   Krizhevsky A., 2012, NIPS 2012
   Larochelle H., 2011, AISTATS 2011
   Lee H., 2007, ADV NEURAL INF PROCE, P801
   Li Y., 2013, CVPR 2013
   Luo H., 2013, AISTATS 2013
   Mnih A., 2014, ICML 2014
   Montavon Gregoire, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P621, DOI 10.1007/978-3-642-35289-8_33
   Ozair S., 2014, ARXIV14100630 U MONT
   Ozair S., 2014, ARXIV13125578 U MONT
   Poon H., 2011, UAI 2011
   Ranzato M., 2007, NIPS 2006
   Rezende D. J., 2014, ARXIV14014082
   Rifai S., 2012, ICML 12
   Salakhutdinov R, 2009, ARTIF INTELL, P448, DOI DOI 10.1109/CVPRW.2009.5206577
   Savard F., 2011, THESIS
   SCHWEITZER PJ, 1968, J APPL PROBAB, V5, P401, DOI 10.2307/3212261
   Seide F., 2011, P INTERSPEECH, P437
   Seung HS, 1998, ADV NEUR IN, V10, P654
   Sohl-Dickstein J., 2015, ARXI150303585
   Tieleman T., 2008, ICML 2008, P1064
   Vincent P., 2008, ICML 2008
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1023/A:1022672621406
   Yao L., 2014, ARXIV14090585 U MONT
   Yosinski J., 2014, NIPS 2014
   Younes L, 1998, STOCHASTICS STOCHAST, P177
   Zhou J., 2014, ICML 2014
   Zohrer M., 2014, NIPS 2014
NR 65
TC 5
Z9 5
U1 1
U2 2
PU OXFORD UNIV PRESS
PI OXFORD
PA GREAT CLARENDON ST, OXFORD OX2 6DP, ENGLAND
SN 2049-8764
EI 2049-8772
J9 INF INFERENCE
JI Inf. Inference
PD JUN
PY 2016
VL 5
IS 2
SI SI
BP 210
EP 249
DI 10.1093/imaiai/iaw003
PG 40
WC Computer Science, Artificial Intelligence
SC Computer Science
GA EH6BW
UT WOS:000391858500005
DA 2020-02-19
ER

PT J
AU Sigaud, O
   Droniou, A
AF Sigaud, Olivier
   Droniou, Alain
TI Towards Deep Developmental Learning
SO IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS
LA English
DT Article
DE Affordances; deep learning; developmental robotics; hierarchical
   predictive processing; sensorimotor contingencies
ID INTRINSICALLY MOTIVATED EXPLORATION; NEURAL-NETWORKS; MODELS;
   REPRESENTATION; ARCHITECTURES; SEQUENCES; ROBOTICS; BEHAVIOR; FUTURE;
   BRAINS
AB Deep learning techniques are having an undeniable impact on general pattern recognition issues. In this paper, from a developmental robotics perspective, we scrutinize deep learning techniques under the light of their capability to construct a hierarchy of meaningful multimodal representations from the raw sensors of robots. These investigations reveal the differences between the methodological constraints of pattern recognition and those of developmental robotics. In particular, we outline the necessity to rely on unsupervised rather than supervised learning methods and we highlight the need for progress towards the implementation of hierarchical predictive processing capabilities. Based on these new tools, we outline the emergence of a new domain that we call deep developmental learning.
C1 [Sigaud, Olivier; Droniou, Alain] UPMC Univ Paris 06, Sorbonne Univ, F-75005 Paris, France.
   [Sigaud, Olivier; Droniou, Alain] CNRS, Inst Syst Intelligents & Robot, UMR7222, Paris, France.
RP Sigaud, O (reprint author), UPMC Univ Paris 06, Sorbonne Univ, F-75005 Paris, France.
EM olivier.sigaud@isir.upmc.fr
FU European Commission, within the CoDyCo Project [FP7-ICT-2011-9, 600716];
   European Union's Horizon 2020 Research and Innovation Program within the
   DREAM Project [640891]
FX This work was supported in part by the European Commission, within the
   CoDyCo Project (FP7-ICT-2011-9, No. 600716), and in part by the European
   Union's Horizon 2020 Research and Innovation Program within the DREAM
   Project under Grant 640891.
CR ACKLEY DH, 1985, COGNITIVE SCI, V9, P147
   Aksoy Eren Erdal, 2013, P IEEE 3 JOINT INT C, P1
   Alain G., 2013, ARXIV12114246V4
   Arel I, 2010, IEEE COMPUT INTELL M, V5, P13, DOI 10.1109/MCI.2010.938364
   Asada M, 2001, ROBOT AUTON SYST, V37, P185, DOI 10.1016/S0921-8890(01)00157-9
   Baranes A., 2011, P 2011 IEEE INT C DE, V2, P1
   Baranes A, 2009, IEEE T AUTON MENT DE, V1, P155, DOI 10.1109/TAMD.2009.2037513
   Bellman R., 1957, DYNAMIC PROGRAMMING
   BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181
   Bengio Yoshua, 2013, Statistical Language and Speech Processing. First International Conference, SLSP 2013. Proceedings: LNCS 7978, P1, DOI 10.1007/978-3-642-39593-2_1
   Bengio Y., 2013, ADV NEURAL INFORM PR, V26, P899
   Bengio Y., 2009, P 26 ANN INT C MACH, P41, DOI DOI 10.1145/1553374.1553380
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   BERTHOZ A, 1997, [No title captured]
   Botvinick M, 1998, NATURE, V391, P756, DOI 10.1038/35784
   BROOKS RA, 1986, IEEE T ROBOTIC AUTOM, V2, P14, DOI 10.1109/JRA.1986.1087032
   Butz MV, 2003, LECT NOTES ARTIF INT, V2684, P1
   Cayton L., 2005, TECH REP
   Chao Weng, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P5532, DOI 10.1109/ICASSP.2014.6854661
   Cho Kyunghyun, 2014, ARXIV14061078
   Ciresan Dan Claudiu, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P581, DOI 10.1007/978-3-642-35289-8_31
   Ciresan D, 2012, NEURAL NETWORKS, V32, P333, DOI 10.1016/j.neunet.2012.02.023
   Clark A., 2008, SUPERSIZING MIND EMB
   Clark A., 1997, BEING THERE PUTTING
   Clark A, 2013, BEHAV BRAIN SCI, V36, P181, DOI 10.1017/S0140525X12000477
   Cristianini N., 2000, INTRO SUPPORT VECTOR
   d'Avella A, 2003, NAT NEUROSCI, V6, P300, DOI 10.1038/nn1010
   Delalleau O., 2011, ADV NEURAL INFORM PR, P666
   Deng L, 2013, INT CONF ACOUST SPEE, P8604, DOI 10.1109/ICASSP.2013.6639345
   Di A. Nuovo, 2015, ICDL EPIROB
   Droniou A., 2014, ROB AUT SYST
   Droniou A., 2014, P ICDL EP, P1
   Droniou Alain, 2013, P INT C MACH LEARN, P1
   Erhan D., 2009, P INT C ART INT STAT, V5
   Fernandez S, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P774
   Friston KJ, 2009, PHILOS T R SOC B, V364, P1211, DOI 10.1098/rstb.2008.0300
   Gaona W, 2014, CONN SCI, V27, P1
   Georgeon OL, 2013, BIOL INSPIR COGN ARC, V6, P46, DOI 10.1016/j.bica.2013.05.006
   Gers FA, 2000, NEURAL COMPUT, V12, P2451, DOI 10.1162/089976600300015015
   Gibson J. J., 1977, THEORY AFFORDANCES
   Glorot X., 2010, JLMR P TRACK, p[249, 2010], DOI DOI 10.1177/1753193409103364.
   Graves A, 2005, LECT NOTES COMPUT SC, V3697, P799
   Guerin F, 2013, IEEE T AUTON MENT DE, V5, P18, DOI 10.1109/TAMD.2012.2209879
   Guyon I., 2012, COMP VIS PATT REC WO, P1, DOI DOI 10.1109/CVPRW.2012.6239178
   HARNAD S, 1990, PHYSICA D, V42, P335, DOI 10.1016/0167-2789(90)90087-6
   Hawkins J., 2007, INTELLIGENCE
   Hinton G., 2012, NEURAL NETWORKS TRIC, V9, P599, DOI DOI 10.1007/978-3-642-35289-8_32
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton G. E, 2012, ARXIV12070580
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hochreiter S, 1997, ADV NEUR IN, V9, P473
   Hochreiter S, 2001, FIELD GUIDE DYNAMICA
   HORNIK K, 1989, NEURAL NETWORKS, V2, P359, DOI 10.1016/0893-6080(89)90020-8
   Ijspeert A., 2002, IEEE INT C ROB AUT I
   Ijspeert AJ, 2013, NEURAL COMPUT, V25, P328, DOI 10.1162/NECO_a_00393
   Ivaldi S, 2014, IEEE T AUTON MENT DE, V6, P56, DOI 10.1109/TAMD.2013.2280614
   Jain AK, 1999, ACM COMPUT SURV, V31, P264, DOI 10.1145/331499.331504
   Jaitly N, 2011, INT CONF ACOUST SPEE, P5884
   JARVILEHTO T, 1998, PSYCOLOQUY, V9, P41
   Jayaraman D., 2015, ARXIV150502206
   Kober J., 2010, ROB SCI SYST ZAR SPA
   Koechlin E, 2003, SCIENCE, V302, P1181, DOI 10.1126/science.1088545
   KOHONEN T, 1982, BIOL CYBERN, V43, P59, DOI 10.1007/BF00337288
   Koller D., 2009, PROBABILISTIC GRAPHI
   Konda K. R., 2013, ARXIV13063162
   Koutnik J, 2014, GECCO'14: PROCEEDINGS OF THE 2014 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P541, DOI 10.1145/2576768.2598358
   Koutnk J., 2014, ARXIV14023511
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Laflaquiere A., 2015, ICDL EPIROB
   Lagoudakis M. G., 2003, J MACHINE LEARNING, V4, P1107, DOI DOI 10.1162/JMLR.2003.4.6.1107
   Lange S, 2010, ESANN
   Lange S., 2010, 2010 INT JOINT C NEU, P1
   Le QV, 2013, INT CONF ACOUST SPEE, P8595, DOI 10.1109/ICASSP.2013.6639343
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y., 1998, THEMNIST DATABASE HA
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lee H., 2009, P 26 ANN INT C MACH, P609, DOI DOI 10.1145/1553374.1553453
   Lee H., 2007, ADV NEURAL INF PROCE, P801
   Lemaignan S, 2010, IEEE INT C INT ROBOT, P3548, DOI 10.1109/IROS.2010.5649547
   Lenz I, 2015, INT J ROBOT RES, V34, P705, DOI 10.1177/0278364914549607
   Levine S, 2014, ADV NEURAL INFORM PR, P1071
   Levine S., 2015, ARXIV150400702
   Lillicrap T. P., 2015, ARXIV150902971
   Lopes M, 2010, IEEE T AUTON MENT DE, V2, P65, DOI 10.1109/TAMD.2010.2052419
   Lungarella M, 2003, CONNECT SCI, V15, P151, DOI 10.1080/09540090310001655110
   MacQueen J, 1967, P 5 BERK S MATH STAT, V1, P281, DOI DOI 10.1234/12345678
   Martens J., 2010, P 27 INT C MACH LEAR, P735, DOI DOI 10.1155/2011/176802
   MCGURK H, 1976, NATURE, V264, P746, DOI 10.1038/264746a0
   MEHTA P., 2014, ARXIV14103831
   Meltzoff AN, 2009, SCIENCE, V325, P284, DOI 10.1126/science.1175626
   Memisevic R., 2013, LEARN RELATE IMAGES, V35, P1829
   Memisevic R., 2010, ADV NEURAL INFORM PR, P1603
   Memisevic R, 2011, IEEE I CONF COMP VIS, P1591, DOI 10.1109/ICCV.2011.6126419
   Memisevic R, 2010, NEURAL COMPUT, V22, P1473, DOI 10.1162/neco.2010.01-09-953
   Michalski V., 2014, ADV NEURAL INFORM PR, P1925
   Mikolov Tomas, 2014, ARXIV14127753
   Miller  P., 2010, THEORIES DEV PSYCHOL
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Mocanu D. C., 2015, PATTERN RECOGNIT LET
   Mohamed AR, 2012, IEEE T AUDIO SPEECH, V20, P14, DOI 10.1109/TASL.2011.2109382
   Moldovan B, 2012, IEEE INT CONF ROBOT, P4373, DOI 10.1109/ICRA.2012.6225042
   Narayanan H., 2010, P ADV NEUR INF PROC, V23
   Nemec B., 2009, IEEE RAS INT C HUM R
   Neverova N, 2014, WORKSH EUR C COMP VI, P474, DOI DOI 10.1007/978-3-319-16178-533
   NEWELL A, 1980, COGNITIVE SCI, V4, P135, DOI 10.1016/S0364-0213(80)80015-2
   O'Regan JK, 2001, BEHAV BRAIN SCI, V24, P939, DOI 10.1017/S0140525X01000115
   Oudeyer P.-Y., 2013, INTRINSICALLY MOTIVA, P303
   Oudeyer PY, 2007, IEEE T EVOLUT COMPUT, V11, P265, DOI 10.1109/TEVC.2006.890271
   Pasa Luca, 2015, NEUROCOMPUTING
   Peyrache A, 2009, NAT NEUROSCI, V12, P919, DOI 10.1038/nn.2337
   Pezzulo G., 2011, EMB GROUND C, P196
   Pezzulo G, 2008, MIND MACH, V18, P179, DOI 10.1007/s11023-008-9095-5
   Pezzulo G, 2007, LECT NOTES ARTIF INT, V4520, P73
   Pezzulo G, 2009, PSYCHOL RES-PSYCH FO, V73, P559, DOI 10.1007/s00426-009-0237-z
   Piaget J., 1954, CONSTRUCTION REALITY, V82
   Piaget J., 1952, ORIGINS INTELLIGENCE
   Ranzato M., 2007, J MACH LEARN RES, V2, P371
   Rao RPN, 1999, NAT NEUROSCI, V2, P79, DOI 10.1038/4580
   Reed S., 2013, WORKSH NIPS
   Rifai S., 2011, ADV NEURAL INFORM PR, P2294
   Rifai S, 2011, LECT NOTES ARTIF INT, V6912, P645, DOI 10.1007/978-3-642-23783-6_41
   Rudy J., 2014, ARXIV14127009
   Rumelhart D.E., 1986, PARALLEL DISTRIBUTED, V2, P7
   Sak H., 2014, ARXIV14021128
   Salah R, 2011, P 28 INT C MACH LEAR, P833, DOI 10.1016/j.wasman.2010.10.006
   Salakhutdinov R, 2009, ARTIF INTELL, P448, DOI DOI 10.1109/CVPRW.2009.5206577
   Salman A, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P103, DOI 10.1109/IJCNN.2011.6033207
   SCHMIDHUBER J, 1991, IEEE IJCNN, P1458, DOI 10.1109/IJCNN.1991.170605
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Scholkopf B., 2006, ADV NEURAL INFORM PR, P1345
   SEARLE JR, 1980, BEHAV BRAIN SCI, V3, P417, DOI 10.1017/S0140525X00006038
   Skaggs WE, 1996, SCIENCE, V271, P1870, DOI 10.1126/science.271.5257.1870
   Smith L, 2005, ARTIF LIFE, V11, P13, DOI 10.1162/1064546053278973
   Smolensky P, 1986, PARALLEL DISTRIBUTED, P194
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Starzyk JA, 2007, IEEE T NEURAL NETWOR, V18, P344, DOI 10.1109/TNN.2006.884681
   Stoianov I, 2012, NAT NEUROSCI, V15, P194, DOI 10.1038/nn.2996
   Stoytchev A, 2005, IEEE INT CONF ROBOT, P3060
   Stoytchev A, 2009, IEEE T AUTON MENT DE, V1, P122, DOI 10.1109/TAMD.2009.2029989
   Stuhlsatz A., 2010, P INT JOINT C NEUR N, P1
   Stulp F., 2013, PALADYN J BEHAV ROBO, V4, P49, DOI DOI 10.2478/PJBR-2013-0003
   Stulp F., 2012, P 29 INT C MACH LEAR, P1
   Stulp F., 2013, P IEEE RAS INT C HUM, P1
   Stulp F, 2015, NEURAL NETWORKS, V69, P60, DOI 10.1016/j.neunet.2015.05.005
   SUTSKEVER I, 2008, NIPS, V21, P2008
   Sutskever I., 2011, P 28 INT C MACH LEAR, P1017
   Sutskever I., 2007, 11 INT C ART INT STA
   Sutton R. S, 1998, REINFORCEMENT LEARNI
   Tani J, 2003, NEURAL NETWORKS, V16, P11, DOI 10.1016/S0893-6080(02)00214-9
   Taylor GW, 2011, J MACH LEARN RES, V12, P1025
   Taylor GW, 2009, P 26 ANN INT C MACH, P1025, DOI DOI 10.1145/1553374.1553505
   Tishby N., 2015, ARXIV150302406
   Ugur E., 2009, INT C EP ROB
   Ugur E., 2014, P INT C DEV LEARN EP
   Ugur E., 2014, INT C DEV LEARN EP R
   Vincent P, 2008, P 25 INT C MACH LEAR, P1096, DOI DOI 10.1145/1390156.1390294
   VYGOTSKY LS, 1967, SOV PSYCHOL-USSR, V5, P6, DOI 10.2753/RPO1061-040505036
   Wagner U, 2004, NATURE, V427, P352, DOI 10.1038/nature02223
   WARD JH, 1963, J AM STAT ASSOC, V58, P236, DOI 10.2307/2282967
   Weng JY, 2001, SCIENCE, V291, P599, DOI 10.1126/science.291.5504.599
   Zorzi M., 2013, FRONTIERS PSYCHOL, V4
NR 162
TC 18
Z9 18
U1 1
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 2379-8920
EI 2379-8939
J9 IEEE T COGN DEV SYST
JI IEEE Trans. Cogn. Dev. Syst.
PD JUN
PY 2016
VL 8
IS 2
BP 99
EP 114
DI 10.1109/TAMD.2015.2496248
PG 16
WC Computer Science, Artificial Intelligence; Robotics; Neurosciences
SC Computer Science; Robotics; Neurosciences & Neurology
GA ED2NP
UT WOS:000388684100003
DA 2020-02-19
ER

PT J
AU Welchowski, T
   Schmid, M
AF Welchowski, Thomas
   Schmid, Matthias
TI A framework for parameter estimation and model selection in kernel deep
   stacking networks
SO ARTIFICIAL INTELLIGENCE IN MEDICINE
LA English
DT Article
DE Deep learning; Artificial neural networks; Kernel regression;
   Model-based optimization
ID ARTIFICIAL NEURAL-NETWORKS; HOSPITAL READMISSION; OPTIMIZATION; RISK
AB Background and objectives: Kernel deep stacking networks (KDSNs) are a novel method for supervised learning in biomedical research. Belonging to the class of deep learning techniques, KDSNs are based on artificial neural network architectures that involve multiple nonlinear transformations of the input data. Unlike traditional artificial neural networks, KDSNs do not rely on backpropagation algorithms but on an efficient fitting procedure that is based on a series of kernel ridge regression models with closed-form solutions. Although being computationally advantageous, KDSN modeling remains a challenging task, as it requires the specification of a large number of tuning parameters.
   Methods and material: We propose a new data-driven framework for parameter estimation, hyperparameter tuning, and model selection in KDSNs. The proposed methodology is based on a combination of model-based optimization and hill climbing approaches that do not require the pre-specification of any of the KDSN tuning parameters. We demonstrate the performance of KDSNs by analyzing three medical data sets on hospital readmission of diabetes patients, coronary artery disease, and hospital costs.
   Results: Our numerical studies show that the run-time of the proposed KDSN methodology is significantly shorter than the respective run-time of grid search strategies for hyperparameter tuning. They also show that KDSN modeling is competitive in terms of prediction accuracy with other state-of-the-art techniques for statistical learning.
   Conclusions: KDSNs are a computationally efficient approximation of backpropagation-based artificial neural network techniques. Application of the proposed methodology results in a fast tuning procedure that generates KDSN fits having a similar prediction accuracy as other techniques in the field of deep learning. (C) 2016 Elsevier B.V. All rights reserved.
C1 [Welchowski, Thomas; Schmid, Matthias] Univ Bonn, Dept Med Biometry Informat & Epidemiol, Sigmund Freud Str 25, D-53127 Bonn, Germany.
RP Welchowski, T (reprint author), Univ Bonn, Dept Med Biometry Informat & Epidemiol, Sigmund Freud Str 25, D-53127 Bonn, Germany.
EM welchow@imbie.meb.uni-bonn.de; schmid@imbie.meb.uni-bonn.de
OI Schmid, Matthias/0000-0002-0788-0317; Welchowski,
   Thomas/0000-0003-2940-647X
FU Deutsche ForschungsgemeinschaftGerman Research Foundation (DFG) [SCHM
   2966/1-2]
FX We thank Bernd Bischl (University of Munich) for valuable discussions on
   model-based optimization, Dirk Eddelbuttel (Debian Project) for helping
   us with the implementation of the MBO procedure, and Peter Welchowski
   for proof reading. Financial support from Deutsche
   Forschungsgemeinschaft (Project SCHM 2966/1-2) is gratefully
   acknowledged.
CR AIELLO T, 2016, H2O R INTERFACE H2O
   Amalakuhan B, 2012, J COMMUNITY HOSP INT, V2, DOI 10.3402/jchimp.v2i1.9915
   Arevalo J, 2015, ARTIF INTELL MED, V64, P131, DOI 10.1016/j.artmed.2015.04.004
   Bengio Y, 2011, LECT NOTES ARTIF INT, V6925, P18, DOI 10.1007/978-3-642-24412-4_3
   BISCHL B, 2015, P 2015 INT WORKSH ME
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Breiman L, 2015, RANDOMFOREST BREIMAN
   Brent R. P., 1973, ALGORITHMS MINIMIZAT
   Broomhead D. S., 1988, Complex Systems, V2, P321
   Chen L, 2015, MED IMAGE ANAL, V23, P1, DOI 10.1016/j.media.2015.03.004
   DENG L, 2012, P IEEE WORKSH SPOK L, P210
   Deng L., 2011, P ANN C INT SPEECH C, P2285
   Deng L., 2014, DEEP LEARNING METHOD
   Dhondalay GK, 2014, ARTIF INTELL MED, V62, P119, DOI 10.1016/j.artmed.2014.07.003
   DIGGLE PJ, 2002, P NIPS WORKSH STAT M
   Drees M., 2015, DARCH PACKAGE DEEP A
   DUBRULE O, 1983, J INT ASS MATH GEOL, V15, P687, DOI 10.1007/BF01033232
   Dungan Kathleen M, 2012, J Diabetes Sci Technol, V6, P1045
   GEISSER S, 1979, J AM STAT ASSOC, V74, P153, DOI 10.2307/2286745
   Glorot X., 2010, P INT C ART INT STAT, P249
   Grun B, 2012, BIOINFORMATICS, V28, P222, DOI 10.1093/bioinformatics/btr653
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hofmann T, 2008, ANN STAT, V36, P1171, DOI 10.1214/009053607000000677
   Horn D, 2015, LECT NOTES COMPUT SC, V9018, P64, DOI 10.1007/978-3-319-15934-8_5
   HORNIK K, 1989, NEURAL NETWORKS, V2, P359, DOI 10.1016/0893-6080(89)90020-8
   Hosseinzadeh A., 2013, P 27 AAAI C ART INT, P1532
   Huang PS, 2013, INT CONF ACOUST SPEE, P3143, DOI 10.1109/ICASSP.2013.6638237
   Hutchinson B, 2013, IEEE T PATTERN ANAL, V35, P1944, DOI 10.1109/TPAMI.2012.268
   JOHNSON ME, 1990, J STAT PLAN INFER, V26, P131, DOI 10.1016/0378-3758(90)90122-B
   Jones DR, 1998, J GLOBAL OPTIM, V13, P455, DOI 10.1023/A:1008306431147
   Kalderstam J, 2013, ARTIF INTELL MED, V58, P125, DOI 10.1016/j.artmed.2013.03.001
   Kansagara D, 2011, JAMA-J AM MED ASSOC, V306, P1688, DOI 10.1001/jama.2011.1515
   KNAUS WA, 1995, ANN INTERN MED, V122, P191, DOI 10.7326/0003-4819-122-3-199502010-00007
   Kuhn M., 2013, APPL PREDICTIVE MODE
   Lancashire LJ, 2009, BRIEF BIOINFORM, V10, P315, DOI 10.1093/bib/bbp012
   Leung MKK, 2014, BIOINFORMATICS, V30, P121, DOI 10.1093/bioinformatics/btu277
   LICHMAN M, 2013, UCI MACHINE LEARNING, P3
   LOPEZIBANEZ M, 2015, IRACE ITERATED RACIN
   Ma JS, 2015, J CHEM INF MODEL, V55, P263, DOI 10.1021/ci500747n
   Montavon G., 2012, NEURAL NETWORKS TRIC
   Quang D, 2015, BIOINFORMATICS, V31, P761, DOI 10.1093/bioinformatics/btu703
   Rahimi A., 2008, ADV NEURAL INFORM PR, P1177
   Roustant O, 2012, J STAT SOFTW, V51, P1
   Roy SB, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1691, DOI 10.1145/2783258.2788585
   Rubin DJ, 2015, CURR DIABETES REP, V15, DOI 10.1007/s11892-015-0584-7
   Saunders C., 1998, P 15 INT C MACH LEAR, P515
   Scholkopf B., 2004, KERNEL METHODS COMPU
   Skiena S., 2008, ALGORITHM DESIGN MAN
   Strack B, 2014, BIOMED RES INT, DOI 10.1155/2014/781670
   van Walraven C, 2010, CAN MED ASSOC J, V182, P551, DOI 10.1503/cmaj.091117
   vanBuuren S, 2015, MICE MULTIVARIATE IM
   WELCHOWSKI T, 2016, KERNDEEPSTACKNET KER
   Wood S., 2006, GEN ADDITIVE MODELS
   Yu SP, 2015, ARTIF INTELL MED, V65, P89, DOI 10.1016/j.artmed.2015.08.005
   Zolfaghar K., 2013, PREDICTING RISK OF R
NR 56
TC 3
Z9 3
U1 0
U2 21
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0933-3657
EI 1873-2860
J9 ARTIF INTELL MED
JI Artif. Intell. Med.
PD JUN
PY 2016
VL 70
BP 31
EP 40
DI 10.1016/j.artmed.2016.04.002
PG 10
WC Computer Science, Artificial Intelligence; Engineering, Biomedical;
   Medical Informatics
SC Computer Science; Engineering; Medical Informatics
GA DT0HF
UT WOS:000381163700003
PM 27431035
DA 2020-02-19
ER

PT J
AU Yu, YH
   Lin, HF
   Meng, JN
   Zhao, ZH
AF Yu, Yuhai
   Lin, Hongfei
   Meng, Jiana
   Zhao, Zhehuan
TI Visual and Textual Sentiment Analysis of a Microblog Using Deep
   Convolutional Neural Networks
SO ALGORITHMS
LA English
DT Article
DE sentiment analysis; convolutional neural network; word vectors;
   microblog
ID FRAMEWORK
AB Sentiment analysis of online social media has attracted significant interest recently. Many studies have been performed, but most existing methods focus on either only textual content or only visual content. In this paper, we utilize deep learning models in a convolutional neural network (CNN) to analyze the sentiment in Chinese microblogs from both textual and visual content. We first train a CNN on top of pre-trained word vectors for textual sentiment analysis and employ a deep convolutional neural network (DNN) with generalized dropout for visual sentiment analysis. We then evaluate our sentiment prediction framework on a dataset collected from a famous Chinese social media network (Sina Weibo) that includes text and related images and demonstrate state-of-the-art results on this Chinese sentiment analysis benchmark.
C1 [Yu, Yuhai; Lin, Hongfei; Zhao, Zhehuan] Dalian Univ Technol, Sch Comp Sci & Technol, Dalian 116024, Peoples R China.
   [Yu, Yuhai; Meng, Jiana] Dalian Nationalities Univ, Sch Comp Sci & Engn, Dalian 116600, Peoples R China.
RP Lin, HF (reprint author), Dalian Univ Technol, Sch Comp Sci & Technol, Dalian 116024, Peoples R China.
EM yuyh@dlnu.edu.cn; hflin@dlut.edu.cn; mengjn@dlnu.edu.cn;
   zhehuan@dlut.edu.cn
OI Yu, Yuhai/0000-0002-7827-3143
CR Borth Damian, 2013, P 21 ACM INT C MULT
   Cambria E., 2015, P AAAI AUST TX US 25
   Cambria E., 2014, P 28 AAAI C ART INT
   Cambria E., 2015, SENTIC COMPUTING COM, V1
   Cambria E, 2016, IEEE INTELL SYST, V31, P102, DOI 10.1109/MIS.2016.31
   Cambria E, 2015, LECT NOTES COMPUT SC, V9042, P3, DOI 10.1007/978-3-319-18117-2_1
   Cambria E, 2014, IEEE COMPUT INTELL M, V9, P48, DOI 10.1109/MCI.2014.2307227
   Cao D., 2014, MULTIMED TOOLS APPL, V73, P1
   Cao DL, 2016, MULTIMEDIA SYST, V22, P479, DOI 10.1007/s00530-014-0407-8
   Chen T, 2014, ARXIV14108586
   Chikersal P, 2015, P INT WORKSH SEM EV
   Chikersal P, 2015, LECT NOTES COMPUT SC, V9042, P49, DOI 10.1007/978-3-319-18117-2_4
   Collobert R, 2011, J MACH LEARN RES, V12, P2493
   dos Santos Cicero Nogueira, 2014, P 25 INT C COMP LING
   Esuli A., 2006, P LREC CIT GEN IT
   Kim Y, 2014, ARXIV14085882
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Maynard D., 2013, P BCS SGAI WORKSH SO
   Mesnil Gregoire, 2014, ARXIV14125335
   Mikolov T., 2013, ADV NEUR INF PROC SY, V26
   Mikolov T., 2013, P HLT NAACL ATL GA U
   MIKOLOV T., 2013, ARXIV13013781
   Pereira M.H., 2016, P INT AAAI C WEB SOC
   Poria S., 2015, P EMNLP LISB PORT 17
   Poria S, 2016, NEUROCOMPUTING, V174, P50, DOI 10.1016/j.neucom.2015.01.095
   Poria S, 2015, NEURAL NETWORKS, V63, P104, DOI 10.1016/j.neunet.2014.10.005
   Poria S, 2014, KNOWL-BASED SYST, V69, P108, DOI 10.1016/j.knosys.2014.06.011
   Rosas VP, 2013, IEEE INTELL SYST, V28, P38, DOI 10.1109/MIS.2013.9
   Strapparava C., 2004, P LREC LISB PORT 26
   Wan L, 2013, P 30 INT C MACH LEAR
   Wang M., 2014, P INT C INT MULT COM
   Xu C., 2014, ARXIV14115731
   You Q., 2015, P 23 ANN ACM C MULT
   Yu Y., 2015, J COMPUT INF SYST, V11, P5403
   Zeiler Matthew D, 2012, ARXIV12125701
NR 35
TC 3
Z9 3
U1 1
U2 17
PU MDPI
PI BASEL
PA ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
SN 1999-4893
J9 ALGORITHMS
JI Algorithms
PD JUN
PY 2016
VL 9
IS 2
AR 41
DI 10.3390/a9020041
PG 11
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods
SC Computer Science
GA DR3WP
UT WOS:000379833700020
OA DOAJ Gold
DA 2020-02-19
ER

PT J
AU Kahou, SE
   Bouthillier, X
   Lamblin, P
   Gulcehre, C
   Michalski, V
   Konda, K
   Jean, S
   Froumenty, P
   Dauphin, Y
   Boulanger-Lewandowski, N
   Ferrari, RC
   Mirza, M
   Warde-Farley, D
   Courville, A
   Vincent, P
   Memisevic, R
   Pal, C
   Bengio, Y
AF Kahou, Samira Ebrahimi
   Bouthillier, Xavier
   Lamblin, Pascal
   Gulcehre, Caglar
   Michalski, Vincent
   Konda, Kishore
   Jean, Sebastien
   Froumenty, Pierre
   Dauphin, Yann
   Boulanger-Lewandowski, Nicolas
   Ferrari, Raul Chandias
   Mirza, Mehdi
   Warde-Farley, David
   Courville, Aaron
   Vincent, Pascal
   Memisevic, Roland
   Pal, Christopher
   Bengio, Yoshua
TI EmoNets: Multimodal deep learning approaches for emotion recognition in
   video
SO JOURNAL ON MULTIMODAL USER INTERFACES
LA English
DT Article
DE Emotion recognition; Deep learning; Model combination; Multimodal
   learning
AB The task of the Emotion Recognition in the Wild (EmotiW) Challenge is to assign one of seven emotions to short video clips extracted from Hollywood style movies. The videos depict acted-out emotions under realistic conditions with a large degree of variation in attributes such as pose and illumination, making it worthwhile to explore approaches which consider combinations of features from multiple modalities for label assignment. In this paper we present our approach to learning several specialist models using deep learning techniques, each focusing on one modality. Among these are a convolutional neural network, focusing on capturing visual information in detected faces, a deep belief net focusing on the representation of the audio stream, a K-Means based "bag-of-mouths" model, which extracts visual features around the mouth region and a relational autoencoder, which addresses spatio-temporal aspects of videos. We explore multiple methods for the combination of cues from these modalities into one common classifier. This achieves a considerably greater accuracy than predictions from our strongest single-modality classifier. Our method was the winning submission in the 2013 Emoti W challenge and achieved a test set accuracy of 47.67 % on the 2014 dataset.
C1 [Kahou, Samira Ebrahimi; Froumenty, Pierre; Pal, Christopher] Univ Montreal, Ecole Polytech Montreal, Montreal, PQ, Canada.
   [Michalski, Vincent; Konda, Kishore] Goethe Univ Frankfurt, D-60054 Frankfurt, Germany.
   [Bouthillier, Xavier; Lamblin, Pascal; Gulcehre, Caglar; Jean, Sebastien; Dauphin, Yann; Boulanger-Lewandowski, Nicolas; Ferrari, Raul Chandias; Mirza, Mehdi; Warde-Farley, David; Courville, Aaron; Vincent, Pascal; Memisevic, Roland; Bengio, Yoshua] Univ Montreal, Montreal Inst Learning Algorithms, Montreal, PQ, Canada.
RP Kahou, SE (reprint author), Univ Montreal, Ecole Polytech Montreal, Montreal, PQ, Canada.
EM samira.ebrahimi.kahou@gmail.com; bouthilx@iro.umontreal.ca;
   lamblinp@iro.umontreal.ca; gulcehrc@iro.umontreal.ca;
   michalskivince@gmail.com; konda.kishorereddy@gmail.com;
   jeasebas@iro.umontreal.ca; pierre.froumenty@polymtl.ca;
   dauphiya@iro.umontreal.ca; boulanni@iro.umontreal.ca;
   chandiar@iro.umontreal.ca; mirzamom@iro.umontreal.ca;
   wardefar@iro.umontreal.ca; courvila@iro.umontreal.ca;
   vincentp@iro.umontreal.ca; memisevr@iro.umontreal.ca;
   christopher.pal@polymtl.ca; bengioy@iro.umontreal.ca
FU NSERCNatural Sciences and Engineering Research Council of Canada;
   Ubisoft; German BMBFFederal Ministry of Education & Research (BMBF)
   [01GQ0841]; CIFARCanadian Institute for Advanced Research (CIFAR)
FX The authors would like to thank the developers of Theano [2, 3]. We
   thank NSERC, Ubisoft, the German BMBF, project 01GQ0841 and CIFAR for
   their support. We also thank Abhishek Aggarwal, Emmanuel Bengio, Jorg
   Bornschein, Pierre-Luc Carrier, Myriam Cote, Guillaume Desjardins, David
   Krueger, Razvan Pascanu, Jean-Philippe Raymond, Arjun Sharma, Atousa
   Torabi, Zhenzhou Wu, and Jeremie Zumer for their work on the 2013
   submission.
CR ADELSON EH, 1985, J OPT SOC AM A, V2, P284, DOI 10.1364/JOSAA.2.000284
   Bastien F., 2012, ARXIV12115590
   Bergstra J, 2010, P PYTH SCI COMP C SC, V4
   Bergstra J, 2012, J MACH LEARN RES, V13, P281
   Carrier PL, 2013, 1365 U MONTR
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chen J., 2014, P 16 INT C MULT INT, P508, DOI [10.1145/2663204.2666277, DOI 10.1145/2663204.2666277]
   Coates Adam, 2011, AISTATS
   Dahl George E, 2013, P ICASSP
   Dhall A., 2013, ACM ICMI
   Dhall A, 2012, IEEE MULTIMEDIA, V19, P34, DOI 10.1109/MMUL.2012.26
   Dhall Abhinav, 2014, P 16 INT C MULT INT, P461
   Gehrig T., 2013, P 2013 EM REC WILD C, P9, DOI 10.1145/2531923.2531924
   Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947
   HAMEL Philippe, 2011, ISMIR, V12, P729
   Heusch G., 2005, LIGHTING NORMALIZATI
   Hinton G. E, 2012, ARXIV12070580
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Kahou S., 2013, P 15 ACM INT C MULT
   Kahou SE, 2015, LECT NOTES COMPUTER, V8926
   Kalchbrenner N., 2014, ARXIV14042188
   Konda K., 2014, ICLR
   Krizhevsky A., 2012, CUDA CONVNET GOOGLE
   Krizhevsky A, 2009, LEARNING MULTIPLE LA
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Le Q., 2011, CVPR
   Liu M., 2014, P 16 INT C MULT INT, P494, DOI DOI 10.1145/2663204.2666274
   Liu MY, 2013, ICMI'13: PROCEEDINGS OF THE 2013 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P525, DOI 10.1145/2522848.2531738
   Neverova N., 2014, ARXIV150100102
   Sikka K, 2013, ICMI'13: PROCEEDINGS OF THE 2013 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P517, DOI 10.1145/2522848.2531741
   STRUC V, 2011, PHOTOMETRIC NORMALIZ, P279
   Struc V, 2009, INFORMATICA-LITHUAN, V20, P115
   Sun B., 2014, P 16 INT C MULT INT, P481
   Susskind J., 2010, 2010001 UTML TR
   Sutskever I, 2013, ICML 2013
   Taylor G. W., 2010, P 11 EUR C COMP VI 6
   Viola P., 2001, CVPR
   Wang Heng, 2009, BMVC
   Zhu X., 2012, CVPR
NR 40
TC 91
Z9 93
U1 5
U2 47
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 1783-7677
EI 1783-8738
J9 J MULTIMODAL USER IN
JI J. Multimodal User Interfaces
PD JUN
PY 2016
VL 10
IS 2
BP 99
EP 111
DI 10.1007/s12193-015-0195-2
PG 13
WC Computer Science, Artificial Intelligence; Computer Science, Cybernetics
SC Computer Science
GA DP6AX
UT WOS:000378580400002
DA 2020-02-19
ER

PT J
AU Li, C
   Sanchez, RV
   Zurita, G
   Cerrada, M
   Cabrera, D
AF Li, Chuan
   Sanchez, Rene-Vinicio
   Zurita, Grover
   Cerrada, Mariela
   Cabrera, Diego
TI Fault Diagnosis for Rotating Machinery Using Vibration Measurement Deep
   Statistical Feature Learning
SO SENSORS
LA English
DT Article
DE fault diagnosis; deep learning; statistical feature; vibration sensor;
   rotating machinery
ID DISCRETE WAVELET TRANSFORM; DEMODULATION; GEARBOXES; SYSTEM; FUSION;
   MODEL
AB Fault diagnosis is important for the maintenance of rotating machinery. The detection of faults and fault patterns is a challenging part of machinery fault diagnosis. To tackle this problem, a model for deep statistical feature learning from vibration measurements of rotating machinery is presented in this paper. Vibration sensor signals collected from rotating mechanical systems are represented in the time, frequency, and time-frequency domains, each of which is then used to produce a statistical feature set. For learning statistical features, real-value Gaussian-Bernoulli restricted Boltzmann machines (GRBMs) are stacked to develop a Gaussian-Bernoulli deep Boltzmann machine (GDBM). The suggested approach is applied as a deep statistical feature learning tool for both gearbox and bearing systems. The fault classification performances in experiments using this approach are 95.17% for the gearbox, and 91.75% for the bearing system. The proposed approach is compared to such standard methods as a support vector machine, GRBM and a combination model. In experiments, the best fault classification rate was detected using the proposed model. The results show that deep learning with statistical feature extraction has an essential improvement potential for diagnosing rotating machinery faults.
C1 [Li, Chuan] Dongguan Univ Technol, Sch Mech Engn, Dongguan 523808, Peoples R China.
   [Sanchez, Rene-Vinicio; Zurita, Grover; Cerrada, Mariela; Cabrera, Diego] Univ Politecn Salesiana, Dept Mech Engn, Cuenca 010105, Ecuador.
RP Li, C (reprint author), Dongguan Univ Technol, Sch Mech Engn, Dongguan 523808, Peoples R China.
EM chuanli@21cn.com; rsanchezl@ups.edu.ec; grover_zurita@hotmail.com;
   cerradam@ula.ve; dcabrera@ups.edu.ec
RI Cabrera, Diego/I-8837-2019; L., Rene Vinicio Sanchez/O-5259-2019
OI Cabrera, Diego/0000-0003-1023-871X; Zurita, Grover/0000-0003-1338-0092;
   Sanchez, Rene Vinicio/0000-0003-0395-9228
FU Secretariat for Higher Education, Science, Technology and Innovation of
   the Republic of Ecuador (GIDTEC) [009-004-2015-07-16]; National Natural
   Science Foundation of ChinaNational Natural Science Foundation of China
   [51375517]; Project of Chongqing Science & Technology Commission
   [cstc2014gjhz70002]
FX This work is supported in part by the Secretariat for Higher Education,
   Science, Technology and Innovation of the Republic of Ecuador (GIDTEC
   project No. 009-004-2015-07-16), the National Natural Science Foundation
   of China (Grant No. 51375517), and the Project of Chongqing Science &
   Technology Commission (Grant No. cstc2014gjhz70002). The valuable
   comments and suggestions from the two anonymous reviewers are very much
   appreciated.
CR Arumugam V, 2014, J COMPOS MATER, V48, P3457, DOI 10.1177/0021998313509504
   Bartelmus W, 2009, MECH SYST SIGNAL PR, V23, P1528, DOI 10.1016/j.ymssp.2009.01.014
   Bartkowiak A, 2011, J PHYS CONF SER, V305, DOI 10.1088/1742-6596/305/1/012031
   Batista L, 2013, EXPERT SYST APPL, V40, P6788, DOI 10.1016/j.eswa.2013.06.033
   Ben Ali J, 2015, APPL ACOUST, V89, P16, DOI 10.1016/j.apacoust.2014.08.016
   Bishop C.M., 1996, NEURAL NETWORKS PATT
   Cao HR, 2008, INT J MACH TOOL MANU, V48, P141, DOI 10.1016/j.ijmachtools.2007.09.001
   Cerrada M, 2015, SENSORS-BASEL, V15, P23903, DOI 10.3390/s150923903
   Chen FF, 2013, MEASUREMENT, V46, P220, DOI 10.1016/j.measurement.2012.06.009
   Cho K. H., 2013, INT JOINT C NEUR NET, P1, DOI [10.1109/IJCNN.2013.6706831, DOI 10.1109/IJCNN.2013.6706831]
   Cho K, 2011, LECT NOTES COMPUT SC, V6791, P10, DOI 10.1007/978-3-642-21735-7_2
   de Souza D. L., 2014, J SAFETY ENG, V3, P18
   Gao ZW, 2015, IEEE T IND ELECTRON, V62, P3768, DOI 10.1109/TIE.2015.2419013
   Hastie T., 1998, ADV NEURAL INFORM PR, V10
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hjelm RD, 2014, NEUROIMAGE, V96, P245, DOI 10.1016/j.neuroimage.2014.03.048
   Hou SM, 2010, STRUCT HEALTH MONIT, V9, P297, DOI 10.1177/1475921709352144
   Khomfoi S, 2007, IEEE T POWER ELECTR, V22, P1062, DOI 10.1109/TPEL.2007.897128
   Kumar R, 2013, MEASUREMENT, V46, P537, DOI 10.1016/j.measurement.2012.08.012
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lei YG, 2014, MEASUREMENT, V48, P292, DOI 10.1016/j.measurement.2013.11.012
   Lei YG, 2012, MEAS SCI TECHNOL, V23, DOI 10.1088/0957-0233/23/5/055605
   Li C, 2016, MECH SYST SIGNAL PR, V76-77, P157, DOI 10.1016/j.ymssp.2016.02.064
   Li C, 2016, ISA T, V60, P274, DOI 10.1016/j.isatra.2015.10.014
   Li C, 2015, NEUROCOMPUTING, V168, P119, DOI 10.1016/j.neucom.2015.06.008
   Li C, 2015, MECH SYST SIGNAL PR, V64-65, P132, DOI 10.1016/j.ymssp.2015.04.004
   Li C, 2014, SENSORS-BASEL, V14, P6207, DOI 10.3390/s140406207
   Li C, 2012, J SOUND VIB, V331, P5864, DOI 10.1016/j.jsv.2012.07.045
   Li C, 2012, MECH SYST SIGNAL PR, V26, P205, DOI 10.1016/j.ymssp.2011.07.001
   Nabney I., 2001, NETLAB ALGORITHMS PA
   Qin Q, 2012, MEASUREMENT, V45, P897, DOI 10.1016/j.measurement.2012.02.005
   Raad A, 2008, MECH SYST SIGNAL PR, V22, P574, DOI 10.1016/j.ymssp.2007.09.011
   Randall RB, 2011, VIBRATION BASED COND
   Randall RB, 2011, MECH SYST SIGNAL PR, V25, P485, DOI 10.1016/j.ymssp.2010.07.017
   Salakhutdinov R., 2009, ADV NEURAL INFORM PR, P1598
   Salakhutdinov Ruslan, 2009, THESIS
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Shen CQ, 2013, MEASUREMENT, V46, P1551, DOI 10.1016/j.measurement.2012.12.011
   Tamilselvan P, 2013, RELIAB ENG SYST SAFE, V115, P124, DOI 10.1016/j.ress.2013.02.022
   Tayarani-Bathaie SS, 2014, NEUROCOMPUTING, V125, P153, DOI 10.1016/j.neucom.2012.06.050
   Tran VT, 2014, EXPERT SYST APPL, V41, P4113, DOI 10.1016/j.eswa.2013.12.026
   Wang D, 2011, MEAS SCI TECHNOL, V22, DOI 10.1088/0957-0233/22/2/025102
   Wang Y, 2015, MECH SYST SIGNAL PR, V54-55, P259, DOI 10.1016/j.ymssp.2014.09.002
   Wong WK, 2010, NEUROCOMPUTING, V74, P164, DOI 10.1016/j.neucom.2010.02.027
   Zhao MB, 2014, EXPERT SYST APPL, V41, P3391, DOI 10.1016/j.eswa.2013.11.026
   Zuo MJ, 2005, J SOUND VIB, V287, P614, DOI 10.1016/j.jsv.2005.02.005
NR 47
TC 56
Z9 60
U1 0
U2 103
PU MDPI
PI BASEL
PA ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
SN 1424-8220
J9 SENSORS-BASEL
JI Sensors
PD JUN
PY 2016
VL 16
IS 6
AR 895
DI 10.3390/s16060895
PG 19
WC Chemistry, Analytical; Engineering, Electrical & Electronic; Instruments
   & Instrumentation
SC Chemistry; Engineering; Instruments & Instrumentation
GA DP8OC
UT WOS:000378756500143
PM 27322273
OA DOAJ Gold, Green Published
DA 2020-02-19
ER

PT J
AU Mocanu, E
   Nguyen, PH
   Gibescu, M
   Kling, WL
AF Mocanu, Elena
   Nguyen, Phuong H.
   Gibescu, Madeleine
   Kling, Wil L.
TI Deep learning for estimating building energy consumption
SO SUSTAINABLE ENERGY GRIDS & NETWORKS
LA English
DT Article
DE Energy prediction; Artificial Neural Networks; Conditional Restricted
   Boltzmann Machine; Factored Conditional Restricted Boltzmann Machine
ID PREDICTION; MODELS
AB To improve the design of the electricity infrastructure and the efficient deployment of distributed and renewable energy sources, a new paradigm for the energy supply chain is emerging, leading to the development of smart grids. There is a need to add intelligence at all levels in the grid, acting over various time horizons. Predicting the behavior of the energy system is crucial to mitigate potential uncertainties. An accurate energy prediction at the customer level will reflect directly in efficiency improvements in the whole system. However, prediction of building energy consumption is complex due to many influencing factors, such as climate, performance of thermal systems, and occupancy patterns. Therefore, current state-of-the-art methods are not able to confine the uncertainty at the building level due to the many fluctuations in influencing variables. As an evolution of artificial neural network (ANN)-based prediction methods, deep learning techniques are expected to increase the prediction accuracy by allowing higher levels of abstraction. In this paper, we investigate two newly developed stochastic models for time series prediction of energy consumption, namely Conditional Restricted Boltzmann Machine (CRBM) and Factored Conditional Restricted Boltzmann Machine (FCRBM). The assessment is made on a benchmark dataset consisting of almost four years of one minute resolution electric power consumption data collected from an individual residential customer. The results show that for the energy prediction problem solved here, FCRBM outperforms ANN, Support Vector Machine (SVM), Recurrent Neural Networks (RNN) and CRBM. (C) 2016 Elsevier Ltd. All rights reserved.
C1 [Mocanu, Elena; Nguyen, Phuong H.; Gibescu, Madeleine; Kling, Wil L.] Eindhoven Univ Technol, Dept Elect Engn, POB 513, NL-5600 MB Eindhoven, Netherlands.
RP Mocanu, E (reprint author), Eindhoven Univ Technol, Dept Elect Engn, POB 513, NL-5600 MB Eindhoven, Netherlands.
EM e.mocanu@tue.nl
RI Nguyen, Phuong H./C-6762-2011
OI Nguyen, Phuong H./0000-0003-1124-2710; Mocanu, Elena/0000-0002-0856-579X
CR Aydinalp-Koksal M, 2008, APPL ENERG, V85, P271, DOI 10.1016/j.apenergy.2006.09.012
   Bache K., 2013, UCI MACHINE LEARNING
   Bakirtzis EA, 2015, ELECTR POW SYST RES, V128, P90, DOI 10.1016/j.epsr.2015.06.025
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Costa A, 2013, APPL ENERG, V101, P310, DOI 10.1016/j.apenergy.2011.10.037
   De Livera AM, 2011, J AM STAT ASSOC, V106, P1513, DOI 10.1198/jasa.2011.tm09771
   DEVLIN SJ, 1975, BIOMETRIKA, V62, P531, DOI 10.2307/2335508
   Dounis AI, 2010, ADV BUILD ENERGY RES, V4, P267, DOI 10.3763/aber.2009.0408
   Fan S, 2012, IEEE T POWER SYST, V27, P134, DOI 10.1109/TPWRS.2011.2162082
   Foucquier A, 2013, RENEW SUST ENERG REV, V23, P272, DOI 10.1016/j.rser.2013.03.004
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hurtado LA, 2015, SUSTAIN ENERGY GRIDS, V2, P32, DOI 10.1016/j.segan.2015.03.003
   Jones N, 2014, NATURE, V505, P146, DOI 10.1038/505146a
   Kalogirou SA, 2006, INT J LOW-CARBON TEC, V1, P201, DOI 10.1093/ijlct/1.3.201
   Krarti M., 2011, MECH AEROSPACE ENG S
   Laserson J., 2011, XRDS, V18, P29, DOI DOI 10.1145/2000775.2000787
   Lehmann E., 2005, SPRINGER TEXTS STAT
   Li XM, 2010, THIRD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING: WKDD 2010, PROCEEDINGS, P522, DOI 10.1109/WKDD.2010.137
   Lukosevicius M., 2012, LECT NOTES COMPUTER, V7700
   Lukosevicius M, 2009, COMPUT SCI REV, V3, P127, DOI 10.1016/j.cosrev.2009.03.005
   MARQUARDT DW, 1963, J SOC IND APPL MATH, V11, P431, DOI 10.1137/0111030
   Mnih V., 2011, P INT C UNC ART INT
   Mocanu E, 2014, IEEE SYS MAN CYBERN, P1, DOI 10.1109/SMC.2014.6973875
   Salakhutdinov R., 2007, MACH LEARN, P791
   Simoes MG, 2012, IEEE T IND APPL, V48, P1154, DOI 10.1109/TIA.2012.2199730
   Smolensky P, 1986, PARALLEL DISTRIBUTED, P194
   Taylor GW, 2011, J MACH LEARN RES, V12, P1025
   Taylor JW, 2010, INT J FORECASTING, V26, P627, DOI 10.1016/j.ijforecast.2010.02.009
   Wong SL, 2010, APPL ENERG, V87, P551, DOI 10.1016/j.apenergy.2009.06.028
   Yang J, 2005, ENERG BUILDINGS, V37, P1250, DOI 10.1016/j.enbuild.2005.02.005
   Yang L, 2014, APPL ENERG, V115, P164, DOI 10.1016/j.apenergy.2013.10.062
   Zhao HX, 2012, RENEW SUST ENERG REV, V16, P3586, DOI 10.1016/j.rser.2012.02.049
NR 33
TC 88
Z9 89
U1 17
U2 66
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 2352-4677
J9 SUSTAIN ENERGY GRIDS
JI Sustain. Energy Grids Netw.
PD JUN
PY 2016
VL 6
BP 91
EP 99
DI 10.1016/j.segan.2016.02.005
PG 9
WC Energy & Fuels; Engineering, Electrical & Electronic
SC Energy & Fuels; Engineering
GA DN9NW
UT WOS:000377407500010
DA 2020-02-19
ER

PT J
AU Zhang, DW
   Han, JW
   Han, JG
   Shao, L
AF Zhang, Dingwen
   Han, Junwei
   Han, Jungong
   Shao, Ling
TI Cosaliency Detection Based on Intrasaliency Prior Transfer and Deep
   Intersaliency Mining
SO IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
LA English
DT Article
DE Cosaliency detection; deep learning; prior transfer; stacked denoising
   autoencoder (SDAE)
ID SALIENT OBJECT DETECTION; REMOTE-SENSING IMAGES; REGION DETECTION;
   REPRESENTATIONS; AUTOENCODERS; SEGMENTATION; EFFICIENT
AB As an interesting and emerging topic, cosaliency detection aims at simultaneously extracting common salient objects in multiple related images. It differs from the conventional saliency detection paradigm in which saliency detection for each image is determined one by one independently without taking advantage of the homogeneity in the data pool of multiple related images. In this paper, we propose a novel cosaliency detection approach using deep learning models. Two new concepts, called intrasaliency prior transfer and deep intersaliency mining, are introduced and explored in the proposed work. For the intrasaliency prior transfer, we build a stacked denoising autoencoder (SDAE) to learn the saliency prior knowledge from auxiliary annotated data sets and then transfer the learned knowledge to estimate the intrasaliency for each image in cosaliency data sets. For the deep intersaliency mining, we formulate it by using the deep reconstruction residual obtained in the highest hidden layer of a self-trained SDAE. The obtained deep intersaliency can extract more intrinsic and general hidden patterns to discover the homogeneity of cosalient objects in terms of some higher level concepts. Finally, the cosaliency maps are generated by weighted integration of the proposed intrasaliency prior, deep intersaliency, and traditional shallow intersaliency. Comprehensive experiments over diverse publicly available benchmark data sets demonstrate consistent performance gains of the proposed method over the state-of-the-art cosaliency detection methods.
C1 [Zhang, Dingwen; Han, Junwei] Northwestern Polytech Univ, Sch Automat, Xian 710072, Peoples R China.
   [Han, Jungong; Shao, Ling] Northumbria Univ, Dept Comp Sci & Digital Technol, Newcastle Upon Tyne NE1 8ST, Tyne & Wear, England.
   [Shao, Ling] Nanjing Univ Informat Sci & Technol, Coll Elect & Informat Engn, Nanjing 210044, Jiangsu, Peoples R China.
RP Han, JW (reprint author), Northwestern Polytech Univ, Sch Automat, Xian 710072, Peoples R China.
EM zhangdingwen2006yyy@gmail.com; junweihan2010@gmail.com;
   jungong.han@northumbria.ac.uk; ling.shao@ieee.org
RI zhang, dingwen/S-9447-2017; Shao, Ling/D-3535-2011
OI zhang, dingwen/0000-0001-8369-8886; Han, Jungong/0000-0003-4361-956X
FU National Science Foundation of ChinaNational Natural Science Foundation
   of China [61473231, 61522207]; Doctorate Foundation through Northwestern
   Polytechnical University
FX This work was supported in part by the National Science Foundation of
   China under Grant 61473231 and Grant 61522207 and in part by the
   Doctorate Foundation through Northwestern Polytechnical University.
   (Corresponding author: Junwei Han.)
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Alexe B, 2012, IEEE T PATTERN ANAL, V34, P2189, DOI 10.1109/TPAMI.2012.28
   Batra D, 2010, PROC CVPR IEEE, P3169, DOI 10.1109/CVPR.2010.5540080
   Bengio Y, 2011, P ICML WORKSH UNS TR, P17
   Bengio Y., 2011, INT C ART INT STAT, P164
   Bengio Y., 1998, LECT NOTES COMPUTER
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Cao X, 2013, P INT C MULT EXP JUL, P1
   Cao XC, 2014, IEEE T IMAGE PROCESS, V23, P4175, DOI 10.1109/TIP.2014.2332399
   Chen HT, 2010, IEEE IMAGE PROC, P1117, DOI 10.1109/ICIP.2010.5650014
   Cheng MM, 2015, IEEE T PATTERN ANAL, V37, P569, DOI 10.1109/TPAMI.2014.2345401
   Deselaers T, 2012, INT J COMPUT VISION, V100, P275, DOI 10.1007/s11263-012-0538-3
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   Fu H., 2013, P IEEE C COMP VIS PA, P3166
   Fu HZ, 2013, IEEE T IMAGE PROCESS, V22, P3766, DOI 10.1109/TIP.2013.2260166
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Guo X, 2013, PROC CVPR IEEE, P3206, DOI 10.1109/CVPR.2013.412
   Han J., IEEE T CYBE IN PRESS
   Han J., 2013, MACH VISION APPL, V25, P1671
   Han JW, 2015, IEEE T CIRC SYST VID, V25, P1309, DOI 10.1109/TCSVT.2014.2381471
   Han JW, 2015, IEEE T CYBERNETICS, V45, P1692, DOI 10.1109/TCYB.2014.2358647
   Han JW, 2015, IEEE T GEOSCI REMOTE, V53, P3325, DOI 10.1109/TGRS.2014.2374218
   Han JW, 2014, ISPRS J PHOTOGRAMM, V89, P37, DOI 10.1016/j.isprsjprs.2013.12.011
   Han JW, 2013, IEEE T CIRC SYST VID, V23, P2009, DOI 10.1109/TCSVT.2013.2242594
   Han JW, 2006, IEEE T CIRC SYST VID, V16, P141, DOI 10.1109/TCSVT.2005.859028
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI 10.1007/978-3-319-10578-9_23
   Jacobs David E., 2010, P 23 ANN ACM S US IN, P219, DOI [10.1145/1866029.1866066, DOI 10.1145/1866029]
   Jiang HZ, 2013, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2013.271
   Kai-Yueh Chang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2129, DOI 10.1109/CVPR.2011.5995415
   Kim G, 2011, IEEE I CONF COMP VIS, P169, DOI 10.1109/ICCV.2011.6126239
   Kuettel D, 2012, PROC CVPR IEEE, P558, DOI 10.1109/CVPR.2012.6247721
   Li HL, 2013, IEEE T MULTIMEDIA, V15, P1896, DOI 10.1109/TMM.2013.2271476
   Li HL, 2011, IEEE T IMAGE PROCESS, V20, P3365, DOI 10.1109/TIP.2011.2156803
   Lin Y., 2014, P WORKSH AAAI C ART
   Liu Z, 2014, IEEE SIGNAL PROC LET, V21, P88, DOI 10.1109/LSP.2013.2292873
   Mukherjee Lopamudra, 2011, Proc IEEE Comput Soc Conf Comput Vis Pattern Recognit, P1881, DOI 10.1109/CVPR.2011.5995420
   Prest A, 2012, IEEE T PATTERN ANAL, V34, P601, DOI 10.1109/TPAMI.2011.158
   Rumelhart D. E., 1988, LEARNING REPRESENTAT
   Sankaran A., 2014, BIOM IJCB 2014 IEEE, P1
   Shao L, 2015, IEEE T NEUR NET LEAR, V26, P1019, DOI 10.1109/TNNLS.2014.2330900
   Shen XH, 2012, PROC CVPR IEEE, P853, DOI 10.1109/CVPR.2012.6247758
   Shi Z., 2012, P 11 INT C DEV POW S, P1
   Shin HC, 2013, IEEE T PATTERN ANAL, V35, P1930, DOI 10.1109/TPAMI.2012.277
   Tan ZY, 2013, INT CONF ACOUST SPEE, P2114, DOI 10.1109/ICASSP.2013.6638027
   Toshev A., 2007, CVPR, P1
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Yang LJ, 2011, IEEE T MULTIMEDIA, V13, P1295, DOI 10.1109/TMM.2011.2162399
   Yoonseop Kang, 2013, Neural Information Processing. 20th International Conference, ICONIP 2013. Proceedings: LNCS 8228, P241, DOI 10.1007/978-3-642-42051-1_31
   Zhang DW, 2015, PROC CVPR IEEE, P2994, DOI 10.1109/CVPR.2015.7298918
   Zhang DW, 2015, IEEE GEOSCI REMOTE S, V12, P701, DOI 10.1109/LGRS.2014.2358994
NR 53
TC 96
Z9 97
U1 5
U2 63
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 2162-237X
EI 2162-2388
J9 IEEE T NEUR NET LEAR
JI IEEE Trans. Neural Netw. Learn. Syst.
PD JUN
PY 2016
VL 27
IS 6
BP 1163
EP 1176
DI 10.1109/TNNLS.2015.2495161
PG 14
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Computer Science, Theory & Methods; Engineering,
   Electrical & Electronic
SC Computer Science; Engineering
GA DN5MR
UT WOS:000377113300005
PM 26571541
OA Green Accepted
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Xia, C
   Qi, F
   Shi, GM
AF Xia, Chen
   Qi, Fei
   Shi, Guangming
TI Bottom-Up Visual Saliency Estimation With Deep Autoencoder-Based Sparse
   Reconstruction
SO IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
LA English
DT Article
DE Autoencoder; center-surround (C-S) difference; deep learning;
   reconstruction; saliency; unsupervised feature learning
ID OBJECT DETECTION; MODEL; ATTENTION; MAP; PREDICT
AB Research on visual perception indicates that the human visual system is sensitive to center-surround (C-S) contrast in the bottom-up saliency-driven attention process. Different from the traditional contrast computation of feature difference, models based on reconstruction have emerged to estimate saliency by starting from original images themselves instead of seeking for certain ad hoc features. However, in the existing reconstruction-based methods, the reconstruction parameters of each area are calculated independently without taking their global correlation into account. In this paper, inspired by the powerful feature learning and data reconstruction ability of deep autoencoders, we construct a deep C-S inference network and train it with the data sampled randomly from the entire image to obtain a unified reconstruction pattern for the current image. In this way, global competition in sampling and learning processes can be integrated into the nonlocal reconstruction and saliency estimation of each pixel, which can achieve better detection results than the models with separate consideration on local and global rarity. Moreover, by learning from the current scene, the proposed model can achieve the feature extraction and interaction simultaneously in an adaptive way, which can form a better generalization ability to handle more types of stimuli. Experimental results show that in accordance with different inputs, the network can learn distinct basic features for saliency modeling in its code layer. Furthermore, in a comprehensive evaluation on several benchmark data sets, the proposed method can outperform the existing state-of-the-art algorithms.
C1 [Xia, Chen; Qi, Fei; Shi, Guangming] Xidian Univ, Minist Educ, Sch Elect Engn, Key Lab Intelligent Percept & Image Understanding, Xian 710071, Peoples R China.
RP Xia, C; Qi, F; Shi, GM (reprint author), Xidian Univ, Minist Educ, Sch Elect Engn, Key Lab Intelligent Percept & Image Understanding, Xian 710071, Peoples R China.
EM cxia@stu.xidian.edu.cn; fred.qi@ieee.org; gmshi@xidian.edu.cn
RI Qi, Fei/G-3978-2013
OI Qi, Fei/0000-0002-2161-1551
FU Major State Basic Research Development Program of China (973 Program)
   within Ministry of Science and Technology, China [2013CB329402];
   National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61572387, 61227004, 61472301]; Ministry of
   Education, ChinaMinistry of Education, China [20130203130001];
   International Cooperation Project of Shaanxi Science and Technology
   Research and Development Program [2014KW01-02]; Fundamental Scientific
   Research Funds of Xidian University [K5051302012]
FX This work was supported in part by the Major State Basic Research
   Development Program of China (973 Program) within the Ministry of
   Science and Technology, China, under Grant 2013CB329402, in part by the
   National Natural Science Foundation of China under Grant 61572387, Grant
   61227004, and Grant 61472301, in part by the Ministry of Education,
   China, under Grant 20130203130001, in part by the International
   Cooperation Project of Shaanxi Science and Technology Research and
   Development Program under Grant 2014KW01-02, and in part by the
   Fundamental Scientific Research Funds of Xidian University under Grant
   K5051302012.
CR Beaucousin V, 2013, FRONT PSYCHOL, V4, DOI 10.3389/fpsyg.2013.00539
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Borji A, 2013, IEEE T IMAGE PROCESS, V22, P55, DOI 10.1109/TIP.2012.2210727
   Borji A, 2013, IEEE T PATTERN ANAL, V35, P185, DOI 10.1109/TPAMI.2012.89
   Borji A, 2012, PROC CVPR IEEE, P478, DOI 10.1109/CVPR.2012.6247711
   Borji A, 2012, PROC CVPR IEEE, P438, DOI 10.1109/CVPR.2012.6247706
   Bruce N. D. B., 2005, ADV NEURAL INFORM PR, V18, P155
   Ciresan D, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P1918, DOI 10.1109/IJCNN.2011.6033458
   Duan LJ, 2011, PROC CVPR IEEE, P473, DOI 10.1109/CVPR.2011.5995676
   Erdem E, 2013, J VISION, V13, DOI 10.1167/13.4.11
   Erhan D, 2014, PROC CVPR IEEE, P2155, DOI 10.1109/CVPR.2014.276
   Gao D, 2004, ADV NEURAL INFORM PR, P481
   Gao DS, 2007, IEEE I CONF COMP VIS, P185
   Garcia-Diaz A, 2012, J VISION, V12, DOI 10.1167/12.6.17
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Goferman S, 2012, IEEE T PATTERN ANAL, V34, P1915, DOI 10.1109/TPAMI.2011.272
   Guo CL, 2010, IEEE T IMAGE PROCESS, V19, P185, DOI 10.1109/TIP.2009.2030969
   Harel J, 2007, ADV NEURAL INF PROCE, P545, DOI DOI 10.7551/mitpress/7503.003.0073
   Hinton G., 2010, UTMLTR2010003 DEP CO
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hinton GE, 2007, TRENDS COGN SCI, V11, P428, DOI 10.1016/j.tics.2007.09.004
   Hou X., 2007, IEEE C COMP VIS PATT, V2007, P1, DOI DOI 10.1109/CVPR.2007.383267
   Hou X., 2008, P ADV NEURAL INFORM, P681
   Hou XD, 2012, IEEE T PATTERN ANAL, V34, P194, DOI 10.1109/TPAMI.2011.146
   Huang YP, 2011, WIRES COGN SCI, V2, P580, DOI 10.1002/wcs.142
   Itti L, 2004, IEEE T IMAGE PROCESS, V13, P1304, DOI 10.1109/TIP.2004.834657
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Itti L, 2001, NAT REV NEUROSCI, V2, P194, DOI 10.1038/35058500
   Itti L, 2009, VISION RES, V49, P1295, DOI 10.1016/j.visres.2008.09.007
   Jiang HZ, 2013, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2013.271
   Judd T., 2012, MITCSAILTR2012001
   Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462
   Klein DA, 2011, IEEE I CONF COMP VIS, P2214, DOI 10.1109/ICCV.2011.6126499
   Kootstra G, 2008, P BRIT MACH VIS C BM, P1115
   Krizhevsky A., 2011, P EUR S ART NEUR NET, P1
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kummerer M., 2015, DEEP GAZE
   Le Q. V., 2012, P 29 INT C MACH LEAR, P1
   Lenz I, 2015, INT J ROBOT RES, V34, P705, DOI 10.1177/0278364914549607
   Li XH, 2013, IEEE I CONF COMP VIS, P2976, DOI 10.1109/ICCV.2013.370
   Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43
   Li ZP, 2002, TRENDS COGN SCI, V6, P9, DOI 10.1016/S1364-6613(00)01817-9
   Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70
   Lu Y, 2012, PROC CVPR IEEE, P1067, DOI 10.1109/CVPR.2012.6247785
   Ma Y.-F., 2003, P 11 ACM INT C MULT, P374, DOI DOI 10.1145/957013.957094
   Min Chen, 2011, IEEE INFOCOM 2011 - IEEE Conference on Computer Communications Workshops, P409, DOI 10.1109/INFCOMW.2011.5928847
   Mnih V, 2010, LECT NOTES COMPUT SC, V6316, P210, DOI 10.1007/978-3-642-15567-3_16
   Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7
   Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0
   Osadchy M, 2007, J MACH LEARN RES, V8, P1197
   Pan J., 2015, END TO END CONVOLUTI
   Peters RJ, 2005, VISION RES, V45, P2397, DOI 10.1016/j.visres.2005.03.019
   Ramanathan S, 2010, LECT NOTES COMPUT SC, V6314, P30, DOI 10.1007/978-3-642-15561-1_3
   Rao RPN, 1999, NAT NEUROSCI, V2, P79, DOI 10.1038/4580
   Ren ZX, 2013, IEEE T IMAGE PROCESS, V22, P3120, DOI 10.1109/TIP.2013.2259837
   Riche N, 2013, IEEE I CONF COMP VIS, P1153, DOI 10.1109/ICCV.2013.147
   Salah AA, 2002, IEEE T PATTERN ANAL, V24, P420, DOI 10.1109/34.990146
   Seo HJ, 2009, J VISION, V9, DOI 10.1167/9.12.15
   Sermanet P, 2013, PROC CVPR IEEE, P3626, DOI 10.1109/CVPR.2013.465
   Shen C., 2012, P IEEE INT C DEP SYS, P1
   Shen CY, 2014, NEUROCOMPUTING, V138, P61, DOI 10.1016/j.neucom.2013.09.053
   Shen XH, 2012, PROC CVPR IEEE, P853, DOI 10.1109/CVPR.2012.6247758
   Siva P, 2013, PROC CVPR IEEE, P3238, DOI 10.1109/CVPR.2013.416
   Spratling MW, 2012, NEURAL NETWORKS, V26, P7, DOI 10.1016/j.neunet.2011.10.002
   Szegedy C., 2013, ADV NEURAL INFORM PR, V26, P2553
   Tatler BW, 2007, J VISION, V7, DOI 10.1167/7.14.4
   Tatler BW, 2005, VISION RES, V45, P643, DOI 10.1016/j.visres.2004.09.017
   Torralba A, 2006, PSYCHOL REV, V113, P766, DOI 10.1037/0033-295X.113.4.766
   TREISMAN AM, 1980, COGNITIVE PSYCHOL, V12, P97, DOI 10.1016/0010-0285(80)90005-5
   van de Weijer J, 2005, IEEE T PATTERN ANAL, V27, P625, DOI 10.1109/TPAMI.2005.75
   Vig E, 2014, PROC CVPR IEEE, P2798, DOI 10.1109/CVPR.2014.358
   Wang JL, 2013, IEEE T IMAGE PROCESS, V22, P2151, DOI 10.1109/TIP.2013.2246176
   Wolfe JM, 2004, NAT REV NEUROSCI, V5, P495, DOI 10.1038/nrn1411
   Wu JJ, 2012, J VIS COMMUN IMAGE R, V23, P1158, DOI 10.1016/j.jvcir.2012.07.010
   Xia C, 2013, IEEE IMAGE PROC, P206, DOI 10.1109/ICIP.2013.6738043
   Xia C, 2015, PATTERN RECOGN, V48, P1337, DOI 10.1016/j.patcog.2014.10.007
   Zeiler MD, 2011, IEEE I CONF COMP VIS, P2018, DOI 10.1109/ICCV.2011.6126474
   Zhang JM, 2013, IEEE I CONF COMP VIS, P153, DOI 10.1109/ICCV.2013.26
   Zhang LY, 2008, J VISION, V8, DOI 10.1167/8.7.32
   Zhao Q, 2013, SIGNAL PROCESS, V93, P1401, DOI 10.1016/j.sigpro.2012.06.014
   Zhao Q, 2011, J VISION, V11, DOI 10.1167/11.3.9
NR 82
TC 21
Z9 22
U1 2
U2 35
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 2162-237X
EI 2162-2388
J9 IEEE T NEUR NET LEAR
JI IEEE Trans. Neural Netw. Learn. Syst.
PD JUN
PY 2016
VL 27
IS 6
BP 1227
EP 1240
DI 10.1109/TNNLS.2015.2512898
PG 14
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Computer Science, Theory & Methods; Engineering,
   Electrical & Electronic
SC Computer Science; Engineering
GA DN5MR
UT WOS:000377113300010
PM 26800552
DA 2020-02-19
ER

PT J
AU Fernandez, JRM
   Vidal, JDB
AF Machado Fernandez, Jose Raul
   Bacallao Vidal, Jesus de la Concepcion
TI Improved Shape Parameter Estimation in K Clutter with Neural Networks
   and Deep Learning
SO INTERNATIONAL JOURNAL OF INTERACTIVE MULTIMEDIA AND ARTIFICIAL
   INTELLIGENCE
LA English
DT Article
DE Sea Clutter; K Distribution; Shape Parameter Estimation; Artificial
   Neural Networks; Deep Learning
ID MAXIMUM-LIKELIHOOD-ESTIMATION
AB The discrimination of the clutter interfering signal is a current problem in modern radars' design, especially in coastal or offshore environments where the histogram of the background signal often displays heavy tails. The statistical characterization of this signal is very important for the cancellation of sea clutter, whose behavior obeys a K distribution according to the commonly accepted criterion. By using neural networks, the authors propose a new method for estimating the K shape parameter, demonstrating its superiority over the classic alternative based on the Method of Moments. Whereas both solutions have a similar performance when the entire range of possible values of the shape parameter is evaluated, the neuronal alternative achieves a much more accurate estimation for the lower Fig.s of the parameter. This is exactly the desired behavior because the best estimate occurs for the most aggressive states of sea clutter. The final design, reached by processing three different sets of computer generated K samples, used a total of nine neural networks whose contribution is synthesized in the final estimate, thus the solution can be interpreted as a deep learning approximation. The results are to be applied in the improvement of radar detectors, particularly for maintaining the operational false alarm probability close to the one conceived in the design.
C1 [Machado Fernandez, Jose Raul; Bacallao Vidal, Jesus de la Concepcion] Inst Super Politecn Jose Antonio Echeverria, Havana, Cuba.
RP Fernandez, JRM (reprint author), Inst Super Politecn Jose Antonio Echeverria, Havana, Cuba.
CR Antipov I, 1998, DSTOTR0647
   BARTON DK, 1998, RADAR TECHNOLOGY ENC
   BLACKNELL D, 1994, IEE P-RADAR SON NAV, V141, P45, DOI 10.1049/ip-rsn:19949885
   Cetin A., 2008, THESIS MIDDLE E TU
   Chong CK, 2012, INT J INTERACT MULTI, V1, P22, DOI 10.9781/ijimai.2012.153
   Chung PJ, 2005, IEEE T SIGNAL PROCES, V53, P397, DOI 10.1109/TSP.2004.840811
   Demuth H., 2007, NEURAL NETWORK TOOLB
   Dong Y., 2006, DSTORR0316 EL WARF R
   Fernandez J. R. Machado, 2016, REV INGENIERIA ELECT
   FERNANDEZ JR, 2015, INTELIGENCIA ARTIFIC, V18, P3, DOI DOI 10.4114/intartif.vol18iss56pp3-13
   Galvez NB, 2012, LAT AM APPL RES, V42, P343
   Greco M, 2004, IEEE J OCEANIC ENG, V29, P269, DOI 10.1109/JOE.2004.828548
   Guerrero J. A. Garzon, 2012, CLASIFICACION BLANCO
   Ishii S., 2011, WIRELESS ENG TECHNOL, V2, P175, DOI DOI 10.4236/WET.2011.23025
   Iskander D, 1999, IEEE T AERO ELEC SYS, V35, P1453, DOI 10.1109/7.805463
   Iskander DR, 1996, TENCON IEEE REGION, P769, DOI 10.1109/TENCON.1996.608442
   JOUGHIN IR, 1993, IEEE T GEOSCI REMOTE, V31, P989, DOI 10.1109/36.263769
   Ling D., 2012, 2 INT C COMP APPL SY
   LOMBARDO P, 1994, IEE P-RADAR SON NAV, V141, P196, DOI 10.1049/ip-rsn:19941318
   Machado Fernandez J. R., 2015, WEIBULL CLUTTER ASSU
   Machado Fernandez J. R., 2014, TELEMATICA, V13, P86
   Machado Fernandez J. R., 2016, OPTIMAL SEL IN PRESS
   Machado Gil A., 2014, RECONOCIMIENTO PARAM
   Meikle H., 2008, MODERN RADAR SYSTEMS
   MENG XD, 2013, RES J APPL SCI ENG T, V5, P1528
   Mezache A., 2010, PARAMETER ESTIMATION
   Mezzoug K., 2008, ETUDE COMP DETECTEUR
   NOHARA TJ, 1991, IEE PROC-F, V138, P80, DOI 10.1049/ip-f-2.1991.0013
   RAGHAVAN RS, 1991, IEEE T AERO ELEC SYS, V27, P238, DOI 10.1109/7.78298
   Rams R. C. Sanchez, 2014, IMPLEMENTACION DETEC
   Roberts WJJ, 2000, IEEE T SIGNAL PROCES, V48, P3303, DOI 10.1109/78.886993
   Sayama S., 2013, WIRELESS ENG TECHNOL, V4, P125, DOI DOI 10.4236/WET.2013.43019
   Semwal VB, 2017, NEURAL COMPUT APPL, V28, P565, DOI 10.1007/s00521-015-2089-3
   Settouti N, 2016, INT J INTERACT MULTI, V4, P46, DOI 10.9781/ijimai.2016.419
   Shang X, 2011, IET RADAR SONAR NAV, V5, P315, DOI 10.1049/iet-rsn.2010.0125
   Singha J., 2015, HAND GESTURE RECOGNI
   Singha J, 2016, J MULTIMODAL USER IN, V10, P77, DOI 10.1007/s12193-016-0212-0
   Singha Joyeeta, 2015, IET COMPUTER VISION
   Skolnik MI, 2008, RADAR HDB
   Tanriverdi G., 2012, THESIS GRADUATE SCH
   Ward K., 2013, SEA CLUTTER SCATTERI
   WATTS S, 1985, IEE PROC-F, V132, P613, DOI 10.1049/ip-f-1.1985.0115
   Yim JZ, 2007, INT OFFSHORE POLAR E, P579
   Zhao Zhijian, 2011, Proceedings of the 2011 IEEE CIE International Conference on Radar (Radar), P1752, DOI 10.1109/CIE-Radar.2011.6159909
NR 44
TC 10
Z9 16
U1 0
U2 7
PU UNIV INT RIOJA-UNIR
PI LOGRONO
PA RECTORADO, AVENIDA DE LA PAZ, 137, LOGRONO, 26006, SPAIN
SN 1989-1660
J9 INT J INTERACT MULTI
JI Int. J. Interact. Multimed. Artif. Intell.
PD JUN
PY 2016
VL 3
IS 7
BP 96
EP 103
DI 10.9781/ijimai.2016.3714
PG 8
WC Computer Science, Artificial Intelligence; Computer Science,
   Interdisciplinary Applications
SC Computer Science
GA DM3WY
UT WOS:000376278600015
OA DOAJ Gold
DA 2020-02-19
ER

PT J
AU Zhang, BT
AF Zhang, Byoung-Tak
TI Humans and Machines in the Evolution of AI in Korea
SO AI MAGAZINE
LA English
DT Article
AB Artificial intelligence in Korea is currently prospering. The media are regularly reporting AI-enabled products such as smart advisors, personal robots, autonomous cars, and human level intelligence machines. The IT industry is investing in deep learning and AI to maintain the global competitive edge in its services and products. The Ministry of Science, ICT, and Future Planning (MSIP) has launched new funding programs in AI and cognitive science to implement the government's newly adopted endeavor of building a creative economy and software-centered society. However, AI did not always flourish as it now does. Similar to the history of AI worldwide, AI research and industry in Korea have faced both ups and downs in their history.
C1 [Zhang, Byoung-Tak] Seoul Natl Univ, Comp Sci, Seoul, South Korea.
   [Zhang, Byoung-Tak] Seoul Natl Univ, ICS, Seoul, South Korea.
   [Zhang, Byoung-Tak] Seoul Natl Univ, CRAIC, Seoul, South Korea.
   [Zhang, Byoung-Tak] German Natl Res Ctr Comp Sci, Bonn, Germany.
   [Zhang, Byoung-Tak] KAIS, Daejeon, South Korea.
RP Zhang, BT (reprint author), Seoul Natl Univ, Comp Sci, Seoul, South Korea.; Zhang, BT (reprint author), Seoul Natl Univ, ICS, Seoul, South Korea.; Zhang, BT (reprint author), Seoul Natl Univ, CRAIC, Seoul, South Korea.
CR Ha J.-W., 2015, P 29 AAAI C ART INT
   Kang S.-S., 1987, Proceedings of TENCON 87: 1987 IEEE Region 10 Conference `Computers and Communications Technology Toward 2000' (Cat. No.87CH2423-2), P509
   Lee J., 2007, P 16 IEEE INT S ROB, P357, DOI 10.1109/ROMAN.2007.4415109
   Ryu PM, 2014, INFORM PROCESS MANAG, V50, P683, DOI 10.1016/j.ipm.2014.04.007
   SEO YW, 2000, P 4 INT C AUT AG BAR, P381
   YOO SI, 1992, EXPERT SYST APPL, V4, P69, DOI 10.1016/0957-4174(92)90042-Q
   Zhang B.-T., 2013, LIFELONG MACHINE LEA
   Zhang B.-T., 1997, 1 INT C COGN SCI ICC
NR 8
TC 3
Z9 3
U1 2
U2 22
PU AMER ASSOC ARTIFICIAL INTELL
PI MENLO PK
PA 445 BURGESS DRIVE, MENLO PK, CA 94025-3496 USA
SN 0738-4602
J9 AI MAG
JI AI Mag.
PD SUM
PY 2016
VL 37
IS 2
BP 108
EP 112
DI 10.1609/aimag.v37i2.2656
PG 5
WC Computer Science, Artificial Intelligence
SC Computer Science
GA DP4LC
UT WOS:000378466400015
OA Bronze
DA 2020-02-19
ER

PT J
AU Kappeler, A
   Yoo, S
   Dai, QQ
   Katsaggelos, AK
AF Kappeler, Armin
   Yoo, Seunghwan
   Dai, Qiqin
   Katsaggelos, Aggelos K.
TI Video Super-Resolution With Convolutional Neural Networks
SO IEEE TRANSACTIONS ON COMPUTATIONAL IMAGING
LA English
DT Article
DE Deep Learning; Deep Neural Networks; Convolutional Neural Networks;
   Video Super-Resolution
ID IMAGE SUPERRESOLUTION; ALGORITHM
AB Convolutional neural networks (CNN) are a special type of deep neural networks (DNN). They have so far been successfully applied to image super-resolution (SR) as well as other image restoration tasks. In this paper, we consider the problem of video super-resolution. We propose a CNN that is trained on both the spatial and the temporal dimensions of videos to enhance their spatial resolution. Consecutive frames are motion compensated and used as input to a CNN that provides super-resolved video frames as output. We investigate different options of combining the video frames within one CNN architecture. While large image databases are available to train deep neural networks, it is more challenging to create a large video database of sufficient quality to train neural nets for video restoration. We show that by using images to pretrain our model, a relatively small video database is sufficient for the training of our model to achieve and even improve upon the current state-of-the-art. We compare our proposed approach to current video as well as image SR algorithms.
C1 [Kappeler, Armin; Yoo, Seunghwan; Dai, Qiqin; Katsaggelos, Aggelos K.] Northwestern Univ, Dept Elect Engn & Comp Sci, Evanston, IL 60208 USA.
RP Kappeler, A (reprint author), Northwestern Univ, Dept Elect Engn & Comp Sci, Evanston, IL 60208 USA.
EM a.kappeler@u.northwestern.edu
RI Katsaggelos, Aggelos K/B-7233-2009
CR Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   Babacan SD, 2011, IEEE T IMAGE PROCESS, V20, P984, DOI 10.1109/TIP.2010.2080278
   Baker S, 2011, INT J COMPUT VISION, V92, P1, DOI 10.1007/s11263-010-0390-2
   Belekos SP, 2010, IEEE T IMAGE PROCESS, V19, P1451, DOI 10.1109/TIP.2010.2042115
   Burger HC, 2012, PROC CVPR IEEE, P2392, DOI 10.1109/CVPR.2012.6247952
   Chen M., 2012, P AS C SIGN SYST COM, P1
   Cui Z., 2014, P IEEE EUR C COMP VI, P1
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Drulea M, 2011, IEEE INT C INTELL TR, P318, DOI 10.1109/ITSC.2011.6082986
   Eigen D, 2013, IEEE I CONF COMP VIS, P633, DOI 10.1109/ICCV.2013.84
   Glasner D, 2009, IEEE I CONF COMP VIS, P349, DOI 10.1109/ICCV.2009.5459271
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Infognition, 2010, VID ENH
   Jain V., 2008, P NEUR INF PROC SYST, P1
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Jia Yangqing, 2014, ARXIV14085093
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Katsaggelos A.K., 2007, SYNTHESIS LECT IMAGE, V1, P1, DOI DOI 10.2200/S00036ED1V01Y200606IVM007
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Liao RJ, 2015, IEEE I CONF COMP VIS, P531, DOI 10.1109/ICCV.2015.68
   Liu C, 2014, IEEE T PATTERN ANAL, V36, P346, DOI 10.1109/TPAMI.2013.127
   Ma Z., 2015, P IEEE C COMP VIS PA, V1
   Maas A. L., 2013, P ICML, V30, P1
   Masci J, 2011, LECT NOTES COMPUT SC, V6791, P52, DOI 10.1007/978-3-642-21735-7_7
   Park MK, 2007, OPT ENG, V46, DOI 10.1117/1.2802611
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Schuler CJ, 2013, PROC CVPR IEEE, P1067, DOI 10.1109/CVPR.2013.142
   Schulter S, 2015, PROC CVPR IEEE, P3791, DOI 10.1109/CVPR.2015.7299003
   Song BC, 2011, IEEE T CIRC SYST VID, V21, P274, DOI 10.1109/TCSVT.2010.2087454
   Sun DQ, 2010, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2010.5539939
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Takeda H, 2009, IEEE T IMAGE PROCESS, V18, P1958, DOI 10.1109/TIP.2009.2023703
   Timofte R, 2013, IEEE I CONF COMP VIS, P1920, DOI 10.1109/ICCV.2013.241
   Wang SL, 2012, PROC CVPR IEEE, P2216, DOI 10.1109/CVPR.2012.6247930
   Xie J., 2012, P NEUR INF PROC SYST, P1, DOI DOI 10.1109/AGRO-GEOINFORMATICS.2012.6311605
   Yang JC, 2012, PROC CVPR IEEE, P2360, DOI 10.1109/CVPR.2012.6247948
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zeyde R, 2012, CURVES SURFACES, P711, DOI [DOI 10.1007/978-3-642-27413-8_47, 10.1007/978-3-642-27413-8_47]
   Zhangyang Wang, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P1, DOI 10.1109/CVPRW.2015.7301266
NR 42
TC 99
Z9 105
U1 11
U2 33
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 2333-9403
J9 IEEE T COMPUT IMAG
JI IEEE Trans. Comput. Imaging
PD JUN
PY 2016
VL 2
IS 2
BP 109
EP 122
DI 10.1109/TCI.2016.2532323
PG 14
WC Engineering, Electrical & Electronic; Imaging Science & Photographic
   Technology
SC Engineering; Imaging Science & Photographic Technology
GA DX2UW
UT WOS:000384228400003
DA 2020-02-19
ER

PT J
AU Wang, Q
   Lin, JZ
   Yuan, Y
AF Wang, Qi
   Lin, Jianzhe
   Yuan, Yuan
TI Salient Band Selection for Hyperspectral Image Classification via
   Manifold Ranking
SO IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
LA English
DT Article
DE Band selection; deep learning; hyperspectral image (HSI) classification;
   manifold ranking (MR); saliency; stacked autoencoders (SAEs)
ID EMPIRICAL MODE DECOMPOSITION; CLONAL SELECTION; DEEP; REPRESENTATIONS;
   INFORMATION; EXTRACTION; REDUCTION; NETWORK; QUALITY
AB Saliency detection has been a hot topic in recent years, and many efforts have been devoted in this area. Unfortunately, the results of saliency detection can hardly be utilized in general applications. The primary reason, we think, is unspecific definition of salient objects, which makes that the previously published methods cannot extend to practical applications. To solve this problem, we claim that saliency should be defined in a context and the salient band selection in hyperspectral image (HSI) is introduced as an example. Unfortunately, the traditional salient band selection methods suffer from the problem of inappropriate measurement of band difference. To tackle this problem, we propose to eliminate the drawbacks of traditional salient band selection methods by manifold ranking. It puts the band vectors in the more accurate manifold space and treats the saliency problem from a novel ranking perspective, which is considered to be the main contributions of this paper. To justify the effectiveness of the proposed method, experiments are conducted on three HSIs, and our method is compared with the six existing competitors. Results show that the proposed method is very effective and can achieve the best performance among the competitors.
C1 [Wang, Qi] Northwestern Polytech Univ, Sch Comp Sci, Xian 710072, Peoples R China.
   [Wang, Qi] Northwestern Polytech Univ, Ctr OPTical IMagery Anal & Learning, Xian 710072, Peoples R China.
   [Lin, Jianzhe; Yuan, Yuan] Chinese Acad Sci, Xian Inst Opt & Precis Mech, State Key Lab Transient Opt & Photon, Ctr OPTical Imagery Anal & Learning, Xian 710119, Peoples R China.
RP Wang, Q (reprint author), Northwestern Polytech Univ, Sch Comp Sci, Xian 710072, Peoples R China.; Wang, Q (reprint author), Northwestern Polytech Univ, Ctr OPTical IMagery Anal & Learning, Xian 710072, Peoples R China.; Lin, JZ; Yuan, Y (reprint author), Chinese Acad Sci, Xian Inst Opt & Precis Mech, State Key Lab Transient Opt & Photon, Ctr OPTical Imagery Anal & Learning, Xian 710119, Peoples R China.
EM crabwq@nwpu.edu.cn; linjianzhe@opt.cn; yuany@opt.ac.cn
OI Wang, Qi/0000-0002-7028-4956
FU National Basic Research Program of ChinaNational Basic Research Program
   of China [2013CB336500]; State Key Program of National Natural Science
   of ChinaNational Natural Science Foundation of China [61232010];
   National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61172143, 61105012, 61379094]; Natural Science
   Foundation Research Project of Shaanxi ProvinceNatural Science
   Foundation of Shaanxi Province [2015JM6264]; Fundamental Research Funds
   for Central UniversitiesFundamental Research Funds for the Central
   Universities [3102014JC02020G07, 3102015BJ(II)JJZ01]; Open Research Fund
   through Key Laboratory of Spectral Imaging Technology, Chinese Academy
   of Sciences
FX This work was supported in part by the National Basic Research Program
   of China (Youth 973 Program) under Grant 2013CB336500, in part by the
   State Key Program of National Natural Science of China under Grant
   61232010, in part by the National Natural Science Foundation of China
   under Grant 61172143, Grant 61105012, and Grant 61379094, in part by the
   Natural Science Foundation Research Project of Shaanxi Province under
   Grant 2015JM6264, in part by the Fundamental Research Funds for Central
   Universities under Grant 3102014JC02020G07 and 3102015BJ(II)JJZ01, and
   in part by the Open Research Fund through the Key Laboratory of Spectral
   Imaging Technology, Chinese Academy of Sciences.
CR Agarwal Abhishek, 2007, 2007 IEEE International Symposium on Signal Processing and Information Technology, P353, DOI 10.1109/ISSPIT.2007.4458191
   Ball J., 2006, P AAAI SPRING S COGN, P1
   Ball JE, 2007, IEEE T GEOSCI REMOTE, V45, P3022, DOI 10.1109/TGRS.2007.905629
   Borji A., 2014, SALIENT OBJECT DETEC
   Cayton L., 2005, CS20080923 U CAL SCH
   Chang CI, 2006, IEEE T GEOSCI REMOTE, V44, P1575, DOI 10.1109/TGRS.2006.864389
   Chang CI, 1999, IEEE T GEOSCI REMOTE, V37, P2631, DOI 10.1109/36.803411
   Chang Y.-L., 2008, P IEEE INT GEOSC REM
   Chen DY, 2012, IEEE T NEUR NET LEAR, V23, P1206, DOI 10.1109/TNNLS.2012.2198888
   de Castro LN, 2002, IEEE T EVOLUT COMPUT, V6, P239, DOI 10.1109/TEVC.2002.1011539
   Demir B., 2008, P 27 IEEE INT GEOSC
   Demir B, 2010, IEEE T GEOSCI REMOTE, V48, P4071, DOI 10.1109/TGRS.2010.2070510
   Du Q, 2008, IEEE GEOSCI REMOTE S, V5, P564, DOI 10.1109/LGRS.2008.2000619
   Duda R.O., 2001, PATTERN CLASSIFICATI
   Erturk A, 2013, IEEE T GEOSCI REMOTE, V51, P2787, DOI 10.1109/TGRS.2012.2217501
   Fauvel M, 2013, P IEEE, V101, P652, DOI 10.1109/JPROC.2012.2197589
   Feng J, 2014, IEEE T GEOSCI REMOTE, V52, P4092, DOI 10.1109/TGRS.2013.2279591
   FROST OL, 1972, PR INST ELECTR ELECT, V60, P926, DOI 10.1109/PROC.1972.8817
   Han JW, 2015, IEEE T CIRC SYST VID, V25, P1309, DOI 10.1109/TCSVT.2014.2381471
   Han JW, 2013, IEEE T CIRC SYST VID, V23, P2009, DOI 10.1109/TCSVT.2013.2242594
   Hasanlou M, 2012, IEEE GEOSCI REMOTE S, V9, P1046, DOI 10.1109/LGRS.2012.2189547
   HUGHES GF, 1968, IEEE T INFORM THEORY, V14, P55, DOI 10.1109/TIT.1968.1054102
   Jia S, 2010, INT GEOSCI REMOTE SE, P72, DOI 10.1109/IGARSS.2010.5652463
   Keshava N, 2004, IEEE T GEOSCI REMOTE, V42, P1552, DOI 10.1109/TGRS.2004.830549
   KOCH C, 1985, HUM NEUROBIOL, V4, P219
   Li J, 2010, IEEE T GEOSCI REMOTE, V48, P4085, DOI 10.1109/TGRS.2010.2060550
   Martinez-Uso A, 2007, IEEE T GEOSCI REMOTE, V45, P4158, DOI 10.1109/TGRS.2007.904951
   Melgani F, 2004, IEEE T GEOSCI REMOTE, V42, P1778, DOI 10.1109/TGRS.2004.831865
   Mendenhall MJ, 2008, IEEE T NEURAL NETWOR, V19, P658, DOI 10.1109/TNN.2007.914156
   Merenyi E, 2014, EURASIP J ADV SIG PR, DOI 10.1186/1687-6180-2014-71
   Shao L, 2015, IEEE T NEUR NET LEAR, V26, P1019, DOI 10.1109/TNNLS.2014.2330900
   Shao L, 2014, IEEE T NEUR NET LEAR, V25, P2303, DOI 10.1109/TNNLS.2014.2308519
   Shao L, 2014, IEEE T NEUR NET LEAR, V25, P1359, DOI 10.1109/TNNLS.2013.2293418
   Simon D, 2013, EVOLUTIONARY OPTIMIZ
   Stuhlsatz A, 2012, IEEE T NEUR NET LEAR, V23, P596, DOI 10.1109/TNNLS.2012.2183645
   Sun K, 2014, IEEE J-STARS, V7, P2697, DOI 10.1109/JSTARS.2014.2320299
   Tarabalka Y, 2010, IEEE GEOSCI REMOTE S, V7, P736, DOI 10.1109/LGRS.2010.2047711
   Venkataraman S, 2005, INT GEOSCI REMOTE SE, P1245
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Wang J, 2006, IEEE T GEOSCI REMOTE, V44, P1586, DOI 10.1109/TGRS.2005.863297
   Wang M., 2007, P 15 ACM INT C MULT, P862, DOI DOI 10.1145/1291233.1291431
   Wang Q, 2014, NEUROCOMPUTING, V131, P348, DOI 10.1016/j.neucom.2013.09.032
   WARD JH, 1963, J AM STAT ASSOC, V58, P236, DOI 10.2307/2282967
   Xia W, 2012, INT GEOSCI REMOTE SE, P3062, DOI 10.1109/IGARSS.2012.6350779
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Yang H, 2011, IEEE J-STARS, V4, P660, DOI 10.1109/JSTARS.2011.2120598
   Yang H, 2011, IEEE GEOSCI REMOTE S, V8, P138, DOI 10.1109/LGRS.2010.2053516
   Yuan Y, 2015, IEEE T GEOSCI REMOTE, V53, P631, DOI 10.1109/TGRS.2014.2326655
   Zheng HC, 2008, IEEE T NEURAL NETWOR, V19, P746, DOI 10.1109/TNN.2007.911741
   Zhong P, 2014, IEEE T NEUR NET LEAR, V25, P1319, DOI 10.1109/TNNLS.2013.2293061
   Zhou D., 2003, P 18 ANN C NEUR INF, P169
   Zhou D., 2003, ADV NEURAL INF PROCE, P321
   Zhu QS, 2015, IEEE T NEUR NET LEAR, V26, P185, DOI 10.1109/TNNLS.2014.2369426
NR 53
TC 212
Z9 215
U1 10
U2 125
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 2162-237X
EI 2162-2388
J9 IEEE T NEUR NET LEAR
JI IEEE Trans. Neural Netw. Learn. Syst.
PD JUN
PY 2016
VL 27
IS 6
BP 1279
EP 1289
DI 10.1109/TNNLS.2015.2477537
PG 11
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Computer Science, Theory & Methods; Engineering,
   Electrical & Electronic
SC Computer Science; Engineering
GA DN5MR
UT WOS:000377113300014
PM 27008675
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Lin, L
   Wang, KZ
   Zuo, WM
   Wang, M
   Luo, JB
   Zhang, L
AF Lin, Liang
   Wang, Keze
   Zuo, Wangmeng
   Wang, Meng
   Luo, Jiebo
   Zhang, Lei
TI A Deep Structured Model with Radius-Margin Bound for 3D Human Activity
   Recognition
SO INTERNATIONAL JOURNAL OF COMPUTER VISION
LA English
DT Article
DE Human action and activity; RGB-depth analysis; Structured model; Deep
   learning
ID VIDEO; GRAMMAR; EVENTS
AB Understanding human activity is very challenging even with the recently developed 3D/depth sensors. To solve this problem, this work investigates a novel deep structured model, which adaptively decomposes an activity instance into temporal parts using the convolutional neural networks. Our model advances the traditional deep learning approaches in two aspects. First, we incorporate latent temporal structure into the deep model, accounting for large temporal variations of diverse human activities. In particular, we utilize the latent variables to decompose the input activity into a number of temporally segmented sub-activities, and accordingly feed them into the parts (i.e. sub-networks) of the deep architecture. Second, we incorporate a radius-margin bound as a regularization term into our deep model, which effectively improves the generalization performance for classification. For model training, we propose a principled learning algorithm that iteratively (i) discovers the optimal latent variables (i.e. the ways of activity decomposition) for all training instances, (ii) updates the classifiers based on the generated features, and (iii) updates the parameters of multi-layer neural networks. In the experiments, our approach is validated on several complex scenarios for human activity recognition and demonstrates superior performances over other state-of-the-art approaches.
C1 [Lin, Liang; Wang, Keze] Sun Yat Sen Univ, Guangzhou 510275, Guangdong, Peoples R China.
   [Lin, Liang; Wang, Keze; Zhang, Lei] Hong Kong Polytech Univ, Dept Comp, Hong Kong, Hong Kong, Peoples R China.
   [Zuo, Wangmeng] Harbin Inst Technol, Sch Comp Sci & Technol, Harbin 150001, Peoples R China.
   [Wang, Meng] Hefei Univ Technol, Hefei, Peoples R China.
   [Luo, Jiebo] Univ Rochester, Dept Comp Sci, Rochester, NY 14627 USA.
RP Zuo, WM (reprint author), Harbin Inst Technol, Sch Comp Sci & Technol, Harbin 150001, Peoples R China.
EM linliang@ieee.org; cswmzuo@gmail.com; eric.mengwang@gmail.com;
   jluo@cs.rochester.edu; cslzhang@comp.polyu.edu.hk
RI Wang, Keze/Z-3605-2019; Zuo, Wangmeng/B-3701-2008
OI Zhang, Lei/0000-0002-2078-4215; Luo, Jiebo/0000-0002-4516-9729
FU Hong Kong Scholar Program; HK PolyU's Joint Supervision Scheme; Chinese
   Mainland University [G-SB20]; Taiwan University [G-SB20]; Macao
   University [G-SB20]; Guangdong Natural Science FoundationNational
   Natural Science Foundation of Guangdong Province [S2013010013432,
   S2013050014548]; Guangdong Science and Technology Program
   [2013B010406005]
FX This work was supported in part by the Hong Kong Scholar Program, and in
   part by the HK PolyU's Joint Supervision Scheme with the Chinese
   Mainland, Taiwan and Macao Universities (Grant no. G-SB20), in part by
   Guangdong Natural Science Foundation (Grant nos. S2013010013432 and
   S2013050014548), and in part by Guangdong Science and Technology Program
   (Grant no. 2013B010406005).
CR Amer MR, 2012, PROC CVPR IEEE, P1314, DOI 10.1109/CVPR.2012.6247816
   Bayer J., 2014, P ICLR
   Brendel W, 2011, IEEE I CONF COMP VIS, P778, DOI 10.1109/ICCV.2011.6126316
   Chapelle O, 2002, MACH LEARN, V46, P131, DOI 10.1023/A:1012450327387
   Chaquet JM, 2013, COMPUT VIS IMAGE UND, V117, P633, DOI 10.1016/j.cviu.2013.01.013
   Cheng Z, 2011, ACM MULTIMEDIA, P1401
   Chung KM, 2003, NEURAL COMPUT, V15, P2643, DOI 10.1162/089976603322385108
   Do H., 2013, ICML
   Do H, 2009, LECT NOTES ARTIF INT, V5781, P315
   Donahue Jeffrey, 2015, CVPR
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Girshick R, 2014, P IEEE C COMP VIS PA
   Gupta R, 2013, P 21 ACM INT C MULT
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Huang F. J., 2006, C COMP VIS PATT REC, P284, DOI [10.1109/CVPR.2006.164, DOI 10.1109/CVPR.2006.164]
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Karpathy Andrej, 2014, CVPR
   Koppula H, 2013, J MACH LEARN RES, P792, DOI DOI 10.1177/0278364913478446
   Koppula HS, 2013, INT J ROBOT RES, V32, P951, DOI 10.1177/0278364913478446
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   LeCun Yann, 1990, ADV NEURAL INFORM PR
   Liang X., 2013, P 21 ACM INT C MULT, P263
   Lin L, 2015, IEEE T PATTERN ANAL, V37, P959, DOI 10.1109/TPAMI.2014.2359888
   Lin L, 2009, PATTERN RECOGN, V42, P1297, DOI 10.1016/j.patcog.2008.10.033
   Luo P., 2014, CVPR
   Luo P, 2013, IEEE I CONF COMP VIS, P2864, DOI 10.1109/ICCV.2013.356
   Ni B., 2013, IEEE INT C WORKSH AU, P1, DOI DOI 10.1109/FG.2013.6553756
   Ni Bingbing, 2013, CONSUMER DEPTH CAMER, P193, DOI DOI 10.1007/978-1-4471-4640-7_10
   Oreifej O, 2013, PROC CVPR IEEE, P716, DOI 10.1109/CVPR.2013.98
   Packer B, 2012, PROC CVPR IEEE, P1378, DOI 10.1109/CVPR.2012.6247824
   Pei MT, 2011, IEEE I CONF COMP VIS, P487, DOI 10.1109/ICCV.2011.6126279
   Sadanand S, 2012, PROC CVPR IEEE, P1234, DOI 10.1109/CVPR.2012.6247806
   Scovanner P., 2007, P 15 INT C MULT, P357, DOI DOI 10.1145/1291233.1291311
   Sermanet P., 2013, CVPR
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Sung JY, 2012, IEEE INT CONF ROBOT, P842, DOI 10.1109/ICRA.2012.6224591
   Tang K, 2012, PROC CVPR IEEE, P1250, DOI 10.1109/CVPR.2012.6247808
   Tu K, 2014, IEEE MULTIMEDIA, V21, P42, DOI 10.1109/MMUL.2014.29
   Vapnik V. N., 1998, STAT LEARNING THEORY
   Venugopalan S., 2015, N AM CHAPT ASS COMP
   Wang CY, 2013, PROC CVPR IEEE, P915, DOI 10.1109/CVPR.2013.123
   Wang J, 2013, IEEE I CONF COMP VIS, P2688, DOI 10.1109/ICCV.2013.334
   Wang J, 2012, PROC CVPR IEEE, P1290, DOI 10.1109/CVPR.2012.6247813
   Wang K., 2014, ACM MM
   Wang Y, 2011, IEEE T PATTERN ANAL, V33, P1310, DOI 10.1109/TPAMI.2010.214
   WU CFJ, 1983, ANN STAT, V11, P95, DOI 10.1214/aos/1176346060
   Wu P, 2013, P 21 ACM INT C MULT, P153, DOI 10.1145/2502081.2502112
   Xia L., 2012, IEEE COMP SOC C COMP, P20, DOI DOI 10.1109/CVPRW.2012.6239233
   Xia L, 2013, PROC CVPR IEEE, P2834, DOI 10.1109/CVPR.2013.365
   Yang X, 2012, P 20 ACM INT C MULT, P1057, DOI DOI 10.1145/2393347.2396382
   Yu K, 2011, PROC CVPR IEEE, P1713, DOI 10.1109/CVPR.2011.5995732
   Yun K., 2012, COMP VIS PATT REC WO
   Zhao X., 2013, P ACM INT C MULT, P273
   Zhou X., 2009, ACM MULTIMEDIA, P229
   Zhu SC, 2006, FOUND TRENDS COMPUT, V2, P259, DOI 10.1561/0600000018
NR 55
TC 30
Z9 31
U1 5
U2 25
PU SPRINGER
PI DORDRECHT
PA VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN 0920-5691
EI 1573-1405
J9 INT J COMPUT VISION
JI Int. J. Comput. Vis.
PD JUN
PY 2016
VL 118
IS 2
SI SI
BP 256
EP 273
DI 10.1007/s11263-015-0876-z
PG 18
WC Computer Science, Artificial Intelligence
SC Computer Science
GA DO0OE
UT WOS:000377477400009
DA 2020-02-19
ER

PT J
AU Lai, HJ
   Yan, P
   Shu, XB
   Wei, YC
   Yan, SC
AF Lai, Hanjiang
   Yan, Pan
   Shu, Xiangbo
   Wei, Yunchao
   Yan, Shuicheng
TI Instance-Aware Hashing for Multi-Label Image Retrieval
SO IEEE TRANSACTIONS ON IMAGE PROCESSING
LA English
DT Article
DE Multi-label; image retrieval; instance-aware image representation;
   category-aware hashing; semantic hashing; deep learning
AB Similarity-preserving hashing is a commonly used method for nearest neighbor search in large-scale image retrieval. For image retrieval, deep-network-based hashing methods are appealing, since they can simultaneously learn effective image representations and compact hash codes. This paper focuses on deep-network-based hashing for multi-label images, each of which may contain objects of multiple categories. In most existing hashing methods, each image is represented by one piece of hash code, which is referred to as semantic hashing. This setting may be suboptimal for multi-label image retrieval. To solve this problem, we propose a deep architecture that learns instance-aware image representations for multi-label image data, which are organized in multiple groups, with each group containing the features for one category. The instance-aware representations not only bring advantages to semantic hashing but also can be used in category-aware hashing, in which an image is represented by multiple pieces of hash codes and each piece of code corresponds to a category. Extensive evaluations conducted on several benchmark data sets demonstrate that for both the semantic hashing and the category-aware hashing, the proposed method shows substantial improvement over the state-of-the-art supervised and unsupervised hashing methods.
C1 [Lai, Hanjiang; Yan, Pan] Sun Yat Sen Univ, Sch Data & Comp Sci, Guangzhou 510006, Guangdong, Peoples R China.
   [Shu, Xiangbo] Sch Comp Sci & Technol, Nanjing 210094, Jiangsu, Peoples R China.
   [Wei, Yunchao] Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.
   [Yan, Shuicheng] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 119077, Singapore.
RP Yan, P (reprint author), Sun Yat Sen Univ, Sch Data & Comp Sci, Guangzhou 510006, Guangdong, Peoples R China.
EM laihanj@gmail.com; panyan5@mail.sysu.edu.cn; shuxb104@gmail.com;
   wychao1987@gmail.com; eleyans@nus.edu.sg
FU Natural Science Foundation of Guangdong ProvinceNational Natural Science
   Foundation of Guangdong Province [S2013010011905]; National Natural
   Science Foundation of ChinaNational Natural Science Foundation of China
   [61370021, U1401256, 61472453, U1501252]
FX This work was supported in part by the Natural Science Foundation of
   Guangdong Province under Grant S2013010011905 and in part by the
   National Natural Science Foundation of China under Grant 61370021, Grant
   U1401256, Grant 61472453, and Grant U1501252. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Dacheng Tao. (Corresponding author: Yan Pan.)
CR Arbelaez P, 2014, PROC CVPR IEEE, P328, DOI 10.1109/CVPR.2014.49
   Baeza-Yates R., 1999, MODERN INFORM RETRIE, V463
   Carreira J, 2012, IEEE T PATTERN ANAL, V34, P1312, DOI 10.1109/TPAMI.2011.231
   Cheng MM, 2014, PROC CVPR IEEE, P3286, DOI 10.1109/CVPR.2014.414
   Erhan D., 2009, J MACHINE LEARNING R, V5, P153
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518
   Gong YC, 2011, PROC CVPR IEEE, P817, DOI 10.1109/CVPR.2011.5995432
   Gong Yunchao, 2013, DEEP CONVOLUTIONAL R
   Gu SM, 2013, INT CONF MACH LEARN, P108, DOI 10.1109/ICMLC.2013.6890453
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI 10.1007/978-3-319-10578-9_23
   Huiskes M. J., 2008, P 1 ACM INT C MULT I, P39, DOI DOI 10.1145/1460096.1460104
   Jarvelin K., 2000, SIGIR Forum, V34, P41
   Jarvelin K, 2002, ACM T INFORM SYST, V20, P422, DOI 10.1145/582415.582418
   Jia Y., 2013, CAFFE OPEN SOURCE CO
   Krahenbuhl P, 2014, LECT NOTES COMPUT SC, V8693, P725, DOI 10.1007/978-3-319-10602-1_47
   Kulis B., 2009, P ADV NEUR INF PROC, P1042
   Kulis B, 2009, IEEE I CONF COMP VIS, P2130, DOI 10.1109/ICCV.2009.5459466
   Lai HJ, 2015, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2015.7298947
   Lin GS, 2013, IEEE I CONF COMP VIS, P2552, DOI 10.1109/ICCV.2013.317
   Lin Kevin, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P27, DOI 10.1109/CVPRW.2015.7301269
   Liu T., 2014, CLASSIFICATION NOISY
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Liu Wei, 2011, Reports in Parasitology, V1, P1
   Norouzi M., 2012, ADV NEURAL INFORM PR, V2, P1061
   Norouzi M., 2011, INT C MACH LEARN, P353
   Salakhutdinov R., 2007, J MACHINE LEARNING R, P412
   Salakhutdinov R, 2009, INT J APPROX REASON, V50, P969, DOI 10.1016/j.ijar.2008.11.006
   Sivic J, 2008, PROC CVPR IEEE, P1, DOI 10.1109/CVPRW.2008.4562950
   Szegedy C., 2014, GOING DEEPER CONVOLU
   Tao DC, 2006, IEEE T PATTERN ANAL, V28, P1088, DOI 10.1109/TPAMI.2006.134
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Wang DX, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P2291
   Wang J, 2010, PROC CVPR IEEE, P3424, DOI 10.1109/CVPR.2010.5539994
   Wei Y., 2014, CNN SINGLE LABEL MUL
   Weiss P, 2008, INT CONF ACOUST SPEE, P1173, DOI 10.1109/ICASSP.2008.4517824
   Xia RK, 2014, PROCEEDINGS OF THE TWENTY-EIGHTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2156
   Zhao F, 2015, PROC CVPR IEEE, P1556, DOI 10.1109/CVPR.2015.7298763
NR 38
TC 29
Z9 31
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1057-7149
EI 1941-0042
J9 IEEE T IMAGE PROCESS
JI IEEE Trans. Image Process.
PD JUN
PY 2016
VL 25
IS 6
BP 2469
EP 2479
DI 10.1109/TIP.2016.2545300
PG 11
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
SC Computer Science; Engineering
GA DK4LU
UT WOS:000374890600004
PM 27019492
DA 2020-02-19
ER

PT J
AU He, T
   Huang, WL
   Qiao, Y
   Yao, J
AF He, Tong
   Huang, Weilin
   Qiao, Yu
   Yao, Jian
TI Text-Attentional Convolutional Neural Network for Scene Text Detection
SO IEEE TRANSACTIONS ON IMAGE PROCESSING
LA English
DT Article
DE Maximally stable extremal regions; text detector; convolutional neural
   networks; multi-level supervised information; multi-task learning
ID READING TEXT; LOCALIZATION
AB Recent deep learning models have demonstrated strong capabilities for classifying text and non-text components in natural images. They extract a high-level feature globally computed from a whole image component (patch), where the cluttered background information may dominate true text features in the deep representation. This leads to less discriminative power and poorer robustness. In this paper, we present a new system for scene text detection by proposing a novel text-attentional convolutional neural network (Text-CNN) that particularly focuses on extracting text-related regions and features from the image components. We develop a new learning mechanism to train the Text-CNN with multi-level and rich supervised information, including text region mask, character label, and binary text/non-text information. The rich supervision information enables the Text-CNN with a strong capability for discriminating ambiguous texts, and also increases its robustness against complicated background components. The training process is formulated as a multi-task learning problem, where low-level supervised information greatly facilitates the main task of text/non-text classification. In addition, a powerful low-level detector called contrast-enhancement maximally stable extremal regions (MSERs) is developed, which extends the widely used MSERs by enhancing intensity contrast between text patterns and background. This allows it to detect highly challenging text patterns, resulting in a higher recall. Our approach achieved promising results on the ICDAR 2013 data set, with an F-measure of 0.82, substantially improving the state-of-the-art results.
C1 [He, Tong; Huang, Weilin; Qiao, Yu] Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen 518055, Peoples R China.
   [He, Tong; Yao, Jian] Wuhan Univ, Sch Remote Sensing & Informat Engn, Wuhan 430072, Peoples R China.
   [Huang, Weilin; Qiao, Yu] Chinese Univ Hong Kong, Multimedia Lab, Hong Kong, Hong Kong, Peoples R China.
RP Huang, WL (reprint author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen 518055, Peoples R China.; Huang, WL (reprint author), Chinese Univ Hong Kong, Multimedia Lab, Hong Kong, Hong Kong, Peoples R China.
EM tong.he@siat.ac.cn; wl.huang@siat.ac.cn; yu.qiao@siat.ac.cn;
   jian.yao@whu.edu.cn
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61503367, 41571436]; Guangdong Natural Science
   FoundationNational Natural Science Foundation of Guangdong Province
   [2015A030310289]; Shenzhen Research Program [JSGG20150925164740726,
   JCYJ20150925163005055, CXZZ20150930104115529]; National High-Tech
   Research and Development Program of ChinaNational High Technology
   Research and Development Program of China [2015AA042303]; Guangdong
   Research Program [2014B050505017, 2015B010129013]
FX This work was supported in part by the National Natural Science
   Foundation of China under Project 61503367 and Project 41571436, in part
   by the Guangdong Natural Science Foundation under Grant 2015A030310289,
   in part by the Shenzhen Research Program under Grant
   JSGG20150925164740726, Grant JCYJ20150925163005055, and Grant
   CXZZ20150930104115529, in part by the National High-Tech Research and
   Development Program of China under Grant 2015AA042303, and in part by
   the Guangdong Research Program under Grant 2014B050505017 and Grant
   2015B010129013. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Xiaochun Cao.
   (Corresponding author: Weilin Huang.)
CR Argyriou A., 2007, P ADV NEUR INF PROC, P1
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bissacco A., 2013, P IEEE INT C COMP VI
   Bosch A, 2007, IEEE I CONF COMP VIS, P1863
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Busta M, 2015, IEEE I CONF COMP VIS, P1206, DOI 10.1109/ICCV.2015.143
   Chen H., 2012, P IEEE INT C IM PROC, P2609
   Chen XR, 2004, PROC CVPR IEEE, P366
   Dalal N, 2005, PROC CVPR IEEE, P886
   Epshtein B, 2010, PROC CVPR IEEE, P2963, DOI 10.1109/CVPR.2010.5540041
   Fu HZ, 2013, IEEE T IMAGE PROCESS, V22, P3766, DOI 10.1109/TIP.2013.2260166
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Gonzalez A, 2012, INT C PATT RECOG, P617
   Gupta A., 2016, P IEEE COMP VIS PATT
   Hanif Shehzad Muhammad, 2009, 2009 10th International Conference on Document Analysis and Recognition (ICDAR), P1, DOI 10.1109/ICDAR.2009.172
   He P., 2016, P 30 AAAI C ART INT
   Huang WL, 2013, IEEE I CONF COMP VIS, P1241, DOI 10.1109/ICCV.2013.157
   Huang WL, 2014, LECT NOTES COMPUT SC, V8692, P497, DOI 10.1007/978-3-319-10593-2_33
   Jaderberg M, 2016, INT J COMPUT VISION, V116, P1, DOI 10.1007/s11263-015-0823-z
   Jaderberg M, 2014, LECT NOTES COMPUT SC, V8692, P512, DOI 10.1007/978-3-319-10593-2_34
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Jing Zhang, 2010, Computer Vision - ACCV 2010. 10th Asian Conference on Computer Vision. Revised Selected Papers, P308, DOI 10.1007/978-3-642-19309-5_24
   Kang L, 2014, PROC CVPR IEEE, P4034, DOI 10.1109/CVPR.2014.514
   Karatzas D, 2013, PROC INT CONF DOC, P1484, DOI 10.1109/ICDAR.2013.221
   Kim KI, 2003, IEEE T PATTERN ANAL, V25, P1631, DOI 10.1109/TPAMI.2003.1251157
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y., 1989, ADV NEURAL INF PROCE, P396
   Li Y, 2014, IEEE T IMAGE PROCESS, V23, P1666, DOI 10.1109/TIP.2014.2302896
   Lucas SM, 2005, PROC INT CONF DOC, P80, DOI 10.1109/ICDAR.2005.231
   Matas J, 2004, IMAGE VISION COMPUT, V22, P761, DOI 10.1016/j.imavis.2004.02.006
   Min Chen, 2011, IEEE INFOCOM 2011 - IEEE Conference on Computer Communications Workshops, P409, DOI 10.1109/INFCOMW.2011.5928847
   Neumann L, 2013, IEEE I CONF COMP VIS, P97, DOI 10.1109/ICCV.2013.19
   Neumann L, 2013, PROC INT CONF DOC, P523, DOI 10.1109/ICDAR.2013.110
   Neumann L, 2012, PROC CVPR IEEE, P3538, DOI 10.1109/CVPR.2012.6248097
   Neumann L, 2011, PROC INT CONF DOC, P687, DOI 10.1109/ICDAR.2011.144
   Nister D, 2008, LECT NOTES COMPUT SC, V5303, P183, DOI 10.1007/978-3-540-88688-4_14
   Ozuysal M., 2007, P IEEE C COMP VIS PA, P1
   Pan YF, 2011, IEEE T IMAGE PROCESS, V20, P800, DOI 10.1109/TIP.2010.2070803
   Shahab A, 2011, PROC INT CONF DOC, P1491, DOI 10.1109/ICDAR.2011.296
   Shi CZ, 2013, PATTERN RECOGN LETT, V34, P107, DOI 10.1016/j.patrec.2012.09.019
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Singh M., 2012, INT J ENG RES TECHNO, V1, P1, DOI DOI 10.1145/2208828.2208857
   Sochman J, 2005, PROC CVPR IEEE, P150
   Sun L, 2015, PATTERN RECOGN, V48, P2906, DOI 10.1016/j.patcog.2015.04.002
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tuzel O, 2006, LECT NOTES COMPUT SC, V3952, P589
   Wang K, 2011, IEEE I CONF COMP VIS, P1457, DOI 10.1109/ICCV.2011.6126402
   Wang T, 2012, INT C PATT RECOG, P3304
   Wolf C, 2006, INT J DOC ANAL RECOG, V8, P280, DOI 10.1007/s10032-006-0014-0
   Yao C, 2014, IEEE T IMAGE PROCESS, V23, P4737, DOI 10.1109/TIP.2014.2353813
   Yao C, 2012, PROC CVPR IEEE, P1083, DOI 10.1109/CVPR.2012.6247787
   Ye QX, 2015, IEEE T PATTERN ANAL, V37, P1480, DOI 10.1109/TPAMI.2014.2366765
   Yi CC, 2013, COMPUT VIS IMAGE UND, V117, P182, DOI 10.1016/j.cviu.2012.11.002
   Yi CC, 2011, IEEE T IMAGE PROCESS, V20, P2594, DOI 10.1109/TIP.2011.2126586
   Yin XC, 2015, IEEE T PATTERN ANAL, V37, P1930, DOI 10.1109/TPAMI.2014.2388210
   Yin XC, 2014, IEEE T PATTERN ANAL, V36, P970, DOI 10.1109/TPAMI.2013.182
   Zamberletti A., 2014, P AS C COMP VIS, P91
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang Z., IEEE T PATTERN ANAL
   Zhang Z, 2015, PROC CVPR IEEE, P2558, DOI 10.1109/CVPR.2015.7298871
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
NR 62
TC 84
Z9 91
U1 4
U2 71
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1057-7149
EI 1941-0042
J9 IEEE T IMAGE PROCESS
JI IEEE Trans. Image Process.
PD JUN
PY 2016
VL 25
IS 6
BP 2529
EP 2541
DI 10.1109/TIP.2016.2547588
PG 13
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
SC Computer Science; Engineering
GA DK4LU
UT WOS:000374890600009
PM 28113809
DA 2020-02-19
ER

PT J
AU Qin, PD
   Xu, WR
   Guo, J
AF Qin, Pengda
   Xu, Weiran
   Guo, Jun
TI An empirical convolutional neural network approach for semantic relation
   classification
SO NEUROCOMPUTING
LA English
DT Article
DE Relation classification; Convolution neural network; Dropout;
   Data-driven
AB In industry, relation classification plays a significant role in today's search engine. Up to now, the state-of-the-art systems have the problems of over-reliance on the quality of handcrafted features annotated by experts and linguistic knowledge derived from linguistic analysis modules, which is costly and leads to the issue of error propagation. Currently, with the data-driven approaches attracting wide attention, deep learning achieves impressive performance in semantic processing tasks without much effort on costly features. In this work, we deal with the relation classification task utilizing a convolutional neural network (CNN) approach to automatically control feature learning from raw sentences and minimize the application of external toolkits and resources. Our proposed method has several distinct features. First, we exploit a simple but rational way to specify which input tokens are the target nominals in the input sentence, instead of Position Feature that used in other neural network relation classification systems. Secondly, a most suitable dropout strategy is used to prevent units in the neural network from co-adapting too much, which significantly reduces over-fitting and improves the performance. Eventually, using only word embedding as input features is sufficient to achieve desirable performance. Our experiments on the SemEval-2010 Task-8 dataset show that our CNN architecture without using any additional extracted features significantly outperforms the state-of-the-art systems and achieves an F1-score of 84.8% only considering the context between the two target nominals. (C) 2016 Elsevier B.V. All rights reserved.
C1 [Qin, Pengda; Xu, Weiran; Guo, Jun] Beijing Univ Posts & Telecommun, Sch Informat & Commun Engn, Beijing 100876, Peoples R China.
RP Qin, PD; Xu, WR; Guo, J (reprint author), Beijing Univ Posts & Telecommun, Sch Informat & Commun Engn, Beijing 100876, Peoples R China.
EM qinpengda0406@gmail.com; xuweiran@bupt.edu.cn; guojun@bupt.edu.cn
FU 111 Project of China [B08004]; National Natural Science Foundation of
   ChinaNational Natural Science Foundation of China [61273217, 61300080];
   Ph.D. Programs Foundation of Ministry of Education of the People's
   Republic of China [20130005110004]
FX This work was supported by 111 Project of China under Grant no. B08004,
   the National Natural Science Foundation of China (61273217 and
   61300080), the Ph.D. Programs Foundation of Ministry of Education of the
   People's Republic of China (20130005110004).
CR Bengio Y, 2003, J MACH LEARN RES, V3, P1137, DOI 10.1162/153244303322533223
   Collobert R, 2011, J MACH LEARN RES, V12, P2493
   Dos Santos C.N., 2015, P 53 ANN M ASS COMP
   Fundel K, 2007, BIOINFORMATICS, V23, P365, DOI 10.1093/bioinformatics/btl616
   Glorot X., 2011, P INT C ART INT STAT
   Grishman R., 2011, INFORM EXTRACTION CA
   Harris Zellig S, 1954, DISTRIBUTIONAL STRUC
   Hendrickx Iris, 2009, P WORKSH SEM EV REC
   Hinton G. E, 2012, ARXIV12070580
   Kalchbrenner N., 2014, ARXIV14042188
   Kambhatla N, 2004, P ACL 2004 INT POST
   Karsoliya S., 2012, INT J ENG TRENDS TEC, V3, P713
   Kim Y, 2014, ARXIV14085882
   LeCun Yann, 1995, HDB BRAIN THEORY NEU, P255, DOI DOI 10.1109/IJCNN.2004.1381049
   Lyyer M., 2014, P 2014 C EMP METH NA
   Mintz M, 2009, P JOINT C 47 ANN M A, V2
   Qiu JB, 2016, IEEE T FUZZY SYST, V24, P388, DOI 10.1109/TFUZZ.2015.2457934
   Rink B., 2010, P 5 INT WORKSH SEM E
   SOCHER Richard, 2012, P 2012 JOINT C EMP M
   Stathakis D, 2009, INT J REMOTE SENS, V30, P2133, DOI 10.1080/01431160802549278
   Weston Jason, 2014, P 2014 C EMP METH NA
   Wu F, 2010, P 48 ANN M ASS COMP
   Xu K, 2015, ARXIV150607650
   Xu W., 2014, INT J COMMUN SYST
   Xu Yan, 2015, P C EMP METH NAT LAN
   Yih Wen-tau, 2014, P ACL
   Yin S., 2015, IEEE T CONTROL SYST
   Yin S, 2016, IEEE T CYBERNETICS, V46, P3135, DOI 10.1109/TCYB.2015.2498194
   Yin S, 2015, IEEE-ASME T MECH, V20, P2613, DOI 10.1109/TMECH.2014.2358674
   Yin S, 2015, IEEE T IND ELECTRON, V62, P3852, DOI 10.1109/TIE.2015.2399396
   Yin S, 2015, P IEEE, V103, P143, DOI 10.1109/JPROC.2015.2388958
   Yin S, 2015, IEEE T IND ELECTRON, V62, P1651, DOI 10.1109/TIE.2014.2345331
   Yin S, 2015, IEEE T IND ELECTRON, V62, P657, DOI 10.1109/TIE.2014.2308133
   Yu M., 2014, P NIPS WORKSH LEARN
   Zeiler Matthew D, 2012, ARXIV12125701
   Zelenko D, 2003, J MACH LEARN RES, V3, P1083, DOI 10.1162/153244303322533205
   Zeng Daojian, 2014, P COLING
   Zhang M., 2006, P 21 INT C COMP LING
   Zhang Ye, 2015, ARXIV151003820
NR 39
TC 34
Z9 38
U1 3
U2 43
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0925-2312
EI 1872-8286
J9 NEUROCOMPUTING
JI Neurocomputing
PD MAY 19
PY 2016
VL 190
BP 1
EP 9
DI 10.1016/j.neucom.2015.12.091
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA DK3FN
UT WOS:000374802600001
DA 2020-02-19
ER

PT J
AU Leng, JW
   Jiang, PY
AF Leng, Jiewu
   Jiang, Pingyu
TI A deep learning approach for relationship extraction from interaction
   context in social manufacturing paradigm
SO KNOWLEDGE-BASED SYSTEMS
LA English
DT Article
DE Manufacturing relationship extraction; Deep learning; Stacked denoising
   auto-encoder; Social manufacturing; Interaction context
ID EXTENDED ENTERPRISE; FRAMEWORK; MODELS
AB There is an increasing unstructured text data produced in cross-enterprise social interaction media, forming a social interaction context that contains massive manufacturing relationships, which can be potentially used as decision support information for cross-enterprise manufacturing demand-capability matchmaking. How to enable decision-makers to capture these relationships remains a challenge. The text-based context contains high levels of noise and irrelevant information, causing both highly complexity and sparsity. Under this circumstance, instead of exploiting man-made features carefully optimized for the relationship extraction task, a deep learning model based on an improved stacked denoising auto-encoder on sentence-level features is proposed to extract manufacturing relationships among various named entities (e.g., enterprises, products, demands, and capabilities) underlying the text-based context. Experiment results show that the proposed approach can achieve a comparable performance with the state-of-the-art learning models, as well as a good practicality of its' web-based implementation in social manufacturing interaction context The ultimate goal of this study is to facilitate knowledge transferring and sharing in the context of enterprise social interaction, thereby supporting the integration of the resources and capabilities among different enterprises. (C) 2016 Elsevier B.V. All rights reserved.
C1 [Leng, Jiewu; Jiang, Pingyu] Xi An Jiao Tong Univ, State Key Lab Mfg Syst Engn, Xian 710049, Shaanxi, Peoples R China.
RP Jiang, PY (reprint author), Xi An Jiao Tong Univ, State Key Lab Mfg Syst Engn, Xian 710049, Shaanxi, Peoples R China.
EM pjiang@mail.xjtu.edu.cn
FU NSFCNational Natural Science Foundation of China [71571142]
FX The research work presented in this study is under the support of NSFC
   with Grant no. 71571142.
CR Agard B, 2004, INT J PROD RES, V42, P2955, DOI 10.1080/00207540410001691929
   Amalraj R, 2015, NEUROCOMPUTING, V168, P160, DOI 10.1016/j.neucom.2015.06.001
   Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Cao W, 2012, APPL MECH MATER, V220-223, P61, DOI 10.4028/www.scientific.net/AMM.220-223.61
   Chen M., 2012, P AS C SIGN SYST COM, P1
   Collobert R., 2008, P 25 ICML, V25, P160, DOI [10.1145/1390156.1390177, DOI 10.1145/1390156.1390177]
   Collobert R, 2011, J MACH LEARN RES, V12, P2493
   Culotta Aron, 2004, P 42 ANN M ASS COMP, P423, DOI DOI 10.3115/1218955.1219009
   Ebrahimi J., 2015, P 2015 C N AM CHAPT, P1244
   Esper TL, 2010, J ACAD MARKET SCI, V38, P5, DOI 10.1007/s11747-009-0135-3
   Fayyad U, 1996, AI MAG, V17, P37
   Feldman R., 2007, TEXT MINING HDB ADV
   Forman G., 2003, Journal of Machine Learning Research, V3, P1289, DOI 10.1162/153244303322753670
   Gangemi A, 2009, HDB ONTOLOGIES, P221, DOI DOI 10.1007/978-3-540-92673-3_10
   Grcar M., 2006, DATA SPARSITY ISSUES
   Harris Z., 1968, MATH STRUCTURES LANG
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hui SC, 2000, INFORM MANAGE-AMSTER, V38, P1, DOI 10.1016/S0378-7206(00)00051-3
   Jagdev HS, 1998, PROD PLAN CONTROL, V9, P216, DOI 10.1080/095372898234190
   Jiang CT, 2010, KNOWL-BASED SYST, V23, P302, DOI 10.1016/j.knosys.2009.11.010
   Jin W, 2007, PROC INT C TOOLS ART, P156, DOI 10.1109/ICTAI.2007.63
   Larochelle H, 2009, J MACH LEARN RES, V10, P1
   Lee H., 2009, P 26 ANN INT C MACH, P609, DOI DOI 10.1145/1553374.1553453
   Leng JW, 2014, INT J PROD RES, V52, P3614, DOI 10.1080/00207543.2013.879344
   Lindsay RK, 1999, J AM SOC INFORM SCI, V50, P574, DOI 10.1002/(SICI)1097-4571(1999)50:7<574::AID-ASI3>3.0.CO;2-Q
   Lopez-Ortega O, 2007, J INTELL MANUF, V18, P371, DOI 10.1007/s10845-007-0035-7
   Min B., 2013, HLT NAACL, P777, DOI DOI 10.1186/S40537-016-0043-6
   Mintz M., 2009, P JOINT C 47 ANN M A, V2, DOI [DOI 10.3115/1690219.1690287, 10.3115/1690219.1690287]
   Mnih A., 2008, ADV NEURAL INF PROCE, P1081
   Moh'd Abdelwadood, 2007, Journal of Computer Sciences, V3, P430, DOI 10.3844/jcssp.2007.430.435
   Naradowsky Jason, 2012, P 2012 JOINT C EMP M, P810
   Nothman J, 2013, ARTIF INTELL, V194, P151, DOI 10.1016/j.artint.2012.03.006
   Parsons L., 2004, ACM SIGKDD EXPLOR NE, V6, P90, DOI DOI 10.1145/1007730.1007731
   Ranzato MA, 2007, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2007.383157
   Seewald AK, 2007, ADV DATA ANAL CLASSI, V1, P221, DOI 10.1007/S11634-007-0012-1
   Suchanek Fabian M, 2006, P 12 ACM SIGKDD INT, P712, DOI DOI 10.1145/1150402.1150492
   Turian J., 2012, WORD REPRESENTATIONS, P384
   Vincent P, 2008, P 25 INT C MACH LEAR, P1096, DOI DOI 10.1145/1390156.1390294
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Wang K, 2007, J INTELL MANUF, V18, P487, DOI 10.1007/s10845-007-0053-5
   Wang SH, 2015, INT J IMAG SYST TECH, V25, P153, DOI 10.1002/ima.22132
   Weichselbraun A, 2014, KNOWL-BASED SYST, V69, P78, DOI 10.1016/j.knosys.2014.04.039
   Wu DZ, 2013, INT CONF ENG DES
   Yang GL, 2016, MULTIMED TOOLS APPL, V75, P15601, DOI 10.1007/s11042-015-2649-7
   Yao X, 1993, Int J Neural Syst, V4, P203, DOI 10.1142/S0129065793000171
   Zeng D., 2014, COLING, P2335
   Zhang CY, 2015, KNOWL-BASED SYST, V83, P128, DOI 10.1016/j.knosys.2015.03.017
   Zhang W, 2009, EXPERT SYST APPL, V36, P9333, DOI 10.1016/j.eswa.2008.12.034
   Zhang YD, 2014, J FOOD ENG, V143, P167, DOI 10.1016/j.jfoodeng.2014.07.001
   Zhou GD, 2011, J COMPUT SCI TECH-CH, V26, P45, DOI 10.1007/s11390-011-9414-9
NR 51
TC 45
Z9 45
U1 8
U2 69
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0950-7051
EI 1872-7409
J9 KNOWL-BASED SYST
JI Knowledge-Based Syst.
PD MAY 5
PY 2016
VL 100
BP 188
EP 199
DI 10.1016/j.knosys.2016.03.008
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA DK3EY
UT WOS:000374801000015
DA 2020-02-19
ER

PT J
AU Abu Alsheikh, M
   Niyato, D
   Lin, SW
   Tan, HP
   Han, Z
AF Abu Alsheikh, Mohammad
   Niyato, Dusit
   Lin, Shaowei
   Tan, Hwee-Pink
   Han, Zhu
TI Mobile Big Data Analytics Using Deep Learning and Apache Spark
SO IEEE NETWORK
LA English
DT Article
ID RECOGNITION
AB The proliferation of mobile devices, such as smartphones and Internet of Things gadgets, has resulted in the recent mobile big data era. Collecting mobile big data is unprofitable unless suitable analytics and learning methods are utilized to extract meaningful information and hidden patterns from data. This article presents an overview and brief tutorial on deep learning in mobile big data analytics and discusses a scalable learning framework over Apache Spark. Specifically, distributed deep learning is executed as an iterative MapReduce computing on many Spark workers. Each Spark worker learns a partial deep model on a partition of the overall mobile, and a master deep model is then built by averaging the parameters of all partial models. This Spark-based framework speeds up the learning of deep models consisting of many hidden layers and millions of parameters. We use a context-aware activity recognition application with a real-world dataset containing millions of samples to validate our framework and assess its speedup effectiveness.
C1 [Abu Alsheikh, Mohammad; Niyato, Dusit] Nanyang Technol Univ, Sch Comp Engn, Singapore, Singapore.
   [Abu Alsheikh, Mohammad] Inst Infocomm Res, Singapore, Singapore.
   [Lin, Shaowei] Singapore Univ Technol & Design, Singapore, Singapore.
   [Tan, Hwee-Pink] Singapore Management Univ, Informat Syst Practice, Singapore 178902, Singapore.
   [Han, Zhu] Univ Houston, Elect & Comp Engn Dept, Houston, TX 77004 USA.
   [Han, Zhu] Univ Houston, Dept Comp Sci, Houston, TX 77004 USA.
RP Abu Alsheikh, M (reprint author), Nanyang Technol Univ, Sch Comp Engn, Singapore, Singapore.; Abu Alsheikh, M (reprint author), Inst Infocomm Res, Singapore, Singapore.
EM stumyhaa@i2r.a-star.edu.sg; dniyato@ntu.edu.sg; shaowei_lin@sutd.edu.sg;
   hptan@smu.edu.sg; zhan2@uh.edu
RI Niyato, Dusit/A-3698-2011; Niyato, Dusit/Y-2769-2019; Abu Alsheikh,
   Mohammad/R-2196-2019
OI Abu Alsheikh, Mohammad/0000-0001-7269-2286; Niyato,
   Dusit/0000-0002-7442-7416; Tan, Hwee-Pink/0000-0002-8279-1429
FU A*STAR Computational Resource CentreAgency for Science Technology &
   Research (ASTAR); National Research Foundation of Korea (NRF) - Korean
   government (MSIP) [2014R1A5A1011478]; Singapore MOE [RG18/13, RG33/12];
   MOEMinistry of Education, SingaporeMinistry of Higher Education &
   Scientific Research (MHESR) [MOE2014-T2-2-015 ARC 4/15]; U.S. National
   Science FoundationNational Science Foundation (NSF) [US NSF
   ECCS-1547201, CCF-1456921, CNS-1443917, ECCS-1405121, NSFC61428101]
FX This work was supported in part by the A*STAR Computational Resource
   Centre through the use of its high-performance computing facilities. It
   was also supported in part by the National Research Foundation of Korea
   (NRF) grant funded by the Korean government (MSIP) (2014R1A5A1011478),
   Singapore MOE Tier 1 (RG18/13 and RG33/12) and MOE Tier 2
   (MOE2014-T2-2-015 ARC 4/15), and the U.S. National Science Foundation
   under Grants US NSF ECCS-1547201, CCF-1456921, CNS-1443917,
   ECCS-1405121, and NSFC61428101. The authors thank Ahmed Selim, Trinity
   College Dublin, for valuable discussions in the early stages of the
   study.
CR [Anonymous], 2016, CISC VIS NETW IND GL
   Apache Spark, 2016, AP SPARK LIGHTN FAST
   Dean J., 2012, ADV NEURAL INFORM PR, V25, P1223
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Klemperer P., 2004, PRINCETON PAPERBACKS
   Lane Nicholas D, 2015, P 16 INT WORKSH MOB, P117, DOI DOI 10.1145/2699343.2699349
   Lara OD, 2013, IEEE COMMUN SURV TUT, V15, P1192, DOI 10.1109/SURV.2012.110112.00192
   Lockhart J. W., 2011, P 5 INT WORKSH KNOWL, P25
   Ngiam J, 2011, IEEE INT C MACH LEAR, V28, P689, DOI DOI 10.1145/2647868
   Perera C, 2014, IEEE COMMUN SURV TUT, V16, P414, DOI 10.1109/SURV.2013.042313.00197
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   von Ahn L, 2008, SCIENCE, V321, P1465, DOI 10.1126/science.1160379
   Wang XY, 2015, IEEE WCNC, P1666, DOI 10.1109/WCNC.2015.7127718
   Weiss G. M., 2012, AAAI WKSP ACT CONT R
   Zhang KL, 2014, IEEE ACCESS, V2, P395, DOI 10.1109/ACCESS.2014.2319813
NR 15
TC 58
Z9 58
U1 4
U2 61
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 0890-8044
EI 1558-156X
J9 IEEE NETWORK
JI IEEE Netw.
PD MAY-JUN
PY 2016
VL 30
IS 3
BP 22
EP 29
PG 8
WC Computer Science, Hardware & Architecture; Computer Science, Information
   Systems; Engineering, Electrical & Electronic; Telecommunications
SC Computer Science; Engineering; Telecommunications
GA DW5JJ
UT WOS:000383679900005
DA 2020-02-19
ER

PT J
AU Wijaya, MA
   Fukawa, K
   Suzuki, H
AF Wijaya, Michael Andri
   Fukawa, Kazuhiko
   Suzuki, Hiroshi
TI Neural Network Based Transmit Power Control and Interference
   Cancellation for MIMO Small Cell Networks
SO IEICE TRANSACTIONS ON COMMUNICATIONS
LA English
DT Article
DE MIMO; small cells; intercell interference coordination; interference
   cancellation; neural network; system capacity
ID COORDINATION; CAPACITY; SYSTEMS
AB The random deployment of small cell base stations (BSs) causes the coverage areas of neighboring cells to overlap, which increases intercell interference and degrades the system capacity. This paper proposes a new intercell interference management (IIM) scheme to improve the system capacity in multiple-input multiple-output (MIMO) small cell networks. The proposed IIM scheme consists of both an interference cancellation (IC) technique on the receiver side, and a neural network (NN) based power control algorithm for intercell interference coordination (ICIC) on the transmitter side. In order to improve the system capacity, the NN power control optimizes downlink transmit power while IC eliminates interfering signals from received signals. Computer simulations compare the system capacity of the MIMO network with several ICIC algorithms: the NN, the greedy search, the belief propagation (BP), the distributed pricing (DP), and the maximum power, all of which can be combined with IC reception. Furthermore, this paper investigates the application of a multi-layered NN structure called deep learning and its pre-training scheme, into the mobile communication field. It is shown that the performance of NN is better than that of BP and very close to that of greedy search. The low complexity of the NN algorithm makes it suitable for IIM. It is also demonstrated that combining IC and sectorization of BSs acquires high capacity gain owing to reduced interference.
C1 [Wijaya, Michael Andri; Fukawa, Kazuhiko; Suzuki, Hiroshi] Tokyo Inst Technol, Tokyo 1528550, Japan.
RP Wijaya, MA (reprint author), Tokyo Inst Technol, Tokyo 1528550, Japan.
EM michaelaw@radio.ce.titech.ac.jp
CR ANDERSEN JB, 1995, IEEE COMMUN MAG, V33, P42, DOI 10.1109/35.339880
   Andrews J. G., 2006, IEEE 10 INT S CONS E, P1
   Andrews JG, 2012, IEEE J SEL AREA COMM, V30, P497, DOI 10.1109/JSAC.2012.120401
   Andrews JG, 2005, IEEE WIREL COMMUN, V12, P19, DOI 10.1109/MWC.2005.1421925
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Blum RS, 2003, IEEE J SEL AREA COMM, V21, P793, DOI 10.1109/JSAC.2003.810345
   Boudreau G, 2009, IEEE COMMUN MAG, V47, P74, DOI 10.1109/MCOM.2009.4907410
   Chandrasekhar V, 2009, IEEE T WIREL COMMUN, V8, P4316, DOI 10.1109/TWC.2009.081386
   Claussen H., 2005, 2005 IEEE 16th International Symposium on Personal, Indoor and Mobile Radio Communications (IEEE Cat. No. 05TH8889), P512
   Dai HY, 2004, IEEE T WIREL COMMUN, V3, P442, DOI 10.1109/TWC.2003.821168
   Deng L, 2013, FOUND TRENDS SIGNAL, V7, pI, DOI 10.1561/2000000039
   Erhan D., 2010, P AISTATS 2010 MAY, V9, P201
   Haykin Simon, 2002, ADAPTIVE FILTER THEO
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Huang JW, 2006, IEEE J SEL AREA COMM, V24, P1074, DOI 10.1109/JSAC.2006.872889
   Kashima T, 2006, IEEE J SEL AREA COMM, V24, P437, DOI 10.1109/JSAC.2005.862396
   Kaufman B., 2011, P IEEE ICC, P1, DOI DOI 10.1109/ICCW.2011.5963551
   Kosta C, 2013, IEEE COMMUN SURV TUT, V15, P973, DOI 10.1109/SURV.2012.121112.00037
   Kschischang FR, 2001, IEEE T INFORM THEORY, V47, P498, DOI 10.1109/18.910572
   Lee HC, 2010, IEEE ICC
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Rangan S, 2012, IEEE J SEL AREA COMM, V30, P631, DOI 10.1109/JSAC.2012.120412
   TETKO IV, 1995, J CHEM INF COMP SCI, V35, P826, DOI 10.1021/ci00027a006
   Zahir T, 2013, IEEE COMMUN SURV TUT, V15, P293, DOI 10.1109/SURV.2012.020212.00101
NR 24
TC 2
Z9 2
U1 0
U2 7
PU IEICE-INST ELECTRONICS INFORMATION COMMUNICATIONS ENG
PI TOKYO
PA KIKAI-SHINKO-KAIKAN BLDG, 3-5-8, SHIBA-KOEN, MINATO-KU, TOKYO, 105-0011,
   JAPAN
SN 0916-8516
EI 1745-1345
J9 IEICE T COMMUN
JI IEICE Trans. Commun.
PD MAY
PY 2016
VL E99B
IS 5
BP 1157
EP 1169
DI 10.1587/transcom.2015EBP3358
PG 13
WC Engineering, Electrical & Electronic; Telecommunications
SC Engineering; Telecommunications
GA DT5ZB
UT WOS:000381561100020
DA 2020-02-19
ER

PT J
AU Wei, JG
   Fang, Q
   Zheng, XY
   Lu, WH
   He, YQ
   Dang, JW
AF Wei, Jianguo
   Fang, Qiang
   Zheng, Xinyuan
   Lu, Wenhuan
   He, Yuqing
   Dang, Jianwu
TI Mapping ultrasound-based articulatory images and vowel sounds with a
   deep neural network framework
SO MULTIMEDIA TOOLS AND APPLICATIONS
LA English
DT Article
DE Articulatory Data; Restricted boltzmann machine; Deep denoising
   autoencoders; DNN
ID UNCERTAINTY; ACOUSTICS; MOVEMENTS
AB Constructing a mapping between articulatory movements and corresponding speech could significantly facilitate speech training and the development of speech aids for voice disorder patients. In this paper, we propose a novel deep learning framework for the creation of a bidirectional mapping between articulatory information and synchronized speech recorded using an ultrasound system. We created a dataset comprising six Chinese vowels and employed the Bimodal Deep Autoencoders algorithm based on the Restricted Boltzmann Machine (RBM) to learn the correlation between speech and ultrasound images of the tongue and the weight matrices of the data representations obtained. Speech and ultrasound images were then reconstructed from the extracted features. The reconstruction error of the ultrasound images created with our method was found to be less than that of the approach based on Principal Components Analysis (PCA). Further, the reconstructed speech approximated the original as the mean formants error (MFE) was small. Following acquisition of their shared representations using the RBM-based deep autoencoder, we carried out mapping between ultrasound images of the tongue and corresponding acoustics signals with a Deep Neural Network (DNN) framework using the revised Deep Denoising Autoencoders. The results obtained indicate that the performance of our proposed method is better than that of a Gaussian Mixture Model (GMM)-based method to which it was compared.
C1 [Wei, Jianguo; Lu, Wenhuan] Tianjin Univ, Sch Comp Software, Tianjin 300072, Peoples R China.
   [Fang, Qiang] Chinese Acad Social Sci, Beijing, Peoples R China.
   [Wei, Jianguo; Zheng, Xinyuan; Dang, Jianwu] Tianjin Univ, Tianjin Key Lab Cognit Computat & Applicat, Tianjin 300072, Peoples R China.
   [He, Yuqing] Tianjin Univ, Sch Elect Informat Engn, Tianjin 300072, Peoples R China.
   [Dang, Jianwu] Japan Adv Inst Sci & Technol, Nomi, Japan.
RP Lu, WH (reprint author), Tianjin Univ, Sch Comp Software, Tianjin 300072, Peoples R China.
EM Jianguo@tju.edu.cn; Wenhuan@tju.edu.cn; Dangjianwu@tju.edu.cn
FU National Basic Research Program of ChinaNational Basic Research Program
   of China [2013CB329305]; National Natural Science Foundation of
   ChinaNational Natural Science Foundation of China [61175016, 61304250]
FX This work was supported in part by the National Basic Research Program
   of China (No. 2013CB329305), and in part by grants from the National
   Natural Science Foundation of China (No. 61175016, No. 61304250).
CR Badino L, 2012, IEEE W SP LANG TECH, P370, DOI 10.1109/SLT.2012.6424252
   Ben-Youssef Atef, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P4573, DOI 10.1109/ICASSP.2014.6854468
   Ghosh PK, 2011, INT CONF ACOUST SPEE, P4624
   Hinton G., 2010, MOMENTUM, V9, P926
   Hiroya S, 2004, IEEE T SPEECH AUDI P, V12, P175, DOI 10.1109/TSA.2003.822636
   Hogden J, 2001, IEEE IMTC P, P1105, DOI 10.1109/IMTC.2001.928251
   Hogden J, 1996, J ACOUST SOC AM, V100, P1819, DOI 10.1121/1.416001
   Huang J, 2013, INT CONF ACOUST SPEE, P7596, DOI 10.1109/ICASSP.2013.6639140
   Kello CT, 2004, J ACOUST SOC AM, V116, P2354, DOI 10.1121/1.1715112
   LADEFOGED P, 1980, LANGUAGE, V56, P485, DOI 10.2307/414446
   Livescu K, 2007, INT CONF ACOUST SPEE, P621
   Nakamura Kenichi, 2006, AC SPEECH SIGN PROC, V1
   Nefian AV, 2002, AC SPEECH SIGN PROC, V2, pII
   Ngiam J, 2011, IEEE INT C MACH LEAR, V28, P689, DOI DOI 10.1145/2647868
   Papandreou G, 2009, IEEE T AUDIO SPEECH, V17, P423, DOI 10.1109/TASL.2008.2011515
   Richmond K, 2003, COMPUT SPEECH LANG, V17, P153, DOI 10.1016/S0885-2308(03)00005-6
   Richmond K, 2006, TRAJECTORY MIXTURE D
   Saenko K., 2004, 6 INT C MULT INT, P152
   SCHROETER J, 1992, ADV SPEECH SIGNAL PR, P231
   Schroeter J, 1994, IEEE T SPEECH AUDI P, V2, P133, DOI 10.1109/89.260356
   Simko J, 2009, SEQUENCING EMBODIED
   Suzuki S., 1998, ICSLP
   Toda T, 2008, SPEECH COMMUN, V50, P215, DOI 10.1016/j.specom.2007.09.001
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Wang LJ, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 1-2, P446
   Xie L, 2007, IEEE T MULTIMEDIA, V9, P500, DOI 10.1109/TMM.2006.888009
NR 26
TC 2
Z9 2
U1 0
U2 17
PU SPRINGER
PI DORDRECHT
PA VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN 1380-7501
EI 1573-7721
J9 MULTIMED TOOLS APPL
JI Multimed. Tools Appl.
PD MAY
PY 2016
VL 75
IS 9
BP 5223
EP 5245
DI 10.1007/s11042-015-3038-y
PG 23
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic
SC Computer Science; Engineering
GA DM8HE
UT WOS:000376601700023
DA 2020-02-19
ER

PT J
AU van Tulder, G
   de Bruijne, M
AF van Tulder, Gijs
   de Bruijne, Marleen
TI Combining Generative and Discriminative Representation Learning for Lung
   CT Analysis With Convolutional Restricted Boltzmann Machines
SO IEEE TRANSACTIONS ON MEDICAL IMAGING
LA English
DT Article
DE Deep learning; lung; machine learning; neural network; pattern
   recognition and classification; representation learning; restricted
   Boltzmann machine; segmentation; X-ray imaging and computed tomography
ID NEURAL-NETWORK; CLASSIFICATION
AB The choice of features greatly influences the performance of a tissue classification system. Despite this, many systems are built with standard, predefined filter banks that are not optimized for that particular application. Representation learning methods such as restricted Boltzmann machines may outperform these standard filter banks because they learn a feature description directly from the training data. Like many other representation learning methods, restricted Boltzmann machines are unsupervised and are trained with a generative learning objective; this allows them to learn representations from unlabeled data, but does not necessarily produce features that are optimal for classification. In this paper we propose the convolutional classification restricted Boltzmann machine, which combines a generative and a discriminative learning objective. This allows it to learn filters that are good both for describing the training data and for classification. We present experiments with feature learning for lung texture classification and airway detection in CT images. In both applications, a combination of learning objectives outperformed purely discriminative or generative learning, increasing, for instance, the lung tissue classification accuracy by 1 to 8 percentage points. This shows that discriminative learning can help an otherwise unsupervised feature learner to learn filters that are optimized for classification.
C1 [van Tulder, Gijs; de Bruijne, Marleen] Erasmus MC, Biomed Imaging Grp, NL-3000 CA Rotterdam, Netherlands.
   [de Bruijne, Marleen] Univ Copenhagen, Dept Comp Sci, DK-2100 Copenhagen, Denmark.
RP van Tulder, G (reprint author), Erasmus MC, Biomed Imaging Grp, NL-3000 CA Rotterdam, Netherlands.
EM g.vantulder@erasmusmc.nl
RI de Bruijne, Marleen/P-4234-2019; de Bruijne, Marleen/G-2662-2010
OI de Bruijne, Marleen/0000-0002-6328-902X; de Bruijne,
   Marleen/0000-0002-6328-902X
FU Netherlands Organization for Scientific Research (NWO)Netherlands
   Organization for Scientific Research (NWO)
FX This research is financed by the Netherlands Organization for Scientific
   Research (NWO). Asterisk indicates corresponding author.
CR Anthimopoulos M, 2014, IEEE ENG MED BIO, P6040, DOI 10.1109/EMBC.2014.6945006
   Asherov M., 2014, P SPIE MED IMAG
   Bengio Y., 2012, REPRESENTATION LEARN
   Bergstra J., 2010, PYTH SCI COMP C SCIP
   Berry J, 2011, INT CONF ACOUST SPEE, P557
   Dash J. K., 2015, P SPIE MED IMAG, V9414
   Depeursinge A., 2011, P SPIE MED IMAG
   Depeursinge A, 2012, LECT NOTES COMPUT SC, V7512, P517, DOI 10.1007/978-3-642-33454-2_64
   Depeursinge A, 2012, IEEE T INF TECHNOL B, V16, P665, DOI 10.1109/TITB.2012.2198829
   Depeursinge A, 2012, COMPUT MED IMAG GRAP, V36, P227, DOI 10.1016/j.compmedimag.2011.07.003
   Depeursinge A, 2011, LECT NOTES COMPUT SC, V6893, P231, DOI 10.1007/978-3-642-23626-6_29
   Desjardins G., 2008, EMPIRICAL EVALUATION
   Erhan D, 2010, J MACH LEARN RES, V11, P625
   Foncubierta-Rodrfguez Antonio, 2012, Medical Content-Based Retrieval for Clinical Decision Support. Second MICCAI International Workshop, MCBR-CDS 2011. Revised Selected Papers, P69, DOI 10.1007/978-3-642-28460-1_7
   Gao M., 2015, WORKSH DEEP LEARN ME
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE., 2010, PRACTICAL GUIDE TRAI
   Kumar D, 2015, 2015 12TH CONFERENCE ON COMPUTER AND ROBOT VISION CRV 2015, P133, DOI 10.1109/CRV.2015.25
   Larochelle H, 2012, J MACH LEARN RES, V13, P643
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee H., 2009, P 26 ANN INT C MACH, P609, DOI DOI 10.1145/1553374.1553453
   Lee H, 2011, COMMUN ACM, V54, P95, DOI 10.1145/2001269.2001295
   Leung T, 2001, INT J COMPUT VISION, V43, P29, DOI 10.1023/A:1011126920638
   Li Q, 2014, I C CONT AUTOMAT ROB, P844, DOI 10.1109/ICARCV.2014.7064414
   Li Q, 2013, IEEE ENG MED BIO, P6079, DOI 10.1109/EMBC.2013.6610939
   Lo SCB, 1995, IEEE T MED IMAGING, V14, P711, DOI 10.1109/42.476112
   Lo SCB, 1995, NEURAL NETWORKS, V8, P1201, DOI 10.1016/0893-6080(95)00061-5
   Norouzi M, 2009, PROC CVPR IEEE, P2727
   Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4
   Pedersen JH, 2009, J THORAC ONCOL, V4, P608, DOI 10.1097/JTO.0b013e3181a0d98f
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Petersen J, 2014, MED IMAGE ANAL, V18, P531, DOI 10.1016/j.media.2014.02.004
   Roth HR, 2014, LECT NOTES COMPUT SC, V8673, P520, DOI 10.1007/978-3-319-10404-1_65
   Salakhutdinov R, 2012, NEURAL COMPUT, V24, P1967, DOI 10.1162/NECO_a_00311
   Saxe A.M., 2011, P INT C MACH LEARN
   Schlegl Thomas, 2015, Inf Process Med Imaging, V24, P437, DOI 10.1007/978-3-319-19992-4_34
   SCHMAH T., 2008, ADV NEURAL INFORM PR, P1409
   Schmid C, 2001, PROC CVPR IEEE, P39
   Shen Wei, 2015, Inf Process Med Imaging, V24, P588, DOI 10.1007/978-3-319-19992-4_46
   Shin S., 2014, P SPIE MED IMAG, V9035
   Song Y, 2015, IEEE T MED IMAGING, V34, P1362, DOI 10.1109/TMI.2015.2393954
   Song Y, 2014, I S BIOMED IMAGING, P1023, DOI 10.1109/ISBI.2014.6868047
   Song Y, 2015, MED IMAGE ANAL, V22, P102, DOI 10.1016/j.media.2015.03.003
   Song Y, 2013, LECT NOTES COMPUT SC, V8150, P452, DOI 10.1007/978-3-642-40763-5_56
   Song Y, 2013, IEEE T MED IMAGING, V32, P797, DOI 10.1109/TMI.2013.2241448
   van Tulder G, 2014, LECT NOTES COMPUT SC, V8848, P47, DOI 10.1007/978-3-319-13972-2_5
   Varma M, 2009, IEEE T PATTERN ANAL, V31, P2032, DOI 10.1109/TPAMI.2008.182
NR 47
TC 46
Z9 46
U1 5
U2 38
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 0278-0062
EI 1558-254X
J9 IEEE T MED IMAGING
JI IEEE Trans. Med. Imaging
PD MAY
PY 2016
VL 35
IS 5
SI SI
BP 1262
EP 1272
DI 10.1109/TMI.2016.2526687
PG 11
WC Computer Science, Interdisciplinary Applications; Engineering,
   Biomedical; Engineering, Electrical & Electronic; Imaging Science &
   Photographic Technology; Radiology, Nuclear Medicine & Medical Imaging
SC Computer Science; Engineering; Imaging Science & Photographic
   Technology; Radiology, Nuclear Medicine & Medical Imaging
GA DL3RI
UT WOS:000375550500011
PM 26886968
OA Green Published
DA 2020-02-19
ER

PT J
AU Albarqouni, S
   Baur, C
   Achilles, F
   Belagiannis, V
   Demirci, S
   Navab, N
AF Albarqouni, Shadi
   Baur, Christoph
   Achilles, Felix
   Belagiannis, Vasileios
   Demirci, Stefanie
   Navab, Nassir
TI AggNet: Deep Learning From Crowds for Mitosis Detection in Breast Cancer
   Histology Images
SO IEEE TRANSACTIONS ON MEDICAL IMAGING
LA English
DT Article
DE Aggregation; crowdsourcing; deep learning; gamification; online learning
AB The lack of publicly available ground-truth data has been identified as the major challenge for transferring recent developments in deep learning to the biomedical imaging domain. Though crowdsourcing has enabled annotation of large scale databases for real world images, its application for biomedical purposes requires a deeper understanding and hence, more precise definition of the actual annotation task. The fact that expert tasks are being outsourced to non-expert users may lead to noisy annotations introducing disagreement between users. Despite being a valuable resource for learning annotation models from crowdsourcing, conventional machine-learning methods may have difficulties dealing with noisy annotations during training. In this manuscript, we present a new concept for learning from crowds that handle data aggregation directly as part of the learning process of the convolutional neural network (CNN) via additional crowdsourcing layer (AggNet). Besides, we present an experimental study on learning from crowds designed to answer the following questions. 1) Can deep CNN be trained with data collected from crowdsourcing? 2) How to adapt the CNN to train on multiple types of annotation datasets (ground truth and crowd-based)? 3) How does the choice of annotation and aggregation affect the accuracy? Our experimental setup involved Annot8, a self-implemented web-platform based on Crowdflower API realizing image annotation tasks for a publicly available biomedical image database. Our results give valuable insights into the functionality of deep CNN learning from crowd annotations and prove the necessity of data aggregation integration.
C1 [Albarqouni, Shadi; Baur, Christoph; Achilles, Felix; Belagiannis, Vasileios; Demirci, Stefanie; Navab, Nassir] Tech Univ Munich, Chair Comp Aided Med Procedure, D-85748 Munich, Germany.
   [Albarqouni, Shadi] Deutsch Zentrum Neurodegenerat Erkrankungen, D-53175 Bonn, Germany.
   [Belagiannis, Vasileios] Univ Oxford, Visual Geometry Grp, Oxford, England.
   [Navab, Nassir] Johns Hopkins Univ, Whiting Sch Engn, Baltimore, MD 21218 USA.
RP Albarqouni, S (reprint author), Tech Univ Munich, Chair Comp Aided Med Procedure, D-85748 Munich, Germany.
EM shadi.albarqouni@tum.de
RI Peters, Terry M/K-6853-2013; Baur, Christoph/K-8109-2017; Albarqouni,
   Shadi/H-5441-2019; Belagiannis, Vasileios/W-2665-2019
OI Peters, Terry M/0000-0003-1440-7488; Baur,
   Christoph/0000-0002-4262-5864; Albarqouni, Shadi/0000-0003-2157-2211;
   Belagiannis, Vasileios/0000-0003-0960-8453
CR Aroyo L, 2015, AI MAG, V36, P15, DOI 10.1609/aimag.v36i1.2564
   Belagiannis V, 2015, IEEE I CONF COMP VIS, P2830, DOI 10.1109/ICCV.2015.324
   Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16
   Carneiro G, 2012, IEEE T IMAGE PROCESS, V21, P968, DOI 10.1109/TIP.2011.2169273
   Celi L. A., 2014, J MED INTERNET RES, V16
   Ciresan DC, 2013, LECT NOTES COMPUT SC, V8150, P411, DOI 10.1007/978-3-642-40763-5_51
   Eigen D., 2014, ADV NEUR INF P SYST
   Estelles-Arolas E, 2012, J INF SCI, V38, P189, DOI 10.1177/0165551512437638
   Foncubierta Rodriguez  A., 2012, P ACM MULT 2012 WORK, P9, DOI [10.1145/2390803.2390808, DOI 10.1145/2390803.2390808]
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Gurari D, 2015, IEEE WINT CONF APPL, P1169, DOI 10.1109/WACV.2015.160
   Howe J, 2006, WIRED MAGAZINE, V14, P1, DOI DOI 10.1086/599595
   Inel O, 2014, LECT NOTES COMPUT SC, V8797, P486, DOI 10.1007/978-3-319-11915-1_31
   Kleemann F. G, 2008, SCI TECHNOLOGY INNOV, V4
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kuncheva LI, 2003, PATTERN ANAL APPL, V6, P22, DOI 10.1007/s10044-002-0173-7
   LeCun Y, 1998, LECT NOTES COMPUT SC, V1524, P9
   Lee K., 2010, P 19 INT C WORLD WID, V10, P1139
   Liu FJ, 2015, LECT NOTES COMPUT SC, V9351, P349, DOI 10.1007/978-3-319-24574-4_42
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Macenko M, 2009, 2009 IEEE INTERNATIONAL SYMPOSIUM ON BIOMEDICAL IMAGING: FROM NANO TO MACRO, VOLS 1 AND 2, P1107, DOI 10.1109/ISBI.2009.5193250
   Maier-Hein L, 2014, LECT NOTES COMPUT SC, V8674, P349, DOI 10.1007/978-3-319-10470-6_44
   Mavandadi S, 2012, GAMES HEALTH J, V1, P373, DOI 10.1089/g4h.2012.0054
   Nair V, 2010, ICML, V27, P807, DOI DOI 10.0RG/PAPERS/432.PDF
   Hung NQV, 2013, LECT NOTES COMPUT SC, V8181, P1, DOI 10.1007/978-3-642-41154-0_1
   Raykar V.C., 2011, ADV NEURAL INFORM PR, V24, P1809
   Raykar VC, 2010, J MACH LEARN RES, V11, P1297
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Sheng V. S., 2008, P 14 ACM SIGKDD INT, P614, DOI DOI 10.1145/1401890.1401965
   Vedaldi A., 2015, P ACM INT C MULT
   Venanzi M, 2014, WWW'14: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P155, DOI 10.1145/2566486.2567989
   Veta M, 2015, MED IMAGE ANAL, V20, P237, DOI 10.1016/j.media.2014.11.010
   Volpi D, 2015, INT J COMPUT ASS RAD, V10, P773, DOI 10.1007/s11548-015-1217-y
   von Ahn L, 2008, SCIENCE, V321, P1465, DOI 10.1126/science.1160379
   von Ahn L, 2006, COMPUTER, V39, P92, DOI 10.1109/MC.2006.196
   Whitehill J., 2009, ADV NEURAL INFORM PR, P2035
   Xie YP, 2015, LECT NOTES COMPUT SC, V9351, P358, DOI 10.1007/978-3-319-24574-4_43
   Yu B, 2013, J MED INTERNET RES, V15, DOI 10.2196/jmir.2513
NR 38
TC 129
Z9 130
U1 3
U2 64
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 0278-0062
EI 1558-254X
J9 IEEE T MED IMAGING
JI IEEE Trans. Med. Imaging
PD MAY
PY 2016
VL 35
IS 5
SI SI
BP 1313
EP 1321
DI 10.1109/TMI.2016.2528120
PG 9
WC Computer Science, Interdisciplinary Applications; Engineering,
   Biomedical; Engineering, Electrical & Electronic; Imaging Science &
   Photographic Technology; Radiology, Nuclear Medicine & Medical Imaging
SC Computer Science; Engineering; Imaging Science & Photographic
   Technology; Radiology, Nuclear Medicine & Medical Imaging
GA DL3RI
UT WOS:000375550500015
PM 26891484
DA 2020-02-19
ER

PT J
AU Chen, LJ
   Qu, H
   Zhao, JH
   Chen, BD
   Principe, JC
AF Chen, Liangjun
   Qu, Hua
   Zhao, Jihong
   Chen, Badong
   Principe, Jose C.
TI Efficient and robust deep learning with Correntropy-induced loss
   function
SO NEURAL COMPUTING & APPLICATIONS
LA English
DT Article
DE Deep learning; Correntropy; Stacked auto-encoders; Unsupervised feature
   learning
AB Deep learning systems aim at using hierarchical models to learning high-level features from low-level features. The progress in deep learning is great in recent years. The robustness of the learning systems with deep architectures is however rarely studied and needs further investigation. In particular, the mean square error (MSE), a commonly used optimization cost function in deep learning, is rather sensitive to outliers (or impulsive noises). Robust methods are needed to improve the learning performance and immunize the harmful influences caused by outliers which are pervasive in real-world data. In this paper, we propose an efficient and robust deep learning model based on stacked auto-encoders and Correntropy-induced loss function (CLF), called CLF-based stacked auto-encoders (CSAE). CLF as a nonlinear measure of similarity is robust to outliers and can approximate different norms (from to ) of data. Essentially, CLF is an MSE in reproducing kernel Hilbert space. Different from conventional stacked auto-encoders, which use, in general, the MSE as the reconstruction loss and KL divergence as the sparsity penalty term, the reconstruction loss and sparsity penalty term in CSAE are both built with CLF. The fine-tuning procedure in CSAE is also based on CLF, which can further enhance the learning performance. The excellent and robust performance of the proposed model is confirmed by simulation experiments on MNIST benchmark dataset.
C1 [Chen, Liangjun; Qu, Hua; Zhao, Jihong; Chen, Badong; Principe, Jose C.] Xi An Jiao Tong Univ, Sch Elect & Informat Engn, Xian 710049, Peoples R China.
   [Principe, Jose C.] Univ Florida, Dept Elect & Comp Engn, Gainesville, FL 32611 USA.
RP Chen, BD (reprint author), Xi An Jiao Tong Univ, Sch Elect & Informat Engn, Xian 710049, Peoples R China.
EM cljun27@stu.xjtu.edu.cn; qh@mail.xjtu.edu.cn; xjtuxtyjy@gmail.com;
   chenbd@mail.xjtu.edu.cn
FU 973 ProgramNational Basic Research Program of China [2015CB351703]; 863
   Project [2014AA01A701]; National Natural Science Foundation of
   ChinaNational Natural Science Foundation of China [61372152, 61371087]
FX This work was supported by 973 Program (No. 2015CB351703), 863 Project
   (No. 2014AA01A701) and National Natural Science Foundation of China
   (Nos. 61372152, 61371087).
CR Ahmed A, 2008, LECT NOTES COMPUT SC, V5304, P69, DOI 10.1007/978-3-540-88690-7_6
   Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Chen BD, 2014, IEEE SIGNAL PROC LET, V21, P880, DOI 10.1109/LSP.2014.2319308
   Chen BD, 2012, IEEE SIGNAL PROC LET, V19, P491, DOI 10.1109/LSP.2012.2204435
   Coates A, 2011, INT C ART INT STAT
   Fidler S, 2006, IEEE T PATTERN ANAL, V28, P337, DOI 10.1109/TPAMI.2006.46
   Freund Y., 1992, ADV NEURAL INFORM PR, P912
   Gunduz A, 2009, SIGNAL PROCESS, V89, P14, DOI 10.1016/j.sigpro.2008.07.005
   He R, 2011, NEURAL COMPUT, V23, P2074, DOI 10.1162/NECO_a_00155
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Larochelle Hugo, 2007, P 24 INT C MACH LEAR
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Liu W., 2010, KERNEL ADAPTIVE FILT
   Liu WF, 2006, IEEE IJCNN, P4919
   Liu WF, 2007, IEEE T SIGNAL PROCES, V55, P5286, DOI 10.1109/TSP.2007.896065
   Martinez AM, 2002, IEEE T PATTERN ANAL, V24, P748, DOI 10.1109/TPAMI.2002.1008382
   Mobahi H., 2009, P 26 ANN INT C MACH
   Pandey G, 2014, P 17 INT C ART INT S, V33
   PRINCIPE JC, 2000, UNSUPERVISED ADAPTIV
   Qi Yu, 2014, AC SPEECH SIGN PROC
   Ranzato M, 2008, ADV NEURAL INFORM PR, P1185
   Ranzato M., 2006, ADV NEURAL INFORM PR, V19, P1137
   Ribeiro B., 2013, PROGR PATTERN RECOGN, V8258, P182
   Seth S, 2008, AC SPEECH SIGN PROC
   Singh A, 2010, Proceedings 2010 IEEE International Test Conference (ITC 2010), DOI 10.1109/TEST.2010.5699305
   Singh A, 2010, NEUR NETW IJCNN 2010
   Singh A, 2014, PATTERN RECOGN, V47, P441, DOI 10.1016/j.patcog.2013.07.017
   Vincent P, 2008, P 25 INT C MACH LEAR, P1096, DOI DOI 10.1145/1390156.1390294
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Weston Jason, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P639, DOI 10.1007/978-3-642-35289-8_34
   Xie J., 2012, P ADV NEUR INF PROC, P350
   Yu WC, 2015, NEUROCOMPUTING, V149, P308, DOI 10.1016/j.neucom.2014.03.077
   Zhao SL, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P2012, DOI 10.1109/IJCNN.2011.6033473
NR 35
TC 22
Z9 25
U1 1
U2 13
PU SPRINGER LONDON LTD
PI LONDON
PA 236 GRAYS INN RD, 6TH FLOOR, LONDON WC1X 8HL, ENGLAND
SN 0941-0643
EI 1433-3058
J9 NEURAL COMPUT APPL
JI Neural Comput. Appl.
PD MAY
PY 2016
VL 27
IS 4
BP 1019
EP 1031
DI 10.1007/s00521-015-1916-x
PG 13
WC Computer Science, Artificial Intelligence
SC Computer Science
GA DK0CA
UT WOS:000374578800018
DA 2020-02-19
ER

PT J
AU Gori, M
   Lippi, M
   Maggini, M
   Melacci, S
AF Gori, Marco
   Lippi, Marco
   Maggini, Marco
   Melacci, Stefano
TI Semantic video labeling by developmental visual agents
SO COMPUTER VISION AND IMAGE UNDERSTANDING
LA English
DT Article
DE Learning from constraints; Life-long learning; Scene understanding;
   Motion estimation; Deep learning
ID OBJECT DETECTORS; FEATURES; RECOGNITION
AB In the recent years, computer vision has been undergoing a period of great development, testified by the many successful applications that are currently available in a variety of industrial products. Yet, when we come to the most challenging and foundational problem of building autonomous agents capable of performing scene understanding in unrestricted videos, there is still a lot to be done. In this paper we focus on semantic labeling of video streams, in which a set of semantic classes must be predicted for each pixel of the video. We propose to attack the problem from bottom to top, by introducing Developmental Visual Agents (DVAs) as general purpose visual systems that can progressively acquire visual skills from video data and experience, by continuously interacting with the environment and following lifelong learning principles. DVAs gradually develop a hierarchy of architectural stages, from unsupervised feature extraction to the symbolic level, where supervisions are provided by external users, pixel-wise. Differently from classic machine learning algorithms applied to computer vision, which typically employ huge datasets of fully labeled images to perform recognition tasks, DVAs can exploit even a few supervisions per semantic category, by enforcing coherence constraints based on motion estimation. Experiments on different vision tasks, performed on a variety of heterogeneous visual worlds, confirm the great potential of the proposed approach. (C) 2016 Elsevier Inc. All rights reserved.
C1 [Gori, Marco; Maggini, Marco; Melacci, Stefano] Univ Siena, Dept Informat Engn & Math, Via Roma 56, I-53100 Siena, Italy.
   [Lippi, Marco] Univ Bologna, Dept Comp Sci & Engn, Viale Risorgimento 2, I-40136 Bologna, Italy.
RP Lippi, M (reprint author), Univ Bologna, Dept Comp Sci & Engn, Viale Risorgimento 2, I-40136 Bologna, Italy.
EM marco@diism.unisi.it; marco.lippi3@unibo.it; maggini@diism.unisi.it;
   mela@diism.unisi.it
OI Lippi, Marco/0000-0002-9663-1071; MELACCI, STEFANO/0000-0002-0415-0888
CR Alvarez JM, 2012, LECT NOTES COMPUT SC, V7584, P586, DOI 10.1007/978-3-642-33868-7_58
   Anselmi F., 2015, THEOR COMPU IN PRESS
   Bekel H, 2004, LECT NOTES COMPUT SC, V3175, P447
   Bengio Y., 2009, P 26 ANN INT C MACH, P41, DOI DOI 10.1145/1553374.1553380
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bo L., 2013, EXPT ROBOTICS, P387, DOI DOI 10.1007/978-3-319-00065-7
   Bouvrie J., 2009, ADV NEURAL INFORM PR, P162
   Branson S, 2010, LECT NOTES COMPUT SC, V6314, P438, DOI 10.1007/978-3-642-15561-1_32
   Brostow GJ, 2008, LECT NOTES COMPUT SC, V5302, P44, DOI 10.1007/978-3-540-88682-2_5
   Brostow GJ, 2009, PATTERN RECOGN LETT, V30, P88, DOI 10.1016/j.patrec.2008.04.005
   Brox T, 2010, LECT NOTES COMPUT SC, V6315, P282, DOI 10.1007/978-3-642-15555-0_21
   Bulo SR, 2014, PROC CVPR IEEE, P81, DOI 10.1109/CVPR.2014.18
   Coates A., 2011, INT C ART INT STAT, V15, P215, DOI 10.1177/1753193410390845
   Coates A., 2011, ADV NEURAL INFORM PR, P2528
   Coates A., 2011, P 28 INT C MACH LEAR, V28, P921
   Couprie C, 2014, J MACH LEARN RES, V15, P3489
   Dalal N, 2005, PROC CVPR IEEE, P886
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Diligenti M, 2012, MACH LEARN, V86, P57, DOI 10.1007/s10994-011-5243-x
   Dosovitskiy A., 2014, ADV NEURAL INFORM PR, V27, P766
   Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   Frohlich B., 2013, LNCS, P218, DOI DOI 10.1007/978-3-642-37331-2_17
   FUKUSHIMA K, 1988, NEURAL NETWORKS, V1, P119, DOI 10.1016/0893-6080(88)90014-7
   Gnecco G, 2015, NEURAL COMPUT, V27, P388, DOI 10.1162/NECO_a_00686
   Gori M., 2014, CVPR WORKSH, P712
   Gori M, 2013, IEEE T NEUR NET LEAR, V24, P825, DOI 10.1109/TNNLS.2013.2241787
   Gori M, 2012, LECT NOTES COMPUT SC, V7577, P864, DOI 10.1007/978-3-642-33783-3_62
   HORN BKP, 1981, P SOC PHOTO-OPT INST, V281, P319
   HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837
   Hui K.Y., 2013, P 30 INT C MACH LEAR, P352
   Kae A, 2014, PROC CVPR IEEE, P272, DOI 10.1109/CVPR.2014.42
   Kavukcuoglu K., 2010, ADV NIPS
   Kontschieder P., 2014, STRUCTURED LABELS RA, P1
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   Ladicky L, 2010, LECT NOTES COMPUT SC, V6314, P424, DOI 10.1007/978-3-642-15561-1_31
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee YJ, 2011, IEEE I CONF COMP VIS, P1995, DOI 10.1109/ICCV.2011.6126471
   Leistner C., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2753, DOI 10.1109/CVPR.2011.5995475
   Li FX, 2012, PROC CVPR IEEE, P2424, DOI 10.1109/CVPR.2012.6247956
   Li LJ, 2010, INT J COMPUT VISION, V88, P147, DOI 10.1007/s11263-009-0265-6
   Lim J, 2004, ADV NEURAL INFORM PR, P793
   Lin T., 2014, P 31 INT C MACH LEAR, P1323
   Liu C, 2011, IEEE T PATTERN ANAL, V33, P2368, DOI 10.1109/TPAMI.2011.131
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mairal J., 2014, ADV NEURAL INFORM PR, V27, P2627
   Marszalek M., 2009, P IEEE C COMP VIS PA, P2929, DOI DOI 10.1109/CVPR.2009.5206557
   Melacci S, 2013, LECT NOTES COMPUT SC, V8157, P101, DOI 10.1007/978-3-642-41184-7_11
   Melacci S, 2012, IEEE T NEUR NET LEAR, V23, P1849, DOI 10.1109/TNNLS.2012.2216899
   Melacci S, 2011, J MACH LEARN RES, V12, P1149
   Miclut B, 2014, LECT NOTES COMPUT SC, V8753, P736, DOI 10.1007/978-3-319-11752-2_62
   Morel JM, 2009, SIAM J IMAGING SCI, V2, P438, DOI 10.1137/080732730
   Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222
   Papazoglou A, 2013, IEEE I CONF COMP VIS, P1777, DOI 10.1109/ICCV.2013.223
   Ramanan D, 2006, IEEE T PATTERN ANAL, V28, P1319, DOI 10.1109/TPAMI.2006.155
   Romero A, 2015, ICLR, P1
   Salah R, 2011, P 28 INT C MACH LEAR, P833, DOI 10.1016/j.wasman.2010.10.006
   Schulter S., 2013, P BRIT MACH VIS C BM, P1
   Sermanet P., 2014, INT C LEARN REPR ICL, P1
   Serre T, 2005, PROC CVPR IEEE, P994
   Sohn K., 2012, INT C MACH LEARN ICM
   Sutherland W. A., 2009, INTRO METRIC TOPOLOG
   Tighe J, 2014, PROC CVPR IEEE, P3748, DOI 10.1109/CVPR.2014.479
   Tighe J, 2013, INT J COMPUT VISION, V101, P329, DOI 10.1007/s11263-012-0574-z
   Vijayanarasimhan S, 2014, INT J COMPUT VISION, V108, P97, DOI 10.1007/s11263-014-0721-9
   Vincent P, 2008, P 25 INT C MACH LEAR, P1096, DOI DOI 10.1145/1390156.1390294
   Wiskott L, 2002, NEURAL COMPUT, V14, P715, DOI 10.1162/089976602317318938
   Wold H., 1975, PATH MODELS LATENT V
   Xu CL, 2012, LECT NOTES COMPUT SC, V7577, P626, DOI 10.1007/978-3-642-33783-3_45
   YOSINSKI J, 2014, ADV NEURAL INFORM PR, P3320, DOI DOI 10.1109/IJCNN.2016.7727519
   Zou W.Y., 2012, ADV NEURAL INFORM PR, P3212
NR 71
TC 3
Z9 3
U1 0
U2 14
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1077-3142
EI 1090-235X
J9 COMPUT VIS IMAGE UND
JI Comput. Vis. Image Underst.
PD MAY
PY 2016
VL 146
BP 9
EP 26
DI 10.1016/j.cviu.2016.02.011
PG 18
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
SC Computer Science; Engineering
GA DJ7YX
UT WOS:000374430200002
DA 2020-02-19
ER

PT J
AU Xu, GS
   Wu, HZ
   Shi, YQ
AF Xu, Guanshuo
   Wu, Han-Zhou
   Shi, Yun-Qing
TI Structural Design of Convolutional Neural Networks for Steganalysis
SO IEEE SIGNAL PROCESSING LETTERS
LA English
DT Article
DE Convolutional neural networks (CNNs); deep learning; forensics;
   steganalysis
AB Recent studies have indicated that the architectures of convolutional neural networks (CNNs) tailored for computer vision may not be best suited to image steganalysis. In this letter, we report a CNN architecture that takes into account knowledge of steganalysis. In the detailed architecture, we take absolute values of elements in the feature maps generated from the first convolutional layer to facilitate and improve statistical modeling in the subsequent layers; to prevent overfitting, we constrain the range of data values with the saturation regions of hyperbolic tangent (TanH) at early stages of the networks and reduce the strength of modeling using 1 x 1 convolutions in deeper layers. Although it learns from only one type of noise residual, the proposed CNN is competitive in terms of detection performance compared with the SRM with ensemble classifiers on the BOSSbase for detecting S-UNIWARD and HILL. The results have implied that well-designed CNNs have the potential to provide a better detection performance in the future.
C1 [Xu, Guanshuo; Shi, Yun-Qing] New Jersey Inst Technol, Dept Elect & Comp Engn, Newark, NJ 07102 USA.
   [Wu, Han-Zhou] Southwest Jiaotong Univ, Chengdu 611756, Peoples R China.
   [Wu, Han-Zhou] New Jersey Inst Technol, Newark, NJ 07102 USA.
RP Xu, GS; Shi, YQ (reprint author), New Jersey Inst Technol, Dept Elect & Comp Engn, Newark, NJ 07102 USA.; Wu, HZ (reprint author), Southwest Jiaotong Univ, Chengdu 611756, Peoples R China.
EM gx3@njit.edu; h.wu.phd@ieee.org; shi@njit.edu
FU NJIT Faculty Seed Grant Initiative
FX This work was supported by NJIT Faculty Seed Grant Initiative.
CR Bas P., 2011, LNCS, V6958, P59, DOI DOI 10.1007/978-3-642-24178-9_5
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Chen JS, 2015, IEEE SIGNAL PROC LET, V22, P1849, DOI 10.1109/LSP.2015.2438008
   Chen L., 2014, P IWDW OCT, V9023, P559
   Denemark T., 2016, P SOC PHOTO-OPT INS, P14
   Denemark T, 2014, IEEE INT WORKS INFOR, P48, DOI 10.1109/WIFS.2014.7084302
   Fridrich J, 2012, IEEE T INF FOREN SEC, V7, P868, DOI 10.1109/TIFS.2012.2190402
   Glorot X., 2010, JLMR P TRACK, p[249, 2010], DOI DOI 10.1177/1753193409103364.
   He K., 2015, ARXIV151203385
   Holub V, 2014, EURASIP J INF SECUR, DOI 10.1186/1687-417X-2014-1
   Holub V, 2015, IEEE T INF FOREN SEC, V10, P219, DOI 10.1109/TIFS.2014.2364918
   Holub V, 2013, IEEE T INF FOREN SEC, V8, P1996, DOI 10.1109/TIFS.2013.2286682
   Holub V, 2012, IEEE INT WORKS INFOR, P234, DOI 10.1109/WIFS.2012.6412655
   Ioffe Sergey, 2015, ARXIV150203167
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Ker A. D., 2008, P SPIE ELECT IMAGING, V6819
   Kodovsky J., 2012, P SPIE MEDIA WATERMA, V8303
   Kodovsky J, 2012, IEEE T INF FOREN SEC, V7, P432, DOI 10.1109/TIFS.2011.2175919
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   LeCun Yann, 1990, ADV NEURAL INFORM PR, P396, DOI DOI 10.1111/DSU.12130
   Li B, 2014, IEEE IMAGE PROC, P4206, DOI 10.1109/ICIP.2014.7025854
   Li B, 2014, IEEE T INF FOREN SEC, V9, P1264, DOI 10.1109/TIFS.2014.2326954
   Luo Y, 2015, ISPRS International Workshop on Spatiotemporal Computing, P19, DOI 10.5194/isprsannals-II-4-W2-19-2015
   Nair V, 2010, ICML, V27, P807, DOI DOI 10.0RG/PAPERS/432.PDF
   Pevny T, 2010, LECT NOTES COMPUT SC, V6387, P161, DOI 10.1007/978-3-642-16435-4_13
   Pevny T, 2010, IEEE T INF FOREN SEC, V5, P215, DOI 10.1109/TIFS.2010.2045842
   Pibre L., 2016, P SOC PHOTO-OPT INS, P14
   Qian YL, 2015, PROC SPIE, V9409, DOI 10.1117/12.2083479
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sedighi V, 2016, IEEE T INF FOREN SEC, V11, P221, DOI 10.1109/TIFS.2015.2486744
   Shi YQ, 2012, LNCS, V2012, P63, DOI DOI 10.1007/978-3-642-36373-3
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tan S., 2014, P AS PAC SIGN INF PR, P1, DOI 10.1109/APSIPA.2014
   Tang W., 2014, P 2 ACM WORKSH INF H, P91, DOI [10.1145/2600918, DOI 10.1145/2600918.2600935]
   Tang WX, 2016, IEEE T INF FOREN SEC, V11, P734, DOI 10.1109/TIFS.2015.2507159
   Zou DK, 2006, 2006 IEEE International Conference on Multimedia and Expo - ICME 2006, Vols 1-5, Proceedings, P1365, DOI 10.1109/ICME.2006.262792
NR 37
TC 102
Z9 103
U1 11
U2 60
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1070-9908
EI 1558-2361
J9 IEEE SIGNAL PROC LET
JI IEEE Signal Process. Lett.
PD MAY
PY 2016
VL 23
IS 5
BP 708
EP 712
DI 10.1109/LSP.2016.2548421
PG 5
WC Engineering, Electrical & Electronic
SC Engineering
GA DK5ZS
UT WOS:000375000500001
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Yu, B
   Lane, I
   Chen, F
AF Yu, Bo
   Lane, Ian
   Chen, Fang
TI 3D Face Detection via Reconstruction Over Hierarchical Features for
   Single Face Situations
SO INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE
LA English
DT Article
DE Image segmentation; deep learning; reconstruction network
ID RECOGNITION
AB There are multiple challenges in face detection, including illumination conditions and diverse poses of the user. Prior works tend to detect faces by segmentation at pixel level, which are generally not computationally efficient. When people are sitting in the car, which can be regarded as single face situations, most face detectors fail to detect faces under various poses and illumination conditions. In this paper, we propose a simple but efficient approach for single face detection. We train a deep learning model that reconstructs face directly from input image by removing background and synthesizing 3D data for only the face region. We apply the proposed model to two public 3D face datasets, and obtain significant improvements in false rejection rate (FRR) of 4.6% (from 4.6% to 0.0%) and 21.7% (from 30.2% to 8.5%), respectively, compared with state-of-art performances in two datasets. Furthermore, we show that our reconstruction approach can be applied using 1/2 the time of a widely used real-time face detector. These results demonstrate that the proposed Reconstruction ConNet (RN) is both more accurate and efficient for real-time face detection than prior works.
C1 [Yu, Bo; Chen, Fang] Chinese Acad Sci, Inst Remote Sensing & Digital Earth, Key Lab Digital Earth Sci, Beijing 100101, Peoples R China.
   [Yu, Bo; Lane, Ian] Carnegie Mellon Univ, NASA Res Pk 23, Moffett Field, CA 94043 USA.
   [Chen, Fang] Chinese Acad Sci, Hainan Key Lab Earth Observat, Inst Remote Sensing & Digital Earth, Sanya 572029, Peoples R China.
RP Chen, F (reprint author), Chinese Acad Sci, Inst Remote Sensing & Digital Earth, Key Lab Digital Earth Sci, Beijing 100101, Peoples R China.; Chen, F (reprint author), Chinese Acad Sci, Hainan Key Lab Earth Observat, Inst Remote Sensing & Digital Earth, Sanya 572029, Peoples R China.
EM yubo@radi.ac.cn; chenfang@radi.ac.cn
FU Hundred Talents Program of Chinese Academy of SciencesChinese Academy of
   Sciences [Y34004101A]; comparative study on global environmental change
   using remote sensing technology [41120114001]; National Natural Science
   Foundation of Major International (regional) Collaborative Research
   Project; High Resolution Earth Observation Systems [14CNIC-032079-32]
FX This work has been supported by the Hundred Talents Program of Chinese
   Academy of Sciences (Y34004101A), and the comparative study on global
   environmental change using remote sensing technology (41120114001), the
   National Natural Science Foundation of Major International (regional)
   Collaborative Research Project and High Resolution Earth Observation
   Systems (14CNIC-032079-32). The authors are greatful to Bing Liu's
   generous help from Carnegie Mellon University in improving language
   quality of our paper.
CR Boureau YL, 2010, PROC CVPR IEEE, P2559, DOI 10.1109/CVPR.2010.5539963
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Colombo A, 2006, PATTERN RECOGN, V39, P444, DOI 10.1016/j.patcog.2005.09.009
   Garcia C, 2004, IEEE T PATTERN ANAL, V26, P1408, DOI 10.1109/TPAMI.2004.97
   Girshick R., 2014, IEEE C COMP VIS PATT
   Gupta SD, 2010, METHODS MOL BIOL, V589, P97, DOI 10.1109/SSIAI.2010.5483908
   Hg RI, 2012, 8TH INTERNATIONAL CONFERENCE ON SIGNAL IMAGE TECHNOLOGY & INTERNET BASED SYSTEMS (SITIS 2012), P42, DOI 10.1109/SITIS.2012.17
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Huynh Tri, 2013, Computer Vision - ACCV 2012 Workshops. ACCV 2012 International Workshops. Revised Selected Papers, P133, DOI 10.1007/978-3-642-37410-4_12
   LeCun Y, 1998, LECT NOTES COMPUT SC, V1524, P9
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Leung C. S., IEEE T NEURAL NETW, V21, P1232
   Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222
   Orr Genevieve B, 2003, NEURAL NETWORKS TRIC
   Pinheiro PH, ARXIV13062795
   Rognvaldsson TS, 1998, LECT NOTES COMPUT SC, V1524, P71
   Segundo MP, 2014, PATTERN RECOGN LETT, V50, P72, DOI 10.1016/j.patrec.2013.09.027
   Sutskever I., 2013, P 30 INT C MACH LEAR, P1139
   Vincent P, 2008, P 25 INT C MACH LEAR, P1096, DOI DOI 10.1145/1390156.1390294
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Wan SH, 2014, PATTERN RECOGN, V47, P1859, DOI 10.1016/j.patcog.2013.11.025
   Yin LJ, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P211
   Zhang HC, 2013, PATTERN RECOGN, V46, P1511, DOI 10.1016/j.patcog.2012.10.025
NR 24
TC 0
Z9 0
U1 0
U2 21
PU WORLD SCIENTIFIC PUBL CO PTE LTD
PI SINGAPORE
PA 5 TOH TUCK LINK, SINGAPORE 596224, SINGAPORE
SN 0218-0014
EI 1793-6381
J9 INT J PATTERN RECOGN
JI Int. J. Pattern Recognit. Artif. Intell.
PD MAY
PY 2016
VL 30
IS 4
AR 1655013
DI 10.1142/S0218001416550132
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA DK7GG
UT WOS:000375092600007
DA 2020-02-19
ER

PT J
AU Charalampous, K
   Gasteratos, A
AF Charalampous, Konstantinos
   Gasteratos, Antonios
TI On-line deep learning method for action recognition
SO PATTERN ANALYSIS AND APPLICATIONS
LA English
DT Article
DE Deep Learning; Spatio-temporal Features; L-1-norm minimization; ART;
   Viterbi; Action Recognition; Unsupervised Learning
ID INFERENCE; CORTEX
AB In this paper an unsupervised on-line deep learning algorithm for action recognition in video sequences is proposed. Deep learning models capable of deriving spatio-temporal data have been proposed in the past with remarkable results, yet, they are mostly restricted to building features from a short window length. The model presented here, on the other hand, considers the entire sample sequence and extracts the description in a frame-by-frame manner. Each computational node of the proposed paradigm forms clusters and computes point representatives, respectively. Subsequently, a first-order transition matrix stores and continuously updates the successive transitions among the clusters. Both the spatial and temporal information are concurrently treated by the Viterbi Algorithm, which maximizes a criterion based upon (a) the temporal transitions and (b) the similarity of the respective input sequence with the cluster representatives. The derived Viterbi path is the node's output, whereas the concatenation of nine vicinal such paths constitute the input to the corresponding upper level node. The engagement of ART and the Viterbi Algorithm in a Deep learning architecture, here, for the first time, leads to a substantially different approach for action recognition. Compared with other deep learning methodologies, in most cases, it is shown to outperform them, in terms of classification accuracy.
C1 [Charalampous, Konstantinos; Gasteratos, Antonios] Democritus Univ Thrace, Dept Prod & Management Engn, Vas Sofias 12,Bldg 1,Off 205, GR-67100 Xanthi, Greece.
RP Charalampous, K (reprint author), Democritus Univ Thrace, Dept Prod & Management Engn, Vas Sofias 12,Bldg 1,Off 205, GR-67100 Xanthi, Greece.
EM kchara@pme.duth.gr; agaster@pme.duth.gr
RI Gasteratos, Antonios/B-7796-2012
OI Gasteratos, Antonios/0000-0002-5421-0332
CR Andilla F. D., 2013, ADV NEURAL INFORM PR, P818
   [Anonymous], 2013, CORR
   Bazzani L., 2011, P 28 INT C MACH LEAR, P937
   Bellman R., 2003, DYNAMIC PROGRAMMING
   Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Candes EJ, 2006, COMMUN PUR APPL MATH, V59, P1207, DOI 10.1002/cpa.20124
   CARPENTER GA, 1990, NEURAL NETWORKS, V3, P129, DOI 10.1016/0893-6080(90)90085-Y
   CARPENTER GA, 1987, COMPUT VISION GRAPH, V37, P54, DOI 10.1016/S0734-189X(87)80014-2
   CARPENTER GA, 1987, APPL OPTICS, V26, P4919, DOI 10.1364/AO.26.004919
   Carpenter GA, 2010, NEURAL NETWORKS, V23, P435, DOI 10.1016/j.neunet.2009.07.025
   Chaquet JM, 2013, COMPUT VIS IMAGE UND, V117, P633, DOI 10.1016/j.cviu.2013.01.013
   Chen B, 2013, IEEE T PATTERN ANAL, V35, P1887, DOI 10.1109/TPAMI.2013.19
   Chen SSB, 2001, SIAM REV, V43, P129, DOI 10.1137/S003614450037906X
   Cheng B, 2010, IEEE T IMAGE PROCESS, V19, P858, DOI 10.1109/TIP.2009.2038764
   Chopra S, 2013, ICML WORKSH CHALL RE
   Denil M., 2013, ADV NEURAL INFORM PR, P2148
   Dollar P., 2005, Proceedings. 2nd Joint IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance (VS-PETS) (IEEE Cat. No. 05EX1178), P65
   Donoho DL, 2006, COMMUN PUR APPL MATH, V59, P797, DOI 10.1002/cpa.20132
   Fan JL, 2010, IEEE T NEURAL NETWOR, V21, P1610, DOI 10.1109/TNN.2010.2066286
   Fazl-Ersi E., 2012, J INTELLIGENT ROBOTI, P1
   George D., 2008, THESIS STANFORD
   Gorelick L, 2007, IEEE T PATTERN ANAL, V29, P2247, DOI 10.1109/TPAMI.2007.70711
   Griffiths TL, 2011, J MACH LEARN RES, V12, P1185
   Grossberg S, 2013, NEURAL NETWORKS, V37, P1, DOI [10.1016/j.neunet.2012.09.017, 10.1016/j.neunet.2011.10.011]
   Gulcehre Caglar, 2013, LEARNED NORM POOLING
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hoffman M., 2010, ADV NEURAL INFORM PR, P856
   Jain V., 2008, P ADV NEUR INF PROC, P769
   Jhuang H, 2007, IEEE I CONF COMP VIS, P1253
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Kavukcuoglu K., 2010, NIPS, V1, P5
   Klaser Alexander, 2008, BMVC, P275
   Kostavelis I, 2012, PATTERN RECOGN LETT, V33, P670, DOI 10.1016/j.patrec.2011.11.017
   Kusy B, 2009, 2009 INTERNATIONAL CONFERENCE ON INFORMATION PROCESSING IN SENSOR NETWORKS (IPSN 2009), P109
   Laptev I, 2008, PROC CVPR IEEE, P3222
   Larochelle H., 2010, P ADV NEUR INF PROC, P1243
   Le Q. V., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3361, DOI 10.1109/CVPR.2011.5995496
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee TS, 2003, J OPT SOC AM A, V20, P1434, DOI 10.1364/JOSAA.20.001434
   Lee TS, 1998, VISION RES, V38, P2429, DOI 10.1016/S0042-6989(97)00464-1
   Levine S., 2013, EXPLORING DEEP RECUR
   Liang P., 2009, P HUM LANG TECHN 200, P611, DOI DOI 10.3115/1620754.1620843
   Mairal J., 2008, P IEEE C COMP VIS PA, P1, DOI [10.1109/CVPR.2008.4587652, DOI 10.1109/CVPR.2008.4587652]
   Marcellin M.W., 2000, OVERVIEW JPEG 2000, P523
   Memisevic R., 2012, INT C MACH LEARN
   Moghaddam B., 2006, P 23 INT C MACH LEAR, P641
   Moghaddam B., 2006, ADV NEURAL INFORM PR, V18, P915
   Murray JF, 2007, NEURAL COMPUT, V19, P2301, DOI 10.1162/neco.2007.19.9.2301
   Niebles JC, 2008, INT J COMPUT VISION, V79, P299, DOI 10.1007/s11263-007-0122-4
   Norouzi Mohammad, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2735, DOI 10.1109/CVPRW.2009.5206577
   Olshausen Bruno A., 1997, SPARSE CODING OVERCO, P3311
   Poppe R, 2010, IMAGE VISION COMPUT, V28, P976, DOI 10.1016/j.imavis.2009.11.014
   Qiao LS, 2010, PATTERN RECOGN, V43, P331, DOI 10.1016/j.patcog.2009.05.005
   Ramasso E, 2008, PATTERN ANAL APPL, V11, P1, DOI 10.1007/s10044-007-0073-y
   Ranzato M., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2857, DOI 10.1109/CVPR.2011.5995710
   Ranzato MA, 2007, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2007.383157
   Rodriguez MD, 2008, PROC CVPR IEEE, P3001
   Salakhutdinov R, 2013, IEEE T PATTERN ANAL, V35, P1958, DOI 10.1109/TPAMI.2012.269
   Saxe A., 2013, ADV NEURAL INFORM PR
   Schuldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462
   Serre T, 2007, IEEE T PATTERN ANAL, V29, P411, DOI 10.1109/TPAMI.2007.56
   Srivastava Nitish, 2013, ADV NEURAL INFORM PR, P2094
   Tang Y., 2013, ADV NEURAL INFORM PR, P530
   Tang Y., 2013, WORKSH CHALL REPR LE
   Tang Y., 2010, P 27 INT C MACH LEAR, P1055
   Taylor GW, 2010, LECT NOTES COMPUT SC, V6316, P140, DOI 10.1007/978-3-642-15567-3_11
   Theodoridis S, 2009, PATTERN RECOGNITION, 4RTH EDITION, P1
   Wang H., 2009, BMVC
   Wang S, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON NANOSCALE ARCHITECTURE, P1
   Welling M., 2005, ADV NEURAL INFORM PR, P1481
   Willems G, 2008, LECT NOTES COMPUT SC, V5303, P650, DOI 10.1007/978-3-540-88688-4_48
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Yang J, 2012, PATTERN RECOGN, V45, P1104, DOI 10.1016/j.patcog.2011.08.022
   Zeiler MD, 2010, PROC CVPR IEEE, P2528, DOI 10.1109/CVPR.2010.5539957
   Zhang L, 2015, MULTIMED TOOLS APPL, V74, P123, DOI 10.1007/s11042-013-1457-1
   Zhang YP, 2013, INT SYM CODE GENER, P171
   Zhou G., 2012, JMLR WORKSH C P, P1453
   Zhou Y, 2013, PATTERN RECOGN, V46, P3208, DOI 10.1016/j.patcog.2013.06.007
   Zou H, 2006, J COMPUT GRAPH STAT, V15, P265, DOI 10.1198/106186006X113430
NR 81
TC 17
Z9 19
U1 2
U2 40
PU SPRINGER
PI NEW YORK
PA 233 SPRING ST, NEW YORK, NY 10013 USA
SN 1433-7541
EI 1433-755X
J9 PATTERN ANAL APPL
JI Pattern Anal. Appl.
PD MAY
PY 2016
VL 19
IS 2
BP 337
EP 354
DI 10.1007/s10044-014-0404-8
PG 18
WC Computer Science, Artificial Intelligence
SC Computer Science
GA DJ4KA
UT WOS:000374172600004
DA 2020-02-19
ER

PT J
AU Sigtia, S
   Benetos, E
   Dixon, S
AF Sigtia, Siddharth
   Benetos, Emmanouil
   Dixon, Simon
TI An End-to-End Neural Network for Polyphonic Piano Music Transcription
SO IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING
LA English
DT Article
DE Automatic music transcription; deep learning; recurrent neural networks;
   music language models
ID HARMONICITY; SMOOTHNESS
AB We present a supervised neural network model for polyphonic piano music transcription. The architecture of the proposed model is analogous to speech recognition systems and comprises an acoustic model and a music language model. The acoustic model is a neural network used for estimating the probabilities of pitches in a frame of audio. The language model is a recurrent neural network that models the correlations between pitch combinations over time. The proposed model is general and can be used to transcribe polyphonic music without imposing any constraints on the polyphony. The acoustic and language model predictions are combined using a probabilistic graphical model. Inference over the output variables is performed using the beam search algorithm. We perform two sets of experiments. We investigate various neural network architectures for the acoustic models and also investigate the effect of combining acoustic and music language model predictions using the proposed architecture. We compare performance of the neural network-based acoustic models with two popular unsupervised acoustic models. Results show that convolutional neural network acoustic models yield the best performance across all evaluation metrics. We also observe improved performance with the application of the music language models. Finally, we present an efficient variant of beam search that improves performance and reduces run-times by an order of magnitude, making the model suitable for real-time applications.
C1 [Sigtia, Siddharth; Benetos, Emmanouil; Dixon, Simon] Queen Mary Univ London, Sch Elect Engn & Comp Sci, Ctr Digital Mus, London E1 4NS, England.
RP Sigtia, S; Benetos, E; Dixon, S (reprint author), Queen Mary Univ London, Sch Elect Engn & Comp Sci, Ctr Digital Mus, London E1 4NS, England.
EM s.s.sigtia@qmul.ac.uk; emmanouil.benetos@qmul.ac.uk;
   s.e.dixon@qmul.ac.uk
RI Benetos, Emmanouil/S-1932-2018; Benetos, Emmanouil/AAC-6601-2019
OI Benetos, Emmanouil/0000-0002-6820-6764; Benetos,
   Emmanouil/0000-0002-6820-6764
FU Royal Academy of Engineering Research FellowshipRoyal Academy of
   Engineering - UK [RF/128]
FX The work of E. Benetos was supported by the Royal Academy of Engineering
   Research Fellowship under Grant RF/128. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Matthew E. P. Davies.
CR Abdallah SA, 2004, 5 INT C MUS INF RETR, P318
   Abdel-hamid O., 2013, P INTERSPEECH, P3366
   Abdel-Hamid O, 2012, INT CONF ACOUST SPEE, P4277, DOI 10.1109/ICASSP.2012.6288864
   Bay M., 2009, ISMIR, P315
   Benetos E., 2012, THESIS QUEEN MARY U
   Benetos E, 2012, COMPUT MUSIC J, V36, P81, DOI 10.1162/COMJ_a_00146
   Bengio Samy, 2015, ADV NEURAL INFORM PR, P1171
   BENGIO Y, 2013, P IEEE INT C AC SPEE, P8624
   Berg-Kirkpatrick T., 2014, ADV NEURAL INFORM PR, V27, P1538
   Bergstra J, 2006, MACH LEARN, V65, P473, DOI 10.1007/s10994-006-9019-7
   Bertin N, 2010, IEEE T AUDIO SPEECH, V18, P538, DOI 10.1109/TASL.2010.2041381
   Bock S, 2012, INT CONF ACOUST SPEE, P121, DOI 10.1109/ICASSP.2012.6287832
   Boulanger-Lewandowski Nicolas, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P5417, DOI 10.1109/ICASSP.2014.6854638
   Boulanger-Lewandowski  N., 2013, ISMIR, P335
   Boulanger-lewandowski N., 2012, P 29 INT C MACH LEAR, P1159
   Boulanger-Lewandowski N, 2013, INT CONF ACOUST SPEE, P3178, DOI 10.1109/ICASSP.2013.6638244
   BROWN JC, 1991, J ACOUST SOC AM, V89, P425, DOI 10.1121/1.400476
   Cormen T.H., 2001, INTRO ALGORITHMS, V2
   Dean J., 2012, ADV NEURAL INFORM PR, V25, P1223
   ECK D, 2002, P 2002 IEEE WORKSH, P747
   Emiya V, 2010, IEEE T AUDIO SPEECH, V18, P1643, DOI 10.1109/TASL.2009.2038819
   Emiya Valentin, 2008, P 16 EUR SIGN PROC C, P1
   Graves A., 2012, P REPR LEARN WORKSH
   Grefenstette E., 2015, ARXIV150602516
   Grindlay G., 2010, P 11 INT SOC MUS INF, P21
   Humphrey EJ, 2013, J INTELL INF SYST, V41, P461, DOI 10.1007/s10844-013-0248-5
   Humphrey EJ, 2012, 2012 11TH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA 2012), VOL 2, P357, DOI 10.1109/ICMLA.2012.220
   Klapuri A, 2007, SIGNAL PROCESSING ME
   Klapuri AP, 2003, IEEE T SPEECH AUDI P, V11, P804, DOI 10.1109/TSA.2003.815516
   Larochelle H., 2011, ARTIF INTELL, P29
   LeCun Y, 1998, LECT NOTES COMPUT SC, V1524, P9
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Nam J., 2011, P INT SOC MUS INF RE, P175
   NEAL RM, 1992, ARTIF INTELL, V56, P71, DOI 10.1016/0004-3702(92)90065-6
   O'Hanlon Ken, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P3112, DOI 10.1109/ICASSP.2014.6854173
   Poliner G., 2007, EURASIP J ADV SIG PR, P154
   Rabiner L. R., 1993, PRENTICE HALL SIGNAL
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   Raczynski SA, 2013, IEEE T AUDIO SPEECH, V21, P1830, DOI 10.1109/TASL.2013.2258012
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Schluter J, 2014, P IEEE INT C AC SPEE, P6979, DOI DOI 10.1109/ICASSP.2014.6854953
   Sigtia S., 2014, P 15 INT SOC MUS INF, P53
   Sigtia S, 2015, INT CONF ACOUST SPEE, P2061, DOI 10.1109/ICASSP.2015.7178333
   SIGTIA Siddharth, 2015, P INT SOC MUS INF RE, P127
   Smaragdis P, 2003, 2003 IEEE WORKSHOP ON APPLICATIONS OF SIGNAL PROCESSING TO AUDIO AND ACOUSTICS PROCEEDINGS, P177, DOI 10.1109/ASPAA.2003.1285860
   Smaragdis P., 2006, WORKSH ADV MOD AC PR, V148
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Vincent E, 2004, LECT NOTES COMPUT SC, V3195, P1197
   Vincent E, 2010, IEEE T AUDIO SPEECH, V18, P528, DOI 10.1109/TASL.2009.2034186
   WERBOS PJ, 1990, P IEEE, V78, P1550, DOI 10.1109/5.58337
   Xavier Glorot, 2011, P 14 INT C ART INT S, P315, DOI DOI 10.1177/1753193410395357
   Zeiler Matthew D, 2012, ARXIV12125701
NR 52
TC 48
Z9 48
U1 3
U2 41
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 2329-9290
J9 IEEE-ACM T AUDIO SPE
JI IEEE-ACM Trans. Audio Speech Lang.
PD MAY
PY 2016
VL 24
IS 5
BP 927
EP 939
DI 10.1109/TASLP.2016.2533858
PG 13
WC Acoustics; Engineering, Electrical & Electronic
SC Acoustics; Engineering
GA DH9EU
UT WOS:000373100000008
DA 2020-02-19
ER

PT J
AU Shuai, B
   Zuo, Z
   Wang, G
   Wang, B
AF Shuai, Bing
   Zuo, Zhen
   Wang, Gang
   Wang, Bing
TI Scene Parsing With Integration of Parametric and Non-Parametric Models
SO IEEE TRANSACTIONS ON IMAGE PROCESSING
LA English
DT Article
DE Scene parsing; convolution neural network; CNN-ensemble; global scene
   constraint; local ambiguity; deep learning
ID IMAGE FEATURES; FORESTS
AB We adopt convolutional neural networks (CNNs) to be our parametric model to learn discriminative features and classifiers for local patch classification. Based on the occurrence frequency distribution of classes, an ensemble of CNNs (CNN-Ensemble) are learned, in which each CNN component focuses on learning different and complementary visual patterns. The local beliefs of pixels are output by CNN-Ensemble. Considering that visually similar pixels are indistinguishable under local context, we leverage the global scene semantics to alleviate the local ambiguity. The global scene constraint is mathematically achieved by adding a global energy term to the labeling energy function, and it is practically estimated in a non-parametric framework. A large margin-based CNN metric learning method is also proposed for better global belief estimation. In the end, the integration of local and global beliefs gives rise to the class likelihood of pixels, based on which maximum marginal inference is performed to generate the label prediction maps. Even without any post-processing, we achieve the state-of-the-art results on the challenging SiftFlow and Barcelona benchmarks.
C1 [Shuai, Bing; Zuo, Zhen; Wang, Gang; Wang, Bing] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
RP Wang, G (reprint author), Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
EM bshuai001@ntu.edu.sg; zzuo1@ntu.edu.sg; wanggang@ntu.edu.sg;
   wang0775@ntu.edu.sg
FU Rapid-Rich Object Search (ROSE) Laboratory, Nanyang Technological
   University, Singapore; Singapore Ministry of EducationMinistry of
   Education, Singapore [Tier 2 ARC28/14]; Singapore Agency for Science,
   Technology and Research within the Science and Engineering Research
   Council [PSF1321202099]
FX This work was supported in part by the Rapid-Rich Object Search (ROSE)
   Laboratory, Nanyang Technological University, Singapore, in part by the
   Singapore Ministry of Education under Grant Tier 2 ARC28/14, and in part
   by the Singapore Agency for Science, Technology and Research within the
   Science and Engineering Research Council under Grant PSF1321202099.
CR Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Bulo SR, 2014, PROC CVPR IEEE, P81, DOI 10.1109/CVPR.2014.18
   Chen L-C., 2014, SEMANTIC IMAGE SEGME
   Criminisi A., 2013, ADV COMPUTER VISION
   Eigen D, 2012, PROC CVPR IEEE, P2799, DOI 10.1109/CVPR.2012.6248004
   Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231
   Freund Y., 1999, J JAPANESE SOC ARTIF, V14, P1612
   Gatta C, 2014, IEEE COMPUT SOC CONF, P504, DOI 10.1109/CVPRW.2014.80
   George M, 2015, PROC CVPR IEEE, P3622, DOI 10.1109/CVPR.2015.7298985
   Gould S, 2014, LECT NOTES COMPUT SC, V8689, P632, DOI 10.1007/978-3-319-10590-1_41
   Gould S, 2012, LECT NOTES COMPUT SC, V7576, P439, DOI 10.1007/978-3-642-33715-4_32
   He XM, 2004, PROC CVPR IEEE, P695
   Hu JL, 2014, PROC CVPR IEEE, P1875, DOI 10.1109/CVPR.2014.242
   Kohli P, 2009, INT J COMPUT VISION, V82, P302, DOI 10.1007/s11263-008-0202-0
   Kontschieder P, 2011, IEEE I CONF COMP VIS, P2190, DOI 10.1109/ICCV.2011.6126496
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lazebnik S., 2006, P IEEE INT C COMP VI, V2, P2169, DOI DOI 10.1109/CVPR.2006.68
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Liu C, 2009, PROC CVPR IEEE, P1972, DOI 10.1109/CVPRW.2009.5206536
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Marquez-Neila P, 2014, LECT NOTES COMPUT SC, V8694, P269, DOI 10.1007/978-3-319-10599-4_18
   Mostajabi M, 2015, MACH VISION APPL, V26, P15, DOI 10.1007/s00138-014-0642-1
   Mostajahi M, 2015, PROC CVPR IEEE, P3376, DOI 10.1109/CVPR.2015.7298959
   Oliva A, 2006, PROG BRAIN RES, V155, P23, DOI 10.1016/S0079-6123(06)55002-2
   Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222
   Perronnin F, 2010, PROC CVPR IEEE, P3384, DOI 10.1109/CVPR.2010.5540009
   Pinheiro P., 2014, P 31 INT C MACH LEAR, V32, P82, DOI DOI 10.1016/J.VETPAR.2009.02.011
   Roy A, 2014, PROC CVPR IEEE, P1178, DOI 10.1109/CVPR.2014.154
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shotton J., 2013, ADV NEURAL INFORM PR, P234
   Shotton J, 2006, LECT NOTES COMPUT SC, V3951, P1
   Shuai B., 2015, DAG RECURRENT NEURAL
   Shuai B, 2015, PROC CVPR IEEE, P4249, DOI 10.1109/CVPR.2015.7299053
   Shuai B, 2015, IEEE SIGNAL PROC LET, V22, P1990, DOI 10.1109/LSP.2015.2441781
   Simonyan K., 2014, VERY DEEP CONVOLUTIO
   Singh G, 2013, PROC CVPR IEEE, P3151, DOI 10.1109/CVPR.2013.405
   Szegedy C., 2014, GOING DEEPER CONVOLU
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Tighe J, 2013, PROC CVPR IEEE, P3001, DOI 10.1109/CVPR.2013.386
   Tighe J, 2010, LECT NOTES COMPUT SC, V6315, P352, DOI 10.1007/978-3-642-15555-0_26
   Tompson J., 2014, ADV NEURAL INFORM PR, V27, P1799
   Tung F, 2016, COMPUT VIS IMAGE UND, V143, P191, DOI 10.1016/j.cviu.2015.08.009
   Tung F, 2014, LECT NOTES COMPUT SC, V8694, P511, DOI 10.1007/978-3-319-10599-4_33
   Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412
   Wang L, 2015, IEEE T IMAGE PROCESS, V24, P1424, DOI 10.1109/TIP.2015.2403231
   Yang JM, 2014, PROC CVPR IEEE, P3294, DOI 10.1109/CVPR.2014.415
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang N., 2013, PANDA POSE ALIGNED N
   Zhang YM, 2012, PROC CVPR IEEE, P582, DOI 10.1109/CVPR.2012.6247724
   Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179
   Zhou ZH, 2012, ENSEMBLE METHODS FDN
   Zuo Z, 2015, PATTERN RECOGN, V48, P3004, DOI 10.1016/j.patcog.2015.02.003
NR 53
TC 8
Z9 8
U1 0
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1057-7149
EI 1941-0042
J9 IEEE T IMAGE PROCESS
JI IEEE Trans. Image Process.
PD MAY
PY 2016
VL 25
IS 5
BP 2379
EP 2391
DI 10.1109/TIP.2016.2533862
PG 13
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
SC Computer Science; Engineering
GA DJ9RS
UT WOS:000374551200012
PM 26929044
DA 2020-02-19
ER

PT J
AU Setio, AAA
   Ciompi, F
   Litjens, G
   Gerke, P
   Jacobs, C
   van Riel, SJ
   Wille, MMW
   Naqibullah, M
   Sanchez, CI
   van Ginneken, B
AF Setio, Arnaud Arindra Adiyoso
   Ciompi, Francesco
   Litjens, Geert
   Gerke, Paul
   Jacobs, Colin
   van Riel, Sarah J.
   Wille, Mathilde Marie Winkler
   Naqibullah, Matiullah
   Sanchez, Clara I.
   van Ginneken, Bram
TI Pulmonary Nodule Detection in CT Images: False Positive Reduction Using
   Multi-View Convolutional Networks
SO IEEE TRANSACTIONS ON MEDICAL IMAGING
LA English
DT Article
DE Computed tomography; computer-aided detection; convolutional networks;
   deep learning; lung cancer; pulmonary nodule
ID COMPUTER-AIDED DETECTION; LUNG-CANCER; AUTOMATIC DETECTION; TOMOGRAPHY
   SCANS; DETECTION SYSTEM; NEURAL-NETWORK; MANAGEMENT
AB We propose a novel Computer-Aided Detection (CAD) system for pulmonary nodules using multi-view convolutional networks (ConvNets), for which discriminative features are automatically learnt from the training data. The network is fed with nodule candidates obtained by combining three candidate detectors specifically designed for solid, subsolid, and large nodules. For each candidate, a set of 2-D patches from differently oriented planes is extracted. The proposed architecture comprises multiple streams of 2-D ConvNets, for which the outputs are combined using a dedicated fusion method to get the final classification. Data augmentation and dropout are applied to avoid overfitting. On 888 scans of the publicly available LIDC-IDRI dataset, our method reaches high detection sensitivities of 85.4% and 90.1% at 1 and 4 false positives per scan, respectively. An additional evaluation on independent datasets from the ANODE09 challenge and DLCST is performed. We showed that the proposed multi-view ConvNets is highly suited to be used for false positive reduction of a CAD system.
C1 [Setio, Arnaud Arindra Adiyoso; Ciompi, Francesco; Litjens, Geert; Gerke, Paul; Jacobs, Colin; van Riel, Sarah J.; Sanchez, Clara I.; van Ginneken, Bram] Radboud Univ Nijmegen, Med Ctr, Dept Radiol & Nucl Med, Diagnost Image Anal Grp, NL-6525 GA Nijmegen, Netherlands.
   [Wille, Mathilde Marie Winkler; Naqibullah, Matiullah] Univ Copenhagen, Gentofte Hosp, Dept Resp Med, DK-2900 Hellerup, Denmark.
   [van Ginneken, Bram] Fraunhofer MEVIS, D-28359 Bremen, Germany.
RP Setio, AAA (reprint author), Radboud Univ Nijmegen, Med Ctr, Dept Radiol & Nucl Med, Diagnost Image Anal Grp, NL-6525 GA Nijmegen, Netherlands.
EM Arnaud.ArindraAdiyoso@radboudumc.nl
RI Gutierrez, Clara Isabel Sanchez/N-3580-2014; Jacobs, Colin/P-6938-2015;
   Litjens, Geert JS/A-2319-2016; Ciompi, Francesco/P-5598-2015; van
   Ginneken, Bram/A-3728-2012
OI Jacobs, Colin/0000-0003-1180-3805; Litjens, Geert
   JS/0000-0003-1554-1291; Ciompi, Francesco/0000-0001-8327-9606; van
   Ginneken, Bram/0000-0003-2028-8972
FU Netherlands Organization for Scientific ResearchNetherlands Organization
   for Scientific Research (NWO) [639.023.207]
FX This project was funded by a research grant from the Netherlands
   Organization for Scientific Research, Project 639.023.207. Asterisk
   indicates corresponding author.
CR Aberle DR, 2011, NEW ENGL J MED, V365, P395, DOI 10.1056/NEJMoa1102873
   Cruz-Roa AA, 2013, LECT NOTES COMPUT SC, V8150, P403, DOI 10.1007/978-3-642-40763-5_50
   American Cancer Society, 2014, CANC FACTS FIG 2014
   Armato SG, 2011, MED PHYS, V38, P915, DOI 10.1118/1.3528204
   Bastien F., 2012, P DEEP LEARN UNS FEA, P1
   Bergstra J., 2010, P 9 PYTH SCI C, P1
   Brosch T, 2014, LECT NOTES COMPUT SC, V8674, P462, DOI 10.1007/978-3-319-10470-6_58
   Brown MS, 2014, EUR RADIOL, V24, P2719, DOI 10.1007/s00330-014-3329-0
   Cascio D, 2012, COMPUT BIOL MED, V42, P1098, DOI 10.1016/j.compbiomed.2012.09.002
   Chatfield K., 2014, BR MACH VIS C
   Choi WJ, 2013, ENTROPY-SWITZ, V15, P507, DOI 10.3390/e15020507
   Ciresan D, 2012, NEURAL NETWORKS, V32, P333, DOI 10.1016/j.neunet.2012.02.023
   Efron B., 1994, INTRO BOOTSTRAP, V57
   Enquobahrie AA, 2007, ACAD RADIOL, V14, P579, DOI 10.1016/j.acra.2007.01.029
   Firmino M, 2014, BIOMED ENG ONLINE, V13, DOI 10.1186/1475-925X-13-41
   Fujita H., 2013, INT J COMPUTER ASSIS, P1
   Glorot X., 2010, JLMR P TRACK, p[249, 2010], DOI DOI 10.1177/1753193409103364.
   Golosio B, 2009, MED PHYS, V36, P3607, DOI 10.1118/1.3160107
   Guo W, 2012, MED PHYS, V39, P5157, DOI 10.1118/1.4737109
   Jacobs C, 2014, MED IMAGE ANAL, V18, P374, DOI 10.1016/j.media.2013.12.001
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   Kuhnigk JM, 2006, IEEE T MED IMAGING, V25, P417, DOI 10.1109/TMI.2006.871547
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Li RJ, 2014, LECT NOTES COMPUT SC, V8675, P305, DOI 10.1007/978-3-319-10443-0_39
   Lo SCB, 1995, NEURAL NETWORKS, V8, P1201, DOI 10.1016/0893-6080(95)00061-5
   Torres EL, 2015, MED PHYS, V42, P1477, DOI 10.1118/1.4907970
   Manos D, 2014, CAN ASSOC RADIOL J, V65, P121, DOI 10.1016/j.carj.2014.03.004
   Messay T, 2010, MED IMAGE ANAL, V14, P390, DOI 10.1016/j.media.2010.02.004
   Murphy K, 2009, MED IMAGE ANAL, V13, P757, DOI 10.1016/j.media.2009.07.001
   Naidich DP, 2013, RADIOLOGY, V266, P304, DOI 10.1148/radiol.12120628
   Niemeijer M, 2011, IEEE T MED IMAGING, V30, P215, DOI 10.1109/TMI.2010.2072789
   Pedersen JH, 2009, J THORAC ONCOL, V4, P608, DOI 10.1097/JTO.0b013e3181a0d98f
   Prasoon A, 2013, LECT NOTES COMPUT SC, V8150, P246, DOI 10.1007/978-3-642-40763-5_31
   Retico A., 2009, P SOC PHOTO-OPT INS, V7260
   Roth HR, 2014, LECT NOTES COMPUT SC, V8673, P520, DOI 10.1007/978-3-319-10404-1_65
   Setio AAA, 2015, MED PHYS, V42, P5642, DOI 10.1118/1.4929562
   Simonyan K., 2014, VERY DEEP CONVOLUTIO
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Tan M., 2013, ARTIF INTELL MED
   Tieleman T., 2012, COURSERA NEURAL NETW
   van Ginneken B, 2015, I S BIOMED IMAGING, P286, DOI 10.1109/ISBI.2015.7163869
   van Ginneken B, 2010, MED IMAGE ANAL, V14, P707, DOI 10.1016/j.media.2010.05.005
   Winkler Wille MM, 2015, EUR RADIOL
   Xu DM, 2006, LUNG CANCER, V54, P177, DOI 10.1016/j.lungcan.2006.08.006
   Zhao YR, 2012, EUR RADIOL, V22, P2076, DOI 10.1007/s00330-012-2437-y
NR 47
TC 268
Z9 295
U1 20
U2 176
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 0278-0062
EI 1558-254X
J9 IEEE T MED IMAGING
JI IEEE Trans. Med. Imaging
PD MAY
PY 2016
VL 35
IS 5
SI SI
BP 1160
EP 1169
DI 10.1109/TMI.2016.2536809
PG 10
WC Computer Science, Interdisciplinary Applications; Engineering,
   Biomedical; Engineering, Electrical & Electronic; Imaging Science &
   Photographic Technology; Radiology, Nuclear Medicine & Medical Imaging
SC Computer Science; Engineering; Imaging Science & Photographic
   Technology; Radiology, Nuclear Medicine & Medical Imaging
GA DL3RI
UT WOS:000375550500002
PM 26955024
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Roth, HR
   Lu, L
   Liu, JM
   Yao, JH
   Seff, A
   Cherry, K
   Kim, L
   Summers, RM
AF Roth, Holger R.
   Lu, Le
   Liu, Jiamin
   Yao, Jianhua
   Seff, Ari
   Cherry, Kevin
   Kim, Lauren
   Summers, Ronald M.
TI Improving Computer-Aided Detection Using Convolutional Neural Networks
   and Random View Aggregation
SO IEEE TRANSACTIONS ON MEDICAL IMAGING
LA English
DT Article
DE Computer aided diagnosis; computed tomography; medical diagnostic
   imaging; machine learning; object detection; artificial neural networks;
   multi-layer neural network; deep learning
ID BONE METASTASIS; SEGMENTATION; POLYPS
AB Automated computer-aided detection (CADe) has been an important tool in clinical practice and research. State-of-the-art methods often show high sensitivities at the cost of high false-positives (FP) per patient rates. We design a two-tiered coarse-to-fine cascade framework that first operates a candidate generation system at sensitivities similar to 100% of but at high FP levels. By leveraging existing CADe systems, coordinates of regions or volumes of interest (ROI or VOI) are generated and function as input for a second tier, which is our focus in this study. In this second stage, we generate 2D (two-dimensional) or 2.5D views via sampling through scale transformations, random translations and rotations. These random views are used to train deep convolutional neural network (ConvNet) classifiers. In testing, the ConvNets assign class (e.g., lesion, pathology) probabilities for a new set of random views that are then averaged to compute a final per-candidate classification probability. This second tier behaves as a highly selective process to reject difficult false positives while preserving high sensitivities. The methods are evaluated on three data sets: 59 patients for sclerotic metastasis detection, 176 patients for lymph node detection, and 1,186 patients for colonic polyp detection. Experimental results show the ability of ConvNets to generalize well to different medical imaging CADe applications and scale elegantly to various data sets. Our proposed methods improve performance markedly in all cases. Sensitivities improved from 57% to 70%, 43% to 77%, and 58% to 75% at 3 FPs per patient for sclerotic metastases, lymph nodes and colonic polyps, respectively.
C1 [Roth, Holger R.; Lu, Le; Liu, Jiamin; Yao, Jianhua; Seff, Ari; Cherry, Kevin; Kim, Lauren; Summers, Ronald M.] NIH, Imaging Biomarkers & Comp Aided Diag Lab, Radiol & Imaging Sci Dept, Ctr Clin, Bldg 10, Bethesda, MD 20892 USA.
RP Roth, HR (reprint author), NIH, Imaging Biomarkers & Comp Aided Diag Lab, Radiol & Imaging Sci Dept, Ctr Clin, Bldg 10, Bethesda, MD 20892 USA.
EM holger.roth@nih.gov
RI Lu, Le/AAD-7619-2020
OI Lu, Le/0000-0002-6799-9416
FU Intramural Research Program of the NIH Clinical CenterUnited States
   Department of Health & Human ServicesNational Institutes of Health (NIH)
   - USA
FX This work was supported by the Intramural Research Program of the NIH
   Clinical Center. Asterisk indicates corresponding author.
CR Barbu A, 2006, LECT NOTES COMPUT SC, V4191, P462
   Barbu A, 2012, IEEE T MED IMAGING, V31, P240, DOI 10.1109/TMI.2011.2168234
   Burns JE, 2013, RADIOLOGY, V268, P69, DOI 10.1148/radiol.13121351
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Cheng HD, 2003, PATTERN RECOGN, V36, P2967, DOI 10.1016/S0031-3203(03)00192-4
   Cherry K. M., 2014, SPIE MED IMAG
   Cire D. C., 2012, ADV NEURAL INFORM PR, V2, P2843
   Ciresan D. C., 2013, MICCAI
   Feuerstein M., 2009, SPIE MED IMAGING
   Feulner J., 2013, MEDIA, V17
   Firmino M, 2014, BIOMED ENG ONLINE, V13, DOI 10.1186/1475-925X-13-41
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Gokturk SB, 2001, IEEE T MED IMAGING, V20, P1251, DOI 10.1109/42.974920
   Hammon M, 2013, EUR RADIOL, V23, P1862, DOI 10.1007/s00330-013-2774-5
   Hinton G. E, 2012, ARXIV12070580
   Johnson CD, 2008, NEW ENGL J MED, V359, P1207, DOI 10.1056/NEJMoa0800996
   Jones N, 2014, NATURE, V505, P146, DOI 10.1038/505146a
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Krizhevsky A., 2012, NIPS
   Krizhevsky Alex, 2014, ARXIV14045997
   Le Lu, 2014, Medical Computer Vision Large Data in Medical Imaging. Third International MICCAI Workshop, MCV 2013. Revised Selected Papers. LNCS: 8331, P161, DOI 10.1007/978-3-319-05530-5_16
   LeCun Y., 1989, NEURAL COMPUTAT, V1
   Li Q, 2014, ICARCV
   Liu J., 2014, SPIE MED IMAG
   Liu M., 2011, ACM C CIKM, P2509
   Lu L, 2008, LECT NOTES COMPUT SC, V5305, P465
   Lu L, 2011, PROC CVPR IEEE, P1049, DOI 10.1109/CVPR.2011.5995359
   Lu L, 2009, LECT NOTES COMPUT SC, V5762, P1009
   Msaouel P, 2008, BEST PRACT RES CL EN, V22, P341, DOI 10.1016/j.beem.2008.01.011
   Nakamura Y., 2013, SPIE MED IMAG
   Park  S., 2015, POLYP DETECTION COLO
   Prasoon A., 2013, MICCAI
   Raykar V.C., 2008, P 25 INT C MACH LEAR, P808
   Roth HR, 2014, LECT NOTES COMPUT SC, V8673, P520, DOI 10.1007/978-3-319-10404-1_65
   Roth HR, 2015, RECENT ADV COMPUTATI, P3, DOI DOI 10.1007/978-3-319-14148-0_1
   Seff A, 2014, LECT NOTES COMPUT SC, V8673, P544, DOI 10.1007/978-3-319-10404-1_68
   Shen W., 2015, INFORM PROCESS MED I
   Simonyan K., 2014, ADV NEURAL INFORM PR, P568, DOI DOI 10.1109/ICCVW.2017.368
   Simonyan K., 2014, ARXIV14091556
   Slabaugh G, 2010, ALGORITHMS, V3, P21, DOI 10.3390/a3010021
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Summers RM, 2002, RADIOLOGY, V225, P391, DOI 10.1148/radiol.2252011619
   Summers RM, 2005, GASTROENTEROLOGY, V129, P1832, DOI 10.1053/j.gastro.2005.08.054
   Szegedy C., 2014, CORR
   Tajbakhsh N., 2015, P MICCAI
   Tajbakhsh N., 2015, INF PROCESS MED IMAG
   Toews M, 2007, IEEE T MED IMAGING, V26, P497, DOI 10.1109/TMI.2007.892510
   Turaga S. C., 2010, NEURAL COMPUTAT, V22
   van Ginneken B, 2015, I S BIOMED IMAGING, P286, DOI 10.1109/ISBI.2015.7163869
   van Ravesteijn VF, 2010, IEEE T MED IMAGING, V29, P120, DOI 10.1109/TMI.2009.2028576
   Wan L., 2013, P INT C MACH LEARN
   Wiese T, 2012, PROC SPIE, V8315, DOI 10.1117/12.911700
   Wiese T, 2011, I S BIOMED IMAGING, P152, DOI 10.1109/ISBI.2011.5872376
   Wu DJ, 2010, PROC CVPR IEEE, P2791, DOI 10.1109/CVPR.2010.5540008
   Yao JH, 2005, P SOC PHOTO-OPT INS, V5746, P384, DOI 10.1117/12.594547
   Yao JH, 2006, P SOC PHOTO-OPT INS, V6144, P14459, DOI 10.1117/12.652288
   Yao JH, 2006, I S BIOMED IMAGING, P390
   Yao JH, 2009, PATTERN RECOGN, V42, P1029, DOI 10.1016/j.patcog.2008.09.034
   YOSINSKI J, 2014, ADV NEURAL INFORM PR, P3320, DOI DOI 10.1109/IJCNN.2016.7727519
NR 59
TC 165
Z9 174
U1 11
U2 38
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 0278-0062
EI 1558-254X
J9 IEEE T MED IMAGING
JI IEEE Trans. Med. Imaging
PD MAY
PY 2016
VL 35
IS 5
SI SI
BP 1170
EP 1181
DI 10.1109/TMI.2015.2482920
PG 12
WC Computer Science, Interdisciplinary Applications; Engineering,
   Biomedical; Engineering, Electrical & Electronic; Imaging Science &
   Photographic Technology; Radiology, Nuclear Medicine & Medical Imaging
SC Computer Science; Engineering; Imaging Science & Photographic
   Technology; Radiology, Nuclear Medicine & Medical Imaging
GA DL3RI
UT WOS:000375550500003
PM 26441412
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Dou, Q
   Chen, H
   Yu, LQ
   Zhao, L
   Qin, J
   Wang, DF
   Mok, VCT
   Shi, L
   Heng, PA
AF Dou, Qi
   Chen, Hao
   Yu, Lequan
   Zhao, Lei
   Qin, Jing
   Wang, Defeng
   Mok, Vincent C. T.
   Shi, Lin
   Heng, Pheng-Ann
TI Automatic Detection of Cerebral Microbleeds From MR Images via 3D
   Convolutional Neural Networks
SO IEEE TRANSACTIONS ON MEDICAL IMAGING
LA English
DT Article
DE 3D convolutional neural networks; biomarker detection; cerebral
   microbleeds; deep learning; susceptibility-weighted imaging
ID CLINICAL-RELEVANCE; DISEASE; BRAIN; RELIABILITY; GUIDE
AB Cerebral microbleeds (CMBs) are small haemorrhages nearby blood vessels. They have been recognized as important diagnostic biomarkers for many cerebrovascular diseases and cognitive dysfunctions. In current clinical routine, CMBs are manually labelled by radiologists but this procedure is laborious, time-consuming, and error prone. In this paper, we propose a novel automatic method to detect CMBs from magnetic resonance (MR) images by exploiting the 3D convolutional neural network (CNN). Compared with previous methods that employed either low-level hand-crafted descriptors or 2D CNNs, our method can take full advantage of spatial contextual information in MR volumes to extract more representative high-level features for CMBs, and hence achieve a much better detection accuracy. To further improve the detection performance while reducing the computational cost, we propose a cascaded framework under 3D CNNs for the task of CMB detection. We first exploit a 3D fully convolutional network (FCN) strategy to retrieve the candidates with high probabilities of being CMBs, and then apply a well-trained 3D CNN discrimination model to distinguish CMBs from hard mimics. Compared with traditional sliding window strategy, the proposed 3D FCN strategy can remove massive redundant computations and dramatically speed up the detection process. We constructed a large dataset with 320 volumetric MR scans and performed extensive experiments to validate the proposed method, which achieved a high sensitivity of 93.16% with an average number of 2.74 false positives per subject, outperforming previous methods using low-level descriptors or 2D CNNs by a significant margin. The proposed method, in principle, can be adapted to other biomarker detection tasks from volumetric medical data.
C1 [Dou, Qi; Chen, Hao; Yu, Lequan; Heng, Pheng-Ann] Chinese Univ Hong Kong, Dept Comp Sci & Engn, Hong Kong, Hong Kong, Peoples R China.
   [Zhao, Lei] Chinese Univ Hong Kong, Dept Med & Therapeut, Hong Kong, Hong Kong, Peoples R China.
   [Qin, Jing] Shenzhen Univ, Sch Med, Natl Reg Key Technol Engn Lab Med Ultrasound, Shenzhen 518060, Peoples R China.
   [Wang, Defeng] Chinese Univ Hong Kong, Dept Imaging & Intervent Radiol, Hong Kong, Hong Kong, Peoples R China.
   [Mok, Vincent C. T.; Shi, Lin] Chinese Univ Hong Kong, Chow Yuk Ho Technol Ctr Innovat Med, Therese Pei Fong Chow Res Ctr Prevent Dementia, Dept Med & Therapeut, Hong Kong, Hong Kong, Peoples R China.
RP Shi, L (reprint author), Chinese Univ Hong Kong, Chow Yuk Ho Technol Ctr Innovat Med, Therese Pei Fong Chow Res Ctr Prevent Dementia, Dept Med & Therapeut, Hong Kong, Hong Kong, Peoples R China.
EM qdou@cse.cuhk.edu.hk; hchen@cse.cuhk.edu.hk; shilin@cuhk.edu.hk
RI Chen, Hao/V-4299-2019; Yu, Lequan/U-5377-2019; Dou, Qi/I-8175-2019; Shi,
   Lin/K-5241-2014; Mok, Vincent/N-6421-2015
OI Chen, Hao/0000-0002-8400-3780; Yu, Lequan/0000-0002-9315-6527; Dou,
   Qi/0000-0002-3416-9950; Shi, Lin/0000-0003-2318-4669; Mok,
   Vincent/0000-0002-8102-8835; Zhao, Lei/0000-0001-5125-974X; Qin,
   Jing/0000-0002-7059-0929; Heng, Pheng Ann/0000-0003-3055-5034
FU National Basic Research Program of China, 973 ProgramNational Basic
   Research Program of China [2015CB351706]; Research Grants Council of the
   Hong Kong Special Administrative Region, ChinaHong Kong Research Grants
   Council [CUHK412412, GRF14203115, CUHK14113214]; Shenzhen-Hong Kong
   Innovation Circle Funding Program [SGLH20131010151755080, GHP/002/13SZ]
FX This work was supported by a grant from the National Basic Research
   Program of China, 973 Program (Project 2015CB351706), grants from the
   Research Grants Council of the Hong Kong Special Administrative Region,
   China (Project CUHK412412, GRF14203115 and CUHK14113214), and a grant
   from the Shenzhen-Hong Kong Innovation Circle Funding Program (Project
   SGLH20131010151755080 and GHP/002/13SZ). The first two authors
   contributed equally to this work. Asterisk indicates corresponding
   author.
CR Akter M, 2007, ACAD RADIOL, V14, P1011, DOI 10.1016/j.acra.2007.05.013
   Barnes SRS, 2011, MAGN RESON IMAGING, V29, P844, DOI 10.1016/j.mri.2011.02.028
   Benedictus MR, 2015, JAMA NEUROL, V72, P539, DOI 10.1001/jamaneurol.2015.14
   Bengio Y., 2015, DEEP LEARNING UNPUB
   Bian W, 2013, NEUROIMAGE-CLIN, V2, P282, DOI 10.1016/j.nicl.2013.01.012
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Charidimou A, 2013, NEURORADIOLOGY, V55, P655, DOI 10.1007/s00234-013-1175-4
   Charidimou A, 2012, J NEUROL SCI, V322, P50, DOI 10.1016/j.jns.2012.05.052
   Charidimou A, 2011, FUTUR NEUROL, V6, P587, DOI 10.2217/FNL.11.42
   Chen H, 2015, I S BIOMED IMAGING, P764, DOI 10.1109/ISBI.2015.7163984
   Chen H, 2015, LECT NOTES COMPUT SC, V9349, P507, DOI 10.1007/978-3-319-24553-9_62
   Cire D. C., 2012, ADV NEURAL INFORM PR, V2, P2843
   Ciresan DC, 2013, LECT NOTES COMPUT SC, V8150, P411, DOI 10.1007/978-3-642-40763-5_51
   Cordonnier C, 2007, BRAIN, V130, P1988, DOI 10.1093/brain/awl387
   de Bresser J, 2013, AM J NEURORADIOL, V34, pE61, DOI 10.3174/ajnr.A2960
   Dou Q, 2015, IEEE ENG MED BIO, P7933, DOI 10.1109/EMBC.2015.7320232
   Faziollahi A, 2014, I S BIOMED IMAGING, P113, DOI 10.1109/ISBI.2014.6867822
   GEMAN S, 1992, NEURAL COMPUT, V4, P1, DOI 10.1162/neco.1992.4.1.1
   Ghafaryasl B, 2012, 2012 9TH IEEE INTERNATIONAL SYMPOSIUM ON BIOMEDICAL IMAGING (ISBI), P138, DOI 10.1109/ISBI.2012.6235503
   Goos JDC, 2011, STROKE, V42, P1894, DOI 10.1161/STROKEAHA.110.599837
   Greenberg SM, 2009, LANCET NEUROL, V8, P165, DOI 10.1016/S1474-4422(09)70013-4
   Gregoire SM, 2009, NEUROLOGY, V73, P1759, DOI 10.1212/WNL.0b013e3181c34a7d
   Havaei M., 2015, ARXIV150506448
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI 10.1007/978-3-319-10578-9_23
   Hinton G. E, 2012, ARXIV12070580
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Jolliffe I. T., 1986, PRINCIPAL COMPONENT, V1
   Kamnitsas K., 2015, ISCHEMIC STROKE LESI, V13, P13, DOI DOI 10.1016/J.MEDIA.2016.10.004
   Kang Kai, 2014, ARXIV14114464
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kuijf HJ, 2012, NEUROIMAGE, V59, P2266, DOI 10.1016/j.neuroimage.2011.09.061
   LI HX, 2015, PROC CVPR IEEE, P5325, DOI DOI 10.1109/CVPR.2015.7299170
   Liaw A., 2002, R NEWS, V23, P18, DOI DOI 10.1177/154405910408300516
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Ning F, 2005, IEEE T IMAGE PROCESS, V14, P1360, DOI 10.1109/TIP.2005.852470
   Prasoon A, 2013, LECT NOTES COMPUT SC, V8150, P246, DOI 10.1007/978-3-642-40763-5_31
   Roth HR, 2014, LECT NOTES COMPUT SC, V8673, P520, DOI 10.1007/978-3-319-10404-1_65
   Roth HR, 2015, I S BIOMED IMAGING, P101, DOI 10.1109/ISBI.2015.7163826
   Roth HR, 2015, ARXIV150606448
   Roth HR, 2015, ARXIV150503046
   Turaga SC, 2010, NEURAL COMPUT, V22, P511, DOI 10.1162/neco.2009.10-08-881
   Urban G, 2014, MICCAI BRATS BRAIN T, P31
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   VANDENHEUVEL T, 2015, P SPIE MED IM INT SO
   Vernooij MW, 2008, NEUROLOGY, V70, P1208, DOI 10.1212/01.wnl.0000307750.41970.d9
   Wang ZL, 2014, STROKE, V45, P2811, DOI 10.1161/STROKEAHA.114.004286
   Whitwell J. L., 2014, ALZHEIMERS DEMENT, V10, pP20
   Wu ZH, 2009, ADV DATA SCI ADAPT, V1, P1, DOI 10.1142/S1793536909000047
   Xavier Glorot, 2011, P 14 INT C ART INT S, P315, DOI DOI 10.1177/1753193410395357
   Zikic D, 2013, LECT NOTES COMPUT SC, V8151, P66, DOI 10.1007/978-3-642-40760-4_9
NR 50
TC 172
Z9 174
U1 7
U2 146
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 0278-0062
EI 1558-254X
J9 IEEE T MED IMAGING
JI IEEE Trans. Med. Imaging
PD MAY
PY 2016
VL 35
IS 5
SI SI
BP 1182
EP 1195
DI 10.1109/TMI.2016.2528129
PG 14
WC Computer Science, Interdisciplinary Applications; Engineering,
   Biomedical; Engineering, Electrical & Electronic; Imaging Science &
   Photographic Technology; Radiology, Nuclear Medicine & Medical Imaging
SC Computer Science; Engineering; Imaging Science & Photographic
   Technology; Radiology, Nuclear Medicine & Medical Imaging
GA DL3RI
UT WOS:000375550500004
PM 26886975
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Sirinukunwattana, K
   Raza, SEA
   Tsang, YW
   Snead, DRJ
   Cree, IA
   Rajpoot, NM
AF Sirinukunwattana, Korsuk
   Raza, Shan E. Ahmed
   Tsang, Yee-Wah
   Snead, David R. J.
   Cree, Ian A.
   Rajpoot, Nasir M.
TI Locality Sensitive Deep Learning for Detection and Classification of
   Nuclei in Routine Colon Cancer Histology Images
SO IEEE TRANSACTIONS ON MEDICAL IMAGING
LA English
DT Article
DE Convolutional neural network; deep learning; histology image analysis;
   nucleus detection
ID HISTOPATHOLOGY; HETEROGENEITY
AB Detection and classification of cell nuclei in histopathology images of cancerous tissue stained with the standard hematoxylin and eosin stain is a challenging task due to cellular heterogeneity. Deep learning approaches have been shown to produce encouraging results on histopathology images in various studies. In this paper, we propose a Spatially Constrained Convolutional Neural Network (SC-CNN) to perform nucleus detection. SC-CNN regresses the likelihood of a pixel being the center of a nucleus, where high probability values are spatially constrained to locate in the vicinity of the centers of nuclei. For classification of nuclei, we propose a novel Neighboring Ensemble Predictor (NEP) coupled with CNN to more accurately predict the class label of detected cell nuclei. The proposed approaches for detection and classification do not require segmentation of nuclei. We have evaluated them on a large dataset of colorectal adenocarcinoma images, consisting of more than 20,000 annotated nuclei belonging to four different classes. Our results show that the joint detection and classification of the proposed SC-CNN and NEP produces the highest average F1 score as compared to other recently published approaches. Prospectively, the proposed methods could offer benefit to pathology practice in terms of quantitative analysis of tissue constituents in whole-slide images, and potentially lead to a better understanding of cancer.
C1 [Sirinukunwattana, Korsuk; Raza, Shan E. Ahmed] Univ Warwick, Dept Comp Sci, Coventry CV4 7AL, W Midlands, England.
   [Tsang, Yee-Wah; Snead, David R. J.; Cree, Ian A.] Univ Hosp Coventry & Warwickshire, Dept Pathol, Coventry CV2 2DX, W Midlands, England.
   [Rajpoot, Nasir M.] Qatar Univ, Dept Comp Sci & Engn, Doha, Qatar.
   [Rajpoot, Nasir M.] Univ Warwick, Dept Comp Sci, Coventry CV4 7AL, W Midlands, England.
RP Rajpoot, NM (reprint author), Qatar Univ, Dept Comp Sci & Engn, Doha, Qatar.; Rajpoot, NM (reprint author), Univ Warwick, Dept Comp Sci, Coventry CV4 7AL, W Midlands, England.
EM k.sirinukunwattana@warwick.ac.uk; nasir.rajpoot@ieee.org
OI Sirinukunwattana, Korsuk/0000-0002-3603-4435; SNEAD,
   DAVID/0000-0002-0766-9650
FU NPRP from the Qatar National Research Fund (a member of Qatar
   Foundation) [NPRP5-1345-1-228]; Department of Computer Science,
   University of Warwick, U.K.
FX This paper was made possible by NPRP grant number NPRP5-1345-1-228 from
   the Qatar National Research Fund (a member of Qatar Foundation). The
   statements made herein are solely the responsibility of the authors. K.
   Sirinukunwattana acknowledges the partial financial support provided by
   the Department of Computer Science, University of Warwick, U.K. Asterisk
   indicates corresponding author.
CR Al-Kofahi Y, 2010, IEEE T BIO-MED ENG, V57, P841, DOI 10.1109/TBME.2009.2035102
   Cruz-Roa AA, 2013, LECT NOTES COMPUT SC, V8150, P403, DOI 10.1007/978-3-642-40763-5_50
   Ali S, 2012, IEEE T MED IMAGING, V31, P1448, DOI 10.1109/TMI.2012.2190089
   Arif M, 2007, INTERNATIONAL CONFERENCE ON MACHINE VISION 2007, PROCEEDINGS, P113
   Arteta C, 2012, LECT NOTES COMPUT SC, V7510, P348, DOI 10.1007/978-3-642-33415-3_43
   Basavanhally A, 2011, J PATHOL INFORM, V2
   Bishop CM, 2006, PATTERN RECOGNITION
   Cire D. C., 2012, ADV NEURAL INFORM PR, V2, P2843
   Ciresan DC, 2013, LECT NOTES COMPUT SC, V8150, P411, DOI 10.1007/978-3-642-40763-5_51
   Cosatto E, 2008, INT C PATT RECOG, P672
   Dalerba P, 2011, NAT BIOTECHNOL, V29, P1120, DOI 10.1038/nbt.2038
   Dalle J.-R., 2009, WACV
   Hand DJ, 2001, MACH LEARN, V45, P171, DOI 10.1023/A:1010920819831
   Irshad Humayun, 2014, IEEE Rev Biomed Eng, V7, P97, DOI 10.1109/RBME.2013.2295804
   Khan AM, 2014, IEEE T BIO-MED ENG, V61, P1729, DOI 10.1109/TBME.2014.2303294
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kuse M., 2011, J PATHOL INFORM, V2, DOI [10.4103/2153-3539.92028, DOI 10.4103/2153-3539.92028]
   Larochelle H, 2009, J MACH LEARN RES, V10, P1
   Lewis JS, 2014, AM J SURG PATHOL, V38, P128, DOI 10.1097/PAS.0000000000000086
   Malon Christopher D., 2013, J PATHOL INFORM, V4
   Mitrovic B, 2012, MODERN PATHOL, V25, P1315, DOI 10.1038/modpathol.2012.94
   Murphy K. P., 2012, MACH LEARNING PROBAB
   Nguyen K., 2011, J PATHOLOGY INFORM, V2, P2
   O'Brien CA, 2007, NATURE, V445, P106, DOI 10.1038/nature05372
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sharma H., 2015, P 10 INT C COMP VIS, P37
   Sirinukunwattana K., 2015, SPIE MED IMAG INT SO
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   VANMUIJEN GNP, 1986, EXP CELL RES, V162, P97, DOI 10.1016/0014-4827(86)90429-5
   Vedaldi A., 2014, CORR
   Veta M, 2014, IEEE T BIO-MED ENG, V61, P1400, DOI 10.1109/TBME.2014.2303852
   Veta M, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0070221
   Vink JP, 2013, J MICROSC-OXFORD, V249, P124, DOI 10.1111/jmi.12001
   Wang H., 2014, P SPIE MED IMAG
   Xie YP, 2015, LECT NOTES COMPUT SC, V9351, P358, DOI 10.1007/978-3-319-24574-4_43
   Xu J, 2016, IEEE T MED IMAGING, V35, P119, DOI 10.1109/TMI.2015.2458702
   Yuan YY, 2012, SCI TRANSL MED, V4, DOI 10.1126/scitranslmed.3004330
NR 37
TC 245
Z9 252
U1 20
U2 181
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 0278-0062
EI 1558-254X
J9 IEEE T MED IMAGING
JI IEEE Trans. Med. Imaging
PD MAY
PY 2016
VL 35
IS 5
SI SI
BP 1196
EP 1206
DI 10.1109/TMI.2016.2525803
PG 11
WC Computer Science, Interdisciplinary Applications; Engineering,
   Biomedical; Engineering, Electrical & Electronic; Imaging Science &
   Photographic Technology; Radiology, Nuclear Medicine & Medical Imaging
SC Computer Science; Engineering; Imaging Science & Photographic
   Technology; Radiology, Nuclear Medicine & Medical Imaging
GA DL3RI
UT WOS:000375550500005
PM 26863654
OA Green Published
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Anthimopoulos, M
   Christodoulidis, S
   Ebner, L
   Christe, A
   Mougiakakou, S
AF Anthimopoulos, Marios
   Christodoulidis, Stergios
   Ebner, Lukas
   Christe, Andreas
   Mougiakakou, Stavroula
TI Lung Pattern Classification for Interstitial Lung Diseases Using a Deep
   Convolutional Neural Network
SO IEEE TRANSACTIONS ON MEDICAL IMAGING
LA English
DT Article
DE Convolutional neural networks; interstitial lung diseases; texture
   classification
ID TISSUE-ANALYSIS
AB Automated tissue characterization is one of the most crucial components of a computer aided diagnosis (CAD) system for interstitial lung diseases (ILDs). Although much research has been conducted in this field, the problem remains challenging. Deep learning techniques have recently achieved impressive results in a variety of computer vision problems, raising expectations that they might be applied in other domains, such as medical image analysis. In this paper, we propose and evaluate a convolutional neural network (CNN), designed for the classification of ILD patterns. The proposed network consists of 5 convolutional layers with 2 x 2 kernels and LeakyReLU activations, followed by average pooling with size equal to the size of the final feature maps and three dense layers. The last dense layer has 7 outputs, equivalent to the classes considered: healthy, ground glass opacity (GGO), micronodules, consolidation, reticulation, honeycombing and a combination of GGO/reticulation. To train and evaluate the CNN, we used a dataset of 14696 image patches, derived by 120 CT scans from different scanners and hospitals. To the best of our knowledge, this is the first deep CNN designed for the specific problem. A comparative analysis proved the effectiveness of the proposed CNN against previous methods in a challenging dataset. The classification performance (similar to 85.5%) demonstrated the potential of CNNs in analyzing lung patterns. Future work includes, extending the CNN to three-dimensional data provided by CT volume scans and integrating the proposed method into a CAD system that aims to provide differential diagnosis for ILDs as a supportive tool for radiologists.
C1 [Anthimopoulos, Marios; Christodoulidis, Stergios; Mougiakakou, Stavroula] Univ Bern, ARTORG Ctr Biomed Engn Res, CH-3008 Bern, Switzerland.
   [Anthimopoulos, Marios; Ebner, Lukas; Christe, Andreas; Mougiakakou, Stavroula] Inselspital Bern, Univ Hosp Bern, Dept Diagnost Intervent & Pediat Radiol, CH-3010 Bern, Switzerland.
   [Anthimopoulos, Marios] Inselspital Bern, Univ Hosp Bern, Dept Emergency Med, CH-3010 Bern, Switzerland.
RP Mougiakakou, S (reprint author), Inselspital Bern, Univ Hosp Bern, Dept Diagnost Intervent & Pediat Radiol, CH-3010 Bern, Switzerland.
EM marios.anthimopoulos@artorg.unibe.ch;
   stergios.christodoulidis@artorg.unibe.ch; lukas.ebner@insel.ch;
   andreas.christe@insel.ch; stavroula.mougiakakou@artorg.unibe.ch
RI Christodoulidis, Stergios/AAH-9236-2019
OI Christodoulidis, Stergios/0000-0002-8773-1070
FU Bern University Hospital, "Inselspital"; Swiss National Science
   Foundation (SNSF)Swiss National Science Foundation (SNSF) [156511]
FX This research was carried out within the framework of the INTACT
   research project, supported by Bern University Hospital, "Inselspital"
   and the Swiss National Science Foundation (SNSF) under Grant 156511. M.
   Anthimopoulos and S. Christodoulidis contributed equally to this work.
   Asterisk indicates corresponding author.
CR Anthimopoulos M, 2014, IEEE ENG MED BIO, P6040, DOI 10.1109/EMBC.2014.6945006
   Bastien F., 2012, P DEEP LEARN UNS FEA, P1
   Carpenter J, 2000, STAT MED, V19, P1141, DOI 10.1002/(SICI)1097-0258(20000515)19:9<1141::AID-SIM479>3.0.CO;2-F
   Cushley MJ, 1999, THORAX, V54, pS1
   Delorme S, 1997, INVEST RADIOL, V32, P566, DOI 10.1097/00004424-199709000-00009
   Demedts M, 2002, EUR RESPIR J, V19, P794, DOI 10.1183/09031936.02.00492002
   Depeursinge A, 2015, I S BIOMED IMAGING, P403, DOI 10.1109/ISBI.2015.7163897
   Depeursinge A, 2012, IEEE T INF TECHNOL B, V16, P665, DOI 10.1109/TITB.2012.2198829
   Depeursinge A, 2012, COMPUT MED IMAG GRAP, V36, P227, DOI 10.1016/j.compmedimag.2011.07.003
   Foncubierta-Rodrfguez Antonio, 2012, Medical Content-Based Retrieval for Clinical Decision Support. Second MICCAI International Workshop, MCBR-CDS 2011. Revised Selected Papers, P69, DOI 10.1007/978-3-642-28460-1_7
   Gangeh MJ, 2010, LECT NOTES COMPUT SC, V6363, P595
   Gao M., 2015, 1 WORKSH DEEP LEARN, P41
   HANLEY JA, 1983, RADIOLOGY, V148, P839, DOI 10.1148/radiology.148.3.6878708
   He K., ARXIV150201852
   Heitmann KR, 1997, EUR RADIOL, V7, P1463, DOI 10.1007/s003300050318
   Jia Yangqing, 2014, ARXIV14085093
   King DB, 2015, ACS SYM SER, V1214, P1
   Korfiatis PD, 2010, IEEE T INF TECHNOL B, V14, P675, DOI 10.1109/TITB.2009.2036166
   Krizhevsky A, 2012, ADV NEURAL INF PROCE, V201, P9
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li Q, 2014, I C CONT AUTOMAT ROB, P844, DOI 10.1109/ICARCV.2014.7064414
   Li Q, 2013, IEEE ENG MED BIO, P6079, DOI 10.1109/EMBC.2013.6610939
   Maas Andrew L., 2013, ICML WORKSH DEEP LEA, V28
   Mariolis I., 2010, IEEE INT C IM SYST T, P135
   Simonyan K, 2015, INT C LEARN REPR
   Sluimer I, 2006, IEEE T MED IMAGING, V25, P385, DOI 10.1109/TMI.2005.862753
   Sluimer IC, 2003, MED PHYS, V30, P3081, DOI 10.1118/1.1624771
   Song Y, 2013, IEEE T MED IMAGING, V32, P797, DOI 10.1109/TMI.2013.2241448
   Sorensen L, 2010, IEEE T MED IMAGING, V29, P559, DOI 10.1109/TMI.2009.2038575
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Tuceryan M., 1998, HDB PATTERN RECOGNIT
   Uchiyama Y, 2003, MED PHYS, V30, P2440, DOI 10.1118/1.1597431
   Uppaluri R, 1999, AM J RESP CRIT CARE, V160, P648, DOI 10.1164/ajrccm.160.2.9804094
   van Tulder G, 2014, LECT NOTES COMPUT SC, V8848, P47, DOI 10.1007/978-3-319-13972-2_5
   Vo KT, 2011, IEEE IMAGE PROC, P441, DOI 10.1109/ICIP.2011.6116545
   Vo KT, 2010, IEEE ENG MED BIO, P3085, DOI 10.1109/IEMBS.2010.5626113
   Xu B., 2015, ICML DEEP LEARN, P1
   Xu R, 2011, LECT NOTES COMPUT SC, V6893, P183, DOI 10.1007/978-3-642-23626-6_23
   Xu Y, 2006, ACAD RADIOL, V13, P969, DOI 10.1016/j.acra.2006.04.017
   Zavaletta VA, 2007, ACAD RADIOL, V14, P772, DOI 10.1016/j.acra.2007.03.009
   Zhang M., 2013, P SPIE, V8670
   Zhao W., 2013, P INT C IEEE ENG MED, P5457
NR 42
TC 272
Z9 288
U1 10
U2 127
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 0278-0062
EI 1558-254X
J9 IEEE T MED IMAGING
JI IEEE Trans. Med. Imaging
PD MAY
PY 2016
VL 35
IS 5
SI SI
BP 1207
EP 1216
DI 10.1109/TMI.2016.2535865
PG 10
WC Computer Science, Interdisciplinary Applications; Engineering,
   Biomedical; Engineering, Electrical & Electronic; Imaging Science &
   Photographic Technology; Radiology, Nuclear Medicine & Medical Imaging
SC Computer Science; Engineering; Imaging Science & Photographic
   Technology; Radiology, Nuclear Medicine & Medical Imaging
GA DL3RI
UT WOS:000375550500006
PM 26955021
OA Green Accepted
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Ghesu, FC
   Krubasik, E
   Georgescu, B
   Singh, V
   Zheng, YF
   Hornegger, J
   Comaniciu, D
AF Ghesu, Florin C.
   Krubasik, Edward
   Georgescu, Bogdan
   Singh, Vivek
   Zheng, Yefeng
   Hornegger, Joachim
   Comaniciu, Dorin
TI Marginal Space Deep Learning: Efficient Architecture for Volumetric
   Image Parsing
SO IEEE TRANSACTIONS ON MEDICAL IMAGING
LA English
DT Article
DE Deep learning; image parsing; marginal space learning; sparse
   representations; three-dimensional (3D) object detection and
   segmentation
ID AUTOMATIC SEGMENTATION; NEURAL-NETWORKS; CARDIAC CT; MODELS; ULTRASOUND
AB Robust and fast solutions for anatomical object detection and segmentation support the entire clinical workflow from diagnosis, patient stratification, therapy planning, intervention and follow-up. Current state-of-the-art techniques for parsing volumetric medical image data are typically based on machine learning methods that exploit large annotated image databases. Two main challenges need to be addressed, these are the efficiency in scanning high-dimensional parametric spaces and the need for representative image features which require significant efforts of manual engineering. We propose a pipeline for object detection and segmentation in the context of volumetric image parsing, solving a two-step learning problem: anatomical pose estimation and boundary delineation. For this task we introduce Marginal Space Deep Learning (MSDL), a novel framework exploiting both the strengths of efficient object parametrization in hierarchical marginal spaces and the automated feature design of Deep Learning (DL) network architectures. In the 3D context, the application of deep learning systems is limited by the very high complexity of the parametrization. More specifically 9 parameters are necessary to describe a restricted affine transformation in 3D, resulting in a prohibitive amount of billions of scanning hypotheses. The mechanism of marginal space learning provides excellent run-time performance by learning classifiers in clustered, high-probability regions in spaces of gradually increasing dimensionality. To further increase computational efficiency and robustness, in our system we learn sparse adaptive data sampling patterns that automatically capture the structure of the input. Given the object localization, we propose a DL-based active shape model to estimate the non-rigid object boundary. Experimental results are presented on the aortic valve in ultrasound using an extensive dataset of 2891 volumes from 869 patients, showing significant improvements of up to 45.2% over the state-of-the-art. To our knowledge, this is the first successful demonstration of the DL potential to detection and segmentation in full 3D data with parametrized representations.
C1 [Ghesu, Florin C.; Krubasik, Edward; Georgescu, Bogdan; Singh, Vivek; Zheng, Yefeng; Comaniciu, Dorin] Siemens Healthcare, Med Imaging Technol, Princeton, NJ 08540 USA.
   [Ghesu, Florin C.; Hornegger, Joachim] Univ Erlangen Nurnberg, Pattern Recognit Lab, D-91058 Erlangen, Germany.
   [Krubasik, Edward] Tech Univ Munich, Robot & Embedded Syst Dept, D-85748 Munich, Germany.
RP Ghesu, FC (reprint author), Siemens Healthcare, Med Imaging Technol, Princeton, NJ 08540 USA.
EM florin.c.ghesu@fau.de
OI Comaniciu, Dorin/0000-0002-5238-8647
CR Bar Y, 2015, I S BIOMED IMAGING, P294, DOI 10.1109/ISBI.2015.7163871
   Bengio Y., 2012, COMPUT RES REPOSIT
   Bengio Y., 2007, C NEUR INF PROC SYST
   Chen H, 2015, IEEE J BIOMED HEALTH, V19, P1627, DOI 10.1109/JBHI.2015.2425041
   Ciresan D, 2012, PROC CVPR IEEE, P3642, DOI 10.1109/CVPR.2012.6248110
   Ciresan DC, 2013, LECT NOTES COMPUT SC, V8150, P411, DOI 10.1007/978-3-642-40763-5_51
   COOTES TF, 1995, COMPUT VIS IMAGE UND, V61, P38, DOI 10.1006/cviu.1995.1004
   Dalal N, 2005, PROC CVPR IEEE, P886
   Elattar M, 2015, INT J CARDIOVAS IMAG, P1
   Feulner J, 2011, IEEE T MED IMAGING, V30, P1252, DOI 10.1109/TMI.2011.2112372
   Ghesu FC, 2015, LECT NOTES COMPUT SC, V9349, P710, DOI 10.1007/978-3-319-24553-9_87
   Grbic S, 2012, MED IMAGE ANAL, V16, P1003, DOI 10.1016/j.media.2012.02.003
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Ionasec RI, 2010, IEEE T MED IMAGING, V29, P1636, DOI 10.1109/TMI.2010.2048756
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lowe D., 1999, P INT C COMP VIS, V2, P1150, DOI [10.1109/ICCV.1999.790410, DOI 10.1109/ICCV.1999.790410]
   Lu L, 2009, IEEE I CONF COMP VIS, P2021, DOI 10.1109/ICCV.2009.5459445
   Martin DR, 2004, IEEE T PATTERN ANAL, V26, P530, DOI 10.1109/TPAMI.2004.1273918
   Mitchell SC, 2002, IEEE T MED IMAGING, V21, P1167, DOI 10.1109/TMI.2002.804425
   Mitiche A, 2006, IEEE T PATTERN ANAL, V28, P1818, DOI 10.1109/TPAMI.2006.232
   Pedersen PC, 2008, ULTRASON, P361, DOI 10.1109/ULTSYM.2008.0089
   Pouch AM, 2014, MED IMAGE ANAL, V18, P118, DOI 10.1016/j.media.2013.10.001
   Pouch AM, 2015, MED IMAGE ANAL, V26, P217, DOI 10.1016/j.media.2015.09.003
   Prasoon A, 2013, LECT NOTES COMPUT SC, V8150, P246, DOI 10.1007/978-3-642-40763-5_31
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Roth HR, 2014, LECT NOTES COMPUT SC, V8673, P520, DOI 10.1007/978-3-319-10404-1_65
   Rumelhart D. E., 1986, PARALLEL DISTRIBUTED
   Salakhutdinov R, 2009, ARTIF INTELL, P448, DOI DOI 10.1109/CVPRW.2009.5206577
   Sawada Y, 2015, 2015 14TH IAPR INTERNATIONAL CONFERENCE ON MACHINE VISION APPLICATIONS (MVA), P110, DOI 10.1109/MVA.2015.7153145
   Schneider RJ, 2011, LECT NOTES COMPUT SC, V6893, P520, DOI 10.1007/978-3-642-23626-6_64
   Sermanet P., 2013, COMPUT RES REPOSITOR
   Smolensky P., 1986, PARALLEL DISTRIBUTED
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Tombari F., 2011, 2011 International Conference on 3D Imaging, Modeling, Processing, Visualization and Transmission (3DIMPVT), P212, DOI 10.1109/3DIMPVT.2011.34
   Tu ZW, 2005, IEEE I CONF COMP VIS, P1589
   van Assen HC, 2006, MED IMAGE ANAL, V10, P286, DOI 10.1016/j.media.2005.12.001
   van Ginneken B, 2002, IEEE T MED IMAGING, V21, P924, DOI 10.1109/TMI.2002.803121
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Viola P, 2001, PROC CVPR IEEE, P511
   Zhang WL, 2015, NEUROIMAGE, V108, P214, DOI 10.1016/j.neuroimage.2014.12.061
   Zheng YF, 2008, IEEE T MED IMAGING, V27, P1668, DOI 10.1109/TMI.2008.2004421
   Zheng YF, 2012, IEEE T MED IMAGING, V31, P2307, DOI 10.1109/TMI.2012.2216541
   Zhukov L, 2002, PROC SPIE, V4684, P1398, DOI 10.1117/12.467105
NR 46
TC 44
Z9 45
U1 1
U2 57
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 0278-0062
EI 1558-254X
J9 IEEE T MED IMAGING
JI IEEE Trans. Med. Imaging
PD MAY
PY 2016
VL 35
IS 5
SI SI
BP 1217
EP 1228
DI 10.1109/TMI.2016.2538802
PG 12
WC Computer Science, Interdisciplinary Applications; Engineering,
   Biomedical; Engineering, Electrical & Electronic; Imaging Science &
   Photographic Technology; Radiology, Nuclear Medicine & Medical Imaging
SC Computer Science; Engineering; Imaging Science & Photographic
   Technology; Radiology, Nuclear Medicine & Medical Imaging
GA DL3RI
UT WOS:000375550500007
PM 27046846
DA 2020-02-19
ER

PT J
AU Brosch, T
   Tang, LYW
   Yoo, Y
   Li, DKB
   Traboulsee, A
   Tam, R
AF Brosch, Tom
   Tang, Lisa Y. W.
   Yoo, Youngjin
   Li, David K. B.
   Traboulsee, Anthony
   Tam, Roger
TI Deep 3D Convolutional Encoder Networks With Shortcuts for Multiscale
   Feature Integration Applied to Multiple Sclerosis Lesion Segmentation
SO IEEE TRANSACTIONS ON MEDICAL IMAGING
LA English
DT Article
DE Convolutional neural networks; deep learning; machine learning; magnetic
   resonance imaging (MRI); multiple sclerosis lesions; segmentation
ID WHITE-MATTER LESIONS
AB We propose a novel segmentation approach based on deep 3D convolutional encoder networks with shortcut connections and apply it to the segmentation of multiple sclerosis (MS) lesions in magnetic resonance images. Our model is a neural network that consists of two interconnected pathways, a convolutional pathway, which learns increasingly more abstract and higher-level image features, and a deconvolutional pathway, which predicts the final segmentation at the voxel level. The joint training of the feature extraction and prediction pathways allows for the automatic learning of features at different scales that are optimized for accuracy for any given combination of image types and segmentation task. In addition, shortcut connections between the two pathways allow high- and low-level features to be integrated, which enables the segmentation of lesions across a wide range of sizes. We have evaluated our method on two publicly available data sets (MICCAI 2008 and ISBI 2015 challenges) with the results showing that our method performs comparably to the top-ranked state-of-the-art methods, even when only relatively small data sets are available for training. In addition, we have compared our method with five freely available and widely used MS lesion segmentation methods (EMS, LST-LPA, LST-LGA, Lesion-TOADS, and SLS) on a large data set from an MS clinical trial. The results show that our method consistently outperforms these other methods across a wide range of lesion sizes.
C1 [Brosch, Tom; Tang, Lisa Y. W.; Yoo, Youngjin; Li, David K. B.; Traboulsee, Anthony; Tam, Roger] Univ British Columbia, Div Neurol, Multiple Sclerosis Magnet Resonance Imaging Res G, Vancouver, BC V6T 2B5, Canada.
   [Brosch, Tom; Yoo, Youngjin] Univ British Columbia, Dept Elect & Comp Engn, Vancouver, BC V6T 1Z4, Canada.
   [Tang, Lisa Y. W.; Li, David K. B.; Tam, Roger] Univ British Columbia, Dept Radiol, Vancouver, BC V6T 1Z4, Canada.
RP Brosch, T (reprint author), Univ British Columbia, Div Neurol, Multiple Sclerosis Magnet Resonance Imaging Res G, Vancouver, BC V6T 2B5, Canada.
EM brosch.tom@gmail.com; lisat@sfu.cu; youngjin@msmri.medicine.ubc.ca;
   david.li@ubc.ca; t.traboulsee@ubc.ca; roger.tam@ubc.ca
RI Traboulsee, Anthony L/AAB-2051-2020
FU Natural Sciences and Engineering Research Council of CanadaNatural
   Sciences and Engineering Research Council of Canada; Milan and Maureen
   Ilich Foundation
FX This work was supported by Natural Sciences and Engineering Research
   Council of Canada and the Milan and Maureen Ilich Foundation. Asterisk
   indicates corresponding author.
CR Bastien F., 2012, P DEEP LEARN UNS FEA, P1
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Brosch T, 2015, LECT NOTES COMPUT SC, V9351, P3, DOI 10.1007/978-3-319-24574-4_1
   Brosch T, 2015, NEURAL COMPUT, V27, P211, DOI 10.1162/NECO_a_00682
   Chetlur S., 2014, CUDNN EFFIC IN PRESS
   Ciresan D. C, 2012, ADV NEURAL INFORM PR, P1
   Collobert R., 2011, P BIGLEARN NIPS WORK, p1~6
   Coupe P, 2011, NEUROIMAGE, V54, P940, DOI 10.1016/j.neuroimage.2010.09.018
   Dauphin Y.N., 2015, RMSPROP EQUILIBRATED
   DICE LR, 1945, ECOLOGY, V26, P297, DOI 10.2307/1932409
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Garcia-Lorenzo D, 2013, MED IMAGE ANAL, V17, P1, DOI 10.1016/j.media.2012.09.004
   Geremia E, 2010, LECT NOTES COMPUT SC, V6361, P111
   Guizard N, 2015, NEUROIMAGE-CLIN, V8, P376, DOI 10.1016/j.nicl.2015.05.001
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hochreiter S., 1991, UNTERSUCHUNGEN DYNAM
   Jenkinson M., 2005, 11 ANN M ORG HUM BRA, V17
   Jerman T., 2015, P MICCAI 2015 BRAIN, P1
   Jesson A., 2015, P 2015 LONG MULT SCL, P1
   Jia Y, 2014, CAFFE CONVOLUTIONAL
   Kang K., 2014, FULLY CONVOLUTIONAL
   Kappos L, 2006, NEUROLOGY, V67, P944, DOI 10.1212/01.wnl.0000237994.95410.ce
   Kingma D.P., 2014, METHOD STOCHASTIC OP
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee H., 2009, P 26 ANN INT C MACH, P609, DOI DOI 10.1145/1553374.1553453
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Maier O., 2015, P 2015 LONG MULT SCL, P1
   Nair V, 2010, ICML, V27, P807, DOI DOI 10.0RG/PAPERS/432.PDF
   Ronneberger O., 2015, P 18 INT C MICCAI, P8
   Roura E, 2015, NEURORADIOLOGY, V57, P1031, DOI 10.1007/s00234-015-1552-2
   Scherer D, 2010, LECT NOTES COMPUT SC, V6354, P92, DOI 10.1007/978-3-642-15825-4_10
   Schmidt P, 2012, NEUROIMAGE, V59, P3774, DOI 10.1016/j.neuroimage.2011.11.032
   Shiee N, 2010, NEUROIMAGE, V49, P1524, DOI 10.1016/j.neuroimage.2009.09.005
   Styner M, 2008, MIDAS J, V2008, P1
   Subbanna N. K., 2009, P MICCAI 2009 WORKSH, P1
   Subbanna Nagesh, 2015, Inf Process Med Imaging, V24, P514, DOI 10.1007/978-3-319-19992-4_40
   Sudre CH, 2015, IEEE T MED IMAGING, V34, P2079, DOI 10.1109/TMI.2015.2419072
   Sutskever I., 2013, P 30 INT C MACH LEAR, P1139
   Tomas-Fernandez X, 2015, IEEE T MED IMAGING, V34, P1349, DOI 10.1109/TMI.2015.2393853
   Traboulsee A, 2008, BMC NEUROL, V8, DOI 10.1186/1471-2377-8-11
   Van Leemput K, 2001, IEEE T MED IMAGING, V20, P677, DOI 10.1109/42.938237
   Weiss N, 2013, LECT NOTES COMPUT SC, V8149, P735, DOI 10.1007/978-3-642-40811-3_92
   Yoo Y, 2014, LECT NOTES COMPUT SC, V8679, P117, DOI 10.1007/978-3-319-10581-9_15
   Zeiler M., 2012, ADADELTA ADAPTIVE LE
   Zeiler MD, 2011, IEEE I CONF COMP VIS, P2018, DOI 10.1109/ICCV.2011.6126474
NR 47
TC 121
Z9 128
U1 6
U2 70
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 0278-0062
EI 1558-254X
J9 IEEE T MED IMAGING
JI IEEE Trans. Med. Imaging
PD MAY
PY 2016
VL 35
IS 5
SI SI
BP 1229
EP 1239
DI 10.1109/TMI.2016.2528821
PG 11
WC Computer Science, Interdisciplinary Applications; Engineering,
   Biomedical; Engineering, Electrical & Electronic; Imaging Science &
   Photographic Technology; Radiology, Nuclear Medicine & Medical Imaging
SC Computer Science; Engineering; Imaging Science & Photographic
   Technology; Radiology, Nuclear Medicine & Medical Imaging
GA DL3RI
UT WOS:000375550500008
PM 26886978
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Pereira, S
   Pinto, A
   Alves, V
   Silva, CA
AF Pereira, Sergio
   Pinto, Adriano
   Alves, Victor
   Silva, Carlos A.
TI Brain Tumor Segmentation Using Convolutional Neural Networks in MRI
   Images
SO IEEE TRANSACTIONS ON MEDICAL IMAGING
LA English
DT Article
DE Brain tumor; brain tumor segmentation; convolutional neural networks;
   deep learning; glioma; magnetic resonance imaging
ID CLASSIFICATION; FORESTS
AB Among brain tumors, gliomas are the most common and aggressive, leading to a very short life expectancy in their highest grade. Thus, treatment planning is a key stage to improve the quality of life of oncological patients. Magnetic resonance imaging (MRI) is a widely used imaging technique to assess these tumors, but the large amount of data produced by MRI prevents manual segmentation in a reasonable time, limiting the use of precise quantitative measurements in the clinical practice. So, automatic and reliable segmentation methods are required; however, the large spatial and structural variability among brain tumors make automatic segmentation a challenging problem. In this paper, we propose an automatic segmentation method based on Convolutional Neural Networks (CNN), exploring small 3 x 3 kernels. The use of small kernels allows designing a deeper architecture, besides having a positive effect against overfitting, given the fewer number of weights in the network. We also investigated the use of intensity normalization as a pre-processing step, which though not common in CNN-based segmentation methods, proved together with data augmentation to be very effective for brain tumor segmentation in MRI images. Our proposal was validated in the Brain Tumor Segmentation Challenge 2013 database (BRATS 2013), obtaining simultaneously the first position for the complete, core, and enhancing regions in Dice Similarity Coefficient metric (0.88, 0.83, 0.77) for the Challenge data set. Also, it obtained the overall first position by the online evaluation platform. We also participated in the on-site BRATS 2015 Challenge using the same model, obtaining the second place, with Dice Similarity Coefficient metric of 0.78, 0.65, and 0.75 for the complete, core, and enhancing regions, respectively.
C1 [Pereira, Sergio; Pinto, Adriano; Silva, Carlos A.] Univ Minho, CMEMS UMinho Res Unit, Campus Azurem, P-4800058 Guimaraes, Portugal.
   [Pereira, Sergio; Alves, Victor] Univ Minho, Ctr Algoritmi, P-4710057 Braga, Portugal.
RP Pereira, S (reprint author), Univ Minho, CMEMS UMinho Res Unit, Campus Azurem, P-4800058 Guimaraes, Portugal.
EM id5692@alunos.uminho.pt; valves@di.uminho.pt; csilva@dei.uminho.pt
RI Pereira, Sergio/K-5591-2019; Alves, Victor/D-1319-2009; Silva,
   Carlos/J-1190-2014
OI Pereira, Sergio/0000-0002-4298-0903; Alves, Victor/0000-0003-1819-7051;
   Silva, Carlos/0000-0002-1015-5095; Azevedo da Silva Ribeiro Pinto, Jose
   Adriano/0000-0001-9397-3722
FU FCTPortuguese Foundation for Science and Technology
   [UID/EEA/04436/2013]; FEDER funds through the COMPETE-Programa
   Operacional Competitividade e Internacionalizacao (POCI)
   [POCI-01-0145-FEDER-006941]; Fundacao para a Ciencia e Tecnologia (FCT),
   Portugal [PD/BD/105803/2014]
FX This work is supported by FCT with the reference project
   UID/EEA/04436/2013, by FEDER funds through the COMPETE 2020-Programa
   Operacional Competitividade e Internacionalizacao (POCI) with the
   reference project POCI-01-0145-FEDER-006941. The work of S. Pereira was
   supported by a scholarship from the Fundacao para a Ciencia e Tecnologia
   (FCT), Portugal (PD/BD/105803/2014). Asterisk indicates corresponding
   author.
CR Bastien F., 2012, DEEP LEARN UNS FEAT
   Bauer S, 2012, P MICCAI BRATS, P10
   Bauer S, 2013, PHYS MED BIOL, V58, pR97, DOI 10.1088/0031-9155/58/13/R97
   Bauer S, 2011, LECT NOTES COMPUT SC, V6893, P354, DOI 10.1007/978-3-642-23626-6_44
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bergstra J., 2010, P PYTH SCI COMP C SC
   Cire D. C., 2012, ADV NEURAL INFORM PR, V2, P2843
   Davy A, 2014, MICCAI MULTIMODAL BR, P31
   DICE LR, 1945, ECOLOGY, V26, P297, DOI 10.2307/1932409
   Dieleman S., 2015, LASAGNE 1 RELEASE, DOI [10.5281/zenodo.27878, DOI 10.5281/ZENODO.27878]
   Dieleman S, 2015, MON NOT R ASTRON SOC, V450, P1441, DOI 10.1093/mnras/stv632
   Dvorak P., 2015, P MULT BRAIN TUM IM, P13
   Geremia E, 2013, I S BIOMED IMAGING, P1344
   Glorot X., 2010, JLMR P TRACK, p[249, 2010], DOI DOI 10.1177/1753193409103364.
   Gooya A, 2012, IEEE T MED IMAGING, V31, P1941, DOI 10.1109/TMI.2012.2210558
   Havaei M, 2015, BRAIN TUMOR SEGMENTA
   Hinton G., 2012, ARXIV12070580V1
   Islam A, 2013, IEEE T BIO-MED ENG, V60, P3204, DOI 10.1109/TBME.2013.2271383
   Jarrett K, 2009, IEEE I CONF COMP VIS, P2146, DOI 10.1109/ICCV.2009.5459469
   Kistler M, 2013, J MED INTERNET RES, V15, DOI 10.2196/jmir.2930
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kwon D., 2014, MICCAI MULTIMODAL BR, P18
   Kwon D, 2014, LECT NOTES COMPUT SC, V8673, P763, DOI 10.1007/978-3-319-10404-1_95
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lee CH, 2008, LECT NOTES COMPUT SC, V5241, P359
   Louis DN, 2007, ACTA NEUROPATHOL, V114, P97, DOI 10.1007/s00401-007-0243-4
   Lyksborg Mark, 2015, Image Analysis. 19th Scandinavian Conference, SCIA 2015. Proceedings: LNCS 9127, P201, DOI 10.1007/978-3-319-19665-7_17
   Maas A.L., 2013, P ICML, V30
   Meier R., 2013, P NCI MICCAI BRATS, P31
   Meier R., 2014, P MICCAI BRATS CHALL
   Meier R, 2014, LECT NOTES COMPUT SC, V8673, P714, DOI 10.1007/978-3-319-10404-1_89
   Menze BH, 2015, IEEE T MED IMAGING, V34, P1993, DOI 10.1109/TMI.2014.2377694
   Menze BH, 2010, LECT NOTES COMPUT SC, V6362, P151
   Nyul LG, 1999, MAGNET RESON MED, V42, P1072, DOI 10.1002/(SICI)1522-2594(199912)42:6<1072::AID-MRM11>3.3.CO;2-D
   Nyul LG, 2000, IEEE T MED IMAGING, V19, P143, DOI 10.1109/42.836373
   Pinto A, 2015, IEEE ENG MED BIO, P3037, DOI 10.1109/EMBC.2015.7319032
   Prastawa M, 2004, MED IMAGE ANAL, V8, P275, DOI 10.1016/j.media.2004.06.007
   Rao V, 2015, MICCAI MULTIMODAL BR, P56
   Reza S, 2014, SPIE MED IMAGING
   Shah M, 2011, MED IMAGE ANAL, V15, P267, DOI 10.1016/j.media.2010.12.003
   Simonyan K., 2014, VERY DEEP CONVOLUTIO
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Tabatabai G, 2010, ACTA NEUROPATHOL, V120, P585, DOI 10.1007/s00401-010-0750-6
   Tustison NJ, 2015, NEUROINFORMATICS, V13, P209, DOI 10.1007/s12021-014-9245-2
   Tustison NJ, 2010, IEEE T MED IMAGING, V29, P1310, DOI 10.1109/TMI.2010.2046908
   Urban G, 2014, MICCAI MULTIMODAL BR, P1
   Van Meir EG, 2010, CA-CANCER J CLIN, V60, P166, DOI 10.3322/caac.20069
   VirtualSkeleton, 2015, VIRTUALSKELETON BRAT
   Zikic D., 2014, P MICCAI BRATS, P36
   Zikic D, 2012, LECT NOTES COMPUT SC, V7512, P369, DOI 10.1007/978-3-642-33454-2_46
NR 52
TC 414
Z9 440
U1 29
U2 271
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 0278-0062
EI 1558-254X
J9 IEEE T MED IMAGING
JI IEEE Trans. Med. Imaging
PD MAY
PY 2016
VL 35
IS 5
SI SI
BP 1240
EP 1251
DI 10.1109/TMI.2016.2538465
PG 12
WC Computer Science, Interdisciplinary Applications; Engineering,
   Biomedical; Engineering, Electrical & Electronic; Imaging Science &
   Photographic Technology; Radiology, Nuclear Medicine & Medical Imaging
SC Computer Science; Engineering; Imaging Science & Photographic
   Technology; Radiology, Nuclear Medicine & Medical Imaging
GA DL3RI
UT WOS:000375550500009
PM 26960222
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Moeskops, P
   Viergever, MA
   Mendrik, AM
   de Vries, LS
   Benders, MJNL
   Isgum, I
AF Moeskops, Pim
   Viergever, Max A.
   Mendrik, Adrienne M.
   de Vries, Linda S.
   Benders, Manon J. N. L.
   Isgum, Ivana
TI Automatic Segmentation of MR Brain Images With a Convolutional Neural
   Network
SO IEEE TRANSACTIONS ON MEDICAL IMAGING
LA English
DT Article
DE Adult brain; automatic image segmentation; convolutional neural
   networks; deep learning; MRI; preterm neonatal brain
ID QUANTIFICATION; ALGORITHMS; FRAMEWORK; INFANTS; NEWBORN; CORTEX; YOUNG
AB Automatic segmentation in MR brain images is important for quantitative analysis in large-scale studies with images acquired at all ages. This paper presents a method for the automatic segmentation of MR brain images into a number of tissue classes using a convolutional neural network. To ensure that the method obtains accurate segmentation details as well as spatial consistency, the network uses multiple patch sizes and multiple convolution kernel sizes to acquire multi-scale information about each voxel. The method is not dependent on explicit features, but learns to recognise the information that is important for the classification based on training data. The method requires a single anatomical MR image only. The segmentation method is applied to five different data sets: coronal T-2-weighted images of preterm infants acquired at 30 weeks postmenstrual age (PMA) and 40 weeks PMA, axial T-2-weighted images of preterm infants acquired at 40 weeks PMA, axial T-1-weighted images of ageing adults acquired at an average age of 70 years, and T-1-weighted images of young adults acquired at an average age of 23 years. The method obtained the following average Dice coefficients over all segmented tissue classes for each data set, respectively: 0.87, 0.82, 0.84, 0.86, and 0.91. The results demonstrate that the method obtains accurate segmentations in all five sets, and hence demonstrates its robustness to differences in age and acquisition protocol.
C1 [Moeskops, Pim; Viergever, Max A.; Mendrik, Adrienne M.; Isgum, Ivana] Univ Med Ctr Utrecht, Image Sci Inst, NL-3584 CX Utrecht, Netherlands.
   [Moeskops, Pim; de Vries, Linda S.; Benders, Manon J. N. L.] Univ Med Ctr Utrecht, Dept Neonatol, NL-3584 EA Utrecht, Netherlands.
RP Moeskops, P (reprint author), Univ Med Ctr Utrecht, Image Sci Inst, NL-3584 CX Utrecht, Netherlands.
EM p.moeskops@umcutrecht.nl
RI Viergever, Max A/J-1215-2014; Moeskops, Pim/K-6728-2017; Isgum,
   Ivana/H-8659-2014
OI Moeskops, Pim/0000-0002-8733-6716; Isgum, Ivana/0000-0003-1869-5034
CR Anbeek P, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0081895
   Bar Y., 2015, P SPIE MED IMAG
   Benders M. J., 2013, P SPIE MED IMAG
   Cardoso MJ, 2013, NEUROIMAGE, V65, P97, DOI 10.1016/j.neuroimage.2012.08.009
   Ciompi F, 2015, MED IMAGE ANAL, V26, P195, DOI 10.1016/j.media.2015.08.001
   de Brebisson A., 2015, CVPR BIOIM COMP WORK
   Dubois J, 2008, BRAIN, V131, P2028, DOI 10.1093/brain/awn137
   Fischl B, 2002, NEURON, V33, P341, DOI 10.1016/S0896-6273(02)00569-X
   Gui L, 2012, MED IMAGE ANAL, V16, P1565, DOI 10.1016/j.media.2012.07.006
   Habas PA, 2010, HUM BRAIN MAPP, V31, P1348, DOI 10.1002/hbm.20935
   Hogstrom LJ, 2013, CEREB CORTEX, V23, P2521, DOI 10.1093/cercor/bhs231
   Isgum I, 2015, MED IMAGE ANAL, V20, P135, DOI 10.1016/j.media.2014.11.001
   Jia Li, 2012, Advances in Neural Networks - ISNN 2012. Proceedings 9th International Symposium on Neural Networks, P110, DOI 10.1007/978-3-642-31362-2_13
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   Landman B. A., 2012, MICCAI 2012 WORKSH M
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li G, 2014, J NEUROSCI, V34, P4228, DOI 10.1523/JNEUROSCI.3976-13.2014
   Likar B, 2001, IEEE T MED IMAGING, V20, P1398, DOI 10.1109/42.974934
   Lyall AE, 2015, CEREB CORTEX, V25, P2204, DOI 10.1093/cercor/bhu027
   Makropoulos A, 2014, IEEE T MED IMAGING, V33, P1818, DOI 10.1109/TMI.2014.2322280
   Marcus DS, 2007, J COGNITIVE NEUROSCI, V19, P1498, DOI 10.1162/jocn.2007.19.9.1498
   Mendrik AM, 2015, COMPUT INTEL NEUROSC, DOI 10.1155/2015/813696
   Moeskops P, 2015, NEUROIMAGE, V118, P628, DOI 10.1016/j.neuroimage.2015.06.007
   Moeskops P, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0131552
   Nair V, 2010, ICML, V27, P807, DOI DOI 10.0RG/PAPERS/432.PDF
   Prasoon A, 2013, LECT NOTES COMPUT SC, V8150, P246, DOI 10.1007/978-3-642-40763-5_31
   Prastawa M, 2005, MED IMAGE ANAL, V9, P457, DOI 10.1016/j.media.2005.05.007
   Rodriguez-Carranza CE, 2008, NEUROIMAGE, V41, P462, DOI 10.1016/j.neuroimage.2008.01.008
   Roth H. R., 2015, SPIE MED IMAGING
   Roth HR, 2014, LECT NOTES COMPUT SC, V8673, P520, DOI 10.1007/978-3-319-10404-1_65
   Roth HR, 2015, LECT NOTES COMPUT SC, V9349, P556, DOI 10.1007/978-3-319-24553-9_68
   Salat DH, 2004, CEREB CORTEX, V14, P721, DOI 10.1093/cercor/bhh032
   Sermanet P, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P2809, DOI 10.1109/IJCNN.2011.6033589
   Shi F, 2010, NEUROIMAGE, V49, P391, DOI 10.1016/j.neuroimage.2009.07.066
   Smith SM, 2002, HUM BRAIN MAPP, V17, P143, DOI 10.1002/hbm.10062
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Thambisetty M, 2010, NEUROIMAGE, V52, P1215, DOI 10.1016/j.neuroimage.2010.04.258
   Tieleman T., 2012, COURSERA NEURAL NETW, V4
   Veta M, 2015, MED IMAGE ANAL, V20, P237, DOI 10.1016/j.media.2014.11.010
   Vrooman HA, 2007, NEUROIMAGE, V37, P71, DOI 10.1016/j.neuroimage.2007.05.018
   Wang L, 2015, NEUROIMAGE, V108, P160, DOI 10.1016/j.neuroimage.2014.12.042
   Weisenfeld NI, 2009, NEUROIMAGE, V47, P564, DOI 10.1016/j.neuroimage.2009.04.068
   Wolterink JM, 2015, LECT NOTES COMPUT SC, V9349, P589, DOI 10.1007/978-3-319-24553-9_72
   Wright R, 2014, NEUROIMAGE, V91, P21, DOI 10.1016/j.neuroimage.2014.01.034
   Xue H, 2007, NEUROIMAGE, V38, P461, DOI 10.1016/j.neuroimage.2007.07.030
   Zhang WL, 2015, NEUROIMAGE, V108, P214, DOI 10.1016/j.neuroimage.2014.12.061
NR 46
TC 239
Z9 247
U1 17
U2 125
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 0278-0062
EI 1558-254X
J9 IEEE T MED IMAGING
JI IEEE Trans. Med. Imaging
PD MAY
PY 2016
VL 35
IS 5
SI SI
BP 1252
EP 1261
DI 10.1109/TMI.2016.2548501
PG 10
WC Computer Science, Interdisciplinary Applications; Engineering,
   Biomedical; Engineering, Electrical & Electronic; Imaging Science &
   Photographic Technology; Radiology, Nuclear Medicine & Medical Imaging
SC Computer Science; Engineering; Imaging Science & Photographic
   Technology; Radiology, Nuclear Medicine & Medical Imaging
GA DL3RI
UT WOS:000375550500010
PM 27046893
HC Y
HP N
DA 2020-02-19
ER

PT J
AU van Grinsven, MJJP
   van Ginneken, B
   Hoyng, CB
   Theelen, T
   Sanchez, CI
AF van Grinsven, Mark J. J. P.
   van Ginneken, Bram
   Hoyng, Carel B.
   Theelen, Thomas
   Sanchez, Clara I.
TI Fast Convolutional Neural Network Training Using Selective Data
   Sampling: Application to Hemorrhage Detection in Color Fundus Images
SO IEEE TRANSACTIONS ON MEDICAL IMAGING
LA English
DT Article
DE Convolutional neural network; deep learning; hemorrhage; selective
   sampling
ID AUTOMATIC DETECTION; MICROANEURYSMS; TRANSFORM; LESIONS
AB Convolutional neural networks (CNNs) are deep learning network architectures that have pushed forward the state-of-the-art in a range of computer vision applications and are increasingly popular in medical image analysis. However, training of CNNs is time-consuming and challenging. In medical image analysis tasks, the majority of training examples are easy to classify and therefore contribute little to the CNN learning process. In this paper, we propose a method to improve and speed-up the CNN training for medical image analysis tasks by dynamically selecting misclassified negative samples during training. Training samples are heuristically sampled based on classification by the current status of the CNN. Weights are assigned to the training samples and informative samples are more likely to be included in the next CNN training iteration. We evaluated and compared our proposed method by training a CNN with (SeS) and without (NSeS) the selective sampling method. We focus on the detection of hemorrhages in color fundus images. A decreased training time from 170 epochs to 60 epochs with an increased performance-on par with two human experts-was achieved with areas under the receiver operating characteristics curve of 0.894 and 0.972 on two data sets. The SeS CNN statistically outperformed the NSeS CNN on an independent test set.
C1 [van Grinsven, Mark J. J. P.; van Ginneken, Bram; Sanchez, Clara I.] Radboud Univ Nijmegen, Med Ctr, Dept Radiol & Nucl Med, Diagnost Image Anal Grp, NL-6525 GA Nijmegen, Netherlands.
   [Hoyng, Carel B.; Theelen, Thomas] Radboud Univ Nijmegen, Dept Ophthalmol, Med Ctr, NL-6525 EX Nijmegen, Netherlands.
   [Sanchez, Clara I.] Radboud Univ Nijmegen, Dept Ophthalmol, Med Ctr, NL-6525 GA Nijmegen, Netherlands.
RP van Grinsven, MJJP (reprint author), Radboud Univ Nijmegen, Med Ctr, Dept Radiol & Nucl Med, Diagnost Image Anal Grp, NL-6525 GA Nijmegen, Netherlands.
EM mark.vangrinsven@radboudumc.nl
RI van Ginneken, Bram/A-3728-2012; Gutierrez, Clara Isabel
   Sanchez/N-3580-2014; Theelen, Thomas/A-3192-2012
OI van Ginneken, Bram/0000-0003-2028-8972; 
FU ZonMwNetherlands Organization for Health Research and Development
   [11.631.0003]
FX This project was funded by ZonMw grant: "A cost effective solution for
   the prevention of blindness using computer-aided diagnosis and fundus
   photography", with Project 11.631.0003. Asterisk indicates corresponding
   author.
CR Abramoff Michael D, 2010, IEEE Rev Biomed Eng, V3, P169, DOI 10.1109/RBME.2010.2084567
   Akram UM, 2012, J MED SYST, V36, P3151, DOI 10.1007/s10916-011-9802-2
   Cruz-Roa AA, 2013, LECT NOTES COMPUT SC, V8150, P403, DOI 10.1007/978-3-642-40763-5_50
   Antal B, 2013, COMPUT MED IMAG GRAP, V37, P403, DOI 10.1016/j.compmedimag.2013.05.001
   BALLARD DH, 1981, PATTERN RECOGN, V13, P111, DOI 10.1016/0031-3203(81)90009-1
   Bergstra J, 2012, J MACH LEARN RES, V13, P281
   Chawla N. V., 2004, ACM SIGKDD EXPLORATI, V6, P1, DOI [10.1145/1007730.1007733, DOI 10.1145/1007730.1007733]
   Chen H, 2015, I S BIOMED IMAGING, P764, DOI 10.1109/ISBI.2015.7163984
   Cheung N, 2010, LANCET, V376, P124, DOI 10.1016/S0140-6736(09)62124-3
   Ciresan D, 2012, NEURAL NETWORKS, V32, P333, DOI 10.1016/j.neunet.2012.02.023
   Ciresan DC, 2013, LECT NOTES COMPUT SC, V8150, P411, DOI 10.1007/978-3-642-40763-5_51
   Efraimidis P., 2008, ENCY ALGORITHMS, P1
   Ertekin S., 2007, P 16 ACM C INF KNOWL, P127, DOI DOI 10.1145/1321440.1321461
   Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231
   Freund Y, 1997, MACH LEARN, V28, P133, DOI 10.1023/A:1007330508534
   Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504
   Graham B., 2015, KAGGLE DIABETIC RETI
   Gubern-Merida A, 2015, MED IMAGE ANAL, V20, P265, DOI 10.1016/j.media.2014.12.001
   Guo YR, 2014, LECT NOTES COMPUT SC, V8674, P308, DOI 10.1007/978-3-319-10470-6_39
   Haibo He, 2009, IEEE Transactions on Knowledge and Data Engineering, V21, P1263, DOI 10.1109/TKDE.2008.239
   Halevy A, 2009, IEEE INTELL SYST, V24, P8, DOI 10.1109/MIS.2009.36
   He K., 2015, ARXIV150201852V1
   He Xin, 2009, J Am Coll Radiol, V6, P652, DOI 10.1016/j.jacr.2009.06.001
   Ioffe Sergey, 2015, ARXIV150203167
   Jitpakdee P., 2012, P 9 INT C EL ENG EL, P1
   Kande GB, 2010, J DIGIT IMAGING, V23, P430, DOI 10.1007/s10278-009-9246-0
   Karssemeijer N, 2003, RADIOLOGY, V227, P192, DOI 10.1148/radiol.2271011962
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lin ML, 2013, IEEE T NEUR NET LEAR, V24, P647, DOI 10.1109/TNNLS.2012.2228231
   Lipowski A, 2012, PHYSICA A, V391, P2193, DOI 10.1016/j.physa.2011.12.004
   Long Jonathan, 2015, ARXIV14114038
   Marrugo Andres G, 2011, Journal of Physics: Conference Series, V274, DOI 10.1088/1742-6596/274/1/012039
   Niemeijer M, 2005, IEEE T MED IMAGING, V24, P584, DOI 10.1109/TMI.2005.843738
   Quellec G, 2008, IEEE T MED IMAGING, V27, P1230, DOI 10.1109/TMI.2008.920619
   RAJAN VY, 1989, INFORM PROCESS LETT, V30, P265, DOI 10.1016/0020-0190(89)90206-8
   Rasta Seyed Hossein, 2015, J Med Signals Sens, V5, P40
   Samuelson FW, 2007, I S BIOMED IMAGING, P492, DOI 10.1109/ISBI.2007.356896
   Samuelson FW, 2006, I S BIOMED IMAGING, P1312
   Schapire RE, 2003, LECT NOTES STAT, V171, P149
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Seff A, 2014, LECT NOTES COMPUT SC, V8673, P544, DOI 10.1007/978-3-319-10404-1_68
   Simonyan K., 2014, ARXIV14091556
   Sinthanayothin C, 2002, DIABETIC MED, V19, P105, DOI 10.1046/j.1464-5491.2002.00613.x
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Tang L, 2013, IEEE T MED IMAGING, V32, P364, DOI 10.1109/TMI.2012.2227119
   van Grinsven MJJP, 2013, INVEST OPHTH VIS SCI, V54, P3019, DOI 10.1167/iovs.12-11449
   Vijayprasath S., 2015, INT J ELECT COMMUN E, V2, P29
NR 48
TC 103
Z9 104
U1 3
U2 71
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 0278-0062
EI 1558-254X
J9 IEEE T MED IMAGING
JI IEEE Trans. Med. Imaging
PD MAY
PY 2016
VL 35
IS 5
SI SI
BP 1273
EP 1284
DI 10.1109/TMI.2016.2526689
PG 12
WC Computer Science, Interdisciplinary Applications; Engineering,
   Biomedical; Engineering, Electrical & Electronic; Imaging Science &
   Photographic Technology; Radiology, Nuclear Medicine & Medical Imaging
SC Computer Science; Engineering; Imaging Science & Photographic
   Technology; Radiology, Nuclear Medicine & Medical Imaging
GA DL3RI
UT WOS:000375550500012
PM 26886969
DA 2020-02-19
ER

PT J
AU Tajbakhsh, N
   Shin, JY
   Gurudu, SR
   Hurst, RT
   Kendall, CB
   Gotway, MB
   Liang, JM
AF Tajbakhsh, Nima
   Shin, Jae Y.
   Gurudu, Suryakanth R.
   Hurst, R. Todd
   Kendall, Christopher B.
   Gotway, Michael B.
   Liang, Jianming
TI Convolutional Neural Networks for Medical Image Analysis: Full Training
   or Fine Tuning?
SO IEEE TRANSACTIONS ON MEDICAL IMAGING
LA English
DT Article
DE Carotid intima-media thickness; computer-aided detection; convolutional
   neural networks; deep learning; fine-tuning; medical image analysis;
   polyp detection; pulmonary embolism detection; video quality assessment
ID MISS RATE; PATTERN-RECOGNITION; COLORECTAL-CANCER; TEXTURE FEATURES;
   ULTRASOUND; SEGMENTATION; COLONOSCOPY; POLYPS; MICROCALCIFICATIONS;
   SHIFT
AB Training a deep convolutional neural network (CNN) from scratch is difficult because it requires a large amount of labeled training data and a great deal of expertise to ensure proper convergence. A promising alternative is to fine-tune a CNN that has been pre-trained using, for instance, a large set of labeled natural images. However, the substantial differences between natural and medical images may advise against such knowledge transfer. In this paper, we seek to answer the following central question in the context of medical image analysis: Can the use of pre-trained deep CNNs with sufficient fine-tuning eliminate the need for training a deep CNN from scratch? To address this question, we considered four distinct medical imaging applications in three specialties (radiology, cardiology, and gastroenterology) involving classification, detection, and segmentation from three different imaging modalities, and investigated how the performance of deep CNNs trained from scratch compared with the pre-trained CNNs fine-tuned in a layer-wise manner. Our experiments consistently demonstrated that 1) the use of a pre-trained CNN with adequate fine-tuning outperformed or, in the worst case, performed as well as a CNN trained from scratch; 2) fine-tuned CNNs were more robust to the size of training sets than CNNs trained from scratch; 3) neither shallow tuning nor deep tuning was the optimal choice for a particular application; and 4) our layer-wise fine-tuning scheme could offer a practical way to reach the best performance for the application at hand based on the amount of available data.
C1 [Tajbakhsh, Nima; Shin, Jae Y.; Liang, Jianming] Arizona State Univ, Dept Biomed Informat, Scottsdale, AZ 85259 USA.
   [Gurudu, Suryakanth R.] Mayo Clin, Div Gastroenterol & Hepatol, Scottsdale, AZ 85259 USA.
   [Hurst, R. Todd; Kendall, Christopher B.] Mayo Clin, Div Cardiovasc Dis, Scottsdale, AZ 85259 USA.
   [Gotway, Michael B.] Mayo Clin, Dept Radiol, Scottsdale, AZ 85259 USA.
RP Liang, JM (reprint author), Arizona State Univ, Dept Biomed Informat, Scottsdale, AZ 85259 USA.
EM nima.tajbakhsh@asu.edu; sejong@asu.edu; gurudu.suryakanth@mayo.edu;
   hurst.r@mayo.edu; kendall.christopher@mayo.edu; gotway.michael@mayo.edu;
   jianming.liang@asu.edu
RI Tajbakhsh, Nima/AAC-1354-2019
CR Abdel-Hamid O, 2014, IEEE-ACM T AUDIO SPE, V22, P1533, DOI 10.1109/TASLP.2014.2339736
   Alexandre LA, 2008, INT CONF BIOMED, P38, DOI 10.1109/BMEI.2008.246
   Arevalo J, 2015, IEEE ENG MED BIO, P797, DOI 10.1109/EMBC.2015.7318482
   Arnold M, 2009, 2009 13TH INTERNATIONAL MACHINE VISION AND IMAGE PROCESSING CONFERENCE, P47, DOI 10.1109/IMVIP.2009.16
   Azizpour H., 2014, GENERIC SPE IN PRESS
   Bar Y., 2015, P SPIE MED IMAG
   Bernal J, 2012, PATTERN RECOGN, V45, P3166, DOI 10.1016/j.patcog.2012.03.002
   Bernal J, 2013, IEEE ENG MED BIO, P7350, DOI 10.1109/EMBC.2013.6611256
   Calder KK, 2005, ANN EMERG MED, V45, P302, DOI 10.1016/j.annemergmed.2004.10.001
   Carneiro G, 2015, LECT NOTES COMPUT SC, V9351, P652, DOI 10.1007/978-3-319-24574-4_78
   CHAN HP, 1995, MED PHYS, V22, P1555, DOI 10.1118/1.597428
   Chen H, 2015, IEEE J BIOMED HEALTH, V19, P1627, DOI 10.1109/JBHI.2015.2425041
   Cire D. C., 2012, ADV NEURAL INFORM PR, V2, P2843
   Ciresan DC, 2013, LECT NOTES COMPUT SC, V8150, P411, DOI 10.1007/978-3-642-40763-5_51
   Edwards DC, 2002, MED PHYS, V29, P2861, DOI 10.1118/1.1524631
   Eigen D., 2013, UNDERSTANDI IN PRESS
   Erhan D., 2009, J MACHINE LEARNING R, V5, P153
   Fairfield J., 1990, Proceedings. 10th International Conference on Pattern Recognition (Cat. No.90CH2898-5), P712, DOI 10.1109/ICPR.1990.118200
   FUKUSHIMA K, 1980, BIOL CYBERN, V36, P193, DOI 10.1007/BF00344251
   Gao M, 2015, 1 WORKSH DEEP LEARN
   Glorot X., 2010, JLMR P TRACK, p[249, 2010], DOI DOI 10.1177/1753193409103364.
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314
   Havaei M, 2015, BRAIN TUMOR SEGMENTA
   He K, 2015, DELVING DEEP RECTIFI
   Heresbach D, 2008, ENDOSCOPY, V40, P284, DOI 10.1055/s-2007-995618
   HUBEL DH, 1959, J PHYSIOL-LONDON, V148, P574, DOI 10.1113/jphysiol.1959.sp006308
   Hwang S., 2007, P IEEE INT C IM PROC, V2, P465
   Iakovidis DK, 2005, COMP MED SY, P575, DOI 10.1109/CBMS.2005.6
   Jia Y, 2014, CONVOLUTIONAL ARCHIT
   Karkanis SA, 2003, IEEE T INF TECHNOL B, V7, P141, DOI 10.1109/TITB.2003.813794
   Kim DH, 2007, NEW ENGL J MED, V357, P1403, DOI 10.1056/NEJMoa070543
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lee H., 2009, P 26 ANN INT C MACH, P609, DOI DOI 10.1145/1553374.1553453
   Leufkens AM, 2012, ENDOSCOPY, V44, P470, DOI 10.1055/s-0031-1291666
   Liang JM, 2007, LECT NOTES COMPUT SC, V4584, P630
   Liang JM, 2006, MED IMAGE ANAL, V10, P215, DOI 10.1016/j.media.2005.09.002
   Lo SCB, 1995, IEEE T MED IMAGING, V14, P711, DOI 10.1109/42.476112
   Margeta J, 2015, COMPUT METHOD BIOMEC, P1, DOI DOI 10.1080/21681163.2015
   Menchon-Lara Rosa-Maria, 2013, Natural and Artificial Computation in Engineering and Medical Applications. 5th International Work-Conference on the Interplay Between Natural and Artificial Computation, IWINAC 2013. Proceedings: LNCS 7931, P241, DOI 10.1007/978-3-642-38622-0_25
   Menchon-Lara RM, 2015, NEUROCOMPUTING, V151, P161, DOI 10.1016/j.neucom.2014.09.066
   MIT Technol. Rev, 10 BREAKTHR TECHN
   Oh J, 2007, MED IMAGE ANAL, V11, P110, DOI 10.1016/j.media.2006.10.003
   Pabby A, 2005, GASTROINTEST ENDOSC, V61, P385, DOI 10.1016/S0016-5107(04)02765-8
   Park SY, 2012, IEEE T BIO-MED ENG, V59, P1408, DOI 10.1109/TBME.2012.2188397
   Penatti O. A., P IEEE C COMP VIS PA, P44
   Petroudi S, 2012, IEEE T BIO-MED ENG, V59, P3060, DOI 10.1109/TBME.2012.2214387
   Prasoon A, 2013, LECT NOTES COMPUT SC, V8150, P246, DOI 10.1007/978-3-642-40763-5_31
   Rabeneck L, 2003, AM J GASTROENTEROL, V98, P471, DOI 10.1016/S0002-9270(02)05928-2
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Roth H. R., 2015, P SPIE MED IMAG
   Roth HR, 2014, LECT NOTES COMPUT SC, V8673, P520, DOI 10.1007/978-3-319-10404-1_65
   Sadigh G., 2011, AM J ROENTGENOL, V196
   Schlegl T, 2014, LECT NOTES COMPUT SC, V8848, P82, DOI 10.1007/978-3-319-13972-2_8
   Sharma H., 2014, P SPIE MED IMAG
   Shin HC, 2015, PROC CVPR IEEE, P1090, DOI 10.1109/CVPR.2015.7298712
   Shin J. Y., 2016, P IEEE C COMP VIS PA
   Szegedy C., 2014, GOING DEEPE IN PRESS
   Tajbakhsh N., 2015, P MICCAI
   Tajbakhsh N, 2016, IEEE T MED IMAGING, V35, P630, DOI 10.1109/TMI.2015.2487997
   Tajbakhsh N, 2015, I S BIOMED IMAGING, P79, DOI 10.1109/ISBI.2015.7163821
   Tajbakhsh Nima, 2015, Inf Process Med Imaging, V24, P327, DOI 10.1007/978-3-319-19992-4_25
   Tajbakhsh N, 2014, I S BIOMED IMAGING, P97, DOI 10.1109/ISBI.2014.6867818
   Tajbakhsh N, 2014, LECT NOTES COMPUT SC, V8676, P151, DOI 10.1007/978-3-319-13692-9_14
   Tajbakhsh N, 2014, LECT NOTES COMPUT SC, V8674, P179, DOI 10.1007/978-3-319-10470-6_23
   Tajbakhsh N, 2013, LECT NOTES COMPUT SC, V8198, P53, DOI 10.1007/978-3-642-41083-3_7
   van Ginneken B, 2015, I S BIOMED IMAGING, P286, DOI 10.1109/ISBI.2015.7163869
   van Rijn JC, 2006, AM J GASTROENTEROL, V101, P343, DOI 10.1111/j.1572-0241.2006.00390.x
   Wang Y, 2014, IEEE J BIOMED HEALTH, V18, P1379, DOI 10.1109/JBHI.2013.2285230
   Wulsin DF, 2011, J NEURAL ENG, V8, DOI 10.1088/1741-2560/8/3/036015
   Xu XY, 2012, COMPUT MED IMAG GRAP, V36, P248, DOI 10.1016/j.compmedimag.2011.06.007
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   ZHANG W, 1994, MED PHYS, V21, P517, DOI 10.1118/1.597177
   Zhang WL, 2015, NEUROIMAGE, V108, P214, DOI 10.1016/j.neuroimage.2014.12.061
   Zheng YF, 2015, LECT NOTES COMPUT SC, V9349, P565, DOI 10.1007/978-3-319-24553-9_69
   Zisserman A., 2014, VERY DEEP C IN PRESS
NR 76
TC 497
Z9 520
U1 55
U2 455
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 0278-0062
EI 1558-254X
J9 IEEE T MED IMAGING
JI IEEE Trans. Med. Imaging
PD MAY
PY 2016
VL 35
IS 5
SI SI
BP 1299
EP 1312
DI 10.1109/TMI.2016.2535302
PG 14
WC Computer Science, Interdisciplinary Applications; Engineering,
   Biomedical; Engineering, Electrical & Electronic; Imaging Science &
   Photographic Technology; Radiology, Nuclear Medicine & Medical Imaging
SC Computer Science; Engineering; Imaging Science & Photographic
   Technology; Radiology, Nuclear Medicine & Medical Imaging
GA DL3RI
UT WOS:000375550500014
PM 26978662
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Kallenberg, M
   Petersen, K
   Nielsen, M
   Ng, AY
   Diao, PF
   Igel, C
   Vachon, CM
   Holland, K
   Winkel, RR
   Karssemeijer, N
   Lillholm, M
AF Kallenberg, Michiel
   Petersen, Kersten
   Nielsen, Mads
   Ng, Andrew Y.
   Diao, Pengfei
   Igel, Christian
   Vachon, Celine M.
   Holland, Katharina
   Winkel, Rikke Rass
   Karssemeijer, Nico
   Lillholm, Martin
TI Unsupervised Deep Learning Applied to Breast Density Segmentation and
   Mammographic Risk Scoring
SO IEEE TRANSACTIONS ON MEDICAL IMAGING
LA English
DT Article
DE Breast cancer; deep learning; mammograms; prognosis; risk factor;
   segmentation; unsupervised feature learning
ID CANCER RISK; PARENCHYMAL PATTERNS; DIGITAL MAMMOGRAPHY; IMAGES;
   NETWORKS; FEATURES; SCALE
AB Mammographic risk scoring has commonly been automated by extracting a set of handcrafted features from mammograms, and relating the responses directly or indirectly to breast cancer risk. We present a method that learns a feature hierarchy from unlabeled data. When the learned features are used as the input to a simple classifier, two different tasks can be addressed: i) breast density segmentation, and ii) scoring of mammographic texture. The proposed model learns features at multiple scales. To control the models capacity a novel sparsity regularizer is introduced that incorporates both lifetime and population sparsity. We evaluated our method on three different clinical datasets. Our state-of-the-art results show that the learned breast density scores have a very strong positive relationship with manual ones, and that the learned texture scores are predictive of breast cancer. The model is easy to apply and generalizes to many other segmentation and scoring problems.
C1 [Kallenberg, Michiel; Petersen, Kersten; Nielsen, Mads; Diao, Pengfei; Igel, Christian; Lillholm, Martin] Univ Copenhagen, Dept Comp Sci, DK-2100 Copenhagen, Denmark.
   [Kallenberg, Michiel] Biomediq AS, DK-2100 Copenhagen, Denmark.
   [Ng, Andrew Y.] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.
   [Vachon, Celine M.] Mayo Clin Hosp, Phoenix, AZ 85054 USA.
   [Holland, Katharina; Karssemeijer, Nico] Radboud Univ Nijmegen, Med Ctr, NL-6525 GA Nijmegen, Netherlands.
   [Winkel, Rikke Rass] Univ Copenhagen Hosp, DK-2100 Copenhagen, Denmark.
RP Kallenberg, M (reprint author), Univ Copenhagen, Dept Comp Sci, DK-2100 Copenhagen, Denmark.
EM michielkallenberg@gmail.com
RI Holland, Katharina/E-1624-2016; Karssemeijer, N./H-8058-2014
FU Innovation Fund Denmark [141-2013-6]; Danish National Advanced
   Technology Foundation; European Seventh Framework Programme FP7
   [306088]; European CommissionEuropean Commission Joint Research Centre
   [PCIG10-GA-2011-303655]; Foundation of Population Screening Mid West
   Netherlands; Villum Fonden [8721]
FX This work was supported by the Innovation Fund Denmark under the grant
   141-2013-6, the Danish National Advanced Technology Foundation under
   grant Personalized Breast Cancer Screening, the European Seventh
   Framework Programme FP7 under grant agreement no 306088, the European
   Commission through project AKMI (PCIG10-GA-2011-303655), and the
   Foundation of Population Screening Mid West Netherlands. M. Kallenberg
   and K. Petersen contributed equally to this work. Asterisk indicates
   corresponding author.
CR ALAIN D, 2013, JMLR WORKSHOP C P, P154
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Boyd NF, 2010, JNCI-J NATL CANCER I, V102, P1224, DOI 10.1093/jnci/djq239
   BYNG JW, 1994, PHYS MED BIOL, V39, P1629, DOI 10.1088/0031-9155/39/10/008
   Byng JW, 1996, PHYS MED BIOL, V41, P909, DOI 10.1088/0031-9155/41/5/007
   Cire D. C., 2012, ADV NEURAL INFORM PR, V2, P2843
   Coates A., 2012, THESIS STANFORD U ST
   Cuzick J, 2011, J NATL CANCER I, V103, P744, DOI 10.1093/jnci/djr079
   Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231
   Ferlay J, 2013, EUR J CANCER, V49, P1374, DOI 10.1016/j.ejca.2012.12.027
   Ferrari RJ, 2004, MED BIOL ENG COMPUT, V42, P378, DOI 10.1007/BF02344714
   Florack L, 2000, IEEE T PATTERN ANAL, V22, P1050, DOI 10.1109/34.877526
   Florack L, 1996, INT J COMPUT VISION, V18, P61, DOI 10.1007/BF00126140
   Fonseca P., 2015, P SPIE MED IMAG
   Gram IT, 1997, EUR J RADIOL, V24, P131, DOI 10.1016/S0720-048X(96)01138-2
   Haberle L, 2012, BREAST CANCER RES, V14, DOI 10.1186/bcr3163
   He WD, 2015, INT J BREAST CANCER, DOI 10.1155/2015/276217
   Heine JJ, 2008, CANCER EPIDEM BIOMAR, V17, P3090, DOI 10.1158/1055-9965.EPI-08-0170
   Heine JJ, 2012, J NATL CANCER I, V104, P1028, DOI 10.1093/jnci/djs254
   HIGHNAM RH, 1999, MAMMOGRAPHIC IMAGE A
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2007, TRENDS COGN SCI, V11, P428, DOI 10.1016/j.tics.2007.09.004
   Huo ZM, 2000, MED PHYS, V27, P4, DOI 10.1118/1.598851
   Jamieson A. R., 2011, P SPIE MED IMAG
   Jamieson A. R., 2012, P SPIE MED IMAG
   Jemal A, 2011, CA-CANCER J CLIN, V61, P134, DOI 10.3322/caac.20115
   Kallenberg MGJ, 2011, PHYS MED BIOL, V56, P2715, DOI 10.1088/0031-9155/56/9/005
   Keller BM, 2015, BREAST CANCER RES, V17, DOI 10.1186/s13058-015-0626-8
   Keller BM, 2012, MED PHYS, V39, P4903, DOI 10.1118/1.4736530
   Le Q.V., 2012, P INT C MACH LEARN
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee H., 2008, ADV NEURAL INFORM PR, V20, P873
   Li H, 2008, J DIGIT IMAGING, V21, P145, DOI 10.1007/s10278-007-9093-9
   Li JM, 2012, BREAST CANCER RES, V14, DOI 10.1186/bcr3238
   Lindeberg T., 1994, SCALE SPACE THEORY C
   Manduca A, 2009, CANCER EPIDEM BIOMAR, V18, P837, DOI 10.1158/1055-9965.EPI-08-0631
   Masci J, 2011, LECT NOTES COMPUT SC, V6791, P52, DOI 10.1007/978-3-642-21735-7_7
   McCormack VA, 2006, CANCER EPIDEM BIOMAR, V15, P1159, DOI 10.1158/1055-9965.EPI-06-0034
   Montavon G., 2012, NEURAL NETWORKS TRIC, V7700
   Nair V, 2010, ICML, V27, P807, DOI DOI 10.0RG/PAPERS/432.PDF
   Nickson C, 2013, BREAST CANCER RES, V15, DOI 10.1186/bcr3474
   Nielsen M, 2011, CANCER EPIDEMIOL, V35, P381, DOI 10.1016/j.canep.2010.10.011
   Nielsen M, 2014, BREAST CANCER RES, V16, DOI 10.1186/bcr3641
   Ning F, 2005, IEEE T IMAGE PROCESS, V14, P1360, DOI 10.1109/TIP.2005.852470
   Oliver A., 2007, MED IMAGE UNDERSTAND, P223
   Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0
   Olson JE, 2012, BREAST CANCER RES, V14, DOI 10.1186/bcr3357
   Paine T. L., 2014, ANAL UNSUPERVISED PR
   Petersen Kersten, 2014, Breast Imaging. 12th International Workshop, IWDM 2014. Proceedings: LNCS 8539, P88, DOI 10.1007/978-3-319-07887-8_13
   Petersen K., 2012, P SPARS TECH MED IM
   Petroudi S, 2003, P ANN INT IEEE EMBS, V25, P798, DOI 10.1109/IEMBS.2003.1279885
   Ranzato MA, 2007, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2007.383157
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Scholkopf B., 2007, ADV NEURAL INFORM PR, V19, P1137, DOI DOI 10.7551/mitpress/7503.003.0147
   Sivaramakrishna R, 2001, ACAD RADIOL, V8, P250, DOI 10.1016/S1076-6332(03)80534-2
   Tabar L, 1999, CANCER, V86, P449, DOI 10.1002/(SICI)1097-0142(19990801)86:3<449::AID-CNCR13>3.3.CO;2-H
   Tan S., 2014, P AS PAC SIGN INF PR, P1, DOI 10.1109/APSIPA.2014
   Torrent A, 2008, LECT NOTES COMPUT SC, V5116, P9, DOI 10.1007/978-3-540-70538-3_2
   Turaga SC, 2010, NEURAL COMPUT, V22, P511, DOI 10.1162/neco.2009.10-08-881
   van Engeland S, 2006, IEEE T MED IMAGING, V25, P273, DOI 10.1109/TMI.2005.862741
   Vincent P, 2008, P 25 INT C MACH LEAR, P1096, DOI DOI 10.1145/1390156.1390294
   Wagner Raimar, 2013, NEUR NETW IJCNN 2013, V8, P1, DOI DOI 10.1109/IJCNN.2013.6706969
   Wang S, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON NANOSCALE ARCHITECTURE, P1
   WEI D, 1995, ACOUST SPEECH SIGNAL, V5, P3483
   WOLFE JN, 1976, CANCER, V37, P2486, DOI 10.1002/1097-0142(197605)37:5<2486::AID-CNCR2820370542>3.0.CO;2-8
   Zheng YJ, 2015, MED PHYS, V42, P4149, DOI 10.1118/1.4921996
NR 67
TC 131
Z9 136
U1 12
U2 84
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 0278-0062
EI 1558-254X
J9 IEEE T MED IMAGING
JI IEEE Trans. Med. Imaging
PD MAY
PY 2016
VL 35
IS 5
SI SI
BP 1322
EP 1331
DI 10.1109/TMI.2016.2532122
PG 10
WC Computer Science, Interdisciplinary Applications; Engineering,
   Biomedical; Engineering, Electrical & Electronic; Imaging Science &
   Photographic Technology; Radiology, Nuclear Medicine & Medical Imaging
SC Computer Science; Engineering; Imaging Science & Photographic
   Technology; Radiology, Nuclear Medicine & Medical Imaging
GA DL3RI
UT WOS:000375550500016
PM 26915120
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Yan, ZN
   Zhan, YQ
   Peng, ZG
   Liao, S
   Shinagawa, Y
   Zhang, ST
   Metaxas, DN
   Zhou, XS
AF Yan, Zhennan
   Zhan, Yiqiang
   Peng, Zhigang
   Liao, Shu
   Shinagawa, Yoshihisa
   Zhang, Shaoting
   Metaxas, Dimitris N.
   Zhou, Xiang Sean
TI Multi-Instance Deep Learning: Discover Discriminative Local Anatomies
   for Bodypart Recognition
SO IEEE TRANSACTIONS ON MEDICAL IMAGING
LA English
DT Article
DE CNN; discriminative local information discovery; multi-instance;
   multi-stage
ID IMAGE FEATURES; GRADIENTS
AB In general image recognition problems, discriminative information often lies in local image patches. For example, most human identity information exists in the image patches containing human faces. The same situation stays in medical images as well. "Bodypart identity" of a transversal slice-which bodypart the slice comes from-is often indicated by local image information, e.g., a cardiac slice and an aorta arch slice are only differentiated by the mediastinum region. In this work, we design a multi-stage deep learning framework for image classification and apply it on bodypart recognition. Specifically, the proposed framework aims at: 1) discover the local regions that are discriminative and non-informative to the image classification problem, and 2) learn a image-level classifier based on these local regions. We achieve these two tasks by the two stages of learning scheme, respectively. In the pre-train stage, a convolutional neural network (CNN) is learned in a multi-instance learning fashion to extract the most discriminative and and non-informative local patches from the training slices. In the boosting stage, the pre-learned CNN is further boosted by these local patches for image classification. The CNN learned by exploiting the discriminative local appearances becomes more accurate than those learned from global image context. The key hallmark of our method is that it automatically discovers the discriminative and non-informative local patches through multi-instance deep learning. Thus, no manual annotation is required. Our method is validated on a synthetic dataset and a large scale CT dataset. It achieves better performances than state-of-the-art approaches, including the standard deep CNN.
C1 [Yan, Zhennan; Metaxas, Dimitris N.] Rutgers State Univ, Dept Comp Sci, Piscataway, NJ 08854 USA.
   [Zhan, Yiqiang; Peng, Zhigang; Liao, Shu; Shinagawa, Yoshihisa; Zhou, Xiang Sean] Siemens Healthcare, Malvern, PA 19355 USA.
   [Zhang, Shaoting] Univ N Carolina, Dept Comp Sci, Charlotte, NC 28223 USA.
RP Zhan, YQ (reprint author), Siemens Healthcare, Malvern, PA 19355 USA.
EM zhennany@cs.rutgers.edu; yiqiang.zhan@siemens.com; szhang16@uncc.edu;
   dnm@cs.rutgers.edu
CR Andrews S, 2002, ADV NEURAL INFORM PR, V15, P561
   Bergstra James, 2010, P PYTH SCI COMP C JU
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Cheng MM, 2014, PROC CVPR IEEE, P3286, DOI 10.1109/CVPR.2014.414
   Ciresan D, 2012, PROC CVPR IEEE, P3642, DOI 10.1109/CVPR.2012.6248110
   COOTES TF, 1995, COMPUT VIS IMAGE UND, V61, P38, DOI 10.1006/cviu.1995.1004
   Criminisi A., 2009, MED IMAGE COMPUTING, P69
   Criminisi A, 2011, LECT NOTES COMPUT SC, V6533, P106, DOI 10.1007/978-3-642-18421-5_11
   Dalal N, 2005, PROC CVPR IEEE, P886
   Donner R, 2013, MED IMAGE ANAL, V17, P1304, DOI 10.1016/j.media.2013.02.004
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Freund Y., 1995, COMPUTATIONAL LEARNI, V904, P23, DOI DOI 10.1007/3-540-59119-2_166
   Guld MO, 2002, P SOC PHOTO-OPT INS, V4685, P280, DOI 10.1117/12.467017
   Hinton G. E, 2012, ARXIV12070580
   Hong S., 2008, U.S. Patent App, Patent No. [11/933 518, 11933518]
   Jia Yangqing, 2014, ARXIV14085093
   Juneja M, 2013, PROC CVPR IEEE, P923, DOI 10.1109/CVPR.2013.124
   Krahenbuhl P., 2012, ARXIV12105644
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kure T.M., 2015, ARXIV150407947
   Lazebnik S., 2006, P IEEE INT C COMP VI, V2, P2169, DOI DOI 10.1109/CVPR.2006.68
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Maron O., 1998, Machine Learning. Proceedings of the Fifteenth International Conference (ICML'98), P341
   Maron O, 1998, ADV NEUR IN, V10, P570
   Nair V, 2010, ICML, V27, P807, DOI DOI 10.0RG/PAPERS/432.PDF
   Oliva A, 2006, PROG BRAIN RES, V155, P23, DOI 10.1016/S0079-6123(06)55002-2
   Papandreou G., 2015, ARXIV150202734V2
   Parikh D, 2011, IEEE I CONF COMP VIS, P519, DOI 10.1109/ICCV.2011.6126283
   Park J, 2006, LECT NOTES COMPUT SC, V4223, P1148
   Pinheiro PO, 2015, PROC CVPR IEEE, P1713, DOI 10.1109/CVPR.2015.7298780
   Qian N, 1999, NEURAL NETWORKS, V12, P145, DOI 10.1016/S0893-6080(98)00116-6
   Roth HR, 2015, I S BIOMED IMAGING, P101, DOI 10.1109/ISBI.2015.7163826
   Sanchez J, 2011, PROC CVPR IEEE, P1665, DOI 10.1109/CVPR.2011.5995504
   Selvi S. T, 2009, ARXIV09122314
   Sermanet P., 2013, ARXIV13126229
   Simard PY, 2003, PROC INT CONF DOC, P958
   Singh S, 2012, LECT NOTES COMPUT SC, V7573, P73, DOI 10.1007/978-3-642-33709-3_6
   Szegedy C., 2013, ADV NEURAL INFORM PR, V26, P2553
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   van Ravesteijn VF, 2010, IEEE T MED IMAGING, V29, P120, DOI 10.1109/TMI.2009.2028576
   Wei Yunchao, 2014, ARXIV14065726
   Wu JJ, 2015, PROC CVPR IEEE, P3460, DOI 10.1109/CVPR.2015.7298968
   Yan Zhennan, 2015, Inf Process Med Imaging, V24, P449, DOI 10.1007/978-3-319-19992-4_35
   Yang JC, 2009, PROC CVPR IEEE, P1794, DOI 10.1109/CVPRW.2009.5206757
   Zha Z.-J., 2008, IEEE C COMP VIS PATT, P1, DOI DOI 10.1109/CVPR.2008.4587384
   Zhan YQ, 2008, LECT NOTES COMPUT SC, V5241, P313, DOI 10.1007/978-3-540-85988-8_38
   Zhang Q, 2002, ADV NEUR IN, V14, P1073
   Zhang ST, 2012, MED IMAGE ANAL, V16, P1385, DOI 10.1016/j.media.2012.07.007
   Zhang ST, 2011, LECT NOTES COMPUT SC, V6892, P451, DOI 10.1007/978-3-642-23629-7_55
   Zhang ST, 2012, MED IMAGE ANAL, V16, P265, DOI 10.1016/j.media.2011.08.004
NR 51
TC 65
Z9 65
U1 3
U2 34
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 0278-0062
EI 1558-254X
J9 IEEE T MED IMAGING
JI IEEE Trans. Med. Imaging
PD MAY
PY 2016
VL 35
IS 5
SI SI
BP 1332
EP 1343
DI 10.1109/TMI.2016.2524985
PG 12
WC Computer Science, Interdisciplinary Applications; Engineering,
   Biomedical; Engineering, Electrical & Electronic; Imaging Science &
   Photographic Technology; Radiology, Nuclear Medicine & Medical Imaging
SC Computer Science; Engineering; Imaging Science & Photographic
   Technology; Radiology, Nuclear Medicine & Medical Imaging
GA DL3RI
UT WOS:000375550500017
PM 26863652
DA 2020-02-19
ER

PT J
AU Golkov, V
   Dosovitskiy, A
   Sperl, JI
   Menzel, MI
   Czisch, M
   Samann, P
   Brox, T
   Cremers, D
AF Golkov, Vladimir
   Dosovitskiy, Alexey
   Sperl, Jonathan I.
   Menzel, Marion I.
   Czisch, Michael
   Saemann, Philipp
   Brox, Thomas
   Cremers, Daniel
TI q-Space Deep Learning: Twelve-Fold Shorter and Model-Free Diffusion MRI
   Scans
SO IEEE TRANSACTIONS ON MEDICAL IMAGING
LA English
DT Article
DE Artificial neural networks; diffusion kurtosis imaging (DKI); diffusion
   magnetic resonance imaging (diffusion MRI); neurite orientation
   dispersion and density imaging (NODDI)
ID GAUSSIAN WATER DIFFUSION; KURTOSIS; ARCHITECTURE; BRAIN
AB Numerous scientific fields rely on elaborate but partly suboptimal data processing pipelines. An example is diffusion magnetic resonance imaging (diffusion MRI), a non-invasive microstructure assessment method with a prominent application in neuroimaging. Advanced diffusion models providing accurate microstructural characterization so far have required long acquisition times and thus have been inapplicable for children and adults who are uncooperative, uncomfortable, or unwell. We show that the long scan time requirements are mainly due to disadvantages of classical data processing. We demonstrate how deep learning, a group of algorithms based on recent advances in the field of artificial neural networks, can be applied to reduce diffusion MRI data processing to a single optimized step. This modification allows obtaining scalar measures from advanced models at twelve-fold reduced scan time and detecting abnormalities without using diffusion models. We set a new state of the art by estimating diffusion kurtosis measures from only 12 data points and neurite orientation dispersion and density measures from only 8 data points. This allows unprecedentedly fast and robust protocols facilitating clinical routine and demonstrates how classical data processing can be streamlined by means of deep learning.
C1 [Golkov, Vladimir] Tech Univ Munich, Dept Comp Sci, D-85748 Munich, Germany.
   [Dosovitskiy, Alexey; Brox, Thomas] Univ Freiburg, Dept Comp Sci, D-79110 Freiburg, Germany.
   [Sperl, Jonathan I.; Menzel, Marion I.] GE Global Res, D-85748 Munich, Germany.
   [Czisch, Michael; Saemann, Philipp] Max Planck Inst Psychiat, D-80804 Munich, Germany.
   [Cremers, Daniel] Tech Univ Munich, Dept Comp Sci, D-85748 Garching, Germany.
RP Golkov, V (reprint author), Tech Univ Munich, Dept Comp Sci, D-85748 Munich, Germany.
EM golkov@cs.tum.edu
RI Menzel, Marion Irene/H-8952-2016
OI Menzel, Marion Irene/0000-0003-0087-9134
FU NIHUnited States Department of Health & Human ServicesNational
   Institutes of Health (NIH) - USA [1U54MH091657]; McDonnell Center for
   Systems Neuroscience at Washington University
FX The authors would like to thank S. Polsterl and B. Menze (TU Munich) for
   discussions. Data for Fig. 2 were provided by the Human Connectome
   Project, WU-Minn Consortium (Principal Investigators: D. Van Essen and
   K. Ugurbil; 1U54MH091657) funded by the 16 NIH Institutes and Centers
   that support the NIH Blueprint for Neuroscience Research; and by the
   McDonnell Center for Systems Neuroscience at Washington University.
CR Alexander DC, 2014, LECT NOTES COMPUT SC, V8675, P225, DOI 10.1007/978-3-319-10443-0_29
   Andersson JLR, 2003, NEUROIMAGE, V20, P870, DOI 10.1016/S1053-8119(03)00336-7
   [Anonymous], 2009, NEOPLASIA, V11, P102, DOI 10.1593/neo.81328
   Ashburner J, 2005, NEUROIMAGE, V26, P839, DOI 10.1016/j.neuroimage.2005.02.018
   Bagher-Ebadian H, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0022626
   Bengio Y., 2012, NEURAL NETWORKS TRIC
   Bilgic B, 2012, MAGN RESON MED, V68, P1747, DOI 10.1002/mrm.24505
   Cheung MM, 2009, NEUROIMAGE, V45, P386, DOI 10.1016/j.neuroimage.2008.12.018
   Feinberg DA, 2010, PLOS ONE, V5, DOI 10.1371/journal.pone.0015710
   Ferizi U, 2015, NEUROIMAGE, V118, P468, DOI 10.1016/j.neuroimage.2015.06.027
   Fischl B, 2012, NEUROIMAGE, V62, P774, DOI 10.1016/j.neuroimage.2012.01.021
   Gillard J., 2010, CLIN MR NEUROIMAGING
   Glasser MF, 2013, NEUROIMAGE, V80, P105, DOI 10.1016/j.neuroimage.2013.04.127
   Golkov V, 2016, I S BIOMED IMAGING, P1233, DOI 10.1109/ISBI.2016.7493489
   Golkov V, 2015, LECT NOTES COMPUT SC, V9349, P37, DOI 10.1007/978-3-319-24553-9_5
   Hansen B., 2014, P JOINT ANN M ISMRM, P2602
   Hansen MS, 2013, MAGN RESON MED, V69, P1768, DOI [10.1002/mrm.24389, 10.1002/mrm.24743]
   Hinton G. E, 2012, ARXIV12070580
   Hori M, 2012, MAGN RESON MED SCI, V11, P221, DOI 10.2463/mrms.11.221
   Hui ES, 2008, NEUROIMAGE, V42, P122, DOI 10.1016/j.neuroimage.2008.04.237
   Jarrett K, 2009, IEEE I CONF COMP VIS, P2146, DOI 10.1109/ICCV.2009.5459469
   Jenkinson M, 2012, NEUROIMAGE, V62, P782, DOI 10.1016/j.neuroimage.2011.09.015
   Jensen JH, 2005, MAGNET RESON MED, V53, P1432, DOI 10.1002/mrm.20508
   Klein S, 2010, IEEE T MED IMAGING, V29, P196, DOI 10.1109/TMI.2009.2035616
   Le Bihan D, 2003, NAT REV NEUROSCI, V4, P469, DOI 10.1038/nrn1119
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lu HZ, 2006, NMR BIOMED, V19, P236, DOI 10.1002/nbm.1020
   Luna A, 2012, DIFFUSION MRI OUTSID
   Montufar G. F., 2014, ADV NEURAL INFORM PR, V27, P2924
   Nair V., 2010, P 27 INT C MACH LEAR, P432
   Nedjati-Gilani G., 2014, JOINT ANN M ISMRM ES, P2626
   Nedjati-Gilani GL, 2014, LECT NOTES COMPUT SC, V8675, P257, DOI 10.1007/978-3-319-10443-0_33
   Neher PF, 2015, LECT NOTES COMPUT SC, V9349, P45, DOI 10.1007/978-3-319-24553-9_6
   Nilsson M, 2010, J MAGN RESON, V206, P59, DOI 10.1016/j.jmr.2010.06.002
   Palm R. B., 2012, THESIS TU DENMARK LY
   Paquette M, 2015, MAGN RESON MED, V73, P401, DOI 10.1002/mrm.25093
   Plis SM, 2014, FRONT NEUROSCI-SWITZ, V8, DOI 10.3389/fnins.2014.00229
   Poot DHJ, 2010, IEEE T MED IMAGING, V29, P819, DOI 10.1109/TMI.2009.2037915
   Qi LQ, 2008, J COMPUT APPL MATH, V221, P150, DOI 10.1016/j.cam.2007.10.012
   Saxe A. M., 2014, P INT C LEARN REPR
   Schultz T, 2012, LECT NOTES COMPUT SC, V7512, P493, DOI 10.1007/978-3-642-33454-2_61
   Setsompop K, 2012, NEUROIMAGE, V63, P569, DOI 10.1016/j.neuroimage.2012.06.033
   Smith SM, 2004, NEUROIMAGE, V23, pS208, DOI 10.1016/j.neuroimage.2004.07.051
   Sotiropoulos SN, 2013, NEUROIMAGE, V80, P125, DOI 10.1016/j.neuroimage.2013.05.057
   Van Essen DC, 2013, NEUROIMAGE, V80, P62, DOI 10.1016/j.neuroimage.2013.05.041
   Van Leemput K, 2001, IEEE T MED IMAGING, V20, P677, DOI 10.1109/42.938237
   Veraart J, 2013, NEUROIMAGE, V81, P335, DOI 10.1016/j.neuroimage.2013.05.028
   Veraart J, 2011, MAGN RESON MED, V66, P678, DOI 10.1002/mrm.22835
   Wedeen VJ, 2005, MAGN RESON MED, V54, P1377, DOI 10.1002/mrm.20642
   Winston GP, 2012, QUANT IMAGING MED SU, V2, P254, DOI 10.3978/j.issn.2223-4292.2012.12.05
   Xu JZ, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0041714
   Zhang H, 2012, NEUROIMAGE, V61, P1000, DOI 10.1016/j.neuroimage.2012.03.072
NR 52
TC 50
Z9 53
U1 1
U2 28
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 0278-0062
EI 1558-254X
J9 IEEE T MED IMAGING
JI IEEE Trans. Med. Imaging
PD MAY
PY 2016
VL 35
IS 5
SI SI
BP 1344
EP 1351
DI 10.1109/TMI.2016.2551324
PG 8
WC Computer Science, Interdisciplinary Applications; Engineering,
   Biomedical; Engineering, Electrical & Electronic; Imaging Science &
   Photographic Technology; Radiology, Nuclear Medicine & Medical Imaging
SC Computer Science; Engineering; Imaging Science & Photographic
   Technology; Radiology, Nuclear Medicine & Medical Imaging
GA DL3RI
UT WOS:000375550500018
PM 27071165
DA 2020-02-19
ER

PT J
AU Miao, S
   Wang, ZJ
   Liao, R
AF Miao, Shun
   Wang, Z. Jane
   Liao, Rui
TI A CNN Regression Approach for Real-Time 2D/3D Registration
SO IEEE TRANSACTIONS ON MEDICAL IMAGING
LA English
DT Article
DE 2-D/3-D registration; convolutional neural network; deep learning; image
   guided intervention
ID X-RAY FLUOROSCOPY; IMAGE; CT
AB In this paper, we present a Convolutional Neural Network (CNN) regression approach to address the two major limitations of existing intensity-based 2-D/3-D registration technology: 1) slow computation and 2) small capture range. Different from optimization-based methods, which iteratively optimize the transformation parameters over a scalar-valued metric function representing the quality of the registration, the proposed method exploits the information embedded in the appearances of the digitally reconstructed radiograph and X-ray images, and employs CNN regressors to directly estimate the transformation parameters. An automatic feature extraction step is introduced to calculate 3-D pose-indexed features that are sensitive to the variables to be regressed while robust to other factors. The CNN regressors are then trained for local zones and applied in a hierarchical manner to break down the complex regression task into multiple simpler sub-tasks that can be learned separately. Weight sharing is furthermore employed in the CNN regression model to reduce the memory footprint. The proposed approach has been quantitatively evaluated on 3 potential clinical applications, demonstrating its significant advantage in providing highly accurate real-time 2-D/3-D registration with a significantly enlarged capture range when compared to intensity-based methods.
C1 [Miao, Shun; Wang, Z. Jane] Univ British Columbia, Dept Elect & Comp Engn, Vancouver, BC V6T 1Z4, Canada.
   [Miao, Shun; Liao, Rui] Siemens Healthcare, Med Imaging Technol, Princeton, NJ 08540 USA.
RP Miao, S (reprint author), Univ British Columbia, Dept Elect & Comp Engn, Vancouver, BC V6T 1Z4, Canada.
EM smiao@ece.ubc.ca; zjwang@ece.ubc.ca; rui.liao@siemens.com
OI Miao, Shun/0000-0002-4688-7087
CR Birkfellner W, 2009, MED PHYS, V36, P3420, DOI 10.1118/1.3157111
   Bronstein MM, 2010, PROC CVPR IEEE, P3594, DOI 10.1109/CVPR.2010.5539928
   Chou CR, 2013, COMPUT VIS IMAGE UND, V117, P1095, DOI 10.1016/j.cviu.2013.02.009
   Dollar P, 2010, PROC CVPR IEEE, P1078, DOI 10.1109/CVPR.2010.5540094
   Gao G, 2012, MED IMAGE ANAL, V16, P38, DOI 10.1016/j.media.2011.05.003
   Gendrin C, 2012, RADIOTHER ONCOL, V102, P274, DOI 10.1016/j.radonc.2011.07.031
   Glorot X., 2010, JLMR P TRACK, p[249, 2010], DOI DOI 10.1177/1753193409103364.
   Gouveia A. R., 2015, COMPUT METHOD BIOMEC, P1
   Hatt C., 2015, P MICCAI
   Jia Y, 2014, CAFFE CONVOLUTIONAL
   Kaiser M., 2014, P WORKSH BILDV MED 2, P312
   Kaiser M, 2014, LECT NOTES COMPUT SC, V8673, P283, DOI 10.1007/978-3-319-10404-1_36
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kruger J., 2003, P 14 IEEE VIS 2003 V, P38, DOI DOI 10.1109/VIS.2003.10001
   Liao R, 2013, IEEE T MULTIMEDIA, V15, P983, DOI 10.1109/TMM.2013.2244869
   Markelj P, 2012, MED IMAGE ANAL, V16, P642, DOI 10.1016/j.media.2010.03.005
   McLaughlin R. A., 2002, Medical Image Computing and Computer-Assisted Intervention - MICCAI 2002. 5th International Conference. Proceedings, Part II (Lecture Notes in Computer Science Vol.2489), P517
   Miao S, 2013, LECT NOTES COMPUT SC, V8090, P97, DOI 10.1007/978-3-642-40843-4_11
   Miao S, 2011, I S BIOMED IMAGING, P1215, DOI 10.1109/ISBI.2011.5872620
   Michel F, 2011, I S BIOMED IMAGING, P1209, DOI 10.1109/ISBI.2011.5872619
   Mottaghi R, 2015, PROC CVPR IEEE, P418, DOI 10.1109/CVPR.2015.7298639
   Schmid J., 2014, COMPUT VIS, P674
   Tomazevic D, 2003, IEEE T MED IMAGING, V22, P1407, DOI 10.1109/TMI.2003.819277
   van de Kraats EB, 2005, IEEE T MED IMAGING, V24, P1177, DOI 10.1109/TMI.2005.853240
   Varnavas A, 2015, MED IMAGE ANAL, V26, P108, DOI 10.1016/j.media.2015.08.005
   Vetter S., 2014, BONE JOINT J ORT S16, V96, P50
   Westover L., 1990, Computer Graphics, V24, P367
   Wohlhart P, 2015, PROC CVPR IEEE, P3109, DOI 10.1109/CVPR.2015.7298930
   Zach C, 2015, PROC CVPR IEEE, P196, DOI 10.1109/CVPR.2015.7298615
   Zhu Z., 2012, WORLD C MED PHYS BIO, P230
   Zollei L, 2001, PROC CVPR IEEE, P696
NR 31
TC 95
Z9 96
U1 9
U2 86
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 0278-0062
EI 1558-254X
J9 IEEE T MED IMAGING
JI IEEE Trans. Med. Imaging
PD MAY
PY 2016
VL 35
IS 5
SI SI
BP 1352
EP 1363
DI 10.1109/TMI.2016.2521800
PG 12
WC Computer Science, Interdisciplinary Applications; Engineering,
   Biomedical; Engineering, Electrical & Electronic; Imaging Science &
   Photographic Technology; Radiology, Nuclear Medicine & Medical Imaging
SC Computer Science; Engineering; Imaging Science & Photographic
   Technology; Radiology, Nuclear Medicine & Medical Imaging
GA DL3RI
UT WOS:000375550500019
PM 26829785
DA 2020-02-19
ER

PT J
AU Zhang, ZP
   Luo, P
   Loy, CC
   Tang, XO
AF Zhang, Zhanpeng
   Luo, Ping
   Loy, Chen Change
   Tang, Xiaoou
TI Learning Deep Representation for Face Alignment with Auxiliary
   Attributes
SO IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
LA English
DT Article
DE Face Alignment; face landmark detection; deep learning; convolutional
   network
AB In this study, we show that landmark detection or face alignment task is not a single and independent problem. Instead, its robustness can be greatly improved with auxiliary information. Specifically, we jointly optimize landmark detection together with the recognition of heterogeneous but subtly correlated facial attributes, such as gender, expression, and appearance attributes. This is non-trivial since different attribute inference tasks have different learning difficulties and convergence rates. To address this problem, we formulate a novel tasks-constrained deep model, which not only learns the inter-task correlation but also employs dynamic task coefficients to facilitate the optimization convergence when learning multiple complex tasks. Extensive evaluations show that the proposed task-constrained learning (i) outperforms existing face alignment methods, especially in dealing with faces with severe occlusion and pose variation, and (ii) reduces model complexity drastically compared to the state-of-the-art methods based on cascaded deep model.
C1 [Zhang, Zhanpeng; Luo, Ping; Loy, Chen Change; Tang, Xiaoou] Chinese Univ Hong Kong, Dept Informat Engn, Hong Kong, Hong Kong, Peoples R China.
RP Zhang, ZP; Luo, P; Loy, CC; Tang, XO (reprint author), Chinese Univ Hong Kong, Dept Informat Engn, Hong Kong, Hong Kong, Peoples R China.
EM zz013@ie.cuhk.edu.hk; pluo@ie.cuhk.edu.hk; ccloy@ie.cuhk.edu.hk;
   xtang@ie.cuhk.edu.hk
RI zhang, zhanpeng/AAE-7897-2019
CR Ahmed A, 2008, LECT NOTES COMPUT SC, V5304, P69, DOI 10.1007/978-3-540-88690-7_6
   Asthana A, 2013, PROC CVPR IEEE, P3444, DOI 10.1109/CVPR.2013.442
   Belhumeur PN, 2011, PROC CVPR IEEE, P545, DOI 10.1109/CVPR.2011.5995602
   Boyd S., 2004, CONVEX OPTIMIZATION
   Burgos-Artizzu XP, 2013, IEEE I CONF COMP VIS, P1513, DOI 10.1109/ICCV.2013.191
   Cao XD, 2012, PROC CVPR IEEE, P2887, DOI 10.1109/CVPR.2012.6248015
   Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734
   Chen D, 2014, LECT NOTES COMPUT SC, V8694, P109, DOI 10.1007/978-3-319-10599-4_8
   Chen K, 2013, PROC CVPR IEEE, P2467, DOI 10.1109/CVPR.2013.319
   Collobert R., 2008, P 25 ICML, V25, P160, DOI [10.1145/1390156.1390177, DOI 10.1145/1390156.1390177]
   COOTES TF, 1995, COMPUT VIS IMAGE UND, V61, P38, DOI 10.1006/cviu.1995.1004
   Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467
   Cootes TF, 2012, LECT NOTES COMPUT SC, V7578, P278, DOI 10.1007/978-3-642-33786-4_21
   Dantone M, 2012, PROC CVPR IEEE, P2578, DOI 10.1109/CVPR.2012.6247976
   Dollar P, 2010, PROC CVPR IEEE, P1078, DOI 10.1109/CVPR.2010.5540094
   GUPTA A, 1999, MATRIX VARIATE DISTR
   Huang ZW, 2013, IEEE I CONF COMP VIS, P3296, DOI 10.1109/ICCV.2013.409
   Jesorsky O, 2001, LECT NOTES COMPUT SC, V2091, P90
   Kazemi V, 2014, PROC CVPR IEEE, P1867, DOI 10.1109/CVPR.2014.241
   Koestinger M., 2011, COMP VIS WORKSH ICCV, P2144, DOI DOI 10.1109/ICCVW.2011.6130513
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kumar N, 2008, LECT NOTES COMPUT SC, V5305, P340, DOI 10.1007/978-3-540-88693-8_25
   Le V, 2012, LECT NOTES COMPUT SC, V7574, P679, DOI 10.1007/978-3-642-33712-3_49
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li SJ, 2015, INT J COMPUT VISION, V113, P19, DOI 10.1007/s11263-014-0767-8
   Liu X, 2007, P IEEE C COMP VIS PA, P1
   LIU YT, 2013, IEEE DATA MINING, P399, DOI DOI 10.1109/ICDMW.2013.158
   Lu C., 2015, AAAI C ART INT AUST
   Luo P, 2013, IEEE I CONF COMP VIS, P2864, DOI 10.1109/ICCV.2013.356
   McCullagh PNJ, 1989, GEN LINEAR MODELS
   Messer K, 1999, P 2 INT C AUD VID BA, V964, P72
   Milborrow S, 2008, LECT NOTES COMPUT SC, V5305, P504, DOI 10.1007/978-3-540-88693-8_37
   Moody J., 1995, ADV NEURAL INFORM PR, V4, P950
   Pedersoli M, 2014, PROC CVPR IEEE, P3694, DOI 10.1109/CVPR.2014.472
   Ren SQ, 2014, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR.2014.218
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Sagonas C, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P397, DOI 10.1109/ICCVW.2013.59
   Shi W. L. Baoguang, 2014, ARXIV14095230
   Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244
   Sun Y, 2013, PROC CVPR IEEE, P3476, DOI 10.1109/CVPR.2013.446
   Tzimiropoulos G, 2014, PROC CVPR IEEE, P1851, DOI 10.1109/CVPR.2014.239
   Valstar M, 2010, PROC CVPR IEEE, P2729, DOI 10.1109/CVPR.2010.5539996
   Weston Jason, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P639, DOI 10.1007/978-3-642-35289-8_34
   Xiong XH, 2013, PROC CVPR IEEE, P532, DOI 10.1109/CVPR.2013.75
   Yang H, 2013, IEEE I CONF COMP VIS, P1936, DOI 10.1109/ICCV.2013.243
   Yu X, 2013, IEEE I CONF COMP VIS, P1944, DOI 10.1109/ICCV.2013.244
   Yuille AL, 2002, ADV NEUR IN, V14, P1033
   Zhang JH, 2014, ELECTRON J QUAL THEO, P1, DOI [10.1007/978-3-319-10605-2_1, 10.14232/ejqtde.2014.1.50]
   Zhang ZP, 2014, LECT NOTES COMPUT SC, V8694, P94, DOI 10.1007/978-3-319-10599-4_7
   Zhu SZ, 2015, PROC CVPR IEEE, P4998, DOI 10.1109/CVPR.2015.7299134
   Zhu XX, 2012, PROC CVPR IEEE, P2879, DOI 10.1109/CVPR.2012.6248014
NR 51
TC 114
Z9 123
U1 9
U2 61
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 0162-8828
EI 1939-3539
J9 IEEE T PATTERN ANAL
JI IEEE Trans. Pattern Anal. Mach. Intell.
PD MAY
PY 2016
VL 38
IS 5
BP 918
EP 930
DI 10.1109/TPAMI.2015.2469286
PG 13
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
SC Computer Science; Engineering
GA DJ4GZ
UT WOS:000374164700007
PM 27046839
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Cimpoi, M
   Maji, S
   Kokkinos, I
   Vedaldi, A
AF Cimpoi, Mircea
   Maji, Subhransu
   Kokkinos, Iasonas
   Vedaldi, Andrea
TI Deep Filter Banks for Texture Recognition, Description, and Segmentation
SO INTERNATIONAL JOURNAL OF COMPUTER VISION
LA English
DT Article
DE Texture and material recognition; Visual attributes; Convolutional
   neural networks; Filter banks; Fisher vectors; Datasets and benchmarks
ID CLASSIFICATION; PERCEPTION; ELEMENTS; TEXTONS; VISION
AB Visual textures have played a key role in image understanding because they convey important semantics of images, and because texture representations that pool local image descriptors in an orderless manner have had a tremendous impact in diverse applications. In this paper we make several contributions to texture understanding. First, instead of focusing on texture instance and material category recognition, we propose a human-interpretable vocabulary of texture attributes to describe common texture patterns, complemented by a new describable texture dataset for benchmarking. Second, we look at the problem of recognizing materials and texture attributes in realistic imaging conditions, including when textures appear in clutter, developing corresponding benchmarks on top of the recently proposed OpenSurfaces dataset. Third, we revisit classic texture represenations, including bag-of-visual-words and the Fisher vectors, in the context of deep learning and show that these have excellent efficiency and generalization properties if the convolutional layers of a deep model are used as filter banks. We obtain in this manner state-of-the-art performance in numerous datasets well beyond textures, an efficient method to apply deep features to image regions, as well as benefit in transferring features from one domain to another.
C1 [Cimpoi, Mircea; Vedaldi, Andrea] Univ Oxford, Oxford, England.
   [Maji, Subhransu] Univ Massachusetts, Amherst, MA 01003 USA.
   [Kokkinos, Iasonas] Cent Supelec INRIA Saclay, Palaiseau, France.
RP Cimpoi, M (reprint author), Univ Oxford, Oxford, England.
EM mircea@robots.ox.ac.uk; smaji@cs.umass.edu; iasonas.kokkinos@ecp.fr;
   vedaldi@robots.ox.ac.uk
FU NSFNational Science Foundation (NSF) [1005411]; ODNI via the JHU HLTCOE;
   Google; ERCEuropean Research Council (ERC) [228180, 638009]; XRCE UAC
   Grant; EUEuropean Union (EU) [FP7-ICT-600825, FP7-ICT-2011-600796]
FX The development of DTD is based on work done at the 2012 CLSP Summer
   Workshop, and was partially supported by NSF Grant #1005411, ODNI via
   the JHU HLTCOE and Google Research. Mircea Cimpoi was supported by the
   ERC Grant VisRec no. 228180 and the XRCE UAC Grant. Iasonas Kokkinos was
   supported by EU Projects RECONFIG FP7-ICT-600825 and MOBOT
   FP7-ICT-2011-600796. Andrea Vedaldi was partially supported by the ERC
   Starting Grant IDIU, No. 638009.
CR Adams A, 2010, COMPUT GRAPH FORUM, V29, P753, DOI 10.1111/j.1467-8659.2009.01645.x
   Adelson E. H., 2001, 4299 SPIE
   AMADASUN M, 1989, IEEE T SYST MAN CYB, V19, P1264, DOI 10.1109/21.44046
   Arandjelovic R., 2012, P COMP VIS PATT REC
   Arbelaez P., 2014, IEEE C COMP VIS PAT
   Badri H, 2014, LECT NOTES COMPUT SC, V8689, P505, DOI 10.1007/978-3-319-10590-1_33
   BAJCSY R, 1973, P 3 INT JOINT C ART
   Bell S., 2014, ARXIV14120623
   Bell S, 2013, P SIGGRAPH
   Bell S., 2015, P COMP VIS PATT REC
   Berg T., 2010, ECCV
   Berlin B, 1991, BASIC COLOR TERMS TH
   Bhushan N, 1997, COGNITIVE SCI, V21, P219
   Bourdev L, 2011, IEEE INT C COMP VIS
   BOVIK AC, 1990, IEEE T PATTERN ANAL, V12, P55, DOI 10.1109/34.41384
   BRODATZ P, 1966, TEXTURES PHOTOGRAPHI
   Burghouts GJ, 2009, PATTERN RECOGN LETT, V30, P306, DOI 10.1016/j.patrec.2008.10.005
   Caputo B., 2005, IEEE INT C COMP VIS
   Chatfield K., 2011, P BMVC
   Chatfield K., 2014, P BMVC
   CHAUDHURI BB, 1995, IEEE T PATTERN ANAL, V17, P72, DOI 10.1109/34.368149
   Chen L., 2014, CORR
   Cimpoi M., 2015, P COMP VIS PATT REC
   Cimpoi M., 2014, P COMP VIS PATT REC
   Clarke A. D. F., 2011, P BMVC
   Csurka G., 2004, P ECCV WORKSH STAT L
   Dana KJ, 1999, ACM T GRAPHIC, V18, P1, DOI 10.1145/300776.300778
   Deng J., 2009, P COMP VIS PATT REC
   DUNN D, 1994, IEEE T PATTERN ANAL, V16, P130, DOI 10.1109/34.273736
   Efros A., 1999, P COMP VIS PATT REC, V2
   Everingham M., 2007, TECHNICAL REPORT
   Everingham M, 2008, PASCAL VISUAL OBJECT
   Farhadi A, 2009, PROC CVPR IEEE, P1778, DOI 10.1109/CVPRW.2009.5206772
   Ferrari V., 2007, P NIPS
   Forsyth DA, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P447, DOI 10.1109/ICCV.2001.937659
   FREEMAN WT, 1991, IEEE T PATTERN ANAL, V13, P891, DOI 10.1109/34.93808
   Garding J., 1992, Journal of Mathematical Imaging and Vision, V2, P327, DOI 10.1007/BF00121877
   Geusebroek JM, 2003, IEEE T IMAGE PROCESS, V12, P938, DOI 10.1109/TIP.2003.812429
   Girshick RB, 2014, P COMP VIS PATT REC
   Gong Yunchao, 2014, P ECCV
   Hariharan B, 2014, LECT NOTES COMPUT SC, V8695, P297, DOI 10.1007/978-3-319-10584-0_20
   Hayman E., 2004, P ECCV
   He K., 2014, P ECCV
   Isola P., 2014, P ECCV
   JAIN AK, 1991, PATTERN RECOGN, V24, P1167, DOI 10.1016/0031-3203(91)90143-S
   Jegou H., 2010, P COMP VIS PATT REC
   Jia Y., 2013, CAFFE OPEN SOURCE CO
   JULESZ B, 1981, NATURE, V290, P91, DOI 10.1038/290091a0
   JULESZ B, 1983, BELL SYST TECH J, V62, P1619, DOI 10.1002/j.1538-7305.1983.tb03502.x
   Krahenbuhl P., 2011, ADV NEURAL INFORM PR, P109
   Krizhevsky A., 2012, P NIPS
   Kumar N, 2011, IEEE T PATTERN ANAL, V33, P1962, DOI 10.1109/TPAMI.2011.48
   Ladicky L, 2010, LECT NOTES COMPUT SC, V6315, P239, DOI 10.1007/978-3-642-15555-0_18
   Lazebnik S., 2006, IEEE COMP SOC C COMP
   Lazebnik S., 2005, PAMI, V28, P2169
   Leung T., 1996, Computer Vision - ECCV '96. 4th Eurpean Conference on Computer Proceedings, P546
   Leung T, 2001, INT J COMPUT VISION, V43, P29, DOI 10.1023/A:1011126920638
   Liu LQ, 2011, IEEE I CONF COMP VIS, P2486, DOI 10.1109/ICCV.2011.6126534
   Lowe D., 1999, P ICCV
   Maji S., 2008, P COMP VIS PATT REC
   MALIK J, 1990, J OPT SOC AM A, V7, P923, DOI 10.1364/JOSAA.7.000923
   Malik J, 1997, INT J COMPUT VISION, V23, P149, DOI 10.1023/A:1007958829620
   MALLAT SG, 1989, IEEE T PATTERN ANAL, V11, P674, DOI 10.1109/34.192463
   MALLIKARJUNA P, 2006, KTH TIPS KTH TIPS2 D
   MANJUNATH BS, 1991, IEEE T PATTERN ANAL, V13, P478, DOI 10.1109/34.134046
   Matthews T., 2013, P COMP VIS PATT REC
   Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Oquab M., 2014, P COMP VIS PATT REC
   Oxholm G, 2012, LECT NOTES COMPUT SC, V7572, P58, DOI 10.1007/978-3-642-33718-5_5
   Parikh D., 2011, P ICCV
   Parkhi O. M., 2014, P COMP VIS PATT REC
   Patterson G., 2012, P COMP VIS PATT REC
   Perronnin F., 2007, P COMP VIS PATT REC
   Perronnin F., 2010, P ECCV
   Perronnin F, 2015, PROC CVPR IEEE, P3743, DOI 10.1109/CVPR.2015.7298998
   Philbin J., 2008, LOST QUANTIZATION IM
   Platt J., 2000, ADV LARGE MARGIN CLA
   Portilla J, 2000, INT J COMPUT VISION, V40, P49, DOI 10.1023/A:1026553619983
   Qi XB, 2014, IEEE T PATTERN ANAL, V36, P2199, DOI 10.1109/TPAMI.2014.2316826
   Rao AR, 1996, VISION RES, V36, P1649
   Razavin A. S., 2014, DEEPVISION WORKSH
   Schwartz G., 2013, P CVCP
   Sharan L., 2009, J VISION, V9, P784, DOI [10.1167/9.8.784, DOI 10.1167/9.8.784]
   Sharan L, 2013, INT J COMPUT VISION, V103, P348, DOI 10.1007/s11263-013-0609-0
   Sharma G., 2012, P ECCV
   Sifre L., 2013, P COMP VIS PATT REC
   Simonyan K., 2014, CORR
   SIVIC J, 2003, [No title captured], P1470
   Sulc M., 2014, TECHNICAL REPORT
   TAMURA H, 1978, IEEE T SYST MAN CYB, V8, P460, DOI 10.1109/TSMC.1978.4309999
   Timofte R., 2012, P BMVC
   Torralba A, 2009, P COMP VIS PATT REC
   Varma M, 2005, INT J COMPUT VISION, V62, P61, DOI 10.1023/B:VISI.0000046589.39864.ee
   Varma M, 2003, PROC CVPR IEEE, P691
   Vedaldi A., 2010, P COMP VIS PATT REC
   Vedaldi A, 2010, VLFEAT OPEN PORTABLE
   Vedaldi A., 2014, ARXIV14124564
   Wah C., 2011, CNSTR2011001 CALTECH
   Wang J., 2009, P BMVC
   Wang JJ, 2010, PROC CVPR IEEE, P3360, DOI 10.1109/CVPR.2010.5540018
   WANG L, 1990, PATTERN RECOGN, V23, P905, DOI 10.1016/0031-3203(90)90135-8
   Wei LY, 2000, COMP GRAPH, P479
   Wei Y., 2014, CNN SINGLE LABEL MUL
   Xu Y, 2009, INT J COMPUT VISION, V83, P85, DOI 10.1007/s11263-009-0220-6
   Zhang N., 2014, P ECCV
   Zheng S., 2015, CORR
   Zhou Bolei, 2014, P NIPS
   Zhou X, 2010, LECT NOTES COMPUT SC, V6315, P141, DOI 10.1007/978-3-642-15555-0_11
NR 109
TC 106
Z9 109
U1 3
U2 39
PU SPRINGER
PI DORDRECHT
PA VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN 0920-5691
EI 1573-1405
J9 INT J COMPUT VISION
JI Int. J. Comput. Vis.
PD MAY
PY 2016
VL 118
IS 1
BP 65
EP 94
DI 10.1007/s11263-015-0872-3
PG 30
WC Computer Science, Artificial Intelligence
SC Computer Science
GA DL6ZT
UT WOS:000375789300004
OA Green Published, Other Gold
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Avendi, MR
   Kheradvar, A
   Jafarkhani, H
AF Avendi, M. R.
   Kheradvar, Arash
   Jafarkhani, Hamid
TI A combined deep-learning and deformable-model approach to fully
   automatic segmentation of the left ventricle in cardiac MRI
SO MEDICAL IMAGE ANALYSIS
LA English
DT Article
DE Caridac MRI; LV segmentation; Deep learning; Machine learning;
   Deformable models
ID SHORT-AXIS; REGISTRATION; IMAGES; SET; ATLAS
AB Segmentation of the left ventricle (LV) from cardiac magnetic resonance imaging (MRI) datasets is an essential step for calculation of clinical indices such as ventricular volume and ejection fraction. In this work, we employ deep learning algorithms combined with deformable models to develop and evaluate a fully automatic LV segmentation tool from short-axis cardiac MRI datasets. The method employs deep learning algorithms to learn the segmentation task from the ground true data. Convolutional networks are employed to automatically detect the LV chamber in MRI dataset. Stacked autoencoders are used to infer the LV shape. The inferred shape is incorporated into deformable models to improve the accuracy and robustness of the segmentation. We validated our method using 45 cardiac MR datasets from the MICCAI 2009 LV segmentation challenge and showed that it outperforms the state-of-the art methods. Excellent agreement with the ground truth was achieved. Validation metrics, percentage of good contours, Dice metric, average perpendicular distance and conformity, were computed as 96.69%, 0.94, 1.81 mm and 0.86, versus those of 79.2 95.62%, 0.87-0.9, 1.76-2.97 mm and 0.67-0.78, obtained by other methods, respectively. (C) 2016 Elsevier B.V. All rights reserved.
C1 [Avendi, M. R.; Jafarkhani, Hamid] Univ Calif Irvine, Ctr Pervas Commun & Comp, Irvine, CA USA.
   [Avendi, M. R.; Kheradvar, Arash] Univ Calif Irvine, Edwards Lifesci Ctr Adv Cardiovasc Technol, Irvine, CA USA.
RP Jafarkhani, H (reprint author), Univ Calif Irvine, Ctr Pervas Commun & Comp, Irvine, CA USA.
EM m.avendi@uci.edu; arashkh@uci.edu; hamidj@uci.edu
OI Kheradvar, Arash/0000-0003-3864-1359
FU American Heart AssociationAmerican Heart Association [14GRNT18800013];
   Conexant-Broadcom Endowed Chair
FX This work is partially supported by a grant from American Heart
   Association (14GRNT18800013) to Prof. Kheradvar and Conexant-Broadcom
   Endowed Chair of Prof. Jafarkhani.
CR Ayed IB, 2009, IEEE T MED IMAGING, V28, P1902, DOI 10.1109/TMI.2009.2022087
   Baldi P., 2012, J MACH LEARN RES P T, V27, P37
   Barajas J., 2006, 1 INT WORKSH COMP VI, P114
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Billet F, 2009, LECT NOTES COMPUT SC, V5528, P376, DOI 10.1007/978-3-642-01932-6_41
   BLAND JM, 1986, LANCET, V1, P307
   Boureau Y., 2010, P 27 INT C MACH LEAR, P111, DOI DOI 10.1016/J.NEUNET.2012.02.023
   Carminati MC, 2014, COMPUT BIOL MED, V46, P42, DOI 10.1016/j.compbiomed.2013.12.013
   Chan TF, 2001, IEEE T IMAGE PROCESS, V10, P266, DOI 10.1109/83.902291
   Chandler AG, 2008, J CARDIOVASC MAGN R, V10, DOI 10.1186/1532-429X-10-13
   Chang HH, 2010, IEEE T VIS COMPUT GR, V16, P854, DOI 10.1109/TVCG.2009.212
   Chang HH, 2009, NEUROIMAGE, V47, P122, DOI 10.1016/j.neuroimage.2009.03.068
   Cobzas D, 2009, PROC CVPR IEEE, P328, DOI 10.1109/CVPRW.2009.5206812
   Cocosco CA, 2008, J MAGN RESON IMAGING, V28, P366, DOI 10.1002/jmri.21451
   Constantinides C, 2012, IEEE ENG MED BIO, P3207, DOI 10.1109/EMBC.2012.6346647
   Cordero-Grande L, 2011, MED IMAGE ANAL, V15, P283, DOI 10.1016/j.media.2011.01.002
   Deng L., 2014, FDN TRENDS SIGNAL PR
   Dreijer JF, 2013, BMC MED IMAGING, V13, DOI 10.1186/1471-2342-13-24
   Eslami A, 2013, MED IMAGE ANAL, V17, P236, DOI 10.1016/j.media.2012.10.005
   Frangi AF, 2001, IEEE T MED IMAGING, V20, P2, DOI 10.1109/42.906421
   Geremia E, 2011, NEUROIMAGE, V57, P378, DOI 10.1016/j.neuroimage.2011.03.080
   Heimann T, 2009, MED IMAGE ANAL, V13, P543, DOI 10.1016/j.media.2009.05.004
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hu HF, 2013, MAGN RESON IMAGING, V31, P575, DOI 10.1016/j.mri.2012.10.004
   Huang R., 2004, P 2004 IEEE COMP SOC, V2, P11
   Huang S, 2011, J DIGIT IMAGING, V24, P598, DOI 10.1007/s10278-010-9315-4
   Jolly M, 2009, MIDAS J CARDIAC MR L, V4
   Jolly MP, 2009, LECT NOTES COMPUT SC, V5762, P910
   Kedenburg G., 2006, AUTOMATIC CARDIAC MR
   Koikkalainen J, 2008, IEEE T MED IMAGING, V27, P1643, DOI 10.1109/TMI.2008.929106
   Kolawole O., 2008, COMP EVALUATION SEGM, P409, DOI [10.1007/978-3-540-85988-8_49, DOI 10.1007/978-3-540-85988-8_49]
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   KULLBACK S, 1951, ANN MATH STAT, V22, P79, DOI 10.1214/aoms/1177729694
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   LeCun Y, 2010, IEEE INT SYMP CIRC S, P253, DOI 10.1109/ISCAS.2010.5537907
   Lempitsky V, 2009, LECT NOTES COMPUT SC, V5528, P447, DOI 10.1007/978-3-642-01932-6_48
   Liew YM, 2015, PHYS MED BIOL, V60, P2715, DOI 10.1088/0031-9155/60/7/2715
   Lima JAC, 2004, J AM COLL CARDIOL, V44, P1164, DOI 10.1016/j.jacc.2004.06.033
   Liu H, 2012, ACAD RADIOL, V19, P723, DOI 10.1016/j.acra.2012.02.011
   Lorenzo-Valdes M, 2004, MED IMAGE ANAL, V8, P255, DOI 10.1016/j.media.2004.06.005
   Lotjonen J, 2004, LECT NOTES COMPUT SC, V3217, P405
   Margeta Jan, 2012, Statistical Atlases and Computational Models of the Heart. Imaging and Modelling Challenges. Second International Workshop, STACOM 2011 Held in Conjunction with MICCAI 2011. Revised Selected Papers, P109, DOI 10.1007/978-3-642-28326-0_11
   Petitjean C, 2015, MED IMAGE ANAL, V19, P187, DOI 10.1016/j.media.2014.10.004
   Petitjean C, 2011, MED IMAGE ANAL, V15, P169, DOI 10.1016/j.media.2010.12.004
   Plueimpitiwiriyawej C, 2005, IEEE T MED IMAGING, V24, P593, DOI 10.1109/TMI.2005.843740
   Queiros S, 2014, MED IMAGE ANAL, V18, P1115, DOI 10.1016/j.media.2014.06.001
   Radau P, 2009, MIDAS J CARIDAC MR L
   Schaerer J, 2010, MED IMAGE ANAL, V14, P738, DOI 10.1016/j.media.2010.05.009
   Sermanet P., 2014, P INT C LEARN REPR I
   Suinesiaputra A, 2014, MED IMAGE ANAL, V18, P50, DOI 10.1016/j.media.2013.09.001
   Szegedy C., 2013, ADV NEURAL INFORM PR, V26, P2553
   Tavakoli V, 2013, COMPUT VIS IMAGE UND, V117, P966, DOI 10.1016/j.cviu.2012.11.017
   Ngo TA, 2014, PROC CVPR IEEE, P3118, DOI 10.1109/CVPR.2014.399
   van Assen HC, 2006, MED IMAGE ANAL, V10, P286, DOI 10.1016/j.media.2005.12.001
   Vincent P, 2008, P 25 INT C MACH LEAR, P1096, DOI DOI 10.1145/1390156.1390294
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Yuan C, 2002, J MAGN RESON IMAGING, V15, P62, DOI 10.1002/jmri.10030
   Zakkaroff C., STACK ALIGNMENT TRAN
   Zhang HH, 2010, IEEE T MED IMAGING, V29, P350, DOI 10.1109/TMI.2009.2030799
   Zhuang X, 2008, PROC SPIE, V6914, DOI 10.1117/12.769445
NR 61
TC 142
Z9 150
U1 8
U2 109
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 1361-8415
EI 1361-8423
J9 MED IMAGE ANAL
JI Med. Image Anal.
PD MAY
PY 2016
VL 30
BP 108
EP 119
DI 10.1016/j.media.2016.01.005
PG 12
WC Computer Science, Artificial Intelligence; Computer Science,
   Interdisciplinary Applications; Engineering, Biomedical; Radiology,
   Nuclear Medicine & Medical Imaging
SC Computer Science; Engineering; Radiology, Nuclear Medicine & Medical
   Imaging
GA DI5NV
UT WOS:000373546800009
PM 26917105
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Dong, Y
   Liu, YN
   Lian, SG
AF Dong, Yuan
   Liu, Yinan
   Lian, Shiguo
TI Automatic age estimation based on deep learning algorithm
SO NEUROCOMPUTING
LA English
DT Article
DE Age estimation; Deep Convolutional Neural Networks; Transfer learning;
   Loss function
ID NETWORK
AB Automatic age estimation has attracted much attention due to its potential applications. Most of the proposed approaches have mainly used low-level handcraft features to encode facial age related visual information and train an age estimation model. In this paper, we focus on age classification task in which face image is assigned to a label that represents an age range. We proposed a deep learning based framework for age classification task. In our proposed algorithm, Deep Convolutional Neural Networks (Deep ConvNets) are used to extract high-level complex age related visual features and predict age range of input face image. Due to lack of age labeled face images, we use the transfer learning strategy to train the Deep ConvNets. In addition, to describe the relationships between labels that compose an ordered sequence, we define a new loss function in the training process of age classification task. The experiments are conducted on a widely used age estimation dataset-Images of Groups of People. The experimental results demonstrate the excellent performance of our proposed algorithm against the state-of-the-art methods. (C) 2015 Elsevier B.V. All rights reserved.
C1 [Dong, Yuan; Liu, Yinan] Beijing Univ Posts & Telecommun, Beijing 100876, Peoples R China.
   [Lian, Shiguo] Huawei Cent Res Inst, Beijing 100086, Peoples R China.
RP Lian, SG (reprint author), Huawei Cent Res Inst, Beijing 100086, Peoples R China.
EM yuandong@bupt.edu.cn; liuyinan1988@126.com; shiguo.lian@ieee.org
FU Beijing University of Posts and Telecommunications [SEV01100474]; France
   Telecom RD [SEV01100474]; Chinese NSFC (National Natural Science
   Fundation of China) Project [61372169]; Chinese NSFC Project [61532018]
FX This work was sponsored by Collaborative Research Project (SEV01100474)
   between Beijing University of Posts and Telecommunications and France
   Telecom R&D, Chinese NSFC (National Natural Science Fundation of China)
   Project 61372169 and Chinese NSFC Project 61532018.
CR Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467
   Dahl GE, 2012, IEEE T AUDIO SPEECH, V20, P30, DOI 10.1109/TASL.2011.2134090
   Fu Y, 2008, IEEE T MULTIMEDIA, V10, P578, DOI 10.1109/TMM.2008.921847
   Fu Y, 2010, IEEE T PATTERN ANAL, V32, P1955, DOI 10.1109/TPAMI.2010.36
   Gallagher AC, 2009, PROC CVPR IEEE, P256, DOI 10.1109/CVPRW.2009.5206828
   Geng X, 2007, IEEE T PATTERN ANAL, V29, P2234, DOI 10.1109/TPAMI.2007.70733
   Guo GD, 2009, PROC CVPR IEEE, P112, DOI 10.1109/CVPRW.2009.5206681
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kusy B, 2009, 2009 INTERNATIONAL CONFERENCE ON INFORMATION PROCESSING IN SENSOR NETWORKS (IPSN 2009), P109
   KWON YH, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P762, DOI 10.1109/CVPR.1994.323894
   Luo P, 2014, PROC CVPR IEEE, P899, DOI 10.1109/CVPR.2014.120
   Ouyang WL, 2012, PROC CVPR IEEE, P3258, DOI 10.1109/CVPR.2012.6248062
   Sermanet P, 2013, PROC CVPR IEEE, P3626, DOI 10.1109/CVPR.2013.465
   Shan C., 2010, P 1 ACM INT WORKSH M, P23, DOI [DOI 10.1145/1878039.1878045, 10.1145/1878039.1878045]
   Simonyan K., 2014, TECHNICAL REPORT
   Song Z, 2011, IEEE I CONF COMP VIS, P241, DOI 10.1109/ICCV.2011.6126248
   Sun Y, 2014, ADV NEURAL INFORM PR, V60, P1988
   Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244
   Sun Y, 2013, PROC CVPR IEEE, P3476, DOI 10.1109/CVPR.2013.446
   Szegedy C, 2014, TECHNICAL REPORT
   Yan S, 2008, PR IEEE COMP DESIGN, P142, DOI 10.1109/ICCD.2008.4751853
   Yu J, 2014, IEEE T CYBERNETICS, V44, P2431, DOI 10.1109/TCYB.2014.2307862
   Yu J, 2014, IEEE T IMAGE PROCESS, V23, P2019, DOI 10.1109/TIP.2014.2311377
   Yu J, 2012, IEEE T IMAGE PROCESS, V21, P3262, DOI 10.1109/TIP.2012.2190083
   Zhang Y, 2010, PROC CVPR IEEE, P2622, DOI 10.1109/CVPR.2010.5539975
   Zhang ZP, 2014, LECT NOTES COMPUT SC, V8694, P94, DOI 10.1007/978-3-319-10599-4_7
NR 27
TC 24
Z9 24
U1 1
U2 37
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0925-2312
EI 1872-8286
J9 NEUROCOMPUTING
JI Neurocomputing
PD APR 26
PY 2016
VL 187
SI SI
BP 4
EP 10
DI 10.1016/j.neucom.2015.09.115
PG 7
WC Computer Science, Artificial Intelligence
SC Computer Science
GA DK0MK
UT WOS:000374606700002
DA 2020-02-19
ER

PT J
AU Zhan, S
   Tao, QQ
   Li, XH
AF Zhan, Shu
   Tao, Qin-Qin
   Li, Xiao-Hong
TI Face detection using representation learning
SO NEUROCOMPUTING
LA English
DT Article
DE Face detection; Convolutional neural network; Deep learning; Support
   vector machine; Adaboost
ID NETWORK; EFFICIENT; FEATURES; CASCADE
AB Face representation is a crucial step of face detection system. In this paper, we present a fast face detection algorithm based on representation learnt using convolutional neural network (CNN) so as to explicitly capture various latent facial features. Firstly, in order to improve the speed of detection in the system, we train an Adaboost background filter which can remove the background most quickly. Secondly, we use the CNN to extract more distinctive features for those face and non-face patterns that have not been filtered by Adaboost. CNN can automatically learn and synthesize a problem-specific feature extractor from a training set, without making any assumptions or using any hand-made design concerning the features to extract or the areas of the face pattern to analyze. Finally, support vector machines (SVM) are used to detect instead of using the classification function of CNN itself. Extensive experiments demonstrate the robustness and efficiency of our system by comparing it with several popular face detection algorithms on the widely used CMU+MIT frontal face dataset and FDDB dataset. (C) 2015 Elsevier B.V. All rights reserved.
C1 [Zhan, Shu; Li, Xiao-Hong] Hefei Univ Technol, Sch Comp & Informat, Hefei, Peoples R China.
   [Tao, Qin-Qin] Hefei Univ Technol, Hefei, Peoples R China.
RP Zhan, S (reprint author), Hefei Univ Technol, Sch Comp & Informat, Hefei, Peoples R China.
OI zhan, shu/0000-0002-5568-0264
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61371156, 61371155]; Anhui Province Science and
   Technology Research Programs [1401B042019]
FX This work was supported by the National Natural Science Foundation of
   China under Grants 61371156 and 61371155, Anhui Province Science and
   Technology Research Programs under Grant 1401B042019.
CR Ahonen T, 2004, LECT NOTES COMPUT SC, V3021, P469
   Cao ZM., 2011, P IEEE INT C COMP VI, P2707
   Chen D, 2013, PROC CVPR IEEE, P3025, DOI 10.1109/CVPR.2013.389
   Chen Y., 2009, IEEE T PATTERN ANAL, V99, P1
   Dalal N, 2005, PROC CVPR IEEE, P886
   Hong CQ, 2015, IEEE T IND ELECTRON, V62, P3742, DOI 10.1109/TIE.2014.2378735
   Jain V., 2011, P IEEE INT C COMP VI
   Jianguo Li, 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P2183, DOI 10.1109/ICCVW.2011.6130518
   Jun B, 2012, PATTERN RECOGN, V45, P3304, DOI 10.1016/j.patcog.2012.02.031
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Markus N., 2013, ARXIV13054537
   Pan H, 2013, COMPUT VIS IMAGE UND, V117, P12, DOI 10.1016/j.cviu.2012.09.003
   Rowley HA, 1998, IEEE T PATTERN ANAL, V20, P23, DOI 10.1109/34.655647
   Segui S., 2012, INT C PATT REC APPL, P9097
   Shih PC, 2006, PATTERN RECOGN, V39, P260, DOI 10.1016/j.patcog.2005.07.003
   Sun Y, 2013, PROC CVPR IEEE, P3476, DOI 10.1109/CVPR.2013.446
   Tivive FHC, 2008, NEUROCOMPUTING, V71, P3253, DOI 10.1016/j.neucom.2008.04.036
   Venkatesh B., 2010, P WORKSH FAC DET EUR
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Wen JB, 2013, NEUROCOMPUTING, V116, P122, DOI 10.1016/j.neucom.2011.12.060
   Yu J, 2015, IEEE T CYBERNETICS, V45, P767, DOI 10.1109/TCYB.2014.2336697
   Yu J, 2014, PATTERN RECOGN, V47, P3512, DOI 10.1016/j.patcog.2014.05.002
   Yu J, 2014, IEEE T CYBERNETICS, V44, P2431, DOI 10.1109/TCYB.2014.2307862
   Yu J, 2014, IEEE T IMAGE PROCESS, V23, P2019, DOI 10.1109/TIP.2014.2311377
   Yu J, 2014, IEEE T MULTIMEDIA, V16, P159, DOI 10.1109/TMM.2013.2284755
   Zhang C, 2014, IEEE WINT CONF APPL, P1036, DOI 10.1109/WACV.2014.6835990
   Zhou S. H., 2014, J COMPUTATIONAL INFO, V10, P1767
NR 27
TC 30
Z9 31
U1 2
U2 38
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0925-2312
EI 1872-8286
J9 NEUROCOMPUTING
JI Neurocomputing
PD APR 26
PY 2016
VL 187
SI SI
BP 19
EP 26
DI 10.1016/j.neucom.2015.07.130
PG 8
WC Computer Science, Artificial Intelligence
SC Computer Science
GA DK0MK
UT WOS:000374606700004
DA 2020-02-19
ER

PT J
AU Qin, HW
   Li, X
   Liang, J
   Peng, YG
   Zhang, CS
AF Qin, Hongwei
   Li, Xiu
   Liang, Jian
   Peng, Yigang
   Zhang, Changshui
TI DeepFish: Accurate underwater live fish recognition with a deep
   architecture
SO NEUROCOMPUTING
LA English
DT Article
DE Deep learning; Object recognition; Underwater; Cascaded network
ID IMAGE; CLASSIFICATION; KERNEL
AB Underwater object recognition is in great demand, while the research is far from enough. The unrestricted natural environment makes it a challenging task. We propose a framework to recognize fish from videos captured by underwater cameras deployed in the ocean observation network. First, we extract the foreground via sparse and low-rank matrix decomposition. Then, a deep architecture is used to extract features of the foreground fish images. In this architecture, principal component analysis (PCA) is used in two convolutional layers, followed by binary hashing in the non-linear layer and block-wise histograms in the feature pooling layer. Then spatial pyramid pooling (SPP) is used to extract information invariant to large poses. Finally, a linear SVM classifier is used for the classification. This deep network model can be trained efficiently. On a real-world fish recognition dataset, we achieve the state-of-the-art accuracy of 98.64%. (C) 2015 Elsevier B.V. All rights reserved.
C1 [Qin, Hongwei; Li, Xiu] Tsinghua Univ, Grad Sch Shenzhen, Shenzhen 518055, Peoples R China.
   [Qin, Hongwei; Li, Xiu; Liang, Jian; Peng, Yigang; Zhang, Changshui] Tsinghua Univ, Dept Automat, Beijing 100084, Peoples R China.
RP Li, X (reprint author), Tsinghua Univ, Grad Sch Shenzhen, Shenzhen 518055, Peoples R China.
EM li.xiu@sz.tsinghua.edu.cn
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [71171121/61033005]; National 863 High Technology
   Research and Development Program of ChinaNational High Technology
   Research and Development Program of China [2012AA09A408]
FX This work is supported by National Natural Science Foundation of China
   (Grant No. 71171121/61033005) and National 863 High Technology Research
   and Development Program of China (Grant No. 2012AA09A408).
CR Bai J, 2015, NEUROCOMPUTING, V165, P280, DOI 10.1016/j.neucom.2015.03.017
   Bengio Y., 2007, LARGE SCALE KERNEL M, V34
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Boom BJ, 2012, INT C PATT RECOG, P1542
   Chan T. H., ARXIV14043606
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chatfield K., ARXIV14053531
   Dalal N, 2005, PROC CVPR IEEE, P886
   Dean J., 2012, ADV NEURAL INFORM PR, V25, P1223
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Goodfellow I.J., ARXIV13024389
   Grauman K, 2005, IEEE I CONF COMP VIS, P1458
   He K., ARXIV150201852
   Heithaus MR, 2002, ECOLOGY, V83, P480, DOI 10.1890/0012-9658(2002)083[0480:FAATSP]2.0.CO;2
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Huang P. X., 2012, LECT NOTES COMPUTER, P422, DOI [10.1007/978-3-642-37331-2, DOI 10.1007/978-3-642-37331-2]
   Huang PX, 2014, IEEE WINT CONF APPL, P371, DOI 10.1109/WACV.2014.6836076
   Jarrett K, 2009, IEEE I CONF COMP VIS, P2146, DOI 10.1109/ICCV.2009.5459469
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Jin JQ, 2014, IEEE T INTELL TRANSP, V15, P1991, DOI 10.1109/TITS.2014.2308281
   Kavukcuoglu K., 2010, ADV NEURAL INFORM PR, V23, P1090
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Larsen R, 2009, LECT NOTES COMPUT SC, V5575, P745
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee DJ, 2004, PROC SPIE, V5606, P37, DOI 10.1117/12.571789
   Lee H., 2009, P 26 ANN INT C MACH, P609, DOI DOI 10.1145/1553374.1553453
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11
   Qin HW, 2014, 2014 ICPR WORKSHOP ON COMPUTER VISION FOR ANALYSIS OF UNDERWATER IMAGERY (CVAUI 2014), P65, DOI 10.1109/CVAUI.2014.16
   Rova A, 2007, IAPR C MACH VIS APPL, V1, P404
   Simonyan K., ARXIV14091556
   Spampinato C, 2010, P 1 ACM INT WORKSH A, P45, DOI [http://doi.acm.org/10.1145/1877868.1877881, DOI 10.1145/1877868.1877881]
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   STRACHAN NJC, 1990, PATTERN RECOGN, V23, P539, DOI 10.1016/0031-3203(90)90074-U
   STRACHAN NJC, 1993, IMAGE VISION COMPUT, V11, P2, DOI 10.1016/0262-8856(93)90027-E
   Szegedy C., ARXIV14094842
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Tang Y., ARXIV13060239
   Tao D., IEEE T CYBERN
   Vedaldi A, 2010, P 18 ACM INT C MULT, P1469, DOI DOI 10.1145/1873951.1874249
   White DJ, 2006, FISH RES, V80, P203, DOI 10.1016/j.fishres.2006.04.009
   Yang JC, 2009, PROC CVPR IEEE, P1794, DOI 10.1109/CVPRW.2009.5206757
   Yu J, 2014, IEEE T CYBERNETICS, V44, P2431, DOI 10.1109/TCYB.2014.2307862
   Yu J, 2012, IEEE T IMAGE PROCESS, V21, P3262, DOI 10.1109/TIP.2012.2190083
NR 44
TC 43
Z9 43
U1 8
U2 60
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0925-2312
EI 1872-8286
J9 NEUROCOMPUTING
JI Neurocomputing
PD APR 26
PY 2016
VL 187
SI SI
BP 49
EP 58
DI 10.1016/j.neucom.2015.10.122
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA DK0MK
UT WOS:000374606700006
DA 2020-02-19
ER

PT J
AU Kim, H
   Koo, J
   Kim, D
   Jung, S
   Shin, JU
   Lee, S
   Myung, H
AF Kim, Hanguen
   Koo, Jungmo
   Kim, Donghoon
   Jung, Sungwook
   Shin, Jae-Uk
   Lee, Serin
   Myung, Hyun
TI Image-Based Monitoring of Jellyfish Using Deep Learning Architecture
SO IEEE SENSORS JOURNAL
LA English
DT Article
DE Jellyfish monitoring; object recognition; convolutional neural network
AB Jellyfish blooms have caused great damage to the fishery industry. In efforts to solve this problem, various systems to remove jellyfish have been proposed. This letter presents preliminary results of applying an image-based jellyfish distribution recognition algorithm to increase the efficiency of an existing jellyfish removal system. By using a convolutional neural network and dedicated image processing techniques, the experimental results show reasonable performance for real-world application.
C1 [Kim, Hanguen; Koo, Jungmo; Kim, Donghoon; Jung, Sungwook; Shin, Jae-Uk; Myung, Hyun] Korea Adv Inst Sci & Technol, Urban Robot Lab, Daejeon 34141, South Korea.
   [Lee, Serin] Inst Infocomm Res, Singapore 138632, Singapore.
RP Myung, H (reprint author), Korea Adv Inst Sci & Technol, Urban Robot Lab, Daejeon 34141, South Korea.
EM sskhk05@kaist.ac.kr; jungmokoo@kaist.ac.kr; dh8607@kaist.ac.kr;
   sungwook87@kaist.ac.kr; jacksju@kaist.ac.kr;
   serin-lee@i2r.a-star.edu.sg; hmyung@kaist.ac.kr
RI Myung, Hyun/C-1698-2011
OI Myung, Hyun/0000-0002-5799-2026
FU Robot Industrial Cluster Construction Program through the Ministry of
   Trade, Industry and Energy; Korea Institute for Advancement of
   Technology; National Research Foundation of Korea within the Ministry of
   Science, ICT and Future Planning through the Korean Government
   [NRF-2015R1A2A1A15056262]; Ministry of Land, Infrastructure and
   Transport through the U-City Master and Doctor Course Grant Program
FX This work was supported in part by the Robot Industrial Cluster
   Construction Program through the Ministry of Trade, Industry and Energy
   and Korea Institute for Advancement of Technology, in part by the
   National Research Foundation of Korea within the Ministry of Science,
   ICT and Future Planning through the Korean Government under Grant
   NRF-2015R1A2A1A15056262, and in part by the Ministry of Land,
   Infrastructure and Transport through the U-City Master and Doctor Course
   Grant Program.
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Deng L., 2014, FDN TRENDS SIGNAL PR, V7, P197, DOI DOI 10.1561/2000000039
   Ekstrand Michael D., 2010, Foundations and Trends in Human-Computer Interaction, V4, P81, DOI 10.1561/1100000009
   Kim D., 2012, P INT C ROB INT TECH, P395
   Kim D., 2012, P INT C HUM SYST ICH, P336
   Kim DH, 2012, FISHERIES SCI, V78, P1147, DOI 10.1007/s12562-012-0533-1
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Rife J, 2003, IEEE J OCEANIC ENG, V28, P595, DOI 10.1109/JOE.2003.819315
NR 8
TC 6
Z9 6
U1 0
U2 30
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1530-437X
EI 1558-1748
J9 IEEE SENS J
JI IEEE Sens. J.
PD APR 15
PY 2016
VL 16
IS 8
BP 2215
EP 2216
DI 10.1109/JSEN.2016.2517823
PG 2
WC Engineering, Electrical & Electronic; Instruments & Instrumentation;
   Physics, Applied
SC Engineering; Instruments & Instrumentation; Physics
GA DG9QO
UT WOS:000372419100003
DA 2020-02-19
ER

PT J
AU Zabalza, J
   Ren, JC
   Zheng, JB
   Zhao, HM
   Qing, CM
   Yang, ZJ
   Du, PJ
   Marshall, S
AF Zabalza, Jaime
   Ren, Jinchang
   Zheng, Jiangbin
   Zhao, Huimin
   Qing, Chunmei
   Yang, Zhijing
   Du, Peijun
   Marshall, Stephen
TI Novel segmented stacked autoencoder for effective dimensionality
   reduction and feature extraction in hyperspectral imaging
SO NEUROCOMPUTING
LA English
DT Article
DE Deep learning (DL); Hyperspectral remote sensing; Data reduction;
   Segmented stacked autoencoder (S-SAE)
ID SPARSE REPRESENTATION; CLASSIFICATION; TRANSFORMATION; PROFILES;
   SUPPORT; IMAGES
AB Stacked autoencoders (SAEs), as part of the deep learning (DL) framework, have been recently proposed for feature extraction in hyperspectral remote sensing. With the help of hidden nodes in deep layers, a high-level abstraction is achieved for data reduction whilst maintaining the key information of the data. As hidden nodes in SAEs have to deal simultaneously with hundreds of features from hypercubes as inputs, this increases the complexity of the process and leads to limited abstraction and performance. As such, segmented SAE (S-SAE) is proposed by confronting the original features into smaller data segments, which are separately processed by different smaller SAEs. This has resulted in reduced complexity but improved efficacy of data abstraction and accuracy of data classification. (C) 2016 Published by Elsevier B.V.
C1 [Zabalza, Jaime; Ren, Jinchang; Marshall, Stephen] Univ Strathclyde, Dept Elect & Elect Engn, Glasgow G1 1XW, Lanark, Scotland.
   [Zheng, Jiangbin] Northwestern Polytech Univ, Sch Microelect & Software, Xian 710072, Peoples R China.
   [Zhao, Huimin] Guangdong Tech Normal Univ, Sch Elect & Informat, Guangzhou, Guangdong, Peoples R China.
   [Qing, Chunmei] S China Univ Technol, Sch Elect & Informat Engn, Guangzhou 510641, Guangdong, Peoples R China.
   [Yang, Zhijing] Guangdong Univ Technol, Sch Informat Engn, Guangzhou, Guangdong, Peoples R China.
   [Du, Peijun] Nanjing Univ, Dept Geog Informat, Nanjing 210008, Jiangsu, Peoples R China.
RP Ren, JC (reprint author), Univ Strathclyde, Ctr Excellence Signal & Image Proc, Glasgow G1 1XW, Lanark, Scotland.
EM Jinchang.Ren@strath.ac.uk
OI Du, Peijun/0000-0002-2488-2656; Marshall, Stephen/0000-0001-7079-5628;
   Ren, Jinchang/0000-0001-6116-3194; Zabalza, Jaime/0000-0002-0634-1725
FU University of Strathclyde; National Natural Science Foundation of
   ChinaNational Natural Science Foundation of China [61272381, 61471132,
   61401163]; Science and Technology Major Project of Education Department
   of Guangdong Province [2014KZDXM060]; Fundamental Research Funds for the
   Central UniversitiesFundamental Research Funds for the Central
   Universities [2015ZZ032]; Science and Technology Project of Guangzhou
   City [2014J4100078]
FX The authors wish to thank the anonymous reviewers and the Associate
   Editor for their constructive comments to further improve the quality of
   this paper. The work is partially supported by the University of
   Strathclyde and the following grants: National Natural Science
   Foundation of China (61272381, 61471132, 61401163), Science and
   Technology Major Project of Education Department of Guangdong Province
   (2014KZDXM060), the Fundamental Research Funds for the Central
   Universities (No.2015ZZ032), and Science and Technology Project of
   Guangzhou City (2014J4100078).
CR Chang C. C., LIBSVM LIB SUPPORT V
   Chen Y, 2011, IEEE T GEOSCI REMOTE, V49, P3973, DOI 10.1109/TGRS.2011.2129595
   Chen YS, 2014, IEEE J-STARS, V7, P2094, DOI 10.1109/JSTARS.2014.2329330
   Demir B, 2010, IEEE T GEOSCI REMOTE, V48, P4071, DOI 10.1109/TGRS.2010.2070510
   Fauvel M, 2008, IEEE T GEOSCI REMOTE, V46, P3804, DOI 10.1109/TGRS.2008.922034
   Foody GM, 2004, PHOTOGRAMM ENG REM S, V70, P627, DOI 10.14358/PERS.70.5.627
   Gao Y, 2014, IEEE T IMAGE PROCESS, V23, P2769, DOI 10.1109/TIP.2014.2319735
   GREEN AA, 1988, IEEE T GEOSCI REMOTE, V26, P65, DOI 10.1109/36.3001
   HAN J, 2014, IEEE T CIRCUITS SYST, V25, P1309
   Han JW, 2015, IEEE T GEOSCI REMOTE, V53, P3325, DOI 10.1109/TGRS.2014.2374218
   HYVRINEN A, 2001, INDEPENDENT COMPONEN
   Ji RR, 2014, IEEE T GEOSCI REMOTE, V52, P1811, DOI 10.1109/TGRS.2013.2255297
   Jia XP, 2013, P IEEE, V101, P676, DOI 10.1109/JPROC.2012.2229082
   Jia XP, 1999, IEEE T GEOSCI REMOTE, V37, P538, DOI 10.1109/36.739109
   Jolliffe I., 1986, PRINCIPAL COMPONENT
   Li J, 2015, IEEE T GEOSCI REMOTE, V53, P1592, DOI 10.1109/TGRS.2014.2345739
   Li K, 2015, IEEE T CYBERNETICS, V45, P1401, DOI 10.1109/TCYB.2014.2351831
   Midhun M.E., 2014, P ICONIAAC
   Ren JC, 2014, J VIS COMMUN IMAGE R, V25, P1558, DOI 10.1016/j.jvcir.2014.07.001
   Ren JC, 2014, IEEE SIGNAL PROC MAG, V31, P149, DOI 10.1109/MSP.2014.2312071
   Xia JS, 2015, IEEE T GEOSCI REMOTE, V53, P4768, DOI 10.1109/TGRS.2015.2409195
   Yang Y., IEEE T NEUR IN PRESS
   Zabalza J, 2015, IEEE T GEOSCI REMOTE, V53, P4418, DOI 10.1109/TGRS.2015.2398468
   Zabalza J, 2014, ISPRS J PHOTOGRAMM, V93, P112, DOI 10.1016/j.isprsjprs.2014.04.006
   Zabalza JM, 2014, IEEE GEOSCI REMOTE S, V11, P1886, DOI 10.1109/LGRS.2014.2312754
   Zhang JG, 2015, J VIS COMMUN IMAGE R, V30, P376, DOI 10.1016/j.jvcir.2015.05.004
   Zhao CH, 2013, INT J REMOTE SENS, V34, P8669, DOI 10.1080/01431161.2013.845924
NR 27
TC 79
Z9 81
U1 11
U2 101
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0925-2312
EI 1872-8286
J9 NEUROCOMPUTING
JI Neurocomputing
PD APR 12
PY 2016
VL 185
BP 1
EP 10
DI 10.1016/j.neucom.2015.11.044
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA DJ7AI
UT WOS:000374363900001
OA Green Accepted
DA 2020-02-19
ER

PT J
AU Guo, Q
   Wang, FL
   Lei, J
   Tu, D
   Li, GH
AF Guo, Qiang
   Wang, Fenglei
   Lei, Jun
   Tu, Dan
   Li, Guohui
TI Convolutional feature learning and Hybrid CNN-HMM for scene number
   recognition
SO NEUROCOMPUTING
LA English
DT Article
DE Scene text recognition; Convolutional neural network; Hidden Markov
   model; Deep learning; Hybrid NN-HMM; GMM-HMM
ID NEURAL-NETWORKS
AB In this work, we investigate to recognize house numbers captured in street view images. We formulate the problem as sequence recognition and present an integrated model by combining Convolutional Neural Network (CNN) and Hidden Markov Model (HMM). Our method utilizes representation capability of CNN to model the highly variable appearance of digits. Meanwhile, HMM is used to handle the dynamics of the image sequence. They are combined in a hybrid way to form the Hybrid CNN-HMM. Using this model, we can perform training and recognition both at the whole image level without explicit segmentation. The model makes CNN applicable to dynamic problems. Experiments show that the Hybrid CNN-HMM can dramatically boost the performance of Gaussian Mixture Model (GMM)-HMM. We evaluate different local features, e.g. LBP, SIFT and HOG, as observations fed into HMM and find CNN features consistently surpass those hand-engineered features with respect to recognition accuracy. To gain insight into performance difference of the features, we map them from the high-dimensional space to a 2-D plane by the t-SNE algorithm to visualize their semantic clustering with respect to the task. The visualization clearly justified the efficiency of features learnt by CNN. (C) 2015 Elsevier B.V. All rights reserved.
C1 [Guo, Qiang; Wang, Fenglei; Lei, Jun; Tu, Dan; Li, Guohui] Natl Univ Def Technol, Coll Informat Syst & Management, Changsha 410072, Hunan, Peoples R China.
RP Guo, Q (reprint author), Natl Univ Def Technol, Coll Informat Syst & Management, Changsha 410072, Hunan, Peoples R China.
EM guoqiang05@nudt.edu.cn; wangfenglei@nudt.edu.cn; leijun1987@gmail.com;
   tudan@nudt.edu.cn; guohli@nudt.edu.cn
FU State Key Laboratory of Mathematical Engineering and Advanced Computing
   [2015A04]
FX This work was supported by Open Project Program of the State Key
   Laboratory of Mathematical Engineering and Advanced Computing
   (No.2015A04).
CR Alsharif O., ARXIV13101811
   Baum L. E., INEQUALITIES, V3
   BAUM LE, 1970, ANN MATH STAT, V41, P164, DOI 10.1214/aoms/1177697196
   Benenson R, 2015, LECT NOTES COMPUT SC, V8926, P613, DOI 10.1007/978-3-319-16181-5_47
   Bissacco A., 2013, ICCV
   Bourlard H.A., 1993, CONNECTIONIST SPEECH
   Cheriet M, 2009, PATTERN RECOGN, V42, P3131, DOI 10.1016/j.patcog.2009.03.014
   Dahl GE, 2012, IEEE T AUDIO SPEECH, V20, P30, DOI 10.1109/TASL.2011.2134090
   Dalal N, 2005, PROC CVPR IEEE, P886
   de Oliveira L. E. S., 2002, AUTOMATIC RECOGNITIO, P1438
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Fergus R., ABS13112901 CORR
   FORNEY GD, 1973, P IEEE, V61, P268, DOI 10.1109/PROC.1973.9030
   Gales M, 2007, FOUND TRENDS SIGNAL, V1, P195, DOI 10.1561/2000000004
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Goodfellow I.J., ARXIV13024389
   Goodfellow I.J., 2013, ARXIV13126082
   Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Jarrett K, 2009, IEEE I CONF COMP VIS, P2146, DOI 10.1109/ICCV.2009.5459469
   Juang BH, 1997, IEEE T SPEECH AUDI P, V5, P257, DOI 10.1109/89.568732
   Kapadia S., 1993, ICASSP-93. 1993 IEEE International Conference on Acoustics, Speech, and Signal Processing (Cat. No.92CH3252-4), P491, DOI 10.1109/ICASSP.1993.319349
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, V1, P4
   Lafferty J.D., 2001, P INT C MACH LEARN, V18, P282
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Ling ZH, 2015, IEEE SIGNAL PROC MAG, V32, P35, DOI 10.1109/MSP.2014.2359987
   Liu C, 2011, IEEE T PATTERN ANAL, V33, P978, DOI 10.1109/TPAMI.2010.147
   Liu L, 2012, IEEE T PATTERN ANAL, V34, P574, DOI 10.1109/TPAMI.2011.145
   Lowe D., 1999, P INT C COMP VIS, V2, P1150, DOI [10.1109/ICCV.1999.790410, DOI 10.1109/ICCV.1999.790410]
   Maas A. L., ABS14067806 CORR
   Matan O, 1991, NIPS, P488
   Meier U, 2011, PROC INT CONF DOC, P1250, DOI 10.1109/ICDAR.2011.252
   MORGAN N, 1995, IEEE SIGNAL PROC MAG, V12, P25
   Nagy G, 2000, IEEE T PATTERN ANAL, V22, P38, DOI 10.1109/34.824820
   Nanni L, 2012, EXPERT SYST APPL, V39, P3634, DOI 10.1016/j.eswa.2011.09.054
   Netzer Y., 2011, NIPS WORKSHOP ON DEE, V2011
   Neumann L., 2013, ICCV
   Neumann L, 2012, PROC CVPR IEEE, P3538, DOI 10.1109/CVPR.2012.6248097
   Neumann L, 2011, LECT NOTES COMPUT SC, V6494, P770, DOI 10.1007/978-3-642-19318-7_60
   OHYA J, 1994, IEEE T PATTERN ANAL, V16, P214, DOI 10.1109/34.273729
   Rabiner L. R., 1993, FUNDAMENTALS SPEECH, p[1, I]
   Richard MD, 1991, NEURAL COMPUT, V3, P461, DOI 10.1162/neco.1991.3.4.461
   Roy Partha Pratim, 2009, 2009 10th International Conference on Document Analysis and Recognition (ICDAR), P11, DOI 10.1109/ICDAR.2009.124
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sainath T. N., 2011, 2011 IEEE Workshop on Automatic Speech Recognition & Understanding (ASRU), P30, DOI 10.1109/ASRU.2011.6163900
   SAYRE KM, 1973, PATTERN RECOGN, V5, P213, DOI 10.1016/0031-3203(73)90044-7
   Sermanet P, 2013, OVERFEAT INTEGRATED
   Simonyan K., 2014, VERY DEEP CONVOLUTIO
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vedaldi A., 2008, VLFEAT OPEN PORTABLE
   Vinciarelli A, 2004, IEEE T PATTERN ANAL, V26, P709, DOI 10.1109/TPAMI.2004.14
   Vinciarelli A, 2002, PATTERN RECOGN, V35, P1433, DOI 10.1016/S0031-3203(01)00129-7
   Wang K, 2011, IEEE I CONF COMP VIS, P1457, DOI 10.1109/ICCV.2011.6126402
   Wang T, 2012, INT C PATT RECOG, P3304
   Yanikoglu B, 1998, PATTERN RECOGN, V31, P1825, DOI 10.1016/S0031-3203(98)00081-8
   Yao C, 2014, PROC CVPR IEEE, P4042, DOI 10.1109/CVPR.2014.515
   Zhang Ning, 2014, PART BASED R CNNS FI
NR 58
TC 19
Z9 19
U1 0
U2 46
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0925-2312
EI 1872-8286
J9 NEUROCOMPUTING
JI Neurocomputing
PD APR 5
PY 2016
VL 184
SI SI
BP 78
EP 90
DI 10.1016/j.neucom.2015.07.135
PG 13
WC Computer Science, Artificial Intelligence
SC Computer Science
GA DJ7AM
UT WOS:000374364300009
DA 2020-02-19
ER

PT J
AU Wang, YS
   Yao, HX
   Zhao, SC
AF Wang, Yasi
   Yao, Hongxun
   Zhao, Sicheng
TI Auto-encoder based dimensionality reduction
SO NEUROCOMPUTING
LA English
DT Article
DE Auto-encoder; Dimensionality reduction; Visualization; Intrinsic
   dimensionality; Dimensionality-accuracy
ID CAPABILITIES; AUTOENCODER; BOUNDS
AB Auto-encoder a tricky three-layered neural network, known as auto-association before, constructs the "building block" of deep learning, which has been demonstrated to achieve good performance in various domains. In this paper, we try to investigate the dimensionality reduction ability of auto-encoder, and see if it has some kind of good property that might accumulate when being stacked and thus contribute to the success of deep learning.
   Based on the above idea, this paper starts from auto-encoder and focuses on its ability to reduce the dimensionality, trying to understand the difference between auto-encoder and state-of-the-art dimensionality reduction methods. Experiments are conducted both on the synthesized data for an intuitive understanding of the method, mainly on two and three-dimensional spaces for better visualization, and on some real datasets, including MNIST and Olivetti face datasets. The results show that auto-encoder can indeed learn something different from other methods. Besides, we preliminarily investigate the influence of the number of hidden layer nodes on the performance of auto-encoder and its possible relation with the intrinsic dimensionality of input data. (C) 2015 Elsevier B.V. All rights reserved.
C1 [Wang, Yasi; Yao, Hongxun; Zhao, Sicheng] Harbin Inst Technol, Sch Comp Sci & Technol, Harbin 150006, Peoples R China.
RP Yao, HX (reprint author), Harbin Inst Technol, Sch Comp Sci & Technol, Harbin 150006, Peoples R China.
EM wangyasi@hit.edu.cn; h.yao@hit.edu.cn; zsc@hit.edu.cn
OI Wang, Yasi/0000-0002-1179-3304
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61472103];  [61133003]
FX This work was supported by the National Natural Science Foundation of
   China (No. 61472103) and Key Program (No. 61133003).
CR Arbib M., 1969, IEEE T INFORM THEORY, V15, P738
   Bengio Yoshua, 2013, Statistical Language and Speech Processing. First International Conference, SLSP 2013. Proceedings: LNCS 7978, P1, DOI 10.1007/978-3-642-39593-2_1
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Bengio Yoshua, 2012, CORR
   BOURLARD H, 1988, BIOL CYBERN, V59, P291, DOI 10.1007/BF00332918
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411
   DeMers D., 1993, ADV NEURAL INFORMATI, V5, P580
   Fergus Rob, 2013, CORR
   Ghodsi Ali, 2006, DIMENSIONALITY REDUC
   Hinton G., 2012, NEURAL NETWORKS TRIC, V9, P599, DOI DOI 10.1007/978-3-642-35289-8_32
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   HOPFIELD JJ, 1985, BIOL CYBERN, V52, P141
   [胡昭华 Hu Zhao-hua], 2009, [电子与信息学报, Journal of Electronics & Information Technology], V31, P1189
   Huang GB, 2003, IEEE T NEURAL NETWOR, V14, P274, DOI 10.1109/TNN.2003.809401
   HUANG SC, 1991, IEEE T NEURAL NETWOR, V2, P47, DOI 10.1109/72.80290
   Japkowicz N, 2000, NEURAL COMPUT, V12, P531, DOI 10.1162/089976600300015691
   KRAMER MA, 1991, AICHE J, V37, P233, DOI 10.1002/aic.690370209
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   Le QV, 2013, INT CONF ACOUST SPEE, P8595, DOI 10.1109/ICASSP.2013.6639343
   Minsky M, 1988, PERCEPTRONS INTRO CO
   SARTORI MA, 1991, IEEE T NEURAL NETWOR, V2, P467, DOI 10.1109/72.88168
   Tamura S, 1997, IEEE T NEURAL NETWOR, V8, P251, DOI 10.1109/72.557662
   TAMURA S, 1991, IEEE IJCNN, P2757, DOI 10.1109/IJCNN.1991.170332
   Wang J, 2012, PROCEDIA COMPUT SCI, V13, P120, DOI 10.1016/j.procs.2012.09.120
   Wang W, 2014, IEEE COMPUT SOC CONF, P496, DOI 10.1109/CVPRW.2014.79
   Xiang C, 2005, IEEE T NEURAL NETWOR, V16, P84, DOI 10.1109/TNN.2004.836197
   Zhu ZY, 2013, IEEE I CONF COMP VIS, P113, DOI 10.1109/ICCV.2013.21
NR 28
TC 49
Z9 55
U1 6
U2 30
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0925-2312
EI 1872-8286
J9 NEUROCOMPUTING
JI Neurocomputing
PD APR 5
PY 2016
VL 184
SI SI
BP 232
EP 242
DI 10.1016/j.neucom.2015.08.104
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA DJ7AM
UT WOS:000374364300022
DA 2020-02-19
ER

PT J
AU Sheng, YQ
   Wang, JL
   Li, CP
   Qi, WN
AF Sheng, Yiqiang
   Wang, Jinlin
   Li, Chaopeng
   Qi, Weining
TI Max-Min-Degree Neural Network for Centralized-Decentralized
   Collaborative Computing
SO IEICE TRANSACTIONS ON COMMUNICATIONS
LA English
DT Article
DE big data; cloud computing; decentralized computing; collaborative
   computing; learning systems
AB In this paper, we propose an undirected model of learning systems, named max-min-degree neural network, to realize centralized-decentralized collaborative computing. The basic idea of the proposal is a max-min-degree constraint which extends a k-degree constraint to improve the communication cost, where k is a user-defined degree of neurons. The max-min-degree constraint is defined such that the degree of each neuron lies between k(min) and k(max). Accordingly, the Boltzmann machine is a special case of the proposal with k(min) = k(max) = n, where n is the full-connected degree of neurons. Evaluations show that the proposal is much better than a state-of-the-art model of deep learning systems with respect to the communication cost. The cost of the above improvement is slower convergent speed with respect to data size, but it does not matter in the case of big data processing.
C1 [Sheng, Yiqiang; Wang, Jinlin; Qi, Weining] Chinese Acad Sci, Natl Network New Media Engn Res Ctr, Beijing 100190, Peoples R China.
   [Li, Chaopeng] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
RP Sheng, YQ (reprint author), Chinese Acad Sci, Natl Network New Media Engn Res Ctr, Beijing 100190, Peoples R China.
EM shengyq@dsp.ac.cn
RI SHENG, Yiqiang/T-8455-2019
OI SHENG, Yiqiang/0000-0002-8452-2492
FU CASDefence Research & Development Organisation (DRDO)Chinese Academy of
   Sciences; CASDefence Research & Development Organisation (DRDO)Chinese
   Academy of Sciences [XDA06040602, XDA06040501]
FX This work is supported by CAS Pioneer Hundred Talents Program and
   Special Fund for Strategic Pilot Technology of CAS under Grant No.
   XDA06040602 and No. XDA06040501. The authors would like to thank Dr. L.
   Wang and Prof. X. Zeng at Chinese Academy of Sciences, thank Prof. A.
   Takahashi and Prof. S. Ueno at Tokyo Institute of Technology, and thank
   the anonymous reviewers for their valuable comments.
CR Bako L, 2012, UKSIM EURO SYMP COMP, P359, DOI 10.1109/EMS.2012.87
   Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153
   Elons AS, 2014, 2014 9TH INTERNATIONAL CONFERENCE ON COMPUTER ENGINEERING & SYSTEMS (ICCES), P360, DOI 10.1109/ICCES.2014.7030986
   Guan ZL, 2010, PROCEEDINGS OF THE 2010 INTERNATIONAL CONFERENCE ON ADVANCED INTELLIGENCE AND AWARENESS INTERNET, AIAI2010, P269
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Kaji I., 1999, Proceedings. Fourth International Symposium on Autonomous Decentralized Systems. - Integration of Heterogeneous Systems -, P150, DOI 10.1109/ISADS.1999.838428
   LeCun Y, 2010, IEEE INT SYMP CIRC S, P253, DOI 10.1109/ISCAS.2010.5537907
   Li BX, 2014, IEEE IJCNN, P4062, DOI 10.1109/IJCNN.2014.6889433
   Low Y, 2012, PROC VLDB ENDOW, V5, P716, DOI 10.14778/2212351.2212354
   Masuishi T, 2005, ISADS 2005: International Symposium on Autonomous Decentralized Systems,Proceedings, P277, DOI 10.1109/ISADS.2005.1452067
   Mazilu S., 2011, Proceedings of the 2011 Tenth International Conference on Machine Learning and Applications (ICMLA 2011), P166, DOI 10.1109/ICMLA.2011.85
   Sandryhaila A, 2014, IEEE SIGNAL PROC MAG, V31, P80, DOI 10.1109/MSP.2014.2329213
   Stojmenovic I, 2014, ACSIS-ANN COMPUT SCI, V2, P1
   Strigl D, 2010, EUROMICRO WORKSHOP P, P317, DOI 10.1109/PDP.2010.43
   TAKAHASHI J, 2013, P 2013 IEEE WIC ACM, P405
   Tomislav B, 2014, IEEE IJCNN, P785, DOI 10.1109/IJCNN.2014.6889487
   Tsuchida Y, 2012, JOINT INT CONF SOFT, P193, DOI 10.1109/SCIS-ISIS.2012.6505093
   Tudoran R, 2014, IEEE ACM INT SYMP, P92, DOI 10.1109/CCGrid.2014.86
   Wang J, 2014, P ROY SOC A-MATH PHY, V470, DOI 10.1098/rspa.2014.0439
   Xiao Z, 2013, IEEE T PARALL DISTR, V24, P1107, DOI 10.1109/TPDS.2012.283
   Yang K, 2013, IEEE T PARALL DISTR, V24, P1717, DOI 10.1109/TPDS.2012.278
   Zhang KL, 2014, IEEE ACCESS, V2, P395, DOI 10.1109/ACCESS.2014.2319813
NR 22
TC 1
Z9 1
U1 0
U2 3
PU IEICE-INST ELECTRONICS INFORMATION COMMUNICATIONS ENG
PI TOKYO
PA KIKAI-SHINKO-KAIKAN BLDG, 3-5-8, SHIBA-KOEN, MINATO-KU, TOKYO, 105-0011,
   JAPAN
SN 0916-8516
EI 1745-1345
J9 IEICE T COMMUN
JI IEICE Trans. Commun.
PD APR
PY 2016
VL E99B
IS 4
BP 841
EP 848
DI 10.1587/transcom.2015ADP0013
PG 8
WC Engineering, Electrical & Electronic; Telecommunications
SC Engineering; Telecommunications
GA DT5ZA
UT WOS:000381561000009
DA 2020-02-19
ER

PT J
AU Gomez, JC
   Tommasi, T
   Zoghbi, S
   Moens, MF
AF Gomez, J. C.
   Tommasi, T.
   Zoghbi, S.
   Moens, M. F.
TI What Would They Say? Predicting User's Comments in Pinterest
SO IEEE LATIN AMERICA TRANSACTIONS
LA Spanish
DT Article
DE Multimodal Clustering; Pinterest; Social Media; User Generated Content;
   Deep-Learning Representation; Automatic Image Annotation
AB When we refer to an image that attracts our attention, it is natural to mention not only what is literally depicted in the image, but also the sentiments, thoughts and opinions that it invokes in ourselves. In this work we deviate from the standard mainstream tasks of associating tags or keywords to an image, or generating content image descriptions, and we introduce the novel task of automatically generate user comments for an image. We present a new dataset collected from the social media Pinterest and we propose a strategy based on building joint textual and visual user models, tailored to the specificity of the mentioned task. We conduct an extensive experimental analysis of our approach on both qualitative and quantitative terms, which allows assessing the value of the proposed approach and shows its encouraging results against several existing image-to-text methods.
C1 [Gomez, J. C.] Katholieke Univ Leuven, Louvain, Belgium.
   [Tommasi, T.] Univ N Carolina, Chapel Hill, NC USA.
   [Zoghbi, S.] Katholieke Univ Leuven, Comp Sci, Louvain, Belgium.
   [Moens, M. F.] Katholieke Univ Leuven, Dept Comp Sci, Louvain, Belgium.
RP Gomez, JC (reprint author), Katholieke Univ Leuven, Louvain, Belgium.
EM jcgcarranza@gmail.com; ttommasi@cs.unc.edu;
   susana.zoghbi@cs.kuleuven.be; sien.moens@cs.kuleuven.be
RI Gomez, Juan Carlos/F-3959-2019
OI Gomez, Juan Carlos/0000-0002-0862-7612; Tommasi,
   Tatiana/0000-0001-8229-7159
CR Donahue J., 2013, ARXIV13101531
   Gallagher A. C., 2008, P ACM INT C MULT, P681
   Kalayeh MM, 2014, PROC CVPR IEEE, P184, DOI 10.1109/CVPR.2014.31
   Kulkarni G, 2011, PROC CVPR IEEE, P1601, DOI 10.1109/CVPR.2011.5995466
   Li X., 2011, P ACM INT C MULT, P233
   Makadia A, 2010, INT J COMPUT VISION, V90, P88, DOI 10.1007/s11263-010-0338-6
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Ordonez H, 2015, IEEE LAT AM T, V13, P769, DOI 10.1109/TLA.2015.7069103
   Ordonez Vicente, 2011, ADV NEURAL INFORM PR, P1143
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311
   Ramnath K, 2014, IEEE WINT CONF APPL, P1050, DOI 10.1109/WACV.2014.6835988
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Sawant Neela, 2010, P INT C MULT INF RET, P231
   Tang JH, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1899412.1899418
   Weston J, 2010, MACH LEARN, V81, P21, DOI 10.1007/s10994-010-5198-3
   Zoghbi Susana, 2016, International Journal of Computer and Electrical Engineering, V8, P31, DOI 10.17706/ijcee.2016.8.1.31-43
   Zoghbi S., 2015, 2015 NIPS MULT MACH
   Zoghbi S, 2016, 2016 INT C MULTIMEDI, P367
NR 18
TC 2
Z9 2
U1 0
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1548-0992
J9 IEEE LAT AM T
JI IEEE Latin Am. Trans.
PD APR
PY 2016
VL 14
IS 4
BP 2013
EP 2019
DI 10.1109/TLA.2016.7483548
PG 7
WC Computer Science, Information Systems; Engineering, Electrical &
   Electronic
SC Computer Science; Engineering
GA DN9BT
UT WOS:000377374800067
DA 2020-02-19
ER

PT J
AU Zabkar, J
   Leonardis, A
AF Zabkar, Jure
   Leonardis, Ales
TI Motor memory: Representation, learning and consolidation
SO BIOLOGICALLY INSPIRED COGNITIVE ARCHITECTURES
LA English
DT Article
DE Motor memory; Compositional hierarchy; Motor memory consolidation; Deep
   learning
ID ADAPTATION; DISCRETE; MOVEMENT; POSITION
AB An efficient representation of motor system is vital to robot control and its ability to learn new skills. While the increasing sensor accuracy and the speed of signal processing failed to bridge the gap between the performance of artificial and human sensorimotor systems, the motor memory architecture seems to remain neglected. Despite the advances in robot skill learning, the latter remains limited to predefined tasks and pre-specified embodiment. We propose a new motor memory architecture that enables information sharing between different skills, on-line learning and off-line memory consolidation. We develop an algorithm for learning and consolidation of motor memory and study the space complexity of the representation in the experiments with humanoid robot Nao. Finally, we propose the integration of motor memory with sensor data into a common sensorimotor memory. (C) 2016 Elsevier B.V. All rights reserved.
C1 [Zabkar, Jure] Univ Ljubljana, Fac Comp & Informat Sci, AI Lab, Vecna Pot 113, SI-1000 Ljubljana, Slovenia.
   [Leonardis, Ales] Univ Birmingham, Sch Comp Sci, Intelligent Robot Lab, Birmingham B15 2TT, W Midlands, England.
   [Leonardis, Ales] Univ Birmingham, Ctr Computat Neurosci & Cognit Robot, Birmingham B15 2TT, W Midlands, England.
RP Zabkar, J (reprint author), Univ Ljubljana, Fac Comp & Informat Sci, AI Lab, Vecna Pot 113, SI-1000 Ljubljana, Slovenia.
EM jure.zabkar@fri.uni-lj.si; a.leonardis@cs.bham.ac.uk
CR Ben Amor H, 2012, P 25 INT C INT ROB S
   Braun DA, 2010, PLOS ONE, V5, DOI 10.1371/journal.pone.0008973
   Braun DA, 2009, CURR BIOL, V19, P352, DOI 10.1016/j.cub.2009.01.036
   Criscimagna-Hemminger SE, 2008, J NEUROSCI, V28, P9610, DOI 10.1523/JNEUROSCI.3071-08.2008
   D'Souza A, 2001, IROS 2001: PROCEEDINGS OF THE 2001 IEEE/RJS INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-4, P298, DOI 10.1109/IROS.2001.973374
   Dayan E, 2011, NEURON, V72, P443, DOI 10.1016/j.neuron.2011.10.008
   Degallier S, 2011, AUTON ROBOT, V31, P155, DOI 10.1007/s10514-011-9235-2
   Demiris Y, 2006, NEURAL NETWORKS, V19, P272, DOI 10.1016/j.neunet.2006.02.005
   Dover G, 2003, J ATHL TRAINING, V38, P304
   Doyon J, 2005, CURR OPIN NEUROBIOL, V15, P161, DOI 10.1016/j.conb.2005.03.004
   Fidler S., 2008, P COMP VIS PATT REC
   Forte D, 2012, ROBOT AUTON SYST, V60, P1327, DOI 10.1016/j.robot.2012.05.004
   jspeert A.J. I., 2002, ADV NEURAL INFORM PR, P1547
   Knoblich G, 2001, PSYCHOL SCI, V12, P467, DOI 10.1111/1467-9280.00387
   Kober J, 2012, AUTON ROBOT, V33, P361, DOI 10.1007/s10514-012-9290-3
   Kruger V, 2010, IEEE ROBOT AUTOM MAG, V17, P30, DOI 10.1109/MRA.2010.936961
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lee KH, 2015, NEURON, V86, P529, DOI 10.1016/j.neuron.2015.03.010
   Lenz I, 2015, INT J ROBOT RES, V34, P705, DOI 10.1177/0278364914549607
   Luft AR, 2005, MOL NEUROBIOL, V32, P205, DOI 10.1385/MN:32:3:205
   Nakanishi J, 2004, ROBOT AUTON SYST, V47, P79, DOI 10.1016/j.robot.2004.03.003
   Nishtala R., 2009, P IEEE INT PAR DISTR, P1, DOI DOI 10.1109/IPDPS.2009.5161076
   Peters J, 2008, NEURAL NETWORKS, V21, P682, DOI 10.1016/j.neunet.2008.02.003
   Proske U, 2012, PHYSIOL REV, V92, P1651, DOI 10.1152/physrev.00048.2011
   Robertson EM, 2012, CURR BIOL, V22, pR66, DOI 10.1016/j.cub.2011.11.051
   Schaal S., 2002, LEARNING ROBOT CONTR
   Schaal S., 2003, INT S AD MOT AN MACH
   Schaal S, 2007, PROG BRAIN RES, V165, P425, DOI 10.1016/S0079-6123(06)65027-9
   Si ZZ, 2013, IEEE T PATTERN ANAL, V35, P2189, DOI 10.1109/TPAMI.2013.35
   Stickgold R, 2005, NATURE, V437, P1272, DOI 10.1038/nature04286
   Tamosiunaite M, 2011, ROBOT AUTON SYST, V59, P910, DOI 10.1016/j.robot.2011.07.004
   Tenenbaum JB, 2011, SCIENCE, V331, P1279, DOI 10.1126/science.1192788
   Ude A, 2010, IEEE T ROBOT, V26, P800, DOI 10.1109/TRO.2010.2065430
   Wolpert DM, 2011, NAT REV NEUROSCI, V12, P739, DOI 10.1038/nrn3112
   Wolpert DM, 2010, CURR BIOL, V20, pR467, DOI 10.1016/j.cub.2010.04.035
   Zabkar J., 2013, P INT DAT AN IDA 201
NR 36
TC 0
Z9 0
U1 2
U2 8
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 2212-683X
EI 2212-6848
J9 BIOL INSPIR COGN ARC
JI Biol. Inspired Cogn. Archit.
PD APR
PY 2016
VL 16
BP 64
EP 74
DI 10.1016/j.bica.2016.03.003
PG 11
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA DN1OK
UT WOS:000376835100006
DA 2020-02-19
ER

PT J
AU Mishra, A
   Alahari, K
   Jawahar, CV
AF Mishra, Anand
   Alahari, Karteek
   Jawahar, C. V.
TI Enhancing energy minimization framework for scene text recognition with
   top-down cues
SO COMPUTER VISION AND IMAGE UNDERSTANDING
LA English
DT Article
DE Scene text understanding; Text recognition; Lexicon priors; Character
   recognition; Random field models
AB Recognizing scene text is a challenging problem, even more so than the recognition of scanned documents. This problem has gained significant attention from the computer vision community in recent years, and several methods based on energy minimization frameworks and deep learning approaches have been proposed. In this work, we focus on the energy minimization framework and propose a model that exploits both bottom-up and top-down cues for recognizing cropped words extracted from street images. The bottom-up cues are derived from individual character detections from an image. We build a conditional random field model on these detections to jointly model the strength of the detections and the interactions between them. These interactions are top-down cues obtained from a lexicon-based prior, i.e., language statistics. The optimal word represented by the text image is obtained by minimizing the energy function corresponding to the random field model. We evaluate our proposed algorithm extensively on a number of cropped scene text benchmark datasets, namely Street View Text, ICDAR 2003, 2011 and 2013 datasets, and IIIT 5K-word, and show better performance than comparable methods. We perform a rigorous analysis of all the steps in our approach and analyze the results. We also show that state-of-the-art convolutional neural network features can be integrated in our framework to further improve the recognition performance. (C) 2016 Elsevier Inc. All rights reserved.
C1 [Mishra, Anand; Jawahar, C. V.] IIIT Hyderabad, Ctr Visual Informat Technol, Hyderabad, Andhra Pradesh, India.
   [Alahari, Karteek] Univ Grenoble Alpes, THOTH Team, Inria Grenoble Rhone Alpes, Lab Jean Kuntzmann,CNRS, Grenoble, France.
RP Mishra, A (reprint author), IIIT Hyderabad, Ctr Visual Informat Technol, Hyderabad, Andhra Pradesh, India.
EM anand.mishra@research.iiit.ac.in
OI Alahari, Karteek/0000-0002-1838-5936
FU Ministry of Communications and Information Technology, Government of
   India, New Delhi; Microsoft Corporation and Microsoft Research India
   under the Microsoft Research India Ph.D fellowship award
FX We thank Jerod Weinman for providing the large lexicon. This work was
   partially supported by the Ministry of Communications and Information
   Technology, Government of India, New Delhi. Anand Mishra is supported by
   Microsoft Corporation and Microsoft Research India under the Microsoft
   Research India Ph.D fellowship award.
CR Almazan J, 2014, IEEE T PATTERN ANAL, V36, P2552, DOI 10.1109/TPAMI.2014.2339814
   [Anonymous], 2011, ICDAR 2003 DATASETS
   [Anonymous], 2011, STREET VIEW TEXT DAT
   Beaufort R., 2007, P 2007 INT C DOC AN
   Bianne-Bernard AL, 2011, IEEE T PATTERN ANAL, V33, P2066, DOI 10.1109/TPAMI.2011.22
   Bissacco A., 2013, P 2013 INT C COMP VI
   Brostow G., 2008, P 10 EUR C COMP VIS
   Chen D., 2002, P 16 INT C PATT REC
   CHEN X, 2004, P 2004 IEEE C COMP V
   Coates A., 2011, P 11 INT C DOC AN RE
   Dalal N., 2005, P 2005 IEEE COMP SOC
   de Campos T.E., 2009, P 4 INT C COMP VIS T
   Desai C., 2009, P 12 INT C COMP VIS
   Elagouni K., 2012, P INT C DOC AN SYST
   Elagouni K., 2011, P 1 ACM INT C MULT R
   Epshtein B., 2010, P 2010 IEEE C COMP V
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Feild J.L., 2013, P 12 INT C DOC AN RE
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Geiger A., 2012, P 2012 IEEE C COMP V
   Goel V., 2013, P 2013 INT C DOC AN
   Gomez L., 2014, COMP VIS ACCV 2014 W, P157
   Goodman J. T., 2001, TECHNICAL REPORT
   Gould S., 2009, P 23 ANN C NEUR INF
   Hong T., 1995, P 1995 INT C DOC AN
   Howe NR, 2009, PATTERN RECOGN, V42, P3338, DOI 10.1016/j.patcog.2009.01.012
   Huang W., 2014, P 13 EUR C COMP VIS
   Jaderberg M., 2014, CORRABS14125903
   Jaderberg M., 2014, P 2014 EUR C COMP VI
   Jaderberg Max, 2014, CORR
   Judd T., 2009, P 12 INT C COMP VIS
   Karatzas D., 2013, P INT C DOC AN REC I
   Karatzas D., 2011, P 11 INT C DOC AN RE
   KATZ SM, 1987, IEEE T ACOUST SPEECH, V35, P400, DOI 10.1109/TASSP.1987.1165125
   Kolmogorov V, 2006, IEEE T PATTERN ANAL, V28, P1568, DOI 10.1109/TPAMI.2006.200
   KORNAI A, 1994, AISB Q, V88, P36
   Kumar D., 2012, P 2012 INT C SIGN PR
   Kumar D., 2013, P INT SOC OPT ENG DO
   Kumar Deepak, 2012, P 8 IND C VIS GRAPH
   Ladicky L., 2010, P 11 EUR C COMP VIS
   Levin A, 2009, INT J COMPUT VISION, V81, P105, DOI 10.1007/s11263-008-0166-0
   Matas J, 2004, IMAGE VISION COMPUT, V22, P761, DOI 10.1016/j.imavis.2004.02.006
   Milyaev S., 2013, PROCEEDINGS OF THE 2
   Mishra A., 2012, P 2012 IEEE C COMP V
   Mishra A., 2013, P 2013 INT C COMP VI
   Mishra A., 2011, P 2011 INT C DOC AN
   Mishra A, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.127
   Mozer M., 1997, P 1997 ANN C NEUR IN
   Nagy G, 2000, IEEE T PATTERN ANAL, V22, P38, DOI 10.1109/34.824820
   Neumann L., 2010, P 10 AS C COMP VIS
   Neumann L., 2012, P 2012 IEEE C COMP V
   Neumann L., 2013, P 2013 INT C DOC AN
   Neumann L., 2012, P 12 EUR C COMP VIS
   Novikova T., 2012, P 2012 EUR C COMP VI
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Pearl J, 1988, PROBABILISTIC REASON
   Pereira F. C, 2001, P 18 INT C MACH LEAR
   PLATT J, 1999, ADV LARGE MARGIN CLA
   Rayner K., 1989, PSYCHOL READING
   RISEMAN EM, 1974, IEEE T COMPUT, VC 23, P480, DOI 10.1109/T-C.1974.223971
   Rodriguez J., 2013, P INT C BRIT MACH VI
   Roy U., 2014, P 12 AS C COMP VIS
   Russell C., 2010, P 26 C UNC ART INT U
   Sheshadri K, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.13
   Shi C., 2013, P 2013 IEEE C COMP V
   Shivakumara P., 2011, P 2011 INT C DOC AN
   Shotton J, 2009, INT J COMPUT VISION, V81, P2, DOI 10.1007/s11263-007-0109-1
   Simard P., 1991, P 1991 ANN C NEUR IN
   Smith R., 2011, P INT C DOC AN REC I
   Thillou C, 2005, EURASIP J APPL SIG P, V2005, P2127, DOI 10.1155/ASP.2005.2127
   Tompson J.J., 2014, P 28 ANN C NEUR INF
   TONG X, 1996, P 4 WORKSH VER LARG
   Toshev A., 2014, P IEEE C COMP VIS PA
   Vedaldi A, 2012, IEEE T PATTERN ANAL, V34, P480, DOI 10.1109/TPAMI.2011.153
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Wang K., 2011, P 2011 C COMP VIS
   Wang K., 2010, P 11 EUR C COMP VIS
   Wang T., 2012, P 2012 INT C PATT RE
   Weinman J.J., 2008, P 2008 INT C PATT RE
   Weinman JJ, 2014, IEEE T PATTERN ANAL, V36, P375, DOI 10.1109/TPAMI.2013.126
   Weinman JJ, 2009, IEEE T PATTERN ANAL, V31, P1733, DOI 10.1109/TPAMI.2009.38
   Yao C., 2014, P 2014 IEEE C COMP V
   Yao J., 2012, P 2012 IEEE C COMP V
   Ye Q., 2014, IEEE T PATTERN ANAL, V1, P1
NR 84
TC 5
Z9 6
U1 2
U2 15
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1077-3142
EI 1090-235X
J9 COMPUT VIS IMAGE UND
JI Comput. Vis. Image Underst.
PD APR
PY 2016
VL 145
BP 30
EP 42
DI 10.1016/j.cviu.2016.01.002
PG 13
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
SC Computer Science; Engineering
GA DG9BQ
UT WOS:000372378200003
DA 2020-02-19
ER

PT J
AU Zhang, SH
   Yang, H
   Yin, ZP
AF Zhang, Shaohua
   Yang, Hua
   Yin, Zhou-Ping
TI Transferred Deep Convolutional Neural Network Features for Extensive
   Facial Landmark Localization
SO IEEE SIGNAL PROCESSING LETTERS
LA English
DT Article
DE Convolutional neural network; deep learning; face alignment; facial
   landmark localization; transfer learning
ID FACE ALIGNMENT; MODELS
AB Features are crucial for extensive facial landmark localization (EFLL), while deep convolutional neural network (DCNN) features lead to breakthroughs in diverse visual recognition tasks. However, there is little study of DCNN features for EFLL, mainly because less labeled data with extensive facial landmarks are available. In this letter, we employ transfer learning to overcome this limitation, and utilize DCNN for EFLL. We concentrate the power of DCNN on feature learning within a cascaded-regression framework (CRF). We present three transfer methods, which show the capacity of DCNN as a generic feature extractor, and the benefit of fine-tuning. The proposed specific fine-tuning method for cascaded regression, named cascade transfer, achieves competitive accuracy with state-of-the-artmethods on the 300-W challenge dataset.
C1 [Zhang, Shaohua; Yang, Hua; Yin, Zhou-Ping] Huazhong Univ Sci & Technol, State Key Lab Digital Mfg Equipment & Technol, Wuhan 430074, Peoples R China.
RP Zhang, SH; Yang, H; Yin, ZP (reprint author), Huazhong Univ Sci & Technol, State Key Lab Digital Mfg Equipment & Technol, Wuhan 430074, Peoples R China.
EM chshnf@163.com; huayang@hust.edu.cn; yinzhp@mail.hust.edu.cn
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [40930532, 51105155]
FX This work was supported by the National Natural Science Foundation of
   China under Grants 40930532 and 51105155. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Philippe Coussy.
CR Belhumeur PN, 2011, PROC CVPR IEEE, P545, DOI 10.1109/CVPR.2011.5995602
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Burgos-Artizzu XP, 2013, IEEE I CONF COMP VIS, P1513, DOI 10.1109/ICCV.2013.191
   Cao C, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2462012
   Cao XD, 2014, INT J COMPUT VISION, V107, P177, DOI 10.1007/s11263-013-0667-3
   Chen D, 2013, PROC CVPR IEEE, P3025, DOI 10.1109/CVPR.2013.389
   COOTES TF, 1995, COMPUT VIS IMAGE UND, V61, P38, DOI 10.1006/cviu.1995.1004
   Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467
   Cristinacce D, 2008, PATTERN RECOGN, V41, P3054, DOI 10.1016/j.patcog.2008.01.024
   Dalal N, 2005, PROC CVPR IEEE, P886
   Deng J., IMAGENET LARGE SCALE
   Dollar P, 2010, PROC CVPR IEEE, P1078, DOI 10.1109/CVPR.2010.5540094
   Donahue J., 2014, P INT C MACH LEARN, P647
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Feng ZH, 2015, IEEE SIGNAL PROC LET, V22, P76, DOI 10.1109/LSP.2014.2347011
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Glorot X., 2010, JLMR P TRACK, p[249, 2010], DOI DOI 10.1177/1753193409103364.
   Jia Y, 2014, CAFFE CONVOLUTIONAL
   Kazemi V, 2014, PROC CVPR IEEE, P1867, DOI 10.1109/CVPR.2014.241
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Le V, 2012, LECT NOTES COMPUT SC, V7574, P679, DOI 10.1007/978-3-642-33712-3_49
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Matthews I, 2004, INT J COMPUT VISION, V60, P135, DOI 10.1023/B:VISI.0000029666.37597.d3
   Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Pantic M, 2000, IEEE T PATTERN ANAL, V22, P1424, DOI 10.1109/34.895976
   Pfister T., 2015, THESIS U OXFORD OXFO
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Ren SQ, 2014, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR.2014.218
   Sagonas C, 2013, IEEE COMPUT SOC CONF, P896, DOI 10.1109/CVPRW.2013.132
   Smith BM, 2014, PROC CVPR IEEE, P1741, DOI 10.1109/CVPR.2014.225
   Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244
   Sun Y, 2013, PROC CVPR IEEE, P3476, DOI 10.1109/CVPR.2013.446
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Tzimiropoulos G, 2014, PROC CVPR IEEE, P1851, DOI 10.1109/CVPR.2014.239
   Xiong XH, 2013, PROC CVPR IEEE, P532, DOI 10.1109/CVPR.2013.75
   Yan JJ, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P392, DOI 10.1109/ICCVW.2013.126
   Yi D., 2014, ARXIV14117923
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang J., 2014, INT J DISTRIB SENS N, V2014, P1, DOI DOI 10.1371/J0URNAL.P0NE.0110734
   Zhang S., 2015, J ELECT IMAG, V24
   Zhao XW, 2014, PROC CVPR IEEE, P1765, DOI 10.1109/CVPR.2014.228
   Zhou EJ, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P386, DOI 10.1109/ICCVW.2013.58
   Zhu XX, 2012, PROC CVPR IEEE, P2879, DOI 10.1109/CVPR.2012.6248014
NR 45
TC 9
Z9 9
U1 2
U2 24
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1070-9908
EI 1558-2361
J9 IEEE SIGNAL PROC LET
JI IEEE Signal Process. Lett.
PD APR
PY 2016
VL 23
IS 4
BP 478
EP 482
DI 10.1109/LSP.2016.2533721
PG 5
WC Engineering, Electrical & Electronic
SC Engineering
GA DH2VV
UT WOS:000372646100005
DA 2020-02-19
ER

PT J
AU Sanchez-Riera, J
   Hua, KL
   Hsiao, YS
   Lim, T
   Hidayati, SC
   Cheng, WH
AF Sanchez-Riera, Jordi
   Hua, Kai-Lung
   Hsiao, Yuan-Sheng
   Lim, Tekoing
   Hidayati, Shintami C.
   Cheng, Wen-Huang
TI A comparative study of data fusion for RGB-D based visual recognition
SO PATTERN RECOGNITION LETTERS
LA English
DT Article
DE RGB-D; Fusion; CNN; DBN; SAE; SVM
AB Data fusion from different modalities has been extensively studied for a better understanding of multimedia contents. On one hand, the emergence of new devices and decreasing storage costs cause growing amounts of data being collected. Though bigger data makes it easier to mine information, methods for big data analytics are not well investigated. On the other hand, new machine learning techniques, such as deep learning, have been shown to be one of the key elements in achieving state-of-the-art inference performances in a variety of applications. Therefore, some of the old questions in data fusion are in need to be addressed again for these new changes. These questions are: What is the most effective way to combine data for various modalities? Does the fusion method affect the performance with different classifiers? To answer these questions, in this paper, we present a comparative study for evaluating early and late fusion schemes with several types of SVM and deep learning classifiers on two challenging RGB-D based visual recognition tasks: hand gesture recognition and generic object recognition. The findings from this study provide useful policy and practical guidance for the development of visual recognition systems. (C) 2015 Elsevier B.V. All rights reserved.
C1 Acad Sinica, MCLab, Taipei 115, Taiwan.
   [Sanchez-Riera, Jordi; Hsiao, Yuan-Sheng; Lim, Tekoing; Hidayati, Shintami C.; Cheng, Wen-Huang] Acad Sinica, MCLab, Res Ctr Informat Technol Innovat CITI, Taipei 115, Taiwan.
   [Hua, Kai-Lung; Hsiao, Yuan-Sheng; Hidayati, Shintami C.] Natl Taiwan Univ Sci & Technol, CSIE, Taipei 106, Taiwan.
RP Cheng, WH (reprint author), Acad Sinica, MCLab, Res Ctr Informat Technol Innovat CITI, Taipei 115, Taiwan.
EM whcheng@citi.sinica.edu.tw
FU Ministry of Science and Technology of TaiwanMinistry of Science and
   Technology, Taiwan [MOST-103-2221-E-001-007-MY2,
   MOST-103-2221-E-011-105]
FX This work was supported by the Ministry of Science and Technology of
   Taiwan under Grants MOST-103-2221-E-001-007-MY2 and
   MOST-103-2221-E-011-105.
CR Atrey PK, 2010, MULTIMEDIA SYST, V16, P345, DOI 10.1007/s00530-010-0182-0
   Bo L., 2011, P IEEE C COMP VIS PA
   Bo L., 2013, P S EXPT ROB
   BROWN M, 2003, [No title captured]
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chen LM, 2013, IEEE ICC
   Fanelli G., 2009, P BRIT MACH VIS C
   Girshick R, 2014, P IEEE C COMP VIS PA
   Gupta S., 2014, P IEEE EUR C COMP VI
   Hoft Nico, 2014, KI 2014: Advances in Artificial Intelligence. 37th Annual German Conference on AI. Proceedings: LNCS 8736, P80, DOI 10.1007/978-3-319-11206-0_9
   Hsiao Y.-S., 2014, P ACM MUTL SYST C
   Huang J.-T., 2013, P INT C AC SPEECH SI
   Janoch A, 2011, 2011 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCV WORKSHOPS)
   Jiang H., 2013, P IEEE INT C ONCOMPU
   Kim KI, 2002, IEEE T PATTERN ANAL, V24, P1542, DOI 10.1109/TPAMI.2002.1046177
   Kittler J, 1998, IEEE T PATTERN ANAL, V20, P226, DOI 10.1109/34.667881
   Lai K., 2011, P IEEE INT C ROB AUT
   Lecun Yann, 1998, P IEEE
   Liu L., 2013, P INT JOINT C ART IN
   Makinen E, 2008, IEEE T PATTERN ANAL, V30, P541, DOI 10.1109/TPAMI.2007.70800
   Ming Y., 2012, P IEEE INT C MULT EX
   Muller A.C., 2014, P IEEE INT C ROB AUT
   NEFIAN AV, 2002, P INT C AC SPEECH SI
   Ouyang W., 2013, P IEEE INT C COMP VI
   Palm R. B., 2012, THESIS TU DENMARK
   Saenko K., 2005, P IEEE INT C COMP VI
   Shahbandi SG, 2012, INT CONF SYST COMPU
   Snoek C.G.M., 2005, P ACM MULT C
   Sonnenburg S, 2010, J MACH LEARN RES, V11, P1799
   Spinello L., 2011, P INT C INT ROB SYST
   van de Sande KEA, 2010, IEEE T PATTERN ANAL, V32, P1582, DOI 10.1109/TPAMI.2009.154
NR 31
TC 18
Z9 20
U1 0
U2 40
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0167-8655
EI 1872-7344
J9 PATTERN RECOGN LETT
JI Pattern Recognit. Lett.
PD APR 1
PY 2016
VL 73
BP 1
EP 6
DI 10.1016/j.patrec.2015.12.006
PG 6
WC Computer Science, Artificial Intelligence
SC Computer Science
GA DH1XZ
UT WOS:000372579600001
DA 2020-02-19
ER

PT J
AU Palangi, H
   Deng, L
   Shen, YL
   Gao, JF
   He, XD
   Chen, JS
   Song, XY
   Ward, R
AF Palangi, Hamid
   Deng, Li
   Shen, Yelong
   Gao, Jianfeng
   He, Xiaodong
   Chen, Jianshu
   Song, Xinying
   Ward, Rabab
TI Deep Sentence Embedding Using Long Short-Term Memory Networks: Analysis
   and Application to Information Retrieval
SO IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING
LA English
DT Article
DE Deep learning; long short-term memory; sentence embedding
AB This paper develops a model that addresses sentence embedding, a hot topic in current natural language processing research, using recurrent neural networks (RNN) with Long Short-Term Memory (LSTM) cells. The proposed LSTM-RNN model sequentially takes each word in a sentence, extracts its information, and embeds it into a semantic vector. Due to its ability to capture long term memory, the LSTM-RNN accumulates increasingly richer information as it goes through the sentence, and when it reaches the last word, the hidden layer of the network provides a semantic representation of the whole sentence. In this paper, the LSTM-RNN is trained in a weakly supervised manner on user click-through data logged by a commercial web search engine. Visualization and analysis are performed to understand how the embedding process works. The model is found to automatically attenuate the unimportant words and detect the salient keywords in the sentence. Furthermore, these detected keywords are found to automatically activate different cells of the LSTM-RNN, where words belonging to a similar topic activate the same cell. As a semantic representation of the sentence, the embedding vector can be used in many different applications. These automatic keyword detection and topic allocation abilities enabled by the LSTM-RNN allow the network to perform document retrieval, a difficult language processing task, where the similarity between the query and documents can be measured by the distance between their corresponding sentence embedding vectors computed by the LSTM-RNN. On a web search task, the LSTM-RNN embedding is shown to significantly outperform several existing state of the art methods. We emphasize that the proposed model generates sentence embedding vectors that are specially useful for web document retrieval tasks. A comparison with a well known general sentence embedding method, the Paragraph Vector, is performed. The results show that the proposed method in this paper significantly outperforms Paragraph Vector method for web document retrieval task.
C1 [Palangi, Hamid; Ward, Rabab] Univ British Columbia, Dept Elect & Comp Engn, Vancouver, BC V6T 1Z4, Canada.
   [Deng, Li; Shen, Yelong; Gao, Jianfeng; He, Xiaodong; Chen, Jianshu; Song, Xinying] Microsoft Res, Redmond, WA 98052 USA.
RP Palangi, H; Ward, R (reprint author), Univ British Columbia, Dept Elect & Comp Engn, Vancouver, BC V6T 1Z4, Canada.; Deng, L; Shen, YL; Gao, JF; He, XD; Chen, JS; Song, XY (reprint author), Microsoft Res, Redmond, WA 98052 USA.
EM hamidp@ece.ubc.ca; deng@microsoft.com; yeshen@microsoft.com;
   jfgao@microsoft.com; xiaohe@microsoft.com; jian-shuc@microsoft.com;
   xinson@microsoft.com; rababw@ece.ubc.ca
CR Bahdanau Dzmitry, 2015, P ICLR 15
   Bengio Y, 2013, INT CONF ACOUST SPEE, P8624, DOI 10.1109/ICASSP.2013.6639349
   Chen J., 2014, P INT C LEARN REPR I
   Chung J., 2014, NIPS DEEP LEARN WORK
   Collobert R., 2008, P INT C MACH LEARN I
   Dahl GE, 2012, IEEE T AUDIO SPEECH, V20, P30, DOI 10.1109/TASL.2011.2134090
   Dahl GE, 2011, INT CONF ACOUST SPEE, P4688
   DENG L, 1994, NEURAL NETWORKS, V7, P331, DOI 10.1016/0893-6080(94)90027-2
   DENG L, 2012, P ICASSP, P2133
   ELMAN JL, 1990, COGNITIVE SCI, V14, P179, DOI 10.1016/0364-0213(90)90002-E
   Gao J., 2014, P EMNLP
   Gao J., 2011, P 34 INT ACM SIGIR C, P675, DOI DOI 10.1145/2009916.2010007
   Gao JF, 2009, PROCEEDINGS 32ND ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P355, DOI 10.1145/1571941.1572003
   Gao Jianfeng, 2014, P ACL
   Gers FA, 2000, NEURAL COMPUT, V12, P2451, DOI 10.1162/089976600300015015
   Gers FA, 2003, J MACH LEARN RES, V3, P115, DOI 10.1162/153244303768966139
   Graves A., 2012, P REPR LEARN WORKSH
   Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947
   Hermann Karl Moritz, 2014, ARXIV14044641
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Hofmann T, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P289
   Hu B., 2014, ADV NEURAL INF PROCE, V27, P2042
   Huang P.-S., 2013, P 22 ACM INT C INF K, V13, P2333, DOI [DOI 10.1145/2505515.2505665, 10.1145/2505515.2505665]
   Jarvelin K., 2000, SIGIR Forum, V34, P41
   Kalchbrenner N., 2014, P 52 ANN M ASS COMP
   Karpathy A., 2015, ARXIV150602078
   Kiros R., 2015, ADV NEURAL INF PROCE
   Le Q V, 2014, INT C MACH LEARN, P1188, DOI DOI 10.1145/2740908.2742760
   Li Deng, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P6844, DOI 10.1109/ICASSP.2014.6854926
   Mesnil G, 2013, P INTERSPEECH
   Mikolov T., 2013, P INT C LEARN REPR I
   Mikolov T, 2013, ADV NEURAL INFORM PR, V27, P3111, DOI DOI 10.1162/JMLR.2003.3.4-5.951
   MIKOLOV T., 2013, ARXIV13013781
   Mikolov T, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 1-2, P1045
   Nesterov Y., 1983, SOV MATH DOKL, V27, P372
   Pascanu R, 2013, J MACHINE LEARNING R, V28, P1310
   Rehurek R, 2010, P LREC 2010 WORKSH N, P45, DOI DOI 10.13140/2.1.2393.1847
   ROBINSON AJ, 1994, IEEE T NEURAL NETWOR, V5, P298, DOI 10.1109/72.279192
   Sak H., 2014, P ANN C INT SPEECH C
   Shen Y., 2014, P CIKM NOV
   Socher R., 2011, P C EMP METH NAT LAN, P151
   Sutskever I., 2014, ADV NEURAL INFORM PR, P3104, DOI DOI 10.1007/S10107-014-0839-0
   Sutskever I., 2013, P 30 INT C MACH LEAR, P1139
   Yu D, 2011, IEEE SIGNAL PROC MAG, V28, P145, DOI 10.1109/MSP.2010.939038
   ZHANG J, 2014, P 9 INT S LIN DRIV I, V1, P1, DOI DOI 10.1016/J.MATPR.2014.09.001
   Zhu  Y., 2015, ARXIV150606724
NR 47
TC 137
Z9 150
U1 30
U2 115
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 2329-9290
EI 2329-9304
J9 IEEE-ACM T AUDIO SPE
JI IEEE-ACM Trans. Audio Speech Lang.
PD APR
PY 2016
VL 24
IS 4
BP 694
EP 707
DI 10.1109/TASLP.2016.2520371
PG 14
WC Acoustics; Engineering, Electrical & Electronic
SC Acoustics; Engineering
GA DH0FR
UT WOS:000372458400008
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Zhang, KH
   Liu, QS
   Wu, Y
   Yang, MH
AF Zhang, Kaihua
   Liu, Qingshan
   Wu, Yi
   Yang, Ming-Hsuan
TI Robust Visual Tracking via Convolutional Networks Without Training
SO IEEE TRANSACTIONS ON IMAGE PROCESSING
LA English
DT Article
DE Visual tracking; convolutional networks; deep learning
ID OBJECT RECOGNITION
AB Deep networks have been successfully applied to visual tracking by learning a generic representation offline from numerous training images. However, the offline training is time-consuming and the learned generic representation may be less discriminative for tracking specific objects. In this paper, we present that, even without offline training with a large amount of auxiliary data, simple two-layer convolutional networks can be powerful enough to learn robust representations for visual tracking. In the first frame, we extract a set of normalized patches from the target region as fixed filters, which integrate a series of adaptive contextual filters surrounding the target to define a set of feature maps in the subsequent frames. These maps measure similarities between each filter and useful local intensity patterns across the target, thereby encoding its local structural information. Furthermore, all the maps together form a global representation, via which the inner geometric layout of the target is also preserved. A simple soft shrinkage method that suppresses noisy values below an adaptive threshold is employed to de-noise the global representation. Our convolutional networks have a lightweight structure and perform favorably against several state-of- the-art methods on the recent tracking benchmark data set with 50 challenging videos.
C1 [Zhang, Kaihua; Liu, Qingshan; Wu, Yi] Nanjing Univ Informat Sci & Technol, Jiangsu Key Lab Big Data Anal Technol, Nanjing 210044, Jiangsu, Peoples R China.
   [Yang, Ming-Hsuan] Univ Calif Merced, Sch Engn, Merced, CA 95344 USA.
RP Zhang, KH; Liu, QS; Wu, Y (reprint author), Nanjing Univ Informat Sci & Technol, Jiangsu Key Lab Big Data Anal Technol, Nanjing 210044, Jiangsu, Peoples R China.; Yang, MH (reprint author), Univ Calif Merced, Sch Engn, Merced, CA 95344 USA.
EM cskhzhang@nuist.edu.cn; qsliu@nuist.edu.cn; ywu@nuist.edu.cn;
   mhyang@ucmerced.edu
RI Yang, Ming-Hsuan/AAE-7350-2019; Yang, Ming-Hsuan/T-9533-2019; Zhang,
   Kaihua/E-1026-2013
OI Yang, Ming-Hsuan/0000-0003-4848-2304; Zhang, Kaihua/0000-0002-1613-3401
FU Natural Science Foundation of ChinaNational Natural Science Foundation
   of China [61402233, 41501377, 61532009, 61272223, 61370036]; National
   Science Foundation of Jiangsu Province [BK20151529]; Startup Foundation
   for Introducing Talent of Nanjing University of Information Science and
   Technology [S8113049001]; National Science Foundation CAREERNational
   Science Foundation (NSF) [1149783]; Information and Intelligent Systems
   Program [1152576]
FX The work of K. Zhang, Q. Liu, and Y. Wu was supported in part by the
   Natural Science Foundation of China under Grant 61402233, Grant
   41501377, Grant 61532009, Grant 61272223, and Grant 61370036, in part by
   the National Science Foundation of Jiangsu Province under Grant
   BK20151529, and in part by the Startup Foundation for Introducing Talent
   of Nanjing University of Information Science and Technology under Grant
   S8113049001. The work of M.-H. Yang was supported in part by the
   National Science Foundation CAREER under Grant 1149783 and in part by
   the Information and Intelligent Systems Program under Grant 1152576. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Dimitrios Tzovaras.
CR Amit Adam, 2006, IEEE C COMP VIS PATT, P798, DOI DOI 10.1109/CVPR.2006.256
   Avidan S, 2004, IEEE T PATTERN ANAL, V26, P1064, DOI 10.1109/TPAMI.2004.53
   Babenko B, 2011, IEEE T PATTERN ANAL, V33, P1619, DOI 10.1109/TPAMI.2010.226
   Baker S, 2004, INT J COMPUT VISION, V56, P221, DOI 10.1023/B:VISI.0000011205.11775.fd
   Bao CL, 2012, PROC CVPR IEEE, P1830, DOI 10.1109/CVPR.2012.6247881
   Ben-Yacoub S, 1999, P 2 INT C AUD VID BA, P31
   Boureau YL, 2010, PROC CVPR IEEE, P2559, DOI 10.1109/CVPR.2010.5539963
   Collins R, 2005, IEEE INT WORKSH PERF, P17
   Comaniciu D, 2003, IEEE T PATTERN ANAL, V25, P564, DOI 10.1109/TPAMI.2003.1195991
   Dinh TB, 2011, PROC CVPR IEEE, P1177, DOI 10.1109/CVPR.2011.5995733
   Donahue J., 2014, P INT C MACH LEARN, P647
   Elad M, 2010, P IEEE, V98, P972, DOI 10.1109/JPROC.2009.2037655
   Fan JL, 2010, IEEE T NEURAL NETWOR, V21, P1610, DOI 10.1109/TNN.2010.2066286
   Gao J, 2014, LECT NOTES COMPUT SC, V8691, P188, DOI 10.1007/978-3-319-10578-9_13
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Grabner H, 2008, LECT NOTES COMPUT SC, V5302, P234, DOI 10.1007/978-3-540-88682-2_19
   Grabner Helmut, 2006, P BMVC, V1, P6, DOI DOI 10.5244/C.20.6
   Hare S, 2011, IEEE I CONF COMP VIS, P263, DOI 10.1109/ICCV.2011.6126251
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Henriques JF, 2012, LECT NOTES COMPUT SC, V7575, P702, DOI 10.1007/978-3-642-33765-9_50
   HUBEL DH, 1959, J PHYSIOL-LONDON, V148, P574, DOI 10.1113/jphysiol.1959.sp006308
   Jia X, 2012, PROC CVPR IEEE, P1822, DOI 10.1109/CVPR.2012.6247880
   Kalal Z, 2010, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2010.5540231
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kwon J, 2011, IEEE I CONF COMP VIS, P1195, DOI 10.1109/ICCV.2011.6126369
   Kwon J, 2010, PROC CVPR IEEE, P1269, DOI 10.1109/CVPR.2010.5539821
   Kwon J, 2009, PROC CVPR IEEE, P1208, DOI 10.1109/CVPRW.2009.5206502
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li HX, 2015, LECT NOTES COMPUT SC, V9007, P194, DOI 10.1007/978-3-319-16814-2_13
   Li X, 2013, ACM T INTEL SYST TEC, V4, DOI 10.1145/2508037.2508039
   Liu BY, 2011, PROC CVPR IEEE, P1313, DOI 10.1109/CVPR.2011.5995730
   Mahadevan Vijay, 2012, P ADV NEUR INF PROC, P1664
   Matthews I, 2004, IEEE T PATTERN ANAL, V26, P810, DOI 10.1109/TPAMI.2004.16
   Mei X, 2011, IEEE T PATTERN ANAL, V33, P2259, DOI 10.1109/TPAMI.2011.66
   Oron S, 2012, PROC CVPR IEEE, P1940, DOI 10.1109/CVPR.2012.6247895
   Perez P, 2002, LECT NOTES COMPUT SC, V2350, P661
   Riesenhuber M, 1999, NAT NEUROSCI, V2, P1019
   Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7
   Serre T, 2007, IEEE T PATTERN ANAL, V29, P411, DOI 10.1109/TPAMI.2007.56
   Sevilla-Lara L, 2012, PROC CVPR IEEE, P1910, DOI 10.1109/CVPR.2012.6247891
   Song HH, 2014, ELECTRON LETT, V50, P1931, DOI 10.1049/el.2014.1911
   Wang L, 2015, IEEE T IMAGE PROCESS, V24, P1424, DOI 10.1109/TIP.2015.2403231
   Wang N., 2013, ADV NEURAL INFORM PR, P809
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Zhang KH, 2014, IEEE T PATTERN ANAL, V36, P2002, DOI 10.1109/TPAMI.2014.2315808
   Zhang KH, 2012, LECT NOTES COMPUT SC, V7574, P864, DOI 10.1007/978-3-642-33712-3_62
   Zhang TZ, 2012, PROC CVPR IEEE, P2042, DOI 10.1109/CVPR.2012.6247908
   Zhong W, 2012, PROC CVPR IEEE, P1838, DOI 10.1109/CVPR.2012.6247882
   Zhou XZ, 2014, IEEE IMAGE PROC, P843, DOI 10.1109/ICIP.2014.7025169
NR 50
TC 185
Z9 203
U1 8
U2 139
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1057-7149
EI 1941-0042
J9 IEEE T IMAGE PROCESS
JI IEEE Trans. Image Process.
PD APR
PY 2016
VL 25
IS 4
BP 1779
EP 1792
DI 10.1109/TIP.2016.2531283
PG 14
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
SC Computer Science; Engineering
GA DH2WN
UT WOS:000372648000003
PM 26890870
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Li, HX
   Li, Y
   Porikli, F
AF Li, Hanxi
   Li, Yi
   Porikli, Fatih
TI DeepTrack: Learning Discriminative Feature Representations Online for
   Robust Visual Tracking
SO IEEE TRANSACTIONS ON IMAGE PROCESSING
LA English
DT Article
DE Convolutional neural network; deep learning; visual tracking
ID OBJECT TRACKING
AB Deep neural networks, albeit their great success on feature learning in various computer vision tasks, are usually considered as impractical for online visual tracking, because they require very long training time and a large number of training samples. In this paper, we present an efficient and very robust tracking algorithm using a single convolutional neural network (CNN) for learning effective feature representations of the target object in a purely online manner. Our contributions are multifold. First, we introduce a novel truncated structural loss function that maintains as many training samples as possible and reduces the risk of tracking error accumulation. Second, we enhance the ordinary stochastic gradient descent approach in CNN training with a robust sample selection mechanism. The sampling mechanism randomly generates positive and negative samples from different temporal distributions, which are generated by taking the temporal relations and label noise into account. Finally, a lazy yet effective updating scheme is designed for CNN training. Equipped with this novel updating algorithm, the CNN model is robust to some long-existing difficulties in visual tracking, such as occlusion or incorrect detections, without loss of the effective adaption for significant appearance changes. In the experiment, our CNN tracker outperforms all compared state-of-the-art methods on two recently proposed benchmarks, which in total involve over 60 video sequences. The remarkable performance improvement over the existing trackers illustrates the superiority of the feature representations, which are learned purely online via the proposed deep learning framework.
C1 [Li, Hanxi] Jiangxi Normal Univ, Sch Comp & Informat Engn, Nanchang 330022, Jiangxi, Peoples R China.
   [Li, Hanxi; Li, Yi; Porikli, Fatih] Natl ICT Australia, Canberra Res Lab, Canberra, ACT 2601, Australia.
   [Li, Yi] Toyota Res Inst, Ann Arbor, MI 48105 USA.
   [Li, Yi; Porikli, Fatih] Australian Natl Univ, Res Sch Informat Sci & Engn, Acton, ACT 2600, Australia.
RP Li, Y (reprint author), Natl ICT Australia, Canberra Res Lab, Canberra, ACT 2601, Australia.; Li, Y (reprint author), Toyota Res Inst, Ann Arbor, MI 48105 USA.; Li, Y (reprint author), Australian Natl Univ, Res Sch Informat Sci & Engn, Acton, ACT 2600, Australia.
EM hanxi.li@nicta.com.au; liyi.umd@gmail.com; fatih.porikli@nicta.com.au
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61462042, 61503168]; Australian Research Council's
   Discovery Projects Funding SchemeAustralian Research Council
   [DP150104645]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61462042 and Grant 61503168, and in part
   by the Australian Research Council's Discovery Projects Funding Scheme
   under Project DP150104645. The associate editor coordinating the review
   of this manuscript and approving it for publication was Prof. Ling Shao.
CR Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   Amit Adam, 2006, IEEE C COMP VIS PATT, P798, DOI DOI 10.1109/CVPR.2006.256
   Arulampalam MS, 2002, IEEE T SIGNAL PROCES, V50, P174, DOI 10.1109/78.978374
   Babenko B, 2011, IEEE T PATTERN ANAL, V33, P1619, DOI 10.1109/TPAMI.2010.226
   Babenko B, 2009, PROC CVPR IEEE, P983, DOI 10.1109/CVPRW.2009.5206737
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Blaschko MB, 2008, LECT NOTES COMPUT SC, V5302, P2, DOI 10.1007/978-3-540-88682-2_2
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76
   Ciresan D, 2012, PROC CVPR IEEE, P3642, DOI 10.1109/CVPR.2012.6248110
   Collins RT, 2005, IEEE T PATTERN ANAL, V27, P1631, DOI 10.1109/TPAMI.2005.205
   Dalal N, 2005, PROC CVPR IEEE, P886
   Dinh TB, 2011, PROC CVPR IEEE, P1177, DOI 10.1109/CVPR.2011.5995733
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fan JL, 2010, IEEE T NEURAL NETWOR, V21, P1610, DOI 10.1109/TNN.2010.2066286
   Felsberg M, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P121, DOI 10.1109/ICCVW.2013.22
   Felzenszwalb Pedro, 2008, P IEEE C COMP VIS PA, V08, P1, DOI DOI 10.1109/CVPR.2008.4587597
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Gao J, 2014, LECT NOTES COMPUT SC, V8691, P188, DOI 10.1007/978-3-319-10578-9_13
   Girshick R. B., 2011, ADV NEURAL INFORM PR, P442
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Girshick Ross, 2015, FAST R CNN
   Hare S, 2011, IEEE I CONF COMP VIS, P263, DOI 10.1109/ICCV.2011.6126251
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI 10.1007/978-3-319-10578-9_23
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Jia X, 2012, PROC CVPR IEEE, P1822, DOI 10.1109/CVPR.2012.6247880
   Kalal Z, 2010, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2010.5540231
   Kavukcuoglu K., 2010, ADV NEURAL INFORM PR, V23, P1090
   Kristan M, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P98, DOI 10.1109/ICCVW.2013.20
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kwon J, 2010, PROC CVPR IEEE, P1269, DOI 10.1109/CVPR.2010.5539821
   Lawrence N. D., 2001, P 18 INT C MACH LEAR, V1, P306
   Li H, 2015, P BMVC, P1, DOI [10.5244/C.28.56, DOI 10.5244/C.28.56]
   Li HX, 2015, LECT NOTES COMPUT SC, V9007, P194, DOI 10.1007/978-3-319-16814-2_13
   Long PM, 2010, MACH LEARN, V78, P287, DOI 10.1007/s10994-009-5165-z
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Ma B., 2015, IEEE T CYBERNETICS, P1
   Ma B, 2015, IEEE T MULTIMEDIA, V17, P1818, DOI 10.1109/TMM.2015.2463221
   Matan O., 1992, NEURAL INFORM PROCES, V4, P488
   Natarajan N., 2013, ADV NEURAL INFORM PR, P1196
   Nawaz T, 2013, IEEE T IMAGE PROCESS, V22, P1354, DOI 10.1109/TIP.2012.2228497
   Perez P, 2002, LECT NOTES COMPUT SC, V2350, P661
   Ren S., 2015, FASTER R CNN REAL TI
   Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7
   Viola P, 2005, ADV NEURAL INFORM PR, P1417
   Vojir T., 2011, P COMP VIS WINT WORK, P91
   Wang N., 2013, ADV NEURAL INFORM PR, P809
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Xiao JJ, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P137, DOI 10.1109/ICCVW.2013.24
   Xing JL, 2013, IEEE I CONF COMP VIS, P665, DOI 10.1109/ICCV.2013.88
   Yang F, 2014, IEEE T IMAGE PROCESS, V23, P1639, DOI 10.1109/TIP.2014.2300823
   Zhang K., 2015, ROBUST VISUAL TRACKI
   Zhang N, 2014, LECT NOTES COMPUT SC, V8689, P834, DOI 10.1007/978-3-319-10590-1_54
   Zhong W, 2012, PROC CVPR IEEE, P1838, DOI 10.1109/CVPR.2012.6247882
NR 53
TC 105
Z9 123
U1 6
U2 94
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1057-7149
EI 1941-0042
J9 IEEE T IMAGE PROCESS
JI IEEE Trans. Image Process.
PD APR
PY 2016
VL 25
IS 4
BP 1834
EP 1848
DI 10.1109/TIP.2015.2510583
PG 15
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
SC Computer Science; Engineering
GA DH2WN
UT WOS:000372648000007
PM 28055867
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Yan, RM
   Shao, L
AF Yan, Ruomei
   Shao, Ling
TI Blind Image Blur Estimation via Deep Learning
SO IEEE TRANSACTIONS ON IMAGE PROCESSING
LA English
DT Article
DE Blur classification; blur parameter estimation; blind image deblurring;
   general regression neural network
ID DECONVOLUTION; EXPERTS
AB Image blur kernel estimation is critical to blind image deblurring. Most existing approaches exploit handcrafted blur features that are optimized for a certain uniform blur across the image, which is unrealistic in a real blind deconvolution setting, where the blur type is often unknown. To deal with this issue, we aim at identifying the blur type for each input image patch, and then estimating the kernel parameter in this paper. A learning-based method using a pre-trained deep neural network (DNN) and a general regression neural network (GRNN) is proposed to first classify the blur type and then estimate its parameters, taking advantages of both the classification ability of DNN and the regression ability of GRNN. To the best of our knowledge, this is the first time that pre-trained DNN and GRNN have been applied to the problem of blur analysis. First, our method identifies the blur type from a mixed input of image patches corrupted by various blurs with different parameters. To this aim, a supervised DNN is trained to project the input samples into a discriminative feature space, in which the blur type can be easily classified. Then, for each blur type, the proposed GRNN estimates the blur parameters with very high accuracy. Experiments demonstrate the effectiveness of the proposed method in several tasks with better or competitive results compared with the state of the art on two standard image data sets, i. e., the Berkeley segmentation data set and the Pascal VOC 2007 data set. In addition, blur region segmentation and deblurring on a number of real photographs show that our method outperforms the previous techniques even for non-uniformly blurred images.
C1 [Yan, Ruomei; Shao, Ling] Nanjing Univ Informat Sci & Technol, Coll Elect & Informat Engn, Nanjing 210044, Jiangsu, Peoples R China.
   [Yan, Ruomei] Univ Sheffield, Dept Elect & Elect Engn, Sheffield S1 3JD, S Yorkshire, England.
   [Shao, Ling] Northumbria Univ, Dept Comp Sci & Digital Technol, Newcastle Upon Tyne NE1 8ST, Tyne & Wear, England.
RP Shao, L (reprint author), Nanjing Univ Informat Sci & Technol, Coll Elect & Informat Engn, Nanjing 210044, Jiangsu, Peoples R China.; Shao, L (reprint author), Northumbria Univ, Dept Comp Sci & Digital Technol, Newcastle Upon Tyne NE1 8ST, Tyne & Wear, England.
EM rmyan2013@gmail.com; ling.shao@ieee.org
RI Shao, Ling/D-3535-2011
CR Almeida MSC, 2010, IEEE T IMAGE PROCESS, V19, P36, DOI 10.1109/TIP.2009.2031231
   Bao P, 2005, IEEE T PATTERN ANAL, V27, P1485, DOI 10.1109/TPAMI.2005.173
   Bengio Y., 2006, P C ADV NEUR INF PRO, V19, P1
   CANNON M, 1976, IEEE T ACOUST SPEECH, V24, P58, DOI 10.1109/TASSP.1976.1162770
   Chen F, 2009, IEEE T SIGNAL PROCES, V57, P2467, DOI 10.1109/TSP.2009.2018358
   Cho S, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618491
   Cho TS, 2011, PROC CVPR IEEE, P241, DOI 10.1109/CVPR.2011.5995479
   Ciancio A, 2011, IEEE T IMAGE PROCESS, V20, P64, DOI 10.1109/TIP.2010.2053549
   Duan S., 2005, P INT C MULT CLASS S
   Erhan D., 2009, J MACHINE LEARNING R, V5, P153
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fergus R, 2006, ACM T GRAPHIC, V25, P787, DOI 10.1145/1141911.1141956
   Gu K, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2439035
   Gu K, 2015, IEEE T MULTIMEDIA, V17, P50, DOI 10.1109/TMM.2014.2373812
   Gunn S. R., 1998, SUPPORT VECTOR MACHI
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hu W, 2012, IEEE T IMAGE PROCESS, V21, P386, DOI 10.1109/TIP.2011.2160073
   Jain V., 2008, P ADV NEUR INF PROC, P769
   Joshi N., 2008, IEEE C COMP VIS PATT, P1, DOI DOI 10.1109/CVPR.2008.4587834
   Krishnan D, 2011, PROC CVPR IEEE, P233, DOI 10.1109/CVPR.2011.5995521
   Krishnan D, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531402
   Kundur D, 1996, IEEE SIGNAL PROC MAG, V13, P43, DOI 10.1109/79.489268
   Lagendijk R, 2000, BASIC METHODS IMAGE
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Lee C., 2014, DEEPLYSUPERVISED NET
   Levin A., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2657, DOI 10.1109/CVPR.2011.5995308
   Levin A, 2009, PROC CVPR IEEE, P1964, DOI 10.1109/CVPRW.2009.5206815
   Li Q, 2009, ENERG CONVERS MANAGE, V50, P90, DOI 10.1016/j.enconman.2008.08.033
   Liu AM, 2012, IEEE T IMAGE PROCESS, V21, P1500, DOI 10.1109/TIP.2011.2175935
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   McIlhagga W, 2011, INT J COMPUT VISION, V91, P251, DOI 10.1007/s11263-010-0392-0
   Mitchell T., 1997, MACHINE LEARNING
   Molina R, 2006, IEEE T IMAGE PROCESS, V15, P3715, DOI 10.1109/TIP.2006.881972
   Palm R., 2012, THESIS TU DENMARK KO
   Renting L., 2008, IEEE C COMP VIS PATT, P1, DOI DOI 10.1109/CVPR.2008.4587465
   Roth S, 2005, PROC CVPR IEEE, P860
   Rugna J., 2003, P SPIE INTERNET IMAG, V5304
   Shan Q, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360672
   SPECHT DF, 1991, IEEE T NEURAL NETWOR, V2, P568, DOI 10.1109/72.97934
   Su B, 2011, P 19 ACM INT C MULT, P1397
   Sun LH, 2013, PROCEEDINGS OF THE 2013 INTERNATIONAL WORKSHOP ON COMPUTER SCIENCE IN SPORTS, P1
   Tomandl D, 2001, NEURAL NETWORKS, V14, P1023, DOI 10.1016/S0893-6080(01)00051-X
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P684, DOI 10.1109/TIP.2013.2293423
   Yan RM, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.70
   Zhong S., 2011, P 19 ACM INT C MULT, P343
   Zoran D, 2011, IEEE I CONF COMP VIS, P479, DOI 10.1109/ICCV.2011.6126278
NR 46
TC 33
Z9 39
U1 3
U2 48
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1057-7149
EI 1941-0042
J9 IEEE T IMAGE PROCESS
JI IEEE Trans. Image Process.
PD APR
PY 2016
VL 25
IS 4
BP 1910
EP 1921
DI 10.1109/TIP.2016.2535273
PG 12
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
SC Computer Science; Engineering
GA DH2WN
UT WOS:000372648000013
PM 26930680
DA 2020-02-19
ER

PT J
AU Guo, YR
   Gao, YZ
   Shen, DG
AF Guo, Yanrong
   Gao, Yaozong
   Shen, Dinggang
TI Deformable MR Prostate Segmentation via Deep Feature Learning and Sparse
   Patch Matching
SO IEEE TRANSACTIONS ON MEDICAL IMAGING
LA English
DT Article
DE Deformable model; MR prostate segmentation; sparse patch matching;
   stacked sparse auto-encoder (SSAE)
ID ACTIVE SHAPE MODELS; IMAGE SEGMENTATION; LABEL FUSION; REPRESENTATION;
   ALGORITHM; DECISION; SEARCH; TRUTH
AB Automatic and reliable segmentation of the prostate is an important but difficult task for various clinical applications such as prostate cancer radiotherapy. The main challenges for accurate MR prostate localization lie in two aspects: (1) inhomogeneous and inconsistent appearance around prostate boundary, and (2) the large shape variation across different patients. To tackle these two problems, we propose a new deformable MR prostate segmentation method by unifying deep feature learning with the sparse patch matching. First, instead of directly using handcrafted features, we propose to learn the latent feature representation from prostate MR images by the stacked sparse auto-encoder (SSAE). Since the deep learning algorithm learns the feature hierarchy from the data, the learned features are often more concise and effective than the handcrafted features in describing the underlying data. To improve the discriminability of learned features, we further refine the feature representation in a supervised fashion. Second, based on the learned features, a sparse patch matching method is proposed to infer a prostate likelihood map by transferring the prostate labels from multiple atlases to the new prostate MR image. Finally, a deformable segmentation is used to integrate a sparse shape model with the prostate likelihood map for achieving the final segmentation. The proposed method has been extensively evaluated on the dataset that contains 66 T2-wighted prostate MR images. Experimental results show that the deep-learned features are more effective than the handcrafted features in guiding MR prostate segmentation. Moreover, our method shows superior performance than other state-of-the-art segmentation methods. Index Terms-Deformable
C1 [Guo, Yanrong; Gao, Yaozong; Shen, Dinggang] Univ N Carolina, Dept Radiol, Chapel Hill, NC 27599 USA.
   [Guo, Yanrong; Gao, Yaozong; Shen, Dinggang] Univ N Carolina, BRIC, Chapel Hill, NC 27599 USA.
   [Shen, Dinggang] Korea Univ, Dept Brain & Cognit Engn, Seoul 02841, South Korea.
RP Shen, DG (reprint author), Univ N Carolina, Dept Radiol, Chapel Hill, NC 27599 USA.; Shen, DG (reprint author), Univ N Carolina, BRIC, Chapel Hill, NC 27599 USA.; Shen, DG (reprint author), Korea Univ, Dept Brain & Cognit Engn, Seoul 02841, South Korea.
EM yrguo@email.unc.edu; yzgao@cs.unc.edu; dgshen@med.unc.edu
FU NIHUnited States Department of Health & Human ServicesNational
   Institutes of Health (NIH) - USA [CA140413]
FX This work was supported in part by NIH grant CA140413. Y. Guo and Y. Gao
   are co-first authors. Asterisk indicates corresponding author.
CR Asman AJ, 2013, MED IMAGE ANAL, V17, P194, DOI 10.1016/j.media.2012.10.002
   Bengio Y, 2006, ADV NEURAL INFORM PR, P153
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bengio Yoshua, 2012, WORKSH UNS TRANSF LE
   Bulo SR, 2014, PROC CVPR IEEE, P81, DOI 10.1109/CVPR.2014.18
   Cao YH, 2012, 2012 11TH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA 2012), VOL 1, P311, DOI 10.1109/ICMLA.2012.232
   Cardoso MJ, 2013, MED IMAGE ANAL, V17, P671, DOI 10.1016/j.media.2013.02.006
   Carneiro G, 2012, IEEE T IMAGE PROCESS, V21, P968, DOI 10.1109/TIP.2011.2169273
   Chan TF, 2001, IEEE T IMAGE PROCESS, V10, P266, DOI 10.1109/83.902291
   Chandra SS, 2012, IEEE T MED IMAGING, V31, P1955, DOI 10.1109/TMI.2012.2211377
   COOTES TF, 1995, COMPUT VIS IMAGE UND, V61, P38, DOI 10.1006/cviu.1995.1004
   Dalal N, 2005, PROC CVPR IEEE, P886
   Farabet C., 2012, P 29 INT C MACH LEAR, P575
   Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231
   Gao YZ, 2012, MED PHYS, V39, P6372, DOI 10.1118/1.4754304
   Heimann T, 2014, MED IMAGE ANAL, V18, P1320, DOI 10.1016/j.media.2014.04.007
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hricak H, 2004, CANCER, V100, P2655, DOI 10.1002/cncr.20319
   Jin Y, 2015, I S BIOMED IMAGING, P140, DOI 10.1109/ISBI.2015.7163835
   Jin Y, 2015, HUM BRAIN MAPP, V36, P4880, DOI 10.1002/hbm.22957
   Jin Y, 2014, NEUROIMAGE, V100, P75, DOI 10.1016/j.neuroimage.2014.04.048
   Kim M, 2013, NEUROIMAGE, V83, P335, DOI 10.1016/j.neuroimage.2013.06.006
   Langerak TR, 2010, IEEE T MED IMAGING, V29, P2000, DOI 10.1109/TMI.2010.2057442
   Lee H, 2009, P 26 ANN INT C MACH
   Liao Shu, 2013, Inf Process Med Imaging, V23, P511, DOI 10.1007/978-3-642-38868-2_43
   Liao S, 2013, IEEE T MED IMAGING, V32, P419, DOI 10.1109/TMI.2012.2230018
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mairal J., 2009, ADV NEURAL INF PROCE
   MALLAT SG, 1989, IEEE T PATTERN ANAL, V11, P674, DOI 10.1109/34.192463
   Modat M, 2010, COMPUT METH PROG BIO, V98, P278, DOI 10.1016/j.cmpb.2009.09.002
   Nouranian S, 2013, LECT NOTES COMPUT SC, V8150, P173, DOI 10.1007/978-3-642-40763-5_22
   Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4
   Ou Y., 2012, PROMISE12
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Pondman KM, 2008, EUR UROL, V54, P517, DOI 10.1016/j.eururo.2008.06.001
   Sabuncu MR, 2010, IEEE T MED IMAGING, V29, P1714, DOI 10.1109/TMI.2010.2050897
   Sawada Y, 2015, 2015 14TH IAPR INTERNATIONAL CONFERENCE ON MACHINE VISION APPLICATIONS (MVA), P110, DOI 10.1109/MVA.2015.7153145
   Shin HC, 2013, IEEE T PATTERN ANAL, V35, P1930, DOI 10.1109/TPAMI.2012.277
   Sled JG, 1998, IEEE T MED IMAGING, V17, P87, DOI 10.1109/42.668698
   Somkantha K, 2011, IEEE T BIO-MED ENG, V58, P567, DOI 10.1109/TBME.2010.2091129
   Suk HI, 2013, LECT NOTES COMPUT SC, V8150, P583, DOI 10.1007/978-3-642-40763-5_72
   Toth R, 2012, IEEE T MED IMAGING, V31, P1638, DOI 10.1109/TMI.2012.2201498
   Toth R, 2011, ACAD RADIOL, V18, P745, DOI 10.1016/j.acra.2011.01.016
   van Opbroek A, 2015, IEEE T MED IMAGING, V34, P1018, DOI 10.1109/TMI.2014.2366792
   Vincent P, 2008, P 25 INT C MACH LEAR, P1096, DOI DOI 10.1145/1390156.1390294
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Viola P, 2001, PROC CVPR IEEE, P511
   Wang M, 2015, IEEE T CYBERNETICS, V45, P1561, DOI 10.1109/TCYB.2014.2356136
   Warfield SK, 2004, IEEE T MED IMAGING, V23, P903, DOI 10.1109/TMI.2004.828354
   Yan PK, 2015, IEEE T CYBERNETICS, V45, P1158, DOI 10.1109/TCYB.2014.2346394
   Yang MJ, 2013, IEEE T BIO-MED ENG, V60, P479, DOI 10.1109/TBME.2012.2228644
   Zhan YQ, 2008, LECT NOTES COMPUT SC, V5241, P313, DOI 10.1007/978-3-540-85988-8_38
   Zhang ST, 2012, MED IMAGE ANAL, V16, P1385, DOI 10.1016/j.media.2012.07.007
NR 54
TC 73
Z9 77
U1 4
U2 58
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 0278-0062
EI 1558-254X
J9 IEEE T MED IMAGING
JI IEEE Trans. Med. Imaging
PD APR
PY 2016
VL 35
IS 4
BP 1077
EP 1089
DI 10.1109/TMI.2015.2508280
PG 13
WC Computer Science, Interdisciplinary Applications; Engineering,
   Biomedical; Engineering, Electrical & Electronic; Imaging Science &
   Photographic Technology; Radiology, Nuclear Medicine & Medical Imaging
SC Computer Science; Engineering; Imaging Science & Photographic
   Technology; Radiology, Nuclear Medicine & Medical Imaging
GA DJ4HA
UT WOS:000374164800014
PM 26685226
OA Green Accepted
DA 2020-02-19
ER

PT J
AU Tang, JX
   Deng, CW
   Huang, GB
AF Tang, Jiexiong
   Deng, Chenwei
   Huang, Guang-Bin
TI Extreme Learning Machine for Multilayer Perceptron
SO IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
LA English
DT Article
DE Deep learning (DL); deep neural network (DNN); extreme learning machine
   (ELM); multilayer perceptron (MLP); random feature mapping
ID FEEDFORWARD NETWORKS; HIDDEN NODES; RECOGNITION; ALGORITHM;
   REPRESENTATION
AB Extreme learning machine (ELM) is an emerging learning algorithm for the generalized single hidden layer feedforward neural networks, of which the hidden node parameters are randomly generated and the output weights are analytically computed. However, due to its shallow architecture, feature learning using ELM may not be effective for natural signals (e.g., images/videos), even with a large number of hidden nodes. To address this issue, in this paper, a new ELM-based hierarchical learning framework is proposed for multilayer perceptron. The proposed architecture is divided into two main components: 1) self-taught feature extraction followed by supervised feature classification and 2) they are bridged by random initialized hidden weights. The novelties of this paper are as follows: 1) unsupervised multilayer encoding is conducted for feature extraction, and an ELM-based sparse autoencoder is developed via l1 constraint. By doing so, it achieves more compact and meaningful feature representations than the original ELM; 2) by exploiting the advantages of ELM random feature mapping, the hierarchically encoded outputs are randomly projected before final decision making, which leads to a better generalization with faster learning speed; and 3) unlike the greedy layerwise training of deep learning (DL), the hidden layers of the proposed framework are trained in a forward manner. Once the previous layer is established, the weights of the current layer are fixed without fine-tuning. Therefore, it has much better learning efficiency than the DL. Extensive experiments on various widely used classification data sets show that the proposed algorithm achieves better and faster convergence than the existing state-of-the-art hierarchical learning methods. Furthermore, multiple applications in computer vision further confirm the generality and capability of the proposed learning scheme.
C1 [Tang, Jiexiong; Deng, Chenwei] Beijing Inst Technol, Sch Informat & Elect, Beijing 100081, Peoples R China.
   [Huang, Guang-Bin] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
RP Deng, CW (reprint author), Beijing Inst Technol, Sch Informat & Elect, Beijing 100081, Peoples R China.
EM jiexiongtang@bit.edu.cn; cwdeng@bit.edu.cn; egbhuang@ntu.edu.sg
FU Excellent Young Scholars Research Fund of Beijing Institute of
   Technology [2013YR0508]; National Natural Science Foundation of
   ChinaNational Natural Science Foundation of China [61301090]
FX This work was supported in part by the Excellent Young Scholars Research
   Fund of Beijing Institute of Technology under Grant 2013YR0508 and in
   part by the National Natural Science Foundation of China under Grant
   61301090.
CR Agarwal S, 2004, IEEE T PATTERN ANAL, V26, P1475, DOI 10.1109/TPAMI.2004.108
   Agarwal S, 2002, LECT NOTES COMPUT SC, V2353, P113
   Beck A, 2009, IEEE T IMAGE PROCESS, V18, P2419, DOI 10.1109/TIP.2009.2028250
   Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Bishop CM, 2006, PATTERN RECOGNITION
   Fergus R, 2003, PROC CVPR IEEE, P264
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Huang G, 2014, IEEE T CYBERNETICS, V44, P2405, DOI 10.1109/TCYB.2014.2307349
   Huang GB, 2008, NEUROCOMPUTING, V71, P3460, DOI 10.1016/j.neucom.2007.10.008
   Huang GB, 2008, NEUROCOMPUTING, V71, P576, DOI 10.1016/j.neucom.2007.07.025
   Huang GB, 2007, NEUROCOMPUTING, V70, P3056, DOI 10.1016/j.neucom.2007.02.009
   Huang GB, 2006, IEEE T NEURAL NETWOR, V17, P879, DOI 10.1109/TNN.2006.875977
   Huang GB, 2014, COGN COMPUT, V6, P376, DOI 10.1007/s12559-014-9255-2
   Huang GB, 2012, IEEE T SYST MAN CY B, V42, P513, DOI 10.1109/TSMCB.2011.2168604
   Kasun LLC, 2013, IEEE INTELL SYST, V28, P31
   Kim T, 2007, P IEEE C COMP VIS PA, P1
   Kim TK, 2007, LECT NOTES COMPUT SC, V4843, P335
   LeCun Y, 2004, PROC CVPR IEEE, P97
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Leibe B., 2004, WORKSH STAT LEARN CO, V2, P7
   Liang NY, 2006, IEEE T NEURAL NETWOR, V17, P1411, DOI 10.1109/TNN.2006.880583
   Lui YM, 2012, IEEE T CIRC SYST VID, V22, P930, DOI 10.1109/TCSVT.2011.2181452
   Minhas R, 2010, NEUROCOMPUTING, V73, P1906, DOI 10.1016/j.neucom.2010.01.020
   Mohammed AA, 2011, PATTERN RECOGN, V44, P2588, DOI 10.1016/j.patcog.2011.03.013
   Pan C, 2012, NEURAL COMPUT APPL, V21, P1217, DOI 10.1007/s00521-011-0522-9
   Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7
   Salakhutdinov R, 2009, ARTIF INTELL, P448, DOI DOI 10.1109/CVPRW.2009.5206577
   Shotton J, 2005, IEEE I CONF COMP VIS, P503
   Vincent P, 2008, P 25 INT C MACH LEAR, P1096, DOI DOI 10.1145/1390156.1390294
   Zhang KH, 2012, LECT NOTES COMPUT SC, V7574, P864, DOI 10.1007/978-3-642-33712-3_62
NR 33
TC 404
Z9 437
U1 54
U2 143
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 2162-237X
EI 2162-2388
J9 IEEE T NEUR NET LEAR
JI IEEE Trans. Neural Netw. Learn. Syst.
PD APR
PY 2016
VL 27
IS 4
BP 809
EP 821
DI 10.1109/TNNLS.2015.2424995
PG 13
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Computer Science, Theory & Methods; Engineering,
   Electrical & Electronic
SC Computer Science; Engineering
GA DH4XT
UT WOS:000372789600009
PM 25966483
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Maria, J
   Amaro, J
   Falcao, G
   Alexandre, LA
AF Maria, Joao
   Amaro, Joao
   Falcao, Gabriel
   Alexandre, Luis A.
TI Stacked Autoencoders Using Low-Power Accelerated Architectures for
   Object Recognition in Autonomous Systems
SO NEURAL PROCESSING LETTERS
LA English
DT Article
DE Deep learning; Neural networks; Stacked autoencoder; Parallel computing;
   FPGAs; Mobile GPUs; OpenCL; Low-power; Autonomous systems
AB This paper investigates low-energy consumption and low-power hardware models and processor architectures for performing the real-time recognition of objects in power-constrained autonomous systems and robots. Most recent developments show that convolutional deep neural networks are currently the state-of-the-art in terms of classification accuracy. In this article we propose to use of a different type of deep neural network-stacked autoencoders-and show that within a limited number of layers and nodes, for accommodating the use of low-power accelerators such as mobile GPUs and FPGAs, we are still able to achieve both classification levels not far from the state-of-the-art and a high number of processed frames per second. We present experiments using the color CIFAR-10 dataset. This enables the adaptation of the architecture to a live feed camera. Another novelty equally proposed for the first time in this work suggests that the training phase can also be performed in these low-power devices, instead of the usual approach that uses a desktop CPU or a GPU to perform this task and only runs the trained network later on the FPGA. This allows incorporating new functionalities as, for example, a robot performing runtime learning.
C1 [Maria, Joao; Amaro, Joao; Falcao, Gabriel] Univ Coimbra, Inst Telecomunicacoes, Dept Elect & Comp Engn, P-3030290 Coimbra, Portugal.
   [Alexandre, Luis A.] Univ Beira Interior, Dept Informat, P-6201001 Covilha, Portugal.
   [Alexandre, Luis A.] Univ Beira Interior, Inst Telecomunicacoes, P-6201001 Covilha, Portugal.
RP Falcao, G (reprint author), Univ Coimbra, Inst Telecomunicacoes, Dept Elect & Comp Engn, P-3030290 Coimbra, Portugal.
EM jmaria@co.it.pt; jamaro@co.it.pt; gff@co.it.pt; luis.alexandre@ubi.pt
RI Falcao, Gabriel/P-9142-2014; Alexandre, Luis/E-8770-2013
OI Falcao, Gabriel/0000-0001-9805-6747; Alexandre, Luis/0000-0002-5133-5025
FU Instituto de Telecomunicacoes; Fundacao para a Ciencia e a Tecnologia
   (FCT)Portuguese Foundation for Science and Technology
   [UID/EEA/50008/2013]
FX This work has been supported by Instituto de Telecomunicacoes and
   Fundacao para a Ciencia e a Tecnologia (FCT) under grant
   UID/EEA/50008/2013.
CR [Anonymous], 2013, SNAPDR 800
   [Anonymous], 2013, PUBL LEAD CIFAR 10 O
   Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Ciresan D, 2012, PROC CVPR IEEE, P3642, DOI 10.1109/CVPR.2012.6248110
   Dundar A., 2013, NEUR INF PROC SYST C
   Falcao G, 2012, IEEE SIGNAL PROC MAG, V29, P81, DOI 10.1109/MSP.2012.2192212
   Farabet C, 2010, IEEE INT SYMP CIRC S, P257, DOI 10.1109/ISCAS.2010.5537908
   Gong Y., 2013, CORR
   Goodfellow Ian J., 2013, CORR
   Group K, 2012, OPENCL SPEC VERS 1 2
   Hardavellas N, 2011, USENIX, V37, P7
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Krizhevsky A, 2009, LEARNING MULTIPLE LA
   Krizhevsky A., CIFAR 10 DATASET
   Krizhevsky A., 2012, ADV NEURAL INFORM PR
   Nallantech, 2012, PCIE 385N ALT STRAT
   Oh KS, 2004, PATTERN RECOGN, V37, P1311, DOI 10.1016/j.patcog.2004.01.013
   Owaida M, 2015, ACM T EMBED COMPUT S, V14, DOI 10.1145/2656207
   Wang G., 2013, IEEE INT C AC SPEECH
NR 20
TC 13
Z9 13
U1 1
U2 43
PU SPRINGER
PI DORDRECHT
PA VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN 1370-4621
EI 1573-773X
J9 NEURAL PROCESS LETT
JI Neural Process. Lett.
PD APR
PY 2016
VL 43
IS 2
SI SI
BP 445
EP 458
DI 10.1007/s11063-015-9430-9
PG 14
WC Computer Science, Artificial Intelligence
SC Computer Science
GA DF8WO
UT WOS:000371641500009
DA 2020-02-19
ER

PT J
AU Abdel-Zaher, AM
   Eldeib, AM
AF Abdel-Zaher, Ahmed M.
   Eldeib, Ayman M.
TI Breast cancer classification using deep belief networks
SO EXPERT SYSTEMS WITH APPLICATIONS
LA English
DT Article
DE Breast cancer diagnosis; CAD; Classification; Deep learning based
   classifier; Pattern recognition
ID RULES
AB Over the last decade, the ever increasing world-wide demand for early detection of breast cancer at many screening sites and hospitals has resulted in the need of new research avenues. According to the World Health Organization (WHO), an early detection of cancer greatly increases the chances of taking the right decision on a successful treatment plan. The Computer-Aided Diagnosis (CAD) systems are applied widely in the detection and differential diagnosis of many different kinds of abnormalities. Therefore, improving the accuracy of a CAD system has become one of the major research areas. In this paper, a CAD scheme for detection of breast cancer has been developed using deep belief network unsupervised path followed by back propagation supervised path. The construction is back-propagation neural network with Liebenberg Marquardt learning function while weights are initialized from the deep belief network path (DBN-NN). Our technique was tested on the Wisconsin Breast Cancer Dataset (WBCD). The classifier complex gives an accuracy of 99.68% indicating promising results over previously-published studies. The proposed system provides an effective classification model for breast cancer. In addition, we examined the architecture at several train-test partitions. (C) 2015 Elsevier Ltd. All rights reserved.
C1 [Abdel-Zaher, Ahmed M.; Eldeib, Ayman M.] Cairo Univ, Dept Syst & Biomed Engn, Giza, Egypt.
RP Abdel-Zaher, AM (reprint author), Cairo Univ, Dept Syst & Biomed Engn, Giza, Egypt.
EM ahmedallah.m.s@gmail.com; eldeib@ieee.org
CR Abonyi J, 2003, PATTERN RECOGN LETT, V24, P2195, DOI 10.1016/S0167-8655(03)00047-3
   Akay MF, 2009, EXPERT SYST APPL, V36, P3240, DOI 10.1016/j.eswa.2008.01.009
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Boser B. E., 1992, Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, P144, DOI 10.1145/130385.130401
   Decoste D, 2002, MACH LEARN, V46, P161, DOI 10.1023/A:1012454411458
   Dheeba J, 2014, J BIOMED INFORM, V49, P45, DOI 10.1016/j.jbi.2014.01.010
   Erhan D, 2010, J MACH LEARN RES, V11, P625
   Goodman D. E., 2002, P ART NEUR NETW ENG, V12, P179
   Hamilton H. J., 1996, RIAC RULE INDUCTION
   Hinton G. E., 2009, DEEP BELIEF NETWORKS
   Hinton G.E., 2009, DEEP BELIEF NETS
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   LeCun Y., 2001, INTELLIGENT SIGNAL P, P306
   Mert A, 2015, COMPUT MATH METHOD M, DOI 10.1155/2015/265138
   Nahato KB, 2015, COMPUT MATH METHOD M, DOI 10.1155/2015/460189
   Nauck D, 1999, ARTIF INTELL MED, V16, P149, DOI 10.1016/S0933-3657(98)00070-0
   Palm R., 2012, DEEP LEARNING TOOLBO
   Paulin F., 2011, INT J COMPUTER SCI E, V3, P327
   Pena-Reyes CA, 1999, ARTIF INTELL MED, V17, P131, DOI 10.1016/S0933-3657(99)00019-6
   Polat K, 2007, DIGIT SIGNAL PROCESS, V17, P694, DOI 10.1016/j.dsp.2006.10.008
   Quinlan JR, 1996, J ARTIF INTELL RES, V4, P77, DOI 10.1613/jair.279
   Salama G., 2012, INT J COMPUT INF SCI, V01, P36
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Setiono R, 2000, ARTIF INTELL MED, V18, P205, DOI 10.1016/S0933-3657(99)00041-X
   Suykens J. A. K., 2003, ADV LEARNING THEORY, V190, P392
   Ubeyli ED, 2007, EXPERT SYST APPL, V33, P1054, DOI 10.1016/j.eswa.2006.08.005
NR 26
TC 85
Z9 93
U1 5
U2 192
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0957-4174
EI 1873-6793
J9 EXPERT SYST APPL
JI Expert Syst. Appl.
PD MAR 15
PY 2016
VL 46
BP 139
EP 144
DI 10.1016/j.eswa.2015.10.015
PG 6
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic; Operations Research & Management Science
SC Computer Science; Engineering; Operations Research & Management Science
GA CZ4ZR
UT WOS:000367112400013
DA 2020-02-19
ER

PT J
AU Yu, Y
   Wang, WW
   Han, P
AF Yu, Yang
   Wang, Wenwu
   Han, Peng
TI Localization based stereo speech source separation using probabilistic
   time-frequency masking and deep neural networks
SO EURASIP JOURNAL ON AUDIO SPEECH AND MUSIC PROCESSING
LA English
DT Article
DE Deep learning; Deep neural networks; Source separation; Soft mask
ID BLIND SOURCE SEPARATION; RECOGNITION
AB Time-frequency (T-F) masking is an effective method for stereo speech source separation. However, reliable estimation of the T-F mask from sound mixtures is a challenging task, especially when room reverberations are present in the mixtures. In this paper, we propose a new stereo speech separation system where deep neural networks are used to generate soft T-F mask for separation. More specifically, the deep neural network, which is composed of two sparse autoencoders and a softmax regression, is used to estimate the orientations of the dominant source at each T-F unit, based on low-level features, such as mixing vector (MV), interaural level, and phase difference (IPD/ILD). The dataset for training the networks was generated by the convolution of binaural room impulse responses (RIRs) and clean speech signals positioned in different angles with respect to the sensors. With the training dataset, we use unsupervised learning to extract high-level features from low-level features and use supervised learning to find the nonlinear functions between high-level features and the orientations of dominant source. By using the trained networks, the probability that each T-F unit belongs to different sources (target and interferers) can be estimated based on the localization cues which is further used to generate the soft mask for source separation. Experiments based on real binaural RIRs and TIMIT dataset are provided to show the performance of the proposed system for reverberant speech mixtures, as compared with a model-based T-F masking technique proposed recently.
C1 [Yu, Yang; Han, Peng] Northwestern Polytech Univ, Sch Marine Sci & Technol, Xian, Peoples R China.
   [Wang, Wenwu] Univ Surrey, Ctr Vis Speech & Signal Proc, Guildford, Surrey, England.
RP Yu, Y (reprint author), Northwestern Polytech Univ, Sch Marine Sci & Technol, Xian, Peoples R China.
EM nwpuyuy@nwpu.edu.cn
FU Natural Science Basis Research Plan in Shaanxi Province of China
   [2014JQ8355]
FX This study was performed when the first author was an academic visitor
   in the Center for Video Speech and Signal Processing, University of
   Surrey, and he wishes to thank Qingju Liu, Philip JB Jackson, and Atiyeh
   Alinaghi for providing help for issues related to the algorithm in [7].
   This research was supported partially by the Natural Science Basis
   Research Plan in Shaanxi Province of China (Program No.2014JQ8355). The
   authors wish to thank the anonymous reviewers for their helpful comments
   in improving this paper.
CR Alinaghi AY, 2014, IEEE-ACM T AUDIO SPE, V22, P1434, DOI 10.1109/TASLP.2014.2320637
   Baker JM, 2009, IEEE SIGNAL PROC MAG, V26, P78, DOI 10.1109/MSP.2009.932707
   Baker JM, 2009, IEEE SIGNAL PROC MAG, V26, P75, DOI 10.1109/MSP.2009.932166
   Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Bishop Christopher M, 2006, PATTERN RECOGNITION, V1
   Blauert J., 1997, SPATIAL HEARING PSYC
   Browns GJ, 2005, SIG COM TEC, P371, DOI 10.1007/3-540-27489-8_16
   Chauvin Y., 1995, BACKPROPAGATION THEO
   Comon P, 2010, HANDBOOK OF BLIND SOURCE SEPARATION: INDEPENDENT COMPONENT ANALYSIS AND APPLICATIONS, P1
   Dean J., 2012, ADV NEURAL INFORM PR, P1223
   Deng L, 2004, IMA V MATH, V138, P115
   Deng L., 1999, COMPUTATIONAL MODELS, P199
   Deng L, 2013, INT CONF ACOUST SPEE, P8604, DOI 10.1109/ICASSP.2013.6639345
   Di Persia L, 2008, SIGNAL PROCESS, V88, P2578, DOI 10.1016/j.sigpro.2008.04.006
   Garofolo J. S., 1993, NASA STI RECON TECHN, V93, P27403
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   HUANG JT, 2013, P ICASSP, P7304, DOI DOI 10.1109/ICASSP.2013.6639081
   Huang M., 2015, IEEE T AFFECT COMPUT, V99, P1, DOI DOI 10.1109/TASLP.2015.2468583
   Hummersone C, 2011, THESIS
   Hyvarinen A, 2000, NEURAL NETWORKS, V13, P411, DOI 10.1016/S0893-6080(00)00026-5
   Hyvarinen A., 2004, INDEPENDENT COMPONEN, V46
   Jiang Y, 2014, IEEE-ACM T AUDIO SPE, V22, P2112, DOI 10.1109/TASLP.2014.2361023
   Jin ZZ, 2009, IEEE T AUDIO SPEECH, V17, P625, DOI 10.1109/TASL.2008.2010633
   Jitong Chen, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P7039, DOI 10.1109/ICASSP.2014.6854965
   Kim G, 2009, J ACOUST SOC AM, V126, P1486, DOI 10.1121/1.3184603
   Mandel MI, 2010, IEEE T AUDIO SPEECH, V18, P382, DOI 10.1109/TASL.2009.2029711
   Narayanan A, 2013, INT CONF ACOUST SPEE, P7092, DOI 10.1109/ICASSP.2013.6639038
   Ng A, LECT NOTES SPARSE AU
   Ng Andrew Y., 2011, CS294A LECT NOTES, V72, P1, DOI DOI 10.1371/JOURNAL.PONE.0006098
   Po-Sen Huang, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P1562, DOI 10.1109/ICASSP.2014.6853860
   Ranzato M., 2009, THESIS
   Ranzato MA, 2007, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2007.383157
   Sawada H, 2011, IEEE T AUDIO SPEECH, V19, P516, DOI 10.1109/TASL.2010.2051355
   Schmidhuber J, 2014, ARXIVURL14047828
   Shinn-Cunningham BG, 2005, J ACOUST SOC AM, V117, P3100, DOI 10.1121/1.1872572
   Siniscalchi SM, 2013, IEEE T AUDIO SPEECH, V21, P2152, DOI 10.1109/TASL.2013.2270370
   Siniscalchi SM, 2013, NEUROCOMPUTING, V106, P148, DOI 10.1016/j.neucom.2012.11.008
   Van Veen B. D., 1988, IEEE ASSP Magazine, V5, P4, DOI 10.1109/53.665
   Vincent E, 2006, IEEE T AUDIO SPEECH, V14, P1462, DOI 10.1109/TSA.2005.858005
   Wang D.L., 2006, COMPUTATIONAL AUDITO
   Wang YX, 2013, IEEE T AUDIO SPEECH, V21, P270, DOI 10.1109/TASL.2012.2221459
   WIGHTMAN FL, 1992, J ACOUST SOC AM, V91, P1648, DOI 10.1121/1.402445
   Xu Y, 2014, IEEE SIGNAL PROC LET, V21, P65, DOI 10.1109/LSP.2013.2291240
   Yu D, 2011, IEEE SIGNAL PROC MAG, V28, P145, DOI 10.1109/MSP.2010.939038
NR 46
TC 57
Z9 59
U1 1
U2 12
PU SPRINGEROPEN
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 1687-4722
J9 EURASIP J AUDIO SPEE
JI EURASIP J. Audio Speech Music Process.
PD MAR 4
PY 2016
AR 7
DI 10.1186/s13636-016-0085-x
PG 18
WC Acoustics; Engineering, Electrical & Electronic
SC Acoustics; Engineering
GA EH2FC
UT WOS:000391582000001
OA DOAJ Gold
DA 2020-02-19
ER

PT J
AU Farahat, M
   Halavati, R
AF Farahat, Mahboubeh
   Halavati, Ramin
TI Noise Robust Speech Recognition Using Deep Belief Networks
SO INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE AND APPLICATIONS
LA English
DT Article
DE Automatic speech recognition; isolated word recognition; deep learning;
   noise robust speech recognition; feature extraction
ID FEATURE-EXTRACTION
AB Most current speech recognition systems use Hidden Markov Models (HMMs) to deal with the temporal variability of speech and Gaussian mixture models (GMMs) to determine how well each state of each HMM fits a frame or a short window of frames of coefficients that represents the acoustic input. In these systems acoustic inputs are represented by Mel Frequency Cepstral Coefficients temporal spectrogram known as frames. But MFCC is not robust to noise. Consequently, with different train and test conditions the accuracy of speech recognition systems decreases. On the other hand, using MFCCs of larger window of frames in GMMs needs more computational power. In this paper, Deep Belief Networks (DBNs) are used to extract discriminative information from larger window of frames. Nonlinear transformations lead to high-order and low-dimensional features which are robust to variation of input speech. Multiple speaker isolated word recognition tasks with 100 and 200 words in clean and noisy environments has been used to test this method. The experimental results indicate that this new method of feature encoding result in much better word recognition accuracy.
C1 [Farahat, Mahboubeh] Univ Isfahan, Dept Comp Engn, Esfahan, Iran.
   [Halavati, Ramin] Sharif Univ Technol, Dept Elect Engn, Tehran, Iran.
RP Farahat, M (reprint author), Univ Isfahan, Dept Comp Engn, Esfahan, Iran.
EM m.farahat@eng.ui.ac.ir; halavati@ee.sharif.edu
CR Almasganj F., 2012, ELECT TELECOMMUNICAT, V35, P100
   Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Bocchieri E, 2013, INT CONF ACOUST SPEE, P6709, DOI 10.1109/ICASSP.2013.6638960
   Chen J, 2003, SICE 2003 ANNUAL CONFERENCE, VOLS 1-3, P469
   Farahani G., 2007, ROBUST SPEECH RECOGN
   Fischer Asja, 2012, Progress in Pattern Recognition, Image Analysis, ComputerVision, and Applications. Proceedings 17th Iberoamerican Congress, CIARP 2012, P14, DOI 10.1007/978-3-642-33275-3_2
   Freund Yoav, 1994, UCSCCRL9425
   Grezl F, 2007, INT CONF ACOUST SPEE, P757
   Hai D. V., 2011, HYBRID ARCHITECTURES
   Halavati R., 2012, Proceedings of the 2012 6th IEEE International Conference Intelligent Systems (IS), P102, DOI 10.1109/IS.2012.6335121
   HERMANSKY H, 1990, J ACOUST SOC AM, V87, P1738, DOI 10.1121/1.399423
   Hermansky H, 1999, INT CONF ACOUST SPEE, P289, DOI 10.1109/ICASSP.1999.758119
   Hermansky H, 2000, INT CONF ACOUST SPEE, P1635, DOI 10.1109/ICASSP.2000.862024
   Hinton G., 2012, LNCS, V7700, P599, DOI DOI 10.1007/978-3-642-35289-8_32
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   HIRSCH HG, 2000, [No title captured], P181
   JUANG BH, 1986, IEEE T INFORM THEORY, V32, P307
   Ketabdar H, 2008, INT CONF ACOUST SPEE, P4065, DOI 10.1109/ICASSP.2008.4518547
   Le Roux N, 2008, NEURAL COMPUT, V20, P1631, DOI 10.1162/neco.2008.04-07-510
   Mohamed AR, 2012, IEEE T AUDIO SPEECH, V20, P14, DOI 10.1109/TASL.2011.2109382
   Nair V, 2009, ADV NEURAL INF PROCE, V22, P1339
   Nasersharif Babak, 2004, 2004 12th European Signal Processing Conference (EUSIPCO), P345
   Nazari M., 2008, IEEE INT C INF COMM, DOI [10.1109/ICTTA.2008.4530026, DOI 10.1109/ICTTA.2008.4530026]
   Pirhosseinloo Sh., 2012, P 4 INT C COGN SCI, P296
   Rahim M., 1997, EUROSPEECH, P75
   Rifai S., 2010, NIPS WORKSH DEEP LEA
   ROBINSON AJ, 1994, IEEE T NEURAL NETWOR, V5, P298, DOI 10.1109/72.279192
   Sameti H, 2011, EURASIP J AUDIO SPEE, DOI 10.1186/1687-4722-2011-426795
   Sameti H, 2008, COMM COM INF SC, V6, P485
   Schwarz P., 2004, TSD
   Schwarz P., 2006, IEEE INT C AC SPEECH
   SHARMA S, 2000, ACOUST SPEECH SIG PR
   Trentin E, 2001, NEUROCOMPUTING, V37, P91, DOI 10.1016/S0925-2312(00)00308-8
   Veisi H., 2006, IEEE INT C INF COMM, P1293
   Vinyals O, 2011, INT CONF ACOUST SPEE, P4596
   WAIBEL A, 1989, IEEE T ACOUST SPEECH, V37, P328, DOI 10.1109/29.21701
   Yu D., 2013, INT C LEARN REPR
   Yu D, 2011, IEEE SIGNAL PROC MAG, V28, P145, DOI 10.1109/MSP.2010.939038
   Yu Dong, 2011, INTERSPEECH
NR 42
TC 3
Z9 4
U1 0
U2 3
PU WORLD SCIENTIFIC PUBL CO PTE LTD
PI SINGAPORE
PA 5 TOH TUCK LINK, SINGAPORE 596224, SINGAPORE
SN 1469-0268
EI 1757-5885
J9 INT J COMPUT INTELL
JI Int. J. Comput. Intell. Appl.
PD MAR
PY 2016
VL 15
IS 1
AR 1650005
DI 10.1142/S146902681650005X
PG 17
WC Computer Science, Artificial Intelligence
SC Computer Science
GA DV0PX
UT WOS:000382622500005
DA 2020-02-19
ER

PT J
AU Zhang, XS
   Zhang, HW
   Zhang, YD
   Yang, Y
   Wang, M
   Luan, HB
   Li, JT
   Chua, TS
AF Zhang, Xishan
   Zhang, Hanwang
   Zhang, Yongdong
   Yang, Yang
   Wang, Meng
   Luan, Huanbo
   Li, Jintao
   Chua, Tat-Seng
TI Deep Fusion of Multiple Semantic Cues for Complex Event Recognition
SO IEEE TRANSACTIONS ON IMAGE PROCESSING
LA English
DT Article
DE Multimedia event recognition; deep learning; fusion
ID CLASSIFICATION; SCALE
AB We present a deep learning strategy to fuse multiple semantic cues for complex event recognition. In particular, we tackle the recognition task by answering how to jointly analyze human actions (who is doing what), objects (what), and scenes (where). First, each type of semantic features (e.g., human action trajectories) is fed into a corresponding multi-layer feature abstraction pathway, followed by a fusion layer connecting all the different pathways. Second, the correlations of how the semantic cues interacting with each other are learned in an unsupervised cross-modality autoencoder fashion. Finally, by fine-tuning a large-margin objective deployed on this deep architecture, we are able to answer the question on how the semantic cues of who, what, and where compose a complex event. As compared with the traditional feature fusion methods (e.g., various early or late strategies), our method jointly learns the essential higher level features that are most effective for fusion and recognition. We perform extensive experiments on two real-world complex event video benchmarks, MED'11 and CCV, and demonstrate that our method outperforms the best published results by 21% and 11%, respectively, on an event recognition task.
C1 [Zhang, Xishan; Zhang, Yongdong; Li, Jintao] Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
   [Zhang, Xishan] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
   [Zhang, Hanwang; Chua, Tat-Seng] Natl Univ Singapore, Sch Comp, Singapore 117417, Singapore.
   [Yang, Yang] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
   [Wang, Meng] Hefei Univ Technol, Sch Comp Sci & Informat Engn, Hefei 230009, Peoples R China.
   [Luan, Huanbo] Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
RP Zhang, YD (reprint author), Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
EM zhangxishan@ict.ac.cn; hanwang@comp.nus.edu.sg; zhyd@ict.ac.cn;
   dlyyang@gmail.com; eric.mengwang@gmail.com; luanhuanbo@gmail.com;
   jtli@ict.ac.cn; chuats@comp.nus.edu.sg
FU National High Technology Research and Development Program of
   ChinaNational High Technology Research and Development Program of China
   [2014AA015202]; National University of Singapore-Tsinghua Extreme Search
   Project [R-252-300-001-490]; National Nature Science Foundation of
   ChinaNational Natural Science Foundation of China [61525206, 61428207,
   61303075]
FX This work was supported in part by the National High Technology Research
   and Development Program of China under Grant 2014AA015202, in part by
   the National University of Singapore-Tsinghua Extreme Search Project
   under Grant R-252-300-001-490, and in part by the National Nature
   Science Foundation of China under Grant 61525206, Grant 61428207, and
   Grant 61303075. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Pierre-Marc
   Jodoin. (Corresponding author: Yongdong Zhang.)
CR Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Chen Jiawei, 2014, P INT C MULT RETR, P1
   Dalal N, 2005, PROC CVPR IEEE, P886
   Donahue Jeff, 2013, DECAF DEEP CONVOLUTI
   Farquhar J., 2005, ADV NEURAL INF PROCE, P355
   Gaidon A., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3201, DOI 10.1109/CVPR.2011.5995646
   Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.2307/2333955
   Hsu CW, 2002, IEEE T NEURAL NETWOR, V13, P415, DOI 10.1109/72.991427
   Ikizler-Cinbis N, 2010, LECT NOTES COMPUT SC, V6311, P494, DOI 10.1007/978-3-642-15549-9_36
   Izadinia H, 2012, LECT NOTES COMPUT SC, V7575, P430, DOI 10.1007/978-3-642-33765-9_31
   Jhuo IH, 2014, MACH VISION APPL, V25, P33, DOI 10.1007/s00138-013-0567-0
   Jiang L., 2012, P ACM INT C MULT, P449
   Jiang Y.-G., 2012, P ACM INT C MULT RET
   Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7
   Lazebnik S., 2006, P IEEE INT C COMP VI, V2, P2169, DOI DOI 10.1109/CVPR.2006.68
   Li L.-J., 2010, ADV NEURAL INFORM PR, P1378
   Liu D, 2013, PROC CVPR IEEE, P803, DOI 10.1109/CVPR.2013.109
   Liu JE, 2013, IEEE WORK APP COMP, P339, DOI 10.1109/WACV.2013.6475038
   Mazloom M., 2013, P 3 ACM C INT C MULT, P255
   Myers GK, 2014, MACH VISION APPL, V25, P17, DOI 10.1007/s00138-013-0527-8
   Natarajan P, 2012, PROC CVPR IEEE, P1298, DOI 10.1109/CVPR.2012.6247814
   Ngiam J., 2011, P 28 INT C MACH LEAR, P1
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Oneata D, 2013, IEEE I CONF COMP VIS, P1817, DOI 10.1109/ICCV.2013.228
   Over A., 2011, P TRECVID, P10
   Papadimitriou C. H., 1998, Proceedings of the Seventeenth ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems. PODS 1998, P159, DOI 10.1145/275487.275505
   Ramanathan V, 2013, IEEE I CONF COMP VIS, P905, DOI 10.1109/ICCV.2013.117
   Sadanand S, 2012, PROC CVPR IEEE, P1234, DOI 10.1109/CVPR.2012.6247806
   Salakhutdinov R, 2009, INT J APPROX REASON, V50, P969, DOI 10.1016/j.ijar.2008.11.006
   Snoek C. G. M., 2005, 13th Annual ACM International Conference on Multimedia, P399
   Srivastava N, 2014, J MACH LEARN RES, V15, P2949
   Sun C., 2014, P INT C MULT RETR, P241
   Sun C, 2013, IEEE I CONF COMP VIS, P913, DOI 10.1109/ICCV.2013.453
   Sun C, 2013, IEEE WORK APP COMP, P15, DOI 10.1109/WACV.2013.6474994
   Tamrakar A, 2012, PROC CVPR IEEE, P3681, DOI 10.1109/CVPR.2012.6248114
   Tang Y, 2013, DEEP LEARNING USING
   Tieleman T., 2008, P ICML
   Torresani L, 2010, LECT NOTES COMPUT SC, V6311, P776, DOI 10.1007/978-3-642-15549-9_56
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wu S, 2014, PROC CVPR IEEE, P2665, DOI 10.1109/CVPR.2014.341
   Wu ZX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P167, DOI 10.1145/2647868.2654931
   XIAO JX, 2010, PROC CVPR IEEE, P3485, DOI DOI 10.1109/CVPR.2010.5539970
   Xu ZW, 2013, IEEE I CONF COMP VIS, P3440, DOI 10.1109/ICCV.2013.427
   Yang Y, 2012, LECT NOTES COMPUT SC, V7574, P722, DOI 10.1007/978-3-642-33712-3_52
   Yang Y, 2013, IEEE T MULTIMEDIA, V15, P572, DOI 10.1109/TMM.2012.2234731
   Ye GN, 2012, PROC CVPR IEEE, P3021, DOI 10.1109/CVPR.2012.6248032
   Younessian E., 2012, P ICMR, P51
NR 48
TC 34
Z9 35
U1 0
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1057-7149
EI 1941-0042
J9 IEEE T IMAGE PROCESS
JI IEEE Trans. Image Process.
PD MAR
PY 2016
VL 25
IS 3
BP 1033
EP 1046
DI 10.1109/TIP.2015.2511585
PG 14
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
SC Computer Science; Engineering
GA DP2BU
UT WOS:000378293900002
PM 26780785
DA 2020-02-19
ER

PT J
AU Fan, HQ
   Zhou, EJ
AF Fan, Haoqiang
   Zhou, Erjin
TI Approaching human level facial landmark localization by deep learning
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Facial landmark localization; Deep learning; Convolutional neural
   network
ID FACE ALIGNMENT
AB In this paper we present our solution to the 300 Faces in the Wild Facial Landmark Localization Challenge. We demonstrate how to achieve very competitive localization performance with a simple deep learning based system. Human study is conducted to show that the accuracy of our system has been very close to human performance. We discuss how this finding would affect our future direction to improve our system. (C) 2015 Elsevier B.V. All rights reserved.
C1 [Fan, Haoqiang; Zhou, Erjin] Megvii Inc, Beijing, Peoples R China.
RP Fan, HQ (reprint author), Megvii Inc, Beijing, Peoples R China.
EM fhq@megvii.com; zej@megvii.com
CR Asthana A, 2013, PROC CVPR IEEE, P3444, DOI 10.1109/CVPR.2013.442
   Belhumeur PN, 2011, PROC CVPR IEEE, P545, DOI 10.1109/CVPR.2011.5995602
   Ber T., 2012, P BRIT MACH VIS C, P7
   Burgos-Artizzu XP, 2013, IEEE I CONF COMP VIS, P1513, DOI 10.1109/ICCV.2013.191
   Cao XD, 2014, INT J COMPUT VISION, V107, P177, DOI 10.1007/s11263-013-0667-3
   Chandra A., DIVIDING CIRCULAR AR
   Chen D, 2013, PROC CVPR IEEE, P3025, DOI 10.1109/CVPR.2013.389
   Cootes T. F., 1998, Computer Vision - ECCV'98. 5th European Conference on Computer Vision. Proceedings, P484
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Fan HQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P933, DOI 10.1145/2647868.2654960
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Felzenszwalb PF, 2005, INT J COMPUT VISION, V61, P55, DOI 10.1023/B:VISI.0000042934.15159.49
   He K., ARXIV150201852
   Kazemi V, 2014, PROC CVPR IEEE, P1867, DOI 10.1109/CVPR.2014.241
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Le V, 2012, LECT NOTES COMPUT SC, V7574, P679, DOI 10.1007/978-3-642-33712-3_49
   Ren SQ, 2014, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR.2014.218
   Sagonas C, 2013, IEEE COMPUT SOC CONF, P896, DOI 10.1109/CVPRW.2013.132
   Sun Y., ARXIV14121265
   Sun Y, 2013, PROC CVPR IEEE, P3476, DOI 10.1109/CVPR.2013.446
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Tzimiropoulos G, 2014, PROC CVPR IEEE, P1851, DOI 10.1109/CVPR.2014.239
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Xiong XH, 2013, PROC CVPR IEEE, P532, DOI 10.1109/CVPR.2013.75
   Yu X, 2013, IEEE I CONF COMP VIS, P1944, DOI 10.1109/ICCV.2013.244
   Zhang JH, 2014, ELECTRON J QUAL THEO, P1, DOI [10.1007/978-3-319-10605-2_1, 10.14232/ejqtde.2014.1.50]
   Zhang Z, 2015, IEEE SYSTEMS J, P1
   Zhou E., ARXIV150104690
   ZHOU EJ, 2013, COMP VIS WORKSH ICCV, P386
   Zhu SZ, 2015, PROC CVPR IEEE, P4998, DOI 10.1109/CVPR.2015.7299134
   Zhu XX, 2012, PROC CVPR IEEE, P2879, DOI 10.1109/CVPR.2012.6248014
NR 31
TC 35
Z9 37
U1 3
U2 15
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD MAR
PY 2016
VL 47
BP 27
EP 35
DI 10.1016/j.imavis.2015.11.004
PG 9
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
SC Computer Science; Engineering; Optics
GA DO5LL
UT WOS:000377824500004
DA 2020-02-19
ER

PT J
AU Kim, IJ
   Choi, C
   Lee, SH
AF Kim, In-Jung
   Choi, Changbeom
   Lee, Sang-Heon
TI Improving discrimination ability of convolutional neural networks by
   hybrid learning
SO INTERNATIONAL JOURNAL ON DOCUMENT ANALYSIS AND RECOGNITION
LA English
DT Article
DE Deep learning; Convolutional neural networks; Hybrid learning;
   Discrimination; Character recognition; Machine learning; Pattern
   recognition
AB The discrimination of similar patterns is important because they are the major sources of the classification error. This paper proposes a novel method to improve the discrimination ability of convolutional neural networks (CNNs) by hybrid learning. The proposed method embeds a collection of discriminators as well as a recognizer in a shared CNN. By visualizing contrastive class saliency, we show that learning with embedded discriminators leads the shared CNN to detect and catch the differences among similar classes. Also proposed is a hybrid learning algorithm that learns recognition and discrimination together. The proposed method learns recognition focusing on the differences among similar classes, and thereby improves the discrimination ability of the CNN. Unlike conventional discrimination methods, the proposed method does not require predefined sets of similar classes or additional step to integrate its result with that of the recognizer. In experiments on two handwritten Hangul databases SERI95a and PE92, the proposed method reduced classification error from 2.56 to 2.33, and from 4.04 to 3.66% respectively. These improvement lead to relative error reduction rates of 8.97% on SERI95a, and 9.42% on PE92. Our best results update the state-of-the-art performance which were 4.04% on SERI95a and 7.08% on PE92.
C1 [Kim, In-Jung] Handong Global Univ, Sch CSEE, Pohang, South Korea.
   [Choi, Changbeom] Handong Global Univ, Sch Creat Convergence Educ, Pohang, South Korea.
   [Lee, Sang-Heon] DGIST, Dept IoT & Robot Convergence Res, Daegu, South Korea.
RP Kim, IJ (reprint author), Handong Global Univ, Sch CSEE, Pohang, South Korea.
EM ijkim@handong.edu; cbchoi@handong.edu; pobbylee@dgist.ac.kr
FU DGIST R&D Program of the Ministry of Science, ICT and Future Planning
   [15-IT-03]
FX This work was supported by the DGIST R&D Program of the Ministry of
   Science, ICT and Future Planning (15-IT-03).
CR Bengio Y., 2009, P 26 ANN INT C MACH, P41, DOI DOI 10.1145/1553374.1553380
   Bo Xu, 2010, Proceedings 2010 12th International Conference on Frontiers in Handwriting Recognition (ICFHR 2010), P527, DOI 10.1109/ICFHR.2010.87
   Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734
   Girshick Ross, 2014, 2014 IEEE C COMP VIS
   Goodfellow I. J., 2013, ARXIV13024389
   Hinton G., 2015, ARXIV150302531
   Ioffe Sergey, 2015, ARXIV150203167
   Kim D.H., 1992, P 4 NAT C KOR LANG I, P152
   Kim D.-I., 1997, P NAT C KOR LANG INF, P152
   Kim IJ, 2002, PATTERN RECOGN, V35, P2259, DOI 10.1016/S0031-3203(01)00199-6
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Leung KC, 2010, PATTERN RECOGN, V43, P949, DOI 10.1016/j.patcog.2009.09.001
   Lin M., 2014, P ICLR
   Ryu S, 2014, INT J DOC ANAL RECOG, V17, P79, DOI 10.1007/s10032-013-0206-3
   Sato A, 1996, ADV NEUR IN, V8, P423
   Simard P., 2003, BEST PRACTICES CONVO, P958
   Simonyan K., 2014, ARXIV14091556
   Simonyan K., 2013, ARXIV13126034
   Socher R., 2012, ADV NEURAL INFORM PR
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Szegedy C., 2014, ARXIV14094842
   Yin F., ICDAR 2013 CHINESE H
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
NR 23
TC 2
Z9 2
U1 0
U2 9
PU SPRINGER HEIDELBERG
PI HEIDELBERG
PA TIERGARTENSTRASSE 17, D-69121 HEIDELBERG, GERMANY
SN 1433-2833
EI 1433-2825
J9 INT J DOC ANAL RECOG
JI Int. J. Doc. Anal. Recognit.
PD MAR
PY 2016
VL 19
IS 1
BP 1
EP 9
DI 10.1007/s10032-015-0256-9
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA DM4CT
UT WOS:000376293900001
DA 2020-02-19
ER

PT J
AU Luo, C
   Wang, J
AF Luo, Chang
   Wang, Jie
TI Fine-grained representation learning in convolutional autoencoders
SO JOURNAL OF ELECTRONIC IMAGING
LA English
DT Article
DE Pooling; fine-grained representation; convolutional neural networks;
   sparse autoencoder; discrimination-invariance tradeoff; unsupervised
   learning; deep learning
AB Convolutional autoencoders (CAEs) have been widely used as unsupervised feature extractors for high-resolution images. As a key component in CAEs, pooling is a biologically inspired operation to achieve scale and shift invariances, and the pooled representation directly affects the CAEs' performance. Fine-grained pooling, which uses small and dense pooling regions, encodes fine-grained visual cues and enhances local characteristics. However, it tends to be sensitive to spatial rearrangements. In most previous works, pooled features were obtained by empirically modulating parameters in CAEs. We see the CAE as a whole and propose a fine-grained representation learning law to extract better fine-grained features. This representation learning law suggests two directions for improvement. First, we probabilistically evaluate the discrimination-invariance tradeoff with fine-grained granularity in the pooled feature maps, and suggest the proper filter scale in the convolutional layer and appropriate whitening parameters in preprocessing step. Second, pooling approaches are combined with the sparsity degree in pooling regions, and we propose the preferable pooling approach. Experimental results on two independent benchmark datasets demonstrate that our representation learning law could guide CAEs to extract better fine-grained features and performs better in multiclass classification task. This paper also provides guidance for selecting appropriate parameters to obtain better fine-grained representation in other convolutional neural networks. (C) 2016 SPIE and IS&T
C1 [Luo, Chang; Wang, Jie] Air Force Engn Univ, Air & Missile Def Coll, 1 East Changle Rd, Xian 710051, CN, Peoples R China.
RP Luo, C (reprint author), Air Force Engn Univ, Air & Missile Def Coll, 1 East Changle Rd, Xian 710051, CN, Peoples R China.
EM luochang1988@126.com
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [71501184]
FX This work was supported by the National Natural Science Foundation of
   China under Grant No. 71501184.
CR Bell AJ, 1997, VISION RES, V37, P3327, DOI 10.1016/S0042-6989(97)00121-1
   Berg T., 2013, CVPR
   Bo L., 2012, ISER
   Boureau Y., 2010, CVPR
   Boureau Y., 2010, P 27 INT C MACH LEAR, P111, DOI DOI 10.1016/J.NEUNET.2012.02.023
   Chatfield K., 2014, ARXIV14053531
   Cires D. C., 2011, P 22 INT JOINT C ART, V22, P1237, DOI DOI 10.5591/978-1-57735-516-8/IJCAI11-210
   Coates A., 2011, INT C ART INT STAT, V15, P215, DOI 10.1177/1753193410390845
   Coates A., 2011, NIPS
   Coates A., 2011, P 14 INT C ART INT S
   Donahue J., 2014, ICML
   Elawady M. E., 2015, ARXIV15110906
   Fanello SR, 2014, PROC CVPR IEEE, P851, DOI 10.1109/CVPR.2014.114
   Gong Yunchao, 2014, ECCV
   He K., 2014, ECCV
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Jia Y., 2012, CVPR
   Jordan MI, 2015, SCIENCE, V349, P255, DOI 10.1126/science.aaa8415
   Kavukcuoglu  K., 2010, NIPS
   Krizhevsky A., 2009, THESIS, V1
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Li C. G., 2015, CVPR
   Li ZH, 2015, EURASIP J ADV SIG PR, DOI 10.1186/s13634-015-0222-1
   Liu L., 2015, CVPR
   Malalur SS, 2015, NEUROCOMPUTING, V149, P1490, DOI 10.1016/j.neucom.2014.08.043
   Malinowski M., 2013, ARXIV13013516
   Ng A., 2014, DEEP LEARNING
   Scherer D, 2010, LECT NOTES COMPUT SC, V6354, P92, DOI 10.1007/978-3-642-15825-4_10
   Sermanet P., 2013, ARXIV13126229
   Simonyan K., 2012, ECCV
   Socher R., 2012, NIPS
   Sukhbaatar S., 2013, ARXIV13013323
   van der Maaten L, 2014, J MACH LEARN RES, V15, P3221
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang W, 2016, VLDB J, V25, P79, DOI 10.1007/s00778-015-0391-4
   Xu C., 2014, CVPR
   Yang J., 2009, CVPR
   Zeiler Matthew D, 2013, ARXIV13013557
NR 40
TC 3
Z9 3
U1 0
U2 11
PU IS&T & SPIE
PI BELLINGHAM
PA 1000 20TH ST, BELLINGHAM, WA 98225 USA
SN 1017-9909
EI 1560-229X
J9 J ELECTRON IMAGING
JI J. Electron. Imaging
PD MAR
PY 2016
VL 25
IS 2
AR 023018
DI 10.1117/1.JEI.25.2.023018
PG 12
WC Engineering, Electrical & Electronic; Optics; Imaging Science &
   Photographic Technology
SC Engineering; Optics; Imaging Science & Photographic Technology
GA DL8ZZ
UT WOS:000375932300020
DA 2020-02-19
ER

PT J
AU Tian, L
   Fan, CX
   Ming, Y
AF Tian, Lei
   Fan, Chunxiao
   Ming, Yue
TI Multiple scales combined principle component analysis deep learning
   network for face recognition
SO JOURNAL OF ELECTRONIC IMAGING
LA English
DT Article
DE multiple scales feature representation; convolution neural network;
   binary hashing; spatial pyramid pooling; two-dimensional face
   recognition; two-dimensional face verification; three-dimensional face
   recognition
ID EXPRESSION RECOGNITION; PATTERNS
AB It is well known that higher level features can represent the abstract semantics of original data. We propose a multiple scales combined deep learning network to learn a set of high-level feature representations through each stage of convolutional neural network for face recognition, which is named as multiscaled principle component analysis (PCA) Network (MS-PCANet). There are two main differences between our model and the traditional deep learning network. On the one hand, we get the prefixed filter kernels by learning the principal component of images' patches using PCA, nonlinearly process the convolutional results by using simple binary hashing, and pool them using spatial pyramid pooling method. On the other hand, in our model, the output features of several stages are fed to the classifier. The purpose of combining feature representations from multiple stages is to provide multiscaled features to the classifier, since the features in the latter stage are more global and invariant than those in the early stage. Therefore, our MS-PCANet feature compactly encodes both holistic abstract information and local specific information. Extensive experimental results show our MS-PCANet model can efficiently extract high-level feature presentations and outperform state-of-the-art face/expression recognition methods on multiple modalities benchmark face-related datasets. (C) 2016 SPIE and IS&T
C1 [Tian, Lei; Fan, Chunxiao; Ming, Yue] Beijing Univ Posts & Telecommun, Sch Elect Engn, Beijing Key Lab Work Safety Intelligent Monitorin, 10 Xitucheng Rd, Beijing 100876, Peoples R China.
RP Ming, Y (reprint author), Beijing Univ Posts & Telecommun, Sch Elect Engn, Beijing Key Lab Work Safety Intelligent Monitorin, 10 Xitucheng Rd, Beijing 100876, Peoples R China.
EM myname35875235@126.com
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [NSFC-61402046, NSFC-61170176]; Fund for Beijing
   University of Posts and Telecommunications [2013XZ10, 2013XD-04]; Fund
   for the Doctoral Program of Higher Education of ChinaSpecialized
   Research Fund for the Doctoral Program of Higher Education (SRFDP)
   [20120005110002]
FX The work presented in this paper was supported by the National Natural
   Science Foundation of China (Grant Nos. NSFC-61402046 and
   NSFC-61170176), Fund for Beijing University of Posts and
   Telecommunications (Grant Nos. 2013XZ10 and 2013XD-04), Fund for the
   Doctoral Program of Higher Education of China (Grant No.
   20120005110002).
CR Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   Al-Osaimi F, 2009, INT J COMPUT VISION, V81, P302, DOI 10.1007/s11263-008-0174-0
   Arasu S., 2013, P IEEE 6 INT C BIOM, P1, DOI DOI 10.1109/BTAS.2013.6712721
   Barkan O, 2013, IEEE I CONF COMP VIS, P1960, DOI 10.1109/ICCV.2013.246
   Berretti S, 2010, IEEE T PATTERN ANAL, V32, P2162, DOI 10.1109/TPAMI.2010.43
   Beveridge J, 2013, BIOM THEOR APPL SYST, P1
   Beveridge J. R., 2013, POINT AND SHOOT FACE
   Bruna J, 2013, IEEE T PATTERN ANAL, V35, P1872, DOI 10.1109/TPAMI.2012.230
   Chai ZH, 2014, IEEE T INF FOREN SEC, V9, P14, DOI 10.1109/TIFS.2013.2290064
   Chan T. H., 2014, ARXIV14043606
   CHAO YW, 2011, 18 IEEE INT C IM PRO, P761
   Chen D, 2013, PROC CVPR IEEE, P3025, DOI 10.1109/CVPR.2013.389
   Deng WH, 2013, PROC CVPR IEEE, P399, DOI 10.1109/CVPR.2013.58
   Deng WH, 2012, IEEE T PATTERN ANAL, V34, P1864, DOI 10.1109/TPAMI.2012.30
   Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464
   Gross R, 2010, IMAGE VISION COMPUT, V28, P807, DOI 10.1016/j.imavis.2009.08.002
   He KM, 2015, IEEE T PATTERN ANAL, V37, P1904, DOI 10.1109/TPAMI.2015.2389824
   Huang G. B., UMCS2014003
   Huang Gary B., 2007, 0749 U MASS
   Huang W., 2015, ARXIV150600481
   Hussain SU, 2012, BRIT MACH VIS C, P11
   Juefei-Xu F, 2015, IEEE T IMAGE PROCESS, V24, P4780, DOI 10.1109/TIP.2015.2468173
   Jun Yi, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P519, DOI 10.1109/ICASSP.2014.6853650
   Kan M, 2014, PROC CVPR IEEE, P1883, DOI 10.1109/CVPR.2014.243
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Le Q.V., 2011, P 28 INT C MACH LEAR, P265
   Lei Z, 2014, IEEE T PATTERN ANAL, V36, P289, DOI 10.1109/TPAMI.2013.112
   Lei Z, 2011, IEEE T IMAGE PROCESS, V20, P247, DOI 10.1109/TIP.2010.2060207
   Lowe D., 1999, P INT C COMP VIS, V2, P1150, DOI [10.1109/ICCV.1999.790410, DOI 10.1109/ICCV.1999.790410]
   Lu JW, 2015, IEEE I CONF COMP VIS, P3721, DOI 10.1109/ICCV.2015.424
   Lu JW, 2015, IEEE T PATTERN ANAL, V37, P2041, DOI 10.1109/TPAMI.2015.2408359
   Lu JW, 2015, IEEE T INF FOREN SEC, V10, P1371, DOI 10.1109/TIFS.2015.2408431
   Martinez AM, 1998, CVC TECHNICAL REPORT, V24
   Maurer T., 2005, P IEEE C COMP VIS PA, V3, P154, DOI DOI 10.1109/CVPR.2005.581
   Mian AS, 2007, IEEE T PATTERN ANAL, V29, P1927, DOI 10.1109/TPAMI.2007.1105
   Ming Y, 2014, NEUROCOMPUTING, V129, P445, DOI 10.1016/j.neucom.2013.09.014
   Ng C., 2015, ARXIV150702049
   Passalis G., 2005, P IEEE WORKSH FAC RE, V3, P171, DOI DOI 10.1109/CVPR.2005.573
   Phillips PJ, 2005, PROC CVPR IEEE, P947
   Phillips PJ, 2000, IEEE T PATTERN ANAL, V22, P1090, DOI 10.1109/34.879790
   Rudovic O, 2013, IEEE T PATTERN ANAL, V35, P1357, DOI 10.1109/TPAMI.2012.233
   Salah R, 2011, P 28 INT C MACH LEAR, P833, DOI 10.1016/j.wasman.2010.10.006
   Sharma A, 2012, PROC CVPR IEEE, P2160, DOI 10.1109/CVPR.2012.6247923
   Sharma A, 2011, PROC CVPR IEEE, P593, DOI 10.1109/CVPR.2011.5995350
   Shen FM, 2016, PATTERN RECOGN, V54, P94, DOI 10.1016/j.patcog.2016.01.010
   Sun Y, 2014, ADV NEURAL INFORM PR, V60, P1988
   Sun Y, 2013, IEEE I CONF COMP VIS, P1489, DOI 10.1109/ICCV.2013.188
   Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Tan DM, 2010, IEEE T IMAGE PROCESS, V19, P374, DOI 10.1109/TIP.2009.2033625
   Tian L, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON DIGITAL SIGNAL PROCESSING (DSP), P1039, DOI 10.1109/ICDSP.2015.7252036
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Vu N.-S., 2012, IEEE T IMAGE PROCESS, V21, P934
   Vu NS, 2013, IEEE T INF FOREN SEC, V8, P295, DOI 10.1109/TIFS.2012.2224866
   Wolf Lior, 2009, Computer Vision - ACCV 2009. 9th Asian Conference on Computer Vision. Revised Selected Papers, P88
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Xie SF, 2010, IEEE T IMAGE PROCESS, V19, P1349, DOI 10.1109/TIP.2010.2041397
   Xiong XH, 2013, PROC CVPR IEEE, P532, DOI 10.1109/CVPR.2013.75
   Yang M, 2011, PROC CVPR IEEE, P625, DOI 10.1109/CVPR.2011.5995393
   Yang M, 2010, LECT NOTES COMPUT SC, V6316, P448, DOI 10.1007/978-3-642-15567-3_33
   YANG W., 2008, P 8 IEEE INT C AUT F, P1
   Yin LJ, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P211
   Ylioinas J, 2014, INT C PATT RECOG, P4471, DOI 10.1109/ICPR.2014.765
   Zhang L, 2011, IEEE I CONF COMP VIS, P471, DOI 10.1109/ICCV.2011.6126277
   Zheng WM, 2014, IEEE T AFFECT COMPUT, V5, P71, DOI 10.1109/TAFFC.2014.2304712
   Zheng WM, 2009, IEEE I CONF COMP VIS, P1901, DOI 10.1109/ICCV.2009.5459421
   Zuo WM, 2013, IEEE I CONF COMP VIS, P217, DOI 10.1109/ICCV.2013.34
NR 67
TC 10
Z9 11
U1 2
U2 43
PU IS&T & SPIE
PI BELLINGHAM
PA 1000 20TH ST, BELLINGHAM, WA 98225 USA
SN 1017-9909
EI 1560-229X
J9 J ELECTRON IMAGING
JI J. Electron. Imaging
PD MAR
PY 2016
VL 25
IS 2
AR 023025
DI 10.1117/1.JEI.25.2.023025
PG 16
WC Engineering, Electrical & Electronic; Optics; Imaging Science &
   Photographic Technology
SC Engineering; Optics; Imaging Science & Photographic Technology
GA DL8ZZ
UT WOS:000375932300027
DA 2020-02-19
ER

PT J
AU Chen, Z
   Ma, L
   Xu, L
   Tan, CM
   Yan, YH
AF Chen, Zhuo
   Ma, Lin
   Xu, Long
   Tan, Chengming
   Yan, Yihua
TI Imaging and representation learning of solar radio spectrums for
   classification
SO MULTIMEDIA TOOLS AND APPLICATIONS
LA English
DT Article
DE Deep learning; Solar radio astronomy; Feature learning; Classification
AB In this paper, the authors make the first attempt to employ the deep learning method for the representation learning of the solar radio spectrums. The original solar radio spectrums are pre-processed, including normalization, enhancement and etc., to generate new images for the next processing. With the expertise of solar radio astronomy for identifying solar radio activity, we build a solar radio activity database, which contains solar radio spectrums as well as their labels indicating the types of solar radio bursts. The employed deep learning network is firstly pre-trained based on the available massive of unlabeled radio solar images. Afterwards, the weights of the network are further fined-tuned based on the labeled data. Experimental results have demonstrated that the employed network can effectively classify the solar radio image into the labeled categories. Moreover, the pre-training process can help improve the classification accuracy.
C1 [Chen, Zhuo; Xu, Long; Tan, Chengming; Yan, Yihua] Chinese Acad Sci, Natl Astron Observ, Key Lab Solar Act, Beijing, Peoples R China.
   [Ma, Lin] Huawei Noahs Ark Lab, Hong Kong, Hong Kong, Peoples R China.
RP Xu, L (reprint author), Chinese Acad Sci, Natl Astron Observ, Key Lab Solar Act, Beijing, Peoples R China.
EM lxu@nao.cas.cn
RI Xu, Long/AAH-9908-2019
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61202242]; 100-Talents Program of Chinese Academy
   of SciencesChinese Academy of Sciences [Y434061V01]
FX This work was partially supported by the National Natural Science
   Foundation of China under Grant 61202242, 100-Talents Program of Chinese
   Academy of Sciences (No. Y434061V01).
CR Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Chen M., 2012, 29 INT C MACH LEARN
   Chen M., 2014, ICML
   Collobert R, 2011, J MACH LEARN RES, V12, P2493
   Deng L., 2012, APSIPA T SIGNAL INFO
   Dong J, 2009, IEEE T CIRC SYST VID, V19, P1462, DOI 10.1109/TCSVT.2009.2026792
   Fawcett T, 2006, PATTERN RECOGN LETT, V27, P861, DOI 10.1016/j.patrec.2005.10.010
   Fu QJ, 2004, SOL PHYS, V222, P167, DOI 10.1023/B:SOLA.0000036876.14446.dd
   Hinton G., 2010, TECHNICAL REPORT
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Le Q. V., 2012, ICML
   Lee Honglak, 2009, NIPS
   Mohamed AR, 2012, IEEE T AUDIO SPEECH, V20, P14, DOI 10.1109/TASL.2011.2109382
   Papoulis A., 2002, PROBABILITY RANDOM V
   Salakhutdinov R., 2008, ICML
   Sohn K., 2011, ICCV
NR 17
TC 5
Z9 6
U1 0
U2 10
PU SPRINGER
PI DORDRECHT
PA VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN 1380-7501
EI 1573-7721
J9 MULTIMED TOOLS APPL
JI Multimed. Tools Appl.
PD MAR
PY 2016
VL 75
IS 5
BP 2859
EP 2875
DI 10.1007/s11042-015-2528-2
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic
SC Computer Science; Engineering
GA DG4FN
UT WOS:000372027000028
DA 2020-02-19
ER

PT J
AU Lv, Q
   Niu, X
   Dou, Y
   Xu, JQ
   Lei, YW
AF Lv, Qi
   Niu, Xin
   Dou, Yong
   Xu, Jiaqing
   Lei, Yuanwu
TI Classification of Hyperspectral Remote Sensing Image Using Hierarchical
   Local-Receptive-Field-Based Extreme Learning Machine
SO IEEE GEOSCIENCE AND REMOTE SENSING LETTERS
LA English
DT Article
DE Deep learning; extreme learning machine (ELM); hyperspectral image (HSI)
   classification; local receptive field (LRF)
ID SPECTRAL-SPATIAL CLASSIFICATION
AB This letter proposes a novel classification approach for a hyperspectral image (HSI) using a hierarchical local-receptive- field (LRF)-based extreme learning machine (ELM). As a fast and accurate pattern classification algorithm, ELM has been applied in numerous fields, including the HSI classification. The LRF concept originates from research in neuroscience. Considering the local correlations of spectral features, it is promising to improve the performance of HSI classification by introducing the LRFs. Recent research on deep learning has shown that hierarchical architectures with more layers can potentially extract abstract representation and invariant features for better classification performance. Therefore, we further extend the LRF-based ELM method to a hierarchical model for HSI classification. Experimental results on two widely used real hyperspectral data sets confirm the effectiveness of the proposed HSI classification approach.
C1 [Lv, Qi; Niu, Xin; Dou, Yong; Xu, Jiaqing; Lei, Yuanwu] Natl Univ Def Technol, Sch Comp, Changsha 410073, Hunan, Peoples R China.
RP Lv, Q (reprint author), Natl Univ Def Technol, Sch Comp, Changsha 410073, Hunan, Peoples R China.
EM lvqi@nudt.edu.cn
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61125201, 61303070, U1435219, 61402507, 61402499]
FX This work was supported by the National Natural Science Foundation of
   China under Grants 61125201, 61303070, U1435219, 61402507, and 61402499.
CR Bazi Y, 2014, IEEE GEOSCI REMOTE S, V11, P1066, DOI 10.1109/LGRS.2013.2286078
   Bioucas-Dias JM, 2013, IEEE GEOSC REM SEN M, V1, P6, DOI 10.1109/MGRS.2013.2244672
   Chen C, 2014, REMOTE SENS-BASEL, V6, P5795, DOI 10.3390/rs6065795
   Chen YS, 2015, IEEE J-STARS, V8, P2381, DOI 10.1109/JSTARS.2015.2388577
   Chen YS, 2014, IEEE J-STARS, V7, P2094, DOI 10.1109/JSTARS.2014.2329330
   Huang G, 2015, NEURAL NETWORKS, V61, P32, DOI 10.1016/j.neunet.2014.10.001
   Huang GB, 2006, NEUROCOMPUTING, V70, P489, DOI 10.1016/j.neucom.2005.12.126
   Huang GB, 2015, IEEE COMPUT INTELL M, V10, P18, DOI 10.1109/MCI.2015.2405316
   Huang GB, 2012, IEEE T SYST MAN CY B, V42, P513, DOI 10.1109/TSMCB.2011.2168604
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Melgani F, 2004, IEEE T GEOSCI REMOTE, V42, P1778, DOI 10.1109/TGRS.2004.831865
   Pal M, 2009, INT J REMOTE SENS, V30, P3835, DOI 10.1080/01431160902788636
   Samat A, 2014, IEEE J-STARS, V7, P1060, DOI 10.1109/JSTARS.2014.2301775
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Soltani-Farani A, 2015, IEEE T GEOSCI REMOTE, V53, P527, DOI 10.1109/TGRS.2014.2325067
   Yue J, 2015, REMOTE SENS LETT, V6, P468, DOI 10.1080/2150704X.2015.1047045
   Zhang LF, 2015, PATTERN RECOGN, V48, P3102, DOI 10.1016/j.patcog.2014.12.016
   Zhang LF, 2012, IEEE T GEOSCI REMOTE, V50, P879, DOI 10.1109/TGRS.2011.2162339
NR 19
TC 21
Z9 21
U1 2
U2 66
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1545-598X
EI 1558-0571
J9 IEEE GEOSCI REMOTE S
JI IEEE Geosci. Remote Sens. Lett.
PD MAR
PY 2016
VL 13
IS 3
BP 434
EP 438
DI 10.1109/LGRS.2016.2517178
PG 5
WC Geochemistry & Geophysics; Engineering, Electrical & Electronic; Remote
   Sensing; Imaging Science & Photographic Technology
SC Geochemistry & Geophysics; Engineering; Remote Sensing; Imaging Science
   & Photographic Technology
GA DG1JY
UT WOS:000371824700028
DA 2020-02-19
ER

PT J
AU Sugimura, D
   Fujimura, T
   Hamamoto, T
AF Sugimura, Daisuke
   Fujimura, Takayuki
   Hamamoto, Takayuki
TI Enhanced Cascading Classifier Using Multi-Scale HOG for Pedestrian
   Detection from Aerial Images
SO INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE
LA English
DT Article
DE Pedestrian detection; aerial imagery; deep learning; online learning;
   motion estimation; low resolution image; noise
ID ORIENTED GRADIENTS; HISTOGRAMS
AB We propose a method for pedestrian detection from aerial images captured by unmanned aerial vehicles (UAVs). Aerial images are captured at considerably low resolution, and they are often subject to heavy noise and blur as a result of atmospheric influences. Furthermore, significant changes to the appearance of pedestrians frequently occur because of UAV motion. In order to address these crucial problems, we propose a cascading classifier that concatenates a pre-trained classifier and an online learning-based classifier. We construct the first classifier using deep belief network (DBN) with an extended input layer. Unlike previous approaches that use raw images as the input layer of the DBN, we exploit multi-scale histogram of oriented gradients (MSHOG) features. The MS-HOG enables us to supply better and richer information than low-resolution aerial images for constructing a reliable deep structure of DBN, because the dimensions of the input features can be expanded. Furthermore, the MS-HOG effectively extracts the necessary edge information while reducing trivial gradients and noise. The second classifier is based on online learning, and it uses predictions of the target appearance using UAV motions. Predicting the target appearance enables us to collect reliable training samples for the classifier's online learning process. Experiments using aerial videos demonstrate the effectiveness of the proposed method.
C1 [Sugimura, Daisuke; Fujimura, Takayuki; Hamamoto, Takayuki] Tokyo Univ Sci, Dept Elect Engn, Katsushika Ku, 6-3-1 Niijuku, Tokyo 1258585, Japan.
RP Sugimura, D (reprint author), Tokyo Univ Sci, Dept Elect Engn, Katsushika Ku, 6-3-1 Niijuku, Tokyo 1258585, Japan.
EM sugimura@ee.kagu.tus.ac.jp; fujimura@isl.ee.kagu.tus.ac.jp;
   hamamoto@ee.kagu.tus.ac.jp
CR Andriluka M, 2008, PROC CVPR IEEE, P1873, DOI 10.1109/CVPR.2008.4587583
   Benenson R, 2012, PROC CVPR IEEE, P2903, DOI 10.1109/CVPR.2012.6248017
   Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153
   Chalechale A, 2004, IEE P-VIS IMAGE SIGN, V151, P93, DOI 10.1049/ip-vis:20040332
   Cheng H., 2006, P IEEE C COMP VIS PA, P586
   Collins R., 2005, P IEEE INT WORKSH PE, P1
   Crammer K, 2006, J MACH LEARN RES, V7, P551
   Dalal N, 2005, PROC CVPR IEEE, P886
   Doherty P, 2007, LECT NOTES COMPUT SC, V4830, P1
   Dollar P, 2012, LECT NOTES COMPUT SC, V7573, P645, DOI 10.1007/978-3-642-33709-3_46
   Dredze M., 2009, P INT C MACH LEARN, P264
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   Gaszczak A., 2011, P SPIE INTELLIGENT R, P1
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Kanade T., 1981, P 7 INT JOINT C ART, P674, DOI DOI 10.1109/HPDC.2004.1323531
   Luo P, 2014, PROC CVPR IEEE, P899, DOI 10.1109/CVPR.2014.120
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Ouyang WL, 2013, IEEE I CONF COMP VIS, P2056, DOI 10.1109/ICCV.2013.257
   Ouyang WL, 2013, PROC CVPR IEEE, P3198, DOI 10.1109/CVPR.2013.411
   Pinheiro AMG, 2010, INT CONF ACOUST SPEE, P1250, DOI 10.1109/ICASSP.2010.5495406
   Pinheiro AMG, 2009, PROCEEDINGS 2009 FOURTH INTERNATIONAL WORKSHOP ON SEMANTIC MEDIA ADAPTATION AND PERSONALIZATION, P73, DOI 10.1109/SMAP.2009.27
   Pollard T., 2012, COMP VIS PATT REC WO, P15, DOI DOI 10.1109/CVPRW.2012.6239201
   Qian Yu, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2671, DOI 10.1109/CVPRW.2009.5206541
   Reilly V, 2010, LECT NOTES COMPUT SC, V6316, P252, DOI 10.1007/978-3-642-15567-3_19
   Rodriguez-Canosa GR, 2012, REMOTE SENS-BASEL, V4, P1090, DOI 10.3390/rs4041090
   ROSENBLATT F, 1958, PSYCHOL REV, V65, P386, DOI 10.1037/h0042519
   Rosten E, 2006, LECT NOTES COMPUT SC, V3951, P430
   ROUSSEEUW PJ, 1984, J AM STAT ASSOC, V79, P871, DOI 10.2307/2288718
   Salakhutdinov R., 2007, P INT C MACH LEARN, V24, P791, DOI DOI 10.1145/1273496.1273596
   Shi XC, 2012, INT C PATT RECOG, P2512
   Siam M., 2012, Proceedings of the 2012 IEEE International Conference on Robotics and Biomimetics (ROBIO), P2399, DOI 10.1109/ROBIO.2012.6491329
   Siam M, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P586, DOI 10.1109/ICCVW.2013.81
   Wang J., 2012, P 29 INT C MACH LEAR, P121
   Watanabe T, 2009, LECT NOTES COMPUT SC, V5414, P37
   XIAO J, 2008, IEEE C COMP VIS PATT, P1
   Xu L, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024208
   Yan JJ, 2013, PROC CVPR IEEE, P3033, DOI 10.1109/CVPR.2013.390
   Yao C, 2014, LECT NOTES COMPUT SC, V8693, P251, DOI 10.1007/978-3-319-10602-1_17
   Zeng XY, 2013, IEEE I CONF COMP VIS, P121, DOI 10.1109/ICCV.2013.22
   Zeng XY, 2014, LECT NOTES COMPUT SC, V8691, P472, DOI 10.1007/978-3-319-10578-9_31
   Zhao T, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P710, DOI 10.1109/ICCV.2001.937593
   Zhu JJ, 2014, PROC CVPR IEEE, P3510, DOI 10.1109/CVPR.2014.449
NR 42
TC 3
Z9 3
U1 1
U2 48
PU WORLD SCIENTIFIC PUBL CO PTE LTD
PI SINGAPORE
PA 5 TOH TUCK LINK, SINGAPORE 596224, SINGAPORE
SN 0218-0014
EI 1793-6381
J9 INT J PATTERN RECOGN
JI Int. J. Pattern Recognit. Artif. Intell.
PD MAR
PY 2016
VL 30
IS 3
AR 1655009
DI 10.1142/S0218001416550090
PG 23
WC Computer Science, Artificial Intelligence
SC Computer Science
GA DF1UP
UT WOS:000371125000004
DA 2020-02-19
ER

PT J
AU Xiong, C
   Liu, LQ
   Zhao, XW
   Yan, SC
   Kim, TK
AF Xiong, Chao
   Liu, Luoqi
   Zhao, Xiaowei
   Yan, Shuicheng
   Kim, Tae-Kyun
TI Convolutional Fusion Network for Face Verification in the Wild
SO IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY
LA English
DT Article
DE Deep learning; face verification; feature learning; mixture model;
   part-based representation
ID FEATURES
AB Part-based methods have seen popular applications for face verification in the wild, since they are more robust to local variations in terms of pose, illumination, and so on. However, most of the part-based approaches are built on hand-crafted features, which may not be suitable for the specific face verification purpose. In this paper, we propose to learn a part-based feature representation under the supervision of face identities through a deep model that ensures that the generated representations are more robust and suitable for face verification. The proposed framework consists of the following two deliberate components: 1) a deep mixture model (DMM) to find accurate patch correspondence and 2) a convolutional fusion network (CFN) to extract the part-based facial features. Specifically, DMM robustly depicts the spatial-appearance distribution of patch features over the faces via several Gaussian mixtures, which provide more accurate patch correspondence even in the presence of local distortions. Then, DMM only feeds the patches which preserve the identity information to the following CFN. The proposed CFN is a two-layer cascade of convolutional neural networks: 1) a local layer built on face patches to deal with local variations and 2) a fusion layer integrating the responses from the local layer. CFN jointly learns and fuses multiple local responses to optimize the verification performance. The composite representation obtained possesses certain robustness to pose and illumination variations and shows comparable performance with the state-of-the-art methods on two benchmark data sets.
C1 [Xiong, Chao; Zhao, Xiaowei; Kim, Tae-Kyun] Univ London Imperial Coll Sci Technol & Med, Dept Elect & Elect Engn, London SW7 2AZ, England.
   [Liu, Luoqi; Yan, Shuicheng] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 119077, Singapore.
RP Xiong, C; Zhao, XW; Kim, TK (reprint author), Univ London Imperial Coll Sci Technol & Med, Dept Elect & Elect Engn, London SW7 2AZ, England.; Liu, LQ; Yan, SC (reprint author), Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 119077, Singapore.
EM chao.xiong10@imperial.ac.uk; llq667@gmail.com; x.zhao@imperial.ac.uk;
   eleyans@nus.edu.sg; tk.kim@imperial.ac.uk
FU Engineering and Physical Sciences Research CouncilEngineering & Physical
   Sciences Research Council (EPSRC) [EP/N007743/1, EP/J012106/1]
CR Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   CHOPRA S, 2005, PROC CVPR IEEE, P539, DOI DOI 10.1109/CVPR.2005.202
   Cui Z, 2013, PROC CVPR IEEE, P3554, DOI 10.1109/CVPR.2013.456
   Erhan D, 2010, J MACH LEARN RES, V11, P625
   Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton G. E., 2012, IMPROVING NEURAL NET
   Hu J., 1875, IEEE C COMP VIS PATT
   Hu Junlin, 2014, P AS C COMP VIS, P252
   Hua G, 2009, IEEE I CONF COMP VIS, P2082, DOI 10.1109/ICCV.2009.5459457
   Huang G., 2012, ADV NEURAL INFORM PR, V25, P773
   Huang GB, 2012, PROC CVPR IEEE, P2518, DOI 10.1109/CVPR.2012.6247968
   Huang Gary B., 2007, 0749 U MASS
   Hussain SU, 2012, BRIT MACH VIS C, P11
   Kim TK, 2005, IMAGE VISION COMPUT, V23, P631, DOI 10.1016/j.imavis.2005.02.005
   Kim TK, 2003, PROC CVPR IEEE, P579
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li H., 2014, P AS C COMP VIS, P17, DOI 10.1007/978-3-319-16811-1_2
   Li HX, 2013, PROC CVPR IEEE, P3499, DOI 10.1109/CVPR.2013.449
   Li JY, 2013, PROCEEDINGS OF THE 2013 INTERNATIONAL CONFERENCE ON ENERGY, P1
   Liao Q., 2013, CAN BIOL PLAUSIBLE H
   Liu CJ, 2002, IEEE T IMAGE PROCESS, V11, P467, DOI 10.1109/TIP.2002.999679
   Liu D., 2012, J GUANGDONG IND U, V12, P12
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu JW, 2015, IEEE T INF FOREN SEC, V10, P79, DOI 10.1109/TIFS.2014.2363792
   Luo P, 2012, PROC CVPR IEEE, P2480, DOI 10.1109/CVPR.2012.6247963
   Mendez-Vazquez H., 2013, P ICB JUN, P1, DOI DOI 10.1109/ICB.2013.6612990
   Nair V., 2010, P 27 INT C MACH LEAR, P1
   Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4
   Ouyang WL, 2013, IEEE I CONF COMP VIS, P2056, DOI 10.1109/ICCV.2013.257
   Pinto Nicolas, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2591, DOI 10.1109/CVPRW.2009.5206605
   Sikka K, 2012, LECT NOTES COMPUT SC, V7584, P250, DOI 10.1007/978-3-642-33868-7_25
   Simonyan K, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.8
   Sun Y, 2014, ADV NEURAL INFORM PR, V60, P1988
   Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244
   Sun Y, 2013, PROC CVPR IEEE, P3476, DOI 10.1109/CVPR.2013.446
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Tan XY, 2010, IEEE T IMAGE PROCESS, V19, P1635, DOI 10.1109/TIP.2010.2042645
   Turk M. A., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P586, DOI 10.1109/CVPR.1991.139758
   Weston J, 2001, ADV NEUR IN, V13, P668
   Wolf L., 2008, P REAL LIF IMI WORKS
   Wolf L, 2013, PROC CVPR IEEE, P3523, DOI 10.1109/CVPR.2013.452
   Wolf L, 2011, PROC CVPR IEEE, P529, DOI 10.1109/CVPR.2011.5995566
   Wright J, 2009, PROC CVPR IEEE, P1502, DOI 10.1109/CVPRW.2009.5206786
   Zhai Y., 2012, P 29 INT C MACH LEAR, P1455
NR 45
TC 17
Z9 18
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1051-8215
EI 1558-2205
J9 IEEE T CIRC SYST VID
JI IEEE Trans. Circuits Syst. Video Technol.
PD MAR
PY 2016
VL 26
IS 3
BP 517
EP 528
DI 10.1109/TCSVT.2015.2406191
PG 12
WC Engineering, Electrical & Electronic
SC Engineering
GA DH1MD
UT WOS:000372547400008
DA 2020-02-19
ER

PT J
AU Romero, A
   Gatta, C
   Camps-Valls, G
AF Romero, Adriana
   Gatta, Carlo
   Camps-Valls, Gustau
TI Unsupervised Deep Feature Extraction for Remote Sensing Image
   Classification
SO IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING
LA English
DT Article
DE Aerial image classification; classification; deep convolutional
   networks; deep learning; feature extraction; hyperspectral (HS) image;
   multispectral (MS) images; segmentation; sparse features learning; very
   high resolution (VHR)
ID NEURAL-NETWORKS; CLOUD; ENVIRONMENT; ALGORITHM
AB This paper introduces the use of single-layer and deep convolutional networks for remote sensing data analysis. Direct application to multi-and hyperspectral imagery of supervised (shallow or deep) convolutional networks is very challenging given the high input data dimensionality and the relatively small amount of available labeled data. Therefore, we propose the use of greedy layerwise unsupervised pretraining coupled with a highly efficient algorithm for unsupervised learning of sparse features. The algorithm is rooted on sparse representations and enforces both population and lifetime sparsity of the extracted features, simultaneously. We successfully illustrate the expressive power of the extracted representations in several scenarios: classification of aerial scenes, as well as land-use classification in very high resolution or land-cover classification from multi-and hyperspectral images. The proposed algorithmclearly outperforms standard principal component analysis (PCA) and its kernel counterpart (kPCA), as well as current state-of-the-art algorithms of aerial classification, while being extremely computationally efficient at learning representations of data. Results show that single-layer convolutional networks can extract powerful discriminative features only when the receptive field accounts for neighboring pixels and are preferred when the classification requires high resolution and detailed results. However, deep architectures significantly outperform single-layer variants, capturing increasing levels of abstraction and complexity throughout the feature hierarchy.
C1 [Romero, Adriana] Univ Barcelona, Dept Appl Math & Anal, E-08007 Barcelona, Spain.
   [Gatta, Carlo] Univ Autonoma Barcelona, Comp Vis Ctr, Barcelona 01873, Spain.
   [Camps-Valls, Gustau] Univ Valencia, Image Proc Lab, Valencia 46980, Spain.
RP Romero, A (reprint author), Univ Barcelona, Dept Appl Math & Anal, E-08007 Barcelona, Spain.; Gatta, C (reprint author), Univ Autonoma Barcelona, Comp Vis Ctr, Barcelona 01873, Spain.; Camps-Valls, G (reprint author), Univ Valencia, Image Proc Lab, Valencia 46980, Spain.
EM adriana.romero@ub.edu; cgatta@cvc.uab.es; gcamps@uv.es
RI Camps-Valls, Gustau/A-2532-2011
OI Camps-Valls, Gustau/0000-0003-1683-2138
CR Arce GR, 2014, IEEE SIGNAL PROC MAG, V31, P105, DOI 10.1109/MSP.2013.2278763
   Arenas-Garcia J, 2013, IEEE SIGNAL PROC MAG, V30, P16, DOI 10.1109/MSP.2013.2250591
   Arriaza JAT, 2003, IEEE T GEOSCI REMOTE, V41, P826, DOI 10.1109/TGRS.2003.809930
   Bachmann CM, 2006, IEEE T GEOSCI REMOTE, V44, P2786, DOI 10.1109/TGRS.2006.881801
   BARALDI A, 1995, IEEE T GEOSCI REMOTE, V33, P305, DOI 10.1109/36.377930
   Bengio Y, 2006, ADV NEURAL INFORM PR, P153
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Camps-Valls G, 2005, IEEE T GEOSCI REMOTE, V43, P1351, DOI 10.1109/TGRS.2005.846154
   Camps-Valls G., 2009, KERNEL METHODS REMOT
   Camps-Valls G., 2011, REMOTE SENSING IMAGE
   Camps-Valls G, 2014, IEEE SIGNAL PROC MAG, V31, P45, DOI 10.1109/MSP.2013.2279179
   Chen XY, 2014, IEEE GEOSCI REMOTE S, V11, P1797, DOI 10.1109/LGRS.2014.2309695
   Chen YS, 2014, IEEE J-STARS, V7, P2094, DOI 10.1109/JSTARS.2014.2329330
   Cheriyadat AM, 2014, IEEE T GEOSCI REMOTE, V52, P439, DOI 10.1109/TGRS.2013.2241444
   Coates A., 2011, P 28 INT C MACH LEAR, V28, P921
   Dai DX, 2011, IEEE GEOSCI REMOTE S, V8, P173, DOI 10.1109/LGRS.2010.2055033
   Del F. F., 2009, HYPERSPECTRAL IMAGE, P1, DOI [10.1109/WHISPERS.2009.5288997, DOI 10.1109/WHISPERS.2009.5288997]
   Donlon C, 2012, REMOTE SENS ENVIRON, V120, P37, DOI 10.1016/j.rse.2011.07.024
   Drusch M, 2012, REMOTE SENS ENVIRON, V120, P25, DOI 10.1016/j.rse.2011.11.026
   FIELD DJ, 1994, NEURAL COMPUT, V6, P559, DOI 10.1162/neco.1994.6.4.559
   Garcia-Vilchez F, 2011, IEEE GEOSCI REMOTE S, V8, P253, DOI 10.1109/LGRS.2010.2062484
   Ghosh S, 2007, IEEE T GEOSCI REMOTE, V45, P778, DOI 10.1109/TGRS.2006.888861
   Gomez-Chova L, 2008, IEEE GEOSCI REMOTE S, V5, P336, DOI 10.1109/LGRS.2008.916070
   Gomez-Chova L, 2007, IEEE T GEOSCI REMOTE, V45, P4105, DOI 10.1109/TGRS.2007.905312
   Gomez-Chova L, 2010, IEEE T GEOSCI REMOTE, V48, P207, DOI 10.1109/TGRS.2009.2026425
   Hill AC, 2010, ECOL SOC, V15
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Jolliffe I, 2002, PRINCIPAL COMPONENT
   Kavukcuoglu K., 2010, ADV NEURAL INFORM PR, V23, P1090
   Kraft S, 2012, INT GEOSCI REMOTE SE, P7125, DOI 10.1109/IGARSS.2012.6352020
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Laparra V, 2011, IEEE T NEURAL NETWOR, V22, P537, DOI 10.1109/TNN.2011.2106511
   LeCun Y, 1998, LECT NOTES COMPUT SC, V1524, P9
   LeCun Y., 2001, INTELLIGENT SIGNAL P, P306
   Lee H., 2009, P 26 ANN INT C MACH, P609, DOI DOI 10.1145/1553374.1553453
   Lee JA, 2007, INFORM SCI STAT, P1
   Li ST, 2013, IEEE T GEOSCI REMOTE, V51, P4779, DOI 10.1109/TGRS.2012.2230332
   Liang S, 2004, QUANTITATIVE REMOTE
   Licciardi G, 2009, INT GEOSCI REMOTE SE, P176, DOI 10.1109/IGARSS.2009.5416882
   Lillesand TM, 2008, REMOTE SENSING IMAGE
   Longbotham N., 2014, P WHISPERS LAUS SWIT, P479
   Ma WK, 2014, IEEE SIGNAL PROC MAG, V31, P67, DOI 10.1109/MSP.2013.2279731
   Masci J, 2011, LECT NOTES COMPUT SC, V6791, P52, DOI 10.1007/978-3-642-21735-7_7
   McIntire TJ, 2002, IEEE T GEOSCI REMOTE, V40, P1956, DOI 10.1109/TGRS.2002.803728
   Mnih V., 2012, P 29 INT C MACH LEAR, P567
   Ngiam J., 2011, ADV NEURAL INFORM PR, P1125
   Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7
   Ranzato M., 2006, ADV NEURAL INFORM PR, V19, P1137
   Rigas I, 2013, IEEE GEOSCI REMOTE S, V10, P1389, DOI 10.1109/LGRS.2013.2243402
   Roberts DA, 2012, REMOTE SENS ENVIRON, V117, P83, DOI 10.1016/j.rse.2011.07.021
   Romero A, 2015, IEEE T PATTERN ANAL, V37, P1716, DOI 10.1109/TPAMI.2014.2366129
   Roy DP, 2014, REMOTE SENS ENVIRON, V145, P154, DOI 10.1016/j.rse.2014.02.001
   Schaul T., 2013, COMP INT GAM CIG 201, P1
   Sermanet P., 2014, P INT C LEARN REPR, P1, DOI DOI 10.1016/J.VISRES.2006.11.009
   Shaw G, 2002, IEEE SIGNAL PROC MAG, V19, P12, DOI 10.1109/79.974715
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Stuffler T, 2007, ACTA ASTRONAUT, V61, P115, DOI 10.1016/j.actaastro.2007.01.033
   Sun H, 2012, IEEE GEOSCI REMOTE S, V9, P109, DOI 10.1109/LGRS.2011.2161569
   Tian B, 1999, IEEE T NEURAL NETWOR, V10, P138, DOI 10.1109/72.737500
   Vaduva C, 2012, EUR SIGNAL PR CONF, P2506
   Wang ZW, 2014, IEEE T GEOSCI REMOTE, V52, P4808, DOI 10.1109/TGRS.2013.2285049
   Whited DC, 2013, RIVER RES APPL, V29, P135, DOI 10.1002/rra.1585
   Willett RM, 2014, IEEE SIGNAL PROC MAG, V31, P116, DOI 10.1109/MSP.2013.2279507
   Willmore B, 2001, NETWORK-COMP NEURAL, V12, P255, DOI 10.1088/0954-898X/12/3/302
   Yang SY, 2014, IEEE GEOSCI REMOTE S, V11, P479, DOI 10.1109/LGRS.2013.2268847
   Yang Y., 2010, P 18 SIGSPATIAL INT, P270, DOI DOI 10.1145/1869790.1869829
NR 67
TC 245
Z9 255
U1 55
U2 393
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 0196-2892
EI 1558-0644
J9 IEEE T GEOSCI REMOTE
JI IEEE Trans. Geosci. Remote Sensing
PD MAR
PY 2016
VL 54
IS 3
BP 1349
EP 1362
DI 10.1109/TGRS.2015.2478379
PG 14
WC Geochemistry & Geophysics; Engineering, Electrical & Electronic; Remote
   Sensing; Imaging Science & Photographic Technology
SC Geochemistry & Geophysics; Engineering; Remote Sensing; Imaging Science
   & Photographic Technology
GA DG8YL
UT WOS:000372369400009
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Gunther, J
   Pilarski, PM
   Helfrich, G
   Shen, H
   Diepold, K
AF Guenther, Johannes
   Pilarski, Patrick M.
   Helfrich, Gerhard
   Shen, Hao
   Diepold, Klaus
TI Intelligent laser welding through representation, prediction, and
   control learning: An architecture with deep neural networks and
   reinforcement learning
SO MECHATRONICS
LA English
DT Article
DE Deep learning; Reinforcement learning; Prediction; Control; Laser
   welding
ID REAL-TIME; BEHAVIOR
AB Laser welding is a widely used but complex industrial process. In this work, we propose the use of an integrated machine intelligence architecture to help address the significant control difficulties that prevent laser welding from seeing its full potential in process engineering and production. This architecture combines three contemporary machine learning techniques to allow a laser welding controller to learn and improve in a self-directed manner. As a first contribution of this work, we show how a deep, auto-encoding neural network is capable of extracting salient, low-dimensional features from real high-dimensional laser welding data. As a second contribution and novel integration step, these features are then used as input to a temporal-difference learning algorithm (in this case a general-value-function learner) to acquire important real-time information about the process of laser welding; temporally extended predictions are used in combination with deep learning to directly map sensor data to the final quality of a welding seam. As a third contribution and final part of our proposed architecture, we suggest that deep learning features and general-value-function predictions can be beneficially combined with actor-critic reinforcement learning to learn context-appropriate control policies to govern welding power in real time. Preliminary control results are demonstrated using multiple runs with a laser welding simulator. The proposed intelligent laser-welding architecture combines representation, prediction, and control learning: three of the main hallmarks of an intelligent system. As such, we suggest that an integration approach like the one described in this work has the capacity to improve laser welding performance without ongoing and time-intensive human assistance. Our architecture therefore promises to address several key requirements of modern industry. To our knowledge, this architecture is the first demonstrated combination of deep learning and general value functions. It also represents the first use of deep learning for laser welding specifically and production engineering in general. We believe that it would be straightforward to adapt our architecture for use in other industrial and production engineering settings. (C) 2015 Elsevier Ltd. All rights reserved.
C1 [Guenther, Johannes; Helfrich, Gerhard; Shen, Hao; Diepold, Klaus] Tech Univ Munich, Dept Elect & Comp Engn, D-80290 Munich, Germany.
   [Pilarski, Patrick M.] Univ Alberta, Dept Med, Edmonton, AB T6G 2E1, Canada.
RP Gunther, J (reprint author), Tech Univ Munich, Dept Elect & Comp Engn, D-80290 Munich, Germany.
EM johannes.guenther@tum.de
FU Federal Ministry of Education and Research - GermanyFederal Ministry of
   Education & Research (BMBF); Alberta Innovates Centre for Machine
   Learning (Canada); Natural Sciences and Engineering Research Council of
   Canada (NSERC)Natural Sciences and Engineering Research Council of
   Canada; Alberta Innovates Technology Futures (Canada)
FX The project CCLW is performed within the framework of the European
   funding program Eurostars and is funded by the Federal Ministry of
   Education and Research - Germany. We also would like to thank all
   project partners, namely the Precitec GmbH & Co. KG, Germany and IREPA
   LASER, Illkirch, France, who provided the laser-welding data. The
   authors also acknowledge support from the Alberta Innovates Centre for
   Machine Learning and Alberta Innovates Technology Futures (Canada), the
   Natural Sciences and Engineering Research Council of Canada (NSERC), and
   thank Richard Sutton and Joseph Modayil for a number of helpful
   conversations.
CR Alippi C, 2001, IEEE IMTC P, P1762, DOI 10.1109/IMTC.2001.929503
   Ancona A, 2001, APPL OPTICS, V40, P6019, DOI 10.1364/AO.40.006019
   Beersiek J., 2001, ICALEO, P1185
   Bengio Yoshua, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P437, DOI 10.1007/978-3-642-35289-8_26
   Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153
   Bergstra J, 2012, J MACH LEARN RES, V13, P281
   Clark A, 2013, BEHAV BRAIN SCI, V36, P181, DOI 10.1017/S0140525X12000477
   Cotter A., 2011, C NEUR INF PROC SYST, V24, P1647
   Degris T, 2012, P AMER CONTR CONF, P2177
   GILBERT D, 2007, STUMBLING HAPPINESS
   Gunther J, 2014, PROC TECH, V15, P474, DOI 10.1016/j.protcy.2014.09.007
   Haykin S, 2012, P IEEE, V100, P3156, DOI 10.1109/JPROC.2012.2215773
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Jager M, 2008, IEEE T IND ELECTRON, V55, P2177, DOI 10.1109/TIE.2008.918637
   Kimura H, 2001, IEEE DECIS CONTR P, P411, DOI 10.1109/CDC.2001.980135
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   KROOS J, 1993, J PHYS D APPL PHYS, V26, P481, DOI 10.1088/0022-3727/26/3/022
   Lange S, 2012, P INT JOINT C NEUR N, P1
   Littman M. L., 2001, NIPS, V14, P1555
   Liu F, 2005, IEEE IJCNN, P809
   Min-Ying H, 2010, MACE, P3734
   Mnih V, 2013, NIPS DEEP LEARN WORK
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Modayil J, 2014, ADAPT BEHAV, V22, P146, DOI 10.1177/1059712313511648
   Modayil J, 2012, IEEE SYS MAN CYBERN, P1903, DOI 10.1109/ICSMC.2012.6378016
   Moody J., 1995, ADV NEURAL INFORM PR, V4, P950
   Peters J, 2008, NEUROCOMPUTING, V71, P1180, DOI 10.1016/j.neucom.2007.11.026
   Pilarski Patrick M, 2013, IEEE Int Conf Rehabil Robot, V2013, P6650435, DOI 10.1109/ICORR.2013.6650435
   Pilarski PM, 2011, INT C REHAB ROBOT
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Schroth G, 2009, 2009 IEEE-RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, P3148, DOI 10.1109/IROS.2009.5354449
   Serre T, 2007, PROG BRAIN RES, V165, P33, DOI 10.1016/S0079-6123(06)65004-8
   Shao J, 2005, J PHYS CONF SER, V15, P101, DOI 10.1088/1742-6596/15/1/017
   Sutskever I., 2013, P 30 INT C MACH LEAR, P1139
   Sutton R. S., 2011, 10 INT C AUT AG MULT, V10, P761
   Sutton Richard S., 1998, INTRO REINFORCEMENT
   Thomas Philip, 2009, Proc Innov Appl Artif Intell Conf, V2009, P165
   Vincent P, 2008, P 25 INT C MACH LEAR, P1096, DOI DOI 10.1145/1390156.1390294
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Xavier Glorot, 2011, P 14 INT C ART INT S, P315, DOI DOI 10.1177/1753193410395357
   Zeiler MD, 2013, INT CONF ACOUST SPEE, P3517, DOI 10.1109/ICASSP.2013.6638312
   Zipkin P, 2001, MIT SLOAN MANAGE REV
NR 42
TC 23
Z9 23
U1 10
U2 93
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0957-4158
J9 MECHATRONICS
JI Mechatronics
PD MAR
PY 2016
VL 34
SI SI
BP 1
EP 11
DI 10.1016/j.mechatronics.2015.09.004
PG 11
WC Automation & Control Systems; Engineering, Electrical & Electronic;
   Engineering, Mechanical; Robotics
SC Automation & Control Systems; Engineering; Robotics
GA DN1EM
UT WOS:000376809100002
DA 2020-02-19
ER

PT J
AU Liu, YN
   Feng, XQ
   Zhou, ZG
AF Liu, Yanan
   Feng, Xiaoqing
   Zhou, Zhiguang
TI Multimodal video classification with stacked contractive autoencoders
SO SIGNAL PROCESSING
LA English
DT Article
DE Multimodal; Video classification; Deep learning; Stacked contractive
   autoencoder
ID EVENT DETECTION; FUSION
AB In this paper we propose a multimodal feature learning mechanism based on deep networks (i.e., stacked contractive autoencoders) for video classification. Considering the three modalities in video, i.e., image, audio and text, we first build one Stacked Contractive Autoencoder (SCAE) for each single modality, whose outputs will be joint together and fed into another Multimodal Stacked Contractive Autoencoder (MSCAE). The first stage preserves intra-modality semantic relations and the second stage discovers inter-modality semantic correlations. Experiments on real world dataset demonstrate that the proposed approach achieves better performance compared with the state-of-the-art methods. (C) 2015 Elsevier B.V. All rights reserved.
C1 [Liu, Yanan; Feng, Xiaoqing; Zhou, Zhiguang] Zhejiang Univ Finance & Econ, Hangzhou, Zhejiang, Peoples R China.
RP Liu, YN (reprint author), Zhejiang Univ Finance & Econ, Hangzhou, Zhejiang, Peoples R China.
EM liuyn@zju.edu.cn; fxq_snake@163.com; zhouzhiguang@zjucadcg.cn
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61100084, 61202197, 61303133]; Zhejiang Province
   Department of Education [Y201223321]; Zhejiang Provincial Natural
   Science Foundation of ChinaNatural Science Foundation of Zhejiang
   Province [LQ13F020003]
FX This work is supported by the National Natural Science Foundation of
   China (Nos. 61100084, 61202197, 61303133), Zhejiang Province Department
   of Education Fund (No. Y201223321) and Zhejiang Provincial Natural
   Science Foundation of China (No. LQ13F020003).
CR Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bronstein MM, 2010, PROC CVPR IEEE, P3594, DOI 10.1109/CVPR.2010.5539928
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Frome A., 2013, ADV NEURAL INFORM PR, P2121
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Li G., 2012, ACM T MULTIM COMPUT, V8, P1
   Ma ZG, 2014, IEEE T PATTERN ANAL, V36, P1789, DOI 10.1109/TPAMI.2014.2306419
   Ma Z, 2013, IEEE T MULTIMEDIA, V15, P1628, DOI 10.1109/TMM.2013.2264928
   Ngiam J, 2011, P 28 INT C MACH LEAR
   Ranzato M., 2006, ADV NEURAL INFORM PR, V19, P1137
   Salah R, 2011, P 28 INT C MACH LEAR, P833, DOI 10.1016/j.wasman.2010.10.006
   Shen H., 2014, MOL PHYS, P1
   Snoek C. G. M., 2006, P 14 ANN ACM INT C M, P421, DOI DOI 10.1145/1180639.1180727
   Srivastava N, 2014, J MACH LEARN RES, V15, P2949
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Wang M, 2012, IEEE T IMAGE PROCESS, V21, P4649, DOI 10.1109/TIP.2012.2207397
   Wang M, 2012, ACM COMPUT SURV, V44, DOI 10.1145/2333112.2333120
   Wang M, 2009, IEEE T MULTIMEDIA, V11, P465, DOI 10.1109/TMM.2009.2012919
   Wang M, 2009, IEEE T CIRC SYST VID, V19, P733, DOI 10.1109/TCSVT.2009.2017400
   Wang W., 2014, P INT C VER LARG DAT
   Xia Y., SCI WORLD J
   Xia YJ, 2013, INT J INTELL SYST, V28, P540, DOI 10.1002/int.21592
   Xu ZW, 2013, IEEE I CONF COMP VIS, P3440, DOI 10.1109/ICCV.2013.427
   Yang Y, 2013, IEEE I CONF COMP VIS, P2104, DOI 10.1109/ICCV.2013.456
   Yang Y, 2013, IEEE T MULTIMEDIA, V15, P572, DOI 10.1109/TMM.2012.2234731
   Zhang LF, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0099946
   Zhang LM, 2014, IEEE T CYBERNETICS, V44, P1408, DOI 10.1109/TCYB.2013.2285219
   Zhang LM, 2014, IEEE T IMAGE PROCESS, V23, DOI 10.1109/TIP.2014.2303650
   Zhang LM, 2014, INFORM SCIENCES, V254, P141, DOI 10.1016/j.ins.2013.08.020
   Zhang LM, 2013, IEEE T IMAGE PROCESS, V22, P5071, DOI 10.1109/TIP.2013.2278465
   Zhang LM, 2013, SIGNAL PROCESS, V93, P1597, DOI 10.1016/j.sigpro.2012.05.012
   Zhuang YT, 2008, IEEE T MULTIMEDIA, V10, P221, DOI 10.1109/TMM.2007.911822
NR 33
TC 32
Z9 35
U1 4
U2 46
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0165-1684
EI 1872-7557
J9 SIGNAL PROCESS
JI Signal Process.
PD MAR
PY 2016
VL 120
BP 761
EP 766
DI 10.1016/j.sigpro.2015.01.001
PG 6
WC Engineering, Electrical & Electronic
SC Engineering
GA DA4FB
UT WOS:000367754400069
DA 2020-02-19
ER

PT J
AU Aryal, S
   Gutierrez-Osuna, R
AF Aryal, Sandesh
   Gutierrez-Osuna, Ricardo
TI Data driven articulatory synthesis with deep neural networks
SO COMPUTER SPEECH AND LANGUAGE
LA English
DT Article
DE Articulatory synthesis; Electromagnetic articulography; Deep learning;
   Gaussian mixture models
ID BOLTZMANN MACHINES; CONVERSION; MOVEMENTS; MODEL
AB The conventional approach for data-driven articulatory synthesis consists of modeling the joint acoustic-articulatory distribution with a Gaussian mixture model (GMM), followed by a post-processing step that optimizes the resulting acoustic trajectories. This final step can significantly improve the accuracy of the GMM frame-by-frame mapping but is computationally intensive and requires that the entire utterance be synthesized beforehand, making it unsuited for real-time synthesis. To address this issue, we present a deep neural network (DNN) articulatory synthesizer that uses a tapped-delay input line, allowing the model to capture context information in the articulatory trajectory without the need for post-processing. We characterize the DNN as a function of the context size and number of hidden layers, and compare it against two GMM articulatory synthesizers, a baseline model that performs a simple frame-by-frame mapping, and a second model that also performs trajectory optimization. Our results show that a DNN with a 60-ms context window and two 512-neuron hidden layers can synthesize speech at four times the frame rate comparable to frame-by-frame mappings, while improving the accuracy of trajectory optimization (a 9.8% reduction in Mel Cepstral distortion). Subjective evaluation through pairwise listening tests also shows a strong preference toward the DNN articulatory synthesizer when compared to GMM trajectory optimization. (C) 2015 Elsevier Ltd. All rights reserved.
C1 [Aryal, Sandesh; Gutierrez-Osuna, Ricardo] Texas A&M Univ, Dept Comp Sci & Engn, College Stn, TX 77843 USA.
RP Gutierrez-Osuna, R (reprint author), Texas A&M Univ, Dept Comp Sci & Engn, College Stn, TX 77843 USA.
EM sandesh@cse.tamu.edu; rgutier@cse.tamu.edu
FU NSFNational Science Foundation (NSF) [0713205]
FX This work was supported by NSF award 0713205. We are grateful to Prof.
   Steve Renals and the Scottish Informatics and Computer Science Alliance
   (SICSA) for their support during RGO's sabbatical stay at CSTR
   (University of Edinburgh), and to Dr. Christian Geng for his assistance
   in performing the EMA recordings. We are also grateful to Prof. Kawahara
   for permission to use the STRAIGHT analysis-synthesis method. We would
   also like to thank anonymous reviewers whose suggestions and comments
   helped us improve the quality of the paper.
CR Andrew G., 2013, P 30 INT C MACH LEAR, P1247
   Arora R, 2013, INT CONF ACOUST SPEE, P7135, DOI 10.1109/ICASSP.2013.6639047
   Aryal S., 2014, P ICASSP FLOR IT, P7744
   Aryal S, 2013, INT CONF ACOUST SPEE, P7952, DOI 10.1109/ICASSP.2013.6639213
   Bao YB, 2012, INT CONF SIGN PROCES, P562, DOI 10.1109/ICoSP.2012.6491550
   Birkholz P, 2006, INT CONF ACOUST SPEE, P873
   Browman C.P., 1984, J ACOUST SOC AM, V75, pS22
   Cho K., 2013, ARXIV13013468
   Cho K., 2013, MATLAB CODE RESTRICT
   Cho K. H., 2013, INT JOINT C NEUR NET, P1, DOI [10.1109/IJCNN.2013.6706831, DOI 10.1109/IJCNN.2013.6706831]
   Denby B, 2010, SPEECH COMMUN, V52, P270, DOI 10.1016/j.specom.2009.08.002
   Erhan D, 2010, J MACH LEARN RES, V11, P625
   Felps D., 2014, P ICASSP FLOR IT, P3051
   Felps D, 2012, IEEE T AUDIO SPEECH, V20, P2301, DOI 10.1109/TASL.2012.2201474
   Geng C, 2009, J ACOUST SOC AM, V125, P3278, DOI 10.1121/1.3106130
   Ghosh PK, 2011, INT CONF ACOUST SPEE, P4624
   Hermansky H., 1989, ICASSP-89: 1989 International Conference on Acoustics, Speech and Signal Processing (IEEE Cat. No.89CH2673-2), P480, DOI 10.1109/ICASSP.1989.266468
   Hinton G., 2012, NEURAL NETWORKS TRIC, V9, P599, DOI DOI 10.1007/978-3-642-35289-8_32
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Hiroya S, 2004, IEEE T SPEECH AUDI P, V12, P175, DOI 10.1109/TSA.2003.822636
   Hu AQ, 2014, CHIN CONT DECIS CONF, P3033, DOI 10.1109/CCDC.2014.6852695
   ITU-T, 2003, REC G 114 ON WAY TRA
   Ji A., 2014, P ICASSP, P7769
   Kaburagi T., 1998, P ICSLP, P433
   Kawahara H, 1997, INT CONF ACOUST SPEE, P1303, DOI 10.1109/ICASSP.1997.596185
   Kello CT, 2004, J ACOUST SOC AM, V116, P2354, DOI 10.1121/1.1715112
   MAEDA S, 1990, NATO ADV SCI I D-BEH, V55, P131
   MERMELSTEIN P, 1973, J ACOUST SOC AM, V53, P1070, DOI 10.1121/1.1913427
   Muramatsu T, 2008, INTERSPEECH 2008: 9TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2008, VOLS 1-5, P1076
   NABNEY IT, 2002, ADV PTRN RECOGNIT, P1
   Nakamura K., 2006, P ICASSP, pI
   Nakashika T, 2013, INTERSPEECH, P369
   Narayanan  S., 2011, INTERSPEECH, P837
   OZBEK IY, 2009, P INTERSPEECH, P2807
   Prabhavalkar R., 2011, 2011 IEEE Workshop on Automatic Speech Recognition & Understanding (ASRU), P77, DOI 10.1109/ASRU.2011.6163909
   Qin C, 2007, INTERSPEECH 2007: 8TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION, VOLS 1-4, P2300
   Richmond K, 2003, COMPUT SPEECH LANG, V17, P153, DOI 10.1016/S0885-2308(03)00005-6
   Rudzicz F., 2010, P 2010 IEEE INT C AC, P4198
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Salakhutdinov R, 2009, ARTIF INTELL, P448, DOI DOI 10.1109/CVPRW.2009.5206577
   Toda T., 2004, P ISCA, pSSW5
   Toda T, 2008, SPEECH COMMUN, V50, P215, DOI 10.1016/j.specom.2007.09.001
   Toda T, 2007, IEEE T AUDIO SPEECH, V15, P2222, DOI 10.1109/TASL.2007.907344
   Toda T, 2012, 13TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2012 (INTERSPEECH 2012), VOLS 1-3, P94
   Toth A.R., 2005, P INTERSPEECH, P2973
   Uria B., 2012, INTERSPEECH, P867
   Westbury J, 1994, XRAY MICROBEAM SPEEC
   Wrench A. A., 2000, P 5 SEM SPEECH PROD, P305
   Xu Y, 2014, IEEE SIGNAL PROC LET, V21, P65, DOI 10.1109/LSP.2013.2291240
   You Z, 2013, INT CONF ACOUST SPEE, P7600, DOI 10.1109/ICASSP.2013.6639141
   Zen HG, 2013, INT CONF ACOUST SPEE, P7962, DOI 10.1109/ICASSP.2013.6639215
   Zhang YD, 2012, INT CONF ACOUST SPEE, P5161, DOI 10.1109/ICASSP.2012.6289082
NR 52
TC 13
Z9 13
U1 1
U2 48
PU ACADEMIC PRESS LTD- ELSEVIER SCIENCE LTD
PI LONDON
PA 24-28 OVAL RD, LONDON NW1 7DX, ENGLAND
SN 0885-2308
EI 1095-8363
J9 COMPUT SPEECH LANG
JI Comput. Speech Lang.
PD MAR
PY 2016
VL 36
BP 260
EP 273
DI 10.1016/j.csl.2015.02.003
PG 14
WC Computer Science, Artificial Intelligence
SC Computer Science
GA CZ5DP
UT WOS:000367123000015
DA 2020-02-19
ER

PT J
AU Xin, M
   Zhang, H
   Wang, HL
   Sun, MG
   Yuan, D
AF Xin, Miao
   Zhang, Hong
   Wang, Helong
   Sun, Mingui
   Yuan, Ding
TI ARCH: Adaptive recurrent-convolutional hybrid networks for long-term
   action recognition
SO NEUROCOMPUTING
LA English
DT Article
DE Action recognition; Deep learning; Hybrid feature learning
ID TRAJECTORIES; PERCEPTION
AB Recognition of human actions from digital video is a challenging task due to complex interfering factors in uncontrolled realistic environments. In this paper, we propose a learning framework using static, dynamic and sequential mixed features to solve three fundamental problems: spatial domain variation, temporal domain polytrope, and intra- and inter-class diversities. Utilizing a cognitive-based data reduction method and a hybrid "network upon networks" architecture, we extract human action representations which are robust against spatial and temporal interferences and adaptive to variations in both action speed and duration. We evaluated our method on the UCF101 and other three challenging datasets. Our results demonstrated a superior performance of the proposed algorithm in human action recognition. (C) 2015 Elsevier B.V. All rights reserved.
C1 [Xin, Miao] Beihang Univ, Image Proc Ctr, Sch Astronaut, Beijing 100191, Peoples R China.
   [Zhang, Hong; Yuan, Ding] Beihang Univ, Image Proc Ctr, Beijing 100191, Peoples R China.
   [Wang, Helong] Luoyang Electroopt Equipment Res Inst, Luoyang, Peoples R China.
   [Sun, Mingui] Univ Pittsburgh, Lab Computat Neurosci, Pittsburgh, PA 15260 USA.
RP Yuan, D (reprint author), Beihang Univ, Image Proc Ctr, Beijing 100191, Peoples R China.
EM dyuan@buaa.edu.cn
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61272351]; National Institutes of Health of the
   United StatesUnited States Department of Health & Human ServicesNational
   Institutes of Health (NIH) - USA [R01CA165255, R21CA172864]; NVIDIA
   Corporation
FX This work was supported by the National Natural Science Foundation of
   China under Grant no. 61272351, and National Institutes of Health Grants
   R01CA165255 and R21CA172864 of the United States.; We gratefully
   acknowledge the support of NVIDIA Corporation and Perry Deng (NVIDIA)
   with the donation of the GPUs used for this research. Thanks to Haoran
   Li, Zehao Huang and Xin Liu for fruitful discussions.
CR Achlioptas D, 2003, J COMPUT SYST SCI, V66, P671, DOI 10.1016/S0022-0000(03)00025-4
   Aggarwal JK, 2011, ACM COMPUT SURV, V43, DOI 10.1145/1922649.1922653
   Aggarwal JK, 1997, IEEE NONRIGID AND ARTICULATED MOTION WORKSHOP, PROCEEDINGS, P90, DOI 10.1109/NAMW.1997.609859
   Aly R., 2013, AXES SUBMISSIONS TRE
   Baccouche Moez, 2011, Human Behavior Unterstanding. Proceedings Second International Workshop, HBU 2011, P29, DOI 10.1007/978-3-642-25446-8_4
   Baird J. C., 2014, SENSATION JUDGMENT C
   Bengio Y., 2015, DEEP LEARNING BOOK P
   Blank M, 2005, IEEE I CONF COMP VIS, P1395
   Boden M., 2002, T200203 SICS
   Bourdev L, 2009, INT C COMP VIS ICCV
   Bourdev L, 2011, IEEE I CONF COMP VIS, P1543, DOI 10.1109/ICCV.2011.6126413
   Cai ZW, 2014, PROC CVPR IEEE, P596, DOI 10.1109/CVPR.2014.83
   Candes EJ, 2008, IEEE SIGNAL PROC MAG, V25, P21, DOI 10.1109/MSP.2007.914731
   Deng J., 2012, LARGE SCALE VISUAL R
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231
   Foucart S., 2013, MATH INTRO COMPRESSI
   Georgiou H. V., ARXIVHEPTH14107100
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Gong SG, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P742, DOI 10.1109/ICCV.2003.1238423
   Grossman E, 2000, J COGNITIVE NEUROSCI, V12, P711, DOI 10.1162/089892900562417
   Grossman ED, 2002, NEURON, V35, P1167, DOI 10.1016/S0896-6273(02)00897-8
   Hammer B, 2000, NEUROCOMPUTING, V31, P107, DOI 10.1016/S0925-2312(99)00174-5
   He K., ARXIVHEPTH150201852
   Heng Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3169, DOI 10.1109/CVPR.2011.5995407
   Jain M, 2013, PROC CVPR IEEE, P2555, DOI 10.1109/CVPR.2013.330
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Jia Y., ARXIVHEPTH14085093
   Karpathy Andrej, 2014, IEEE C COMP VIS PATT
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kuehne H, 2013, HIGH PERFORMANCE COMPUTING IN SCIENCE AND ENGINEERING '12: TRANSACTIONS OF THE HIGH PERFORMANCE COMPUTING CENTER, STUTTGART (HLRS) 2012, P571, DOI 10.1007/978-3-642-33374-3_41
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li LJ, 2007, IEEE I CONF COMP VIS, P345
   Liang X., 2013, P 21 ACM INT C MULT, P263
   Liebe S, 2012, NAT NEUROSCI, V15, P456, DOI 10.1038/nn.3038
   Marszalek M., 2009, IEEE C COMP VIS PATT
   Martens J., 2011, P 28 INT C MACH LEAR, P1033
   Martens J., 2010, P 27 INT C MACH LEAR, P735, DOI DOI 10.1155/2011/176802
   Mikolov T., 2012, THESIS BRNO U TECHNO
   Nguyen NT, 2005, PROC CVPR IEEE, P955
   Oneata D, 2013, IEEE I CONF COMP VIS, P1817, DOI 10.1109/ICCV.2013.228
   Richard S., 2014, THESIS STANFORD U
   Robinson T., 1996, AUTOMATIC SPEECH SPE, V355, P233
   Rodriguez A, 2014, SCIENCE, V344, P1492, DOI 10.1126/science.1242072
   Ryoo MS, 2008, 2008 IEEE WORKSHOP ON MOTION AND VIDEO COMPUTING, P95
   Sapienza M., ARXIVHEPTH14057545
   Schindler K, 2008, IEEE C COMP VIS PATT, P1, DOI DOI 10.1109/CVPR.2008.4587730
   Schuldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462
   Sermanet P., ARXIVHEPTH13126229
   Simonyan K., 2014, ADV NEURAL INFORM PR, P568, DOI DOI 10.1109/ICCVW.2017.368
   Soomro K., ARXIVHEPTH12120402
   Srivastava N., ARXIVHEPTH150204681
   Sutskever I., 2011, P 28 INT C MACH LEAR, P1017
   Sutskever I, 2013, THESIS U TORONTO
   Szegedy C., ARXIVHEPTH14094842
   Taylor GW, 2010, LECT NOTES COMPUT SC, V6316, P140, DOI 10.1007/978-3-642-15567-3_11
   Tompson J., ARXIVHEPTH14062984
   Tran D., EPRINT ARXIV
   Veeraraghavan A., 2006, P IEEE COMP SOC C CO, P959, DOI DOI 10.1109/CVPR.2006.304
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang H, 2013, INT J COMPUT VISION, V103, P60, DOI 10.1007/s11263-012-0594-8
   Weinland D, 2006, COMPUT VIS IMAGE UND, V104, P249, DOI 10.1016/j.cviu.2006.07.013
   Weinland D, 2011, COMPUT VIS IMAGE UND, V115, P224, DOI 10.1016/j.cviu.2010.10.002
   Williams R. J., 1995, GRADIENT BASED LEARN, P433
   Zhang D, 2006, IEEE T MULTIMEDIA, V8, P509, DOI 10.1109/TMM.2006.870735
   Zhang KH, 2012, LECT NOTES COMPUT SC, V7574, P864, DOI 10.1007/978-3-642-33712-3_62
   Zhou F, 2012, PROC CVPR IEEE, P1282, DOI 10.1109/CVPR.2012.6247812
   Zisserman A., 2014, ARXIVHEPTH14091556
NR 68
TC 22
Z9 23
U1 0
U2 25
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0925-2312
EI 1872-8286
J9 NEUROCOMPUTING
JI Neurocomputing
PD FEB 20
PY 2016
VL 178
SI SI
BP 87
EP 102
DI 10.1016/j.neucom.2015.09.112
PG 16
WC Computer Science, Artificial Intelligence
SC Computer Science
GA DD7HE
UT WOS:000370093500009
PM 29290647
OA Green Accepted
DA 2020-02-19
ER

PT J
AU Sheng, YQ
   Wang, JL
   Deng, HJ
   Li, CP
AF Sheng, Yiqiang
   Wang, Jinlin
   Deng, Haojiang
   Li, Chaopeng
TI k-Degree Layer-Wise Network for Geo-Distributed Computing between Cloud
   and IoT
SO IEICE TRANSACTIONS ON COMMUNICATIONS
LA English
DT Article
DE big data; internet of things; geo-distributed computing; deep learning;
   cloud computing
ID MACHINES
AB In this paper, we propose a novel architecture for a deep learning system, named k-degree layer-wise network, to realize efficient geo-distributed computing between Cloud and Internet of Things (IoT). The geo-distributed computing extends Cloud to the geographical verge of the network in the neighbor of IoT. The basic ideas of the proposal include a k-degree constraint and a layer-wise constraint. The k-degree constraint is defined such that the degree of each vertex on the h-th layer is exactly k(h) to extend the existing deep belief networks and control the communication cost. The layer-wise constraint is defined such that the layer-wise degrees are monotonically decreasing in positive direction to gradually reduce the dimension of data. We prove the k-degree layer-wise network is sparse, while a typical deep neural network is dense. In an evaluation on the M-distributed MNIST database, the proposal is superior to a state-of-the-art model in terms of communication cost and learning time with scalability.
C1 [Sheng, Yiqiang; Wang, Jinlin; Deng, Haojiang; Li, Chaopeng] Chinese Acad Sci, Natl Network New Media Engn Res Ctr, Beijing 100190, Peoples R China.
RP Sheng, YQ (reprint author), Chinese Acad Sci, Natl Network New Media Engn Res Ctr, Beijing 100190, Peoples R China.
EM shengyq@dsp.ac.cn
RI SHENG, Yiqiang/T-8455-2019
OI SHENG, Yiqiang/0000-0002-8452-2492
FU CAS Pioneer Hundred Talents Program; Special Funds for Strategic Pilot
   Technology of CAS [XDA06010300, XDA06040501]
FX This work is supported by CAS Pioneer Hundred Talents Program and
   Special Funds for Strategic Pilot Technology of CAS under Grant No.
   XDA06010300 and No. XDA06040501. The authors would like to thank Prof.
   X. Zeng, Dr. L. Wang and Dr. W. Qi at Chinese Academy of Sciences, thank
   Prof. A. Takahashi and Prof. S. Ueno at Tokyo Institute of Technology,
   and thank the anonymous reviewers for their valuable comments.
CR AMARLINGAM M, 2014, P 2014 IEEE WORLD FO, P452
   Bellavista P, 2014, INT CONF UTIL CLOUD, P363, DOI 10.1109/UCC.2014.46
   Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153
   Deng L, 2013, INT CONF ACOUST SPEE, P8599, DOI 10.1109/ICASSP.2013.6639344
   Guan ZL, 2010, PROCEEDINGS OF THE 2010 INTERNATIONAL CONFERENCE ON ADVANCED INTELLIGENCE AND AWARENESS INTERNET, AIAI2010, P269
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   LeCun Y, 2010, IEEE INT SYMP CIRC S, P253, DOI 10.1109/ISCAS.2010.5537907
   Lee I.J., 2014, 16 AS PAC NETW OP MA, P1
   Low Y, 2012, PROC VLDB ENDOW, V5, P716, DOI 10.14778/2212351.2212354
   Mesnil G, 2015, IEEE-ACM T AUDIO SPE, V23, P530, DOI 10.1109/TASLP.2014.2383614
   Ottenwalder B., 2013, P 7 ACM INT C DISTR, DOI [10.1145/2488222.2488265, DOI 10.1145/2488222.2488265]
   Priyankara S, 2011, IEEE GLOBE WORK, P281, DOI 10.1109/GLOCOMW.2011.6162453
   Rennie SJ, 2012, INT CONF ACOUST SPEE, P4297, DOI 10.1109/ICASSP.2012.6288869
   Roy Partha Pratim, 2014, Proceedings. 2014 14th International Conference on Frontiers in Handwriting Recognition (ICFHR), P506, DOI 10.1109/ICFHR.2014.91
   Shtykh RY, 2014, 2014 IEEE FOURTH INTERNATIONAL CONFERENCE ON BIG DATA AND CLOUD COMPUTING (BDCLOUD), P267, DOI 10.1109/BDCloud.2014.54
   Stojmenovic I, 2014, ACSIS-ANN COMPUT SCI, V2, P1
   Stolfo S. J., 2012, 2012 IEEE CS Security and Privacy Workshops (SPW 2012), P125, DOI 10.1109/SPW.2012.19
   Tudoran R, 2014, IEEE ACM INT SYMP, P92, DOI 10.1109/CCGrid.2014.86
   Wang J, 2014, P ROY SOC A-MATH PHY, V470, DOI 10.1098/rspa.2014.0439
   Xiao Z, 2013, IEEE T PARALL DISTR, V24, P1107, DOI 10.1109/TPDS.2012.283
   Yang K, 2013, IEEE T PARALL DISTR, V24, P1717, DOI 10.1109/TPDS.2012.278
   Yin XC, 2014, INT C PATT RECOG, P1904, DOI 10.1109/ICPR.2014.333
   Zhang KL, 2014, IEEE ACCESS, V2, P395, DOI 10.1109/ACCESS.2014.2319813
NR 23
TC 1
Z9 1
U1 0
U2 9
PU IEICE-INST ELECTRONICS INFORMATION COMMUNICATIONS ENG
PI TOKYO
PA KIKAI-SHINKO-KAIKAN BLDG, 3-5-8, SHIBA-KOEN, MINATO-KU, TOKYO, 105-0011,
   JAPAN
SN 0916-8516
EI 1745-1345
J9 IEICE T COMMUN
JI IEICE Trans. Commun.
PD FEB
PY 2016
VL E99B
IS 2
BP 307
EP 314
DI 10.1587/transcom.2015ITP0009
PG 8
WC Engineering, Electrical & Electronic; Telecommunications
SC Engineering; Telecommunications
GA DK2XY
UT WOS:000374778900003
DA 2020-02-19
ER

PT J
AU Lipton, ZC
   Elkan, C
AF Lipton, Zachary C.
   Elkan, Charles
TI Playing the Imitation Game With Deep Learning
SO IEEE SPECTRUM
LA English
DT Article
NR 0
TC 1
Z9 1
U1 0
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 0018-9235
EI 1939-9340
J9 IEEE SPECTRUM
JI IEEE Spectr.
PD FEB
PY 2016
VL 53
IS 2
BP 40
EP 45
DI 10.1109/MSPEC.2016.7419799
PG 6
WC Engineering, Electrical & Electronic
SC Engineering
GA DG8TS
UT WOS:000372357100016
DA 2020-02-19
ER

PT J
AU Ding, JW
   Huang, YZ
   Liu, W
   Huang, KQ
AF Ding, Jianwei
   Huang, Yongzhen
   Liu, Wei
   Huang, Kaiqi
TI Severely Blurred Object Tracking by Learning Deep Image Representations
SO IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY
LA English
DT Article
DE Deep learning; object tracking; severe blur
AB An implicit assumption in many generic object trackers is that the videos are blur free. However, motion blur is very common in real videos. The performance of a generic object tracker may drop significantly when it is applied to videos with severe motion blur. In this paper, we propose a new Tracking-Learning-Data approach to transfer a generic object tracker to a blur-invariant object tracker without deblurring image sequences. Before object tracking, a large set of unlabeled images is used to learn objects' visual prior knowledge, which is then transferred to the appearance model of a specific target. During object tracking, online training samples are collected from the tracking results and the context information. Different blur kernels are involved with the training samples to increase the robustness of the appearance model to severe blur, and the motion parameters of the object are estimated in the particle filter framework. Extensive experimental results demonstrate that the proposed algorithm can robustly track objects not only in severely blurred videos but also in other challenging scenes.
C1 [Ding, Jianwei] Peoples Publ Secur Univ China, Beijing 430072, Peoples R China.
   [Liu, Wei] Nanyang Normal Univ, Nanyang 450001, Henan, Peoples R China.
   [Huang, Yongzhen; Huang, Kaiqi] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
RP Ding, JW (reprint author), Peoples Publ Secur Univ China, Beijing 430072, Peoples R China.; Liu, W (reprint author), Nanyang Normal Univ, Nanyang 450001, Henan, Peoples R China.; Huang, YZ; Huang, KQ (reprint author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
EM flydjw@gmail.com; yzhuang@nlpr.ia.ac.cn; lw3171796@163.com;
   kqhuang@nlpr.ia.ac.cn
FU Fundamental Research Funds for Central UniversitiesFundamental Research
   Funds for the Central Universities [2014JKF01116]; National High
   Technology Research and Development Program of ChinaNational High
   Technology Research and Development Program of China [2013AA014604];
   National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61402484, 61203252]; SAMSUNG Global Research
   Outreach Program; CCF-Tencent Program; 360 OpenLab Program
FX This work was supported in part by the Fundamental Research Funds for
   the Central Universities under Grant 2014JKF01116, in part by the
   National High Technology Research and Development Program of China under
   Grant 2013AA014604, in part by the National Natural Science Foundation
   of China under Grant 61402484 and Grant 61203252, and in part by the
   SAMSUNG Global Research Outreach Program, CCF-Tencent Program and 360
   OpenLab Program. This paper was recommended by Associate Editor J.-M.
   Odobez.
CR Babenko B, 2011, IEEE T PATTERN ANAL, V33, P1619, DOI 10.1109/TPAMI.2010.226
   Bao CL, 2012, PROC CVPR IEEE, P1830, DOI 10.1109/CVPR.2012.6247881
   Breitenstein MD, 2009, IEEE I CONF COMP VIS, P1515, DOI 10.1109/ICCV.2009.5459278
   Comaniciu D, 2003, IEEE T PATTERN ANAL, V25, P564, DOI 10.1109/TPAMI.2003.1195991
   Comaniciu D, 2000, PROC CVPR IEEE, P142, DOI 10.1109/CVPR.2000.854761
   Dai SY, 2006, IEEE IMAGE PROC, P2389, DOI 10.1109/ICIP.2006.312943
   Ding JW, 2012, 2012 IEEE NINTH INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL-BASED SURVEILLANCE (AVSS), P331, DOI 10.1109/AVSS.2012.78
   Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231
   Fergus R, 2006, ACM T GRAPHIC, V25, P787, DOI 10.1145/1141911.1141956
   Gordon N, 2001, SEQUENTIAL MONTE CAR
   Grabner H, 2008, LECT NOTES COMPUT SC, V5302, P234, DOI 10.1007/978-3-540-88682-2_19
   Grabner Helmut, 2006, P BMVC, V1, P6, DOI DOI 10.5244/C.20.6
   Hare S, 2011, IEEE I CONF COMP VIS, P263, DOI 10.1109/ICCV.2011.6126251
   Hinton G., 2012, NEURAL NETWORKS TRIC, V9, P599, DOI DOI 10.1007/978-3-642-35289-8_32
   Hua Y, 2014, LECT NOTES COMPUT SC, V8694, P172, DOI 10.1007/978-3-319-10599-4_12
   Jin HL, 2005, PROC CVPR IEEE, P18
   Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239
   Krizhevsky A., 2009, THESIS U TORONTO TOR
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kwon J, 2010, PROC CVPR IEEE, P1269, DOI 10.1109/CVPR.2010.5539821
   LeCun Y, 2012, LECT NOTES COMPUT SC, V7583, P496, DOI 10.1007/978-3-642-33863-2_51
   Mei X, 2009, IEEE I CONF COMP VIS, P1436, DOI 10.1109/ICCV.2009.5459292
   Nair V, 2009, ADV NEURAL INF PROCE, V22, P1339
   Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7
   Sermanet P, 2013, PROC CVPR IEEE, P3626, DOI 10.1109/CVPR.2013.465
   Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128
   Vincent P, 2008, P 25 INT C MACH LEAR, P1096, DOI DOI 10.1145/1390156.1390294
   Wang N., 2013, ADV NEURAL INFORM PR
   Wang Q, 2012, IEEE T IMAGE PROCESS, V21, P3296, DOI 10.1109/TIP.2012.2190085
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Wu Y, 2011, IEEE I CONF COMP VIS, P1100, DOI 10.1109/ICCV.2011.6126357
   Yang F, 2014, IEEE T IMAGE PROCESS, V23, P1639, DOI 10.1109/TIP.2014.2300823
   Yilmaz A, 2006, ACM COMPUT SURV, V38, DOI 10.1145/1177352.1177355
   Zhang KH, 2012, LECT NOTES COMPUT SC, V7574, P864, DOI 10.1007/978-3-642-33712-3_62
   Zhang TZ, 2012, PROC CVPR IEEE, P2042, DOI 10.1109/CVPR.2012.6247908
NR 35
TC 15
Z9 20
U1 5
U2 46
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1051-8215
EI 1558-2205
J9 IEEE T CIRC SYST VID
JI IEEE Trans. Circuits Syst. Video Technol.
PD FEB
PY 2016
VL 26
IS 2
BP 319
EP 331
DI 10.1109/TCSVT.2015.2406231
PG 13
WC Engineering, Electrical & Electronic
SC Engineering
GA DE9DN
UT WOS:000370935900005
DA 2020-02-19
ER

PT J
AU Painsky, A
   Rosset, S
   Feder, M
AF Painsky, Amichai
   Rosset, Saharon
   Feder, Meir
TI Generalized Independent Component Analysis Over Finite Alphabets
SO IEEE TRANSACTIONS ON INFORMATION THEORY
LA English
DT Article
DE Independent component analysis; BICA; ICA over galois field; blind
   source separation; minimal redundancy representation; minimum entropy
   codes; factorial codes; predictive coding; distributed source coding;
   neural networks
ID INFORMATION; EXTRACTION; FIELDS; ICA
AB Independent component analysis (ICA) is a statistical method for transforming an observable multi-dimensional random vector into components that are as statistically independent as possible from each other. Usually, the ICA framework assumes a model according to which the observations are generated (such as a linear transformation with additive noise). ICA over finite fields is a special case of ICA in which both the observations and the independent components are over a finite alphabet. In this paper, we consider a generalization of this framework in which an observation vector is decomposed to its independent components (as much as possible) with no prior assumption on the way it was generated. This generalization is also known as Barlow's minimal redundancy representation problem and is considered an open problem. We propose several theorems and show that this hard problem can be accurately solved with a branch and bound search tree algorithm, or tightly approximated with a series of linear problems. Our contribution provides the first efficient set of solutions to Barlow's problem. The minimal redundancy representation (also known as factorial code) has many applications, mainly in the fields of neural networks and deep learning. The binary ICA is also shown to have applications in several domains, including medical diagnosis, multi-cluster assignment, network tomography, and internet resource management. In this paper, we show that this formulation further applies to multiple disciplines in source coding, such as predictive coding, distributed source coding, and coding of large alphabet sources.
C1 [Painsky, Amichai; Rosset, Saharon] Tel Aviv Univ, Dept Stat, IL-6997801 Tel Aviv, Israel.
   [Feder, Meir] Tel Aviv Univ, Dept Elect Engn, IL-6997801 Tel Aviv, Israel.
RP Painsky, A; Rosset, S (reprint author), Tel Aviv Univ, Dept Stat, IL-6997801 Tel Aviv, Israel.; Feder, M (reprint author), Tel Aviv Univ, Dept Elect Engn, IL-6997801 Tel Aviv, Israel.
EM amichaip@eng.tau.ac.il; saharon@post.tau.ac.il; meir@eng.tau.ac.il
FU Israeli Ministry of Science [3]; Israeli Science FoundationIsrael
   Science Foundation [1487/12, 634/09]
FX This paper was supported in part by a returning scientists grant to
   Amichai Painsky from the Israeli Ministry of Science, and by Israeli
   Science Foundation grants 1487/12 and 634/09. This paper was presented
   at the 2014 IEEE International Symposium on Information Theory.
CR Atick JJ, 1990, NEURAL COMPUT, V2, P308, DOI 10.1162/neco.1990.2.3.308
   Barlow HB, 1989, NEURAL COMPUT, V1, P412, DOI 10.1162/neco.1989.1.3.412
   Bartlett MS, 2007, NEUROCOMPUTING, V70, P2204, DOI 10.1016/j.neucom.2006.02.025
   Bartlett MS, 2002, IEEE T NEURAL NETWOR, V13, P1450, DOI 10.1109/TNN.2002.804287
   Becker S, 1996, APPL INTELL, V6, P185, DOI 10.1007/BF00126625
   Becker S., 1988, P 1988 CONN MOD SUMM, P29
   Belohlavek R, 2010, J COMPUT SYST SCI, V76, P3, DOI 10.1016/j.jcss.2009.05.002
   Chakrabarty K, 1998, IEE P-COMPUT DIG T, V145, P52, DOI 10.1049/ip-cdt:19981769
   Choi S, 2000, LECT NOTES COMPUT SC, V1811, P42
   DAVISSON LD, 1973, IEEE T INFORM THEORY, V19, P783, DOI 10.1109/TIT.1973.1055092
   Deco G., 1996, INFORM THEORETIC APP
   Diamantaras KI, 2006, IEEE T SIGNAL PROCES, V54, P3720, DOI 10.1109/TSP.2006.880259
   e Silva Daniel G., 2011, IEEE Information Theory Workshop (ITW 2011), P618, DOI 10.1109/ITW.2011.6089571
   ELIAS P, 1955, IRE T INFORM THEOR, V1, P16, DOI 10.1109/TIT.1955.1055126
   GAVRILOVIC MM, 1975, J MATH ANAL APPL, V52, P260, DOI 10.1016/0022-247X(75)90095-5
   Griffiths T., 2005, P ADV NEUR INF PROC
   Gutch HW, 2012, SIGNAL PROCESS, V92, P1796, DOI 10.1016/j.sigpro.2011.10.003
   Hoare C. A. R., 1961, COMMUN ACM, V4, P321, DOI DOI 10.1145/366622.366644
   Nguyen H, 2011, IEEE T SIGNAL PROCES, V59, P3168, DOI 10.1109/TSP.2011.2144975
   Hyvarinen A., 2004, INDEPENDENT COMPONEN, V46
   Jones MN, 2004, BEHAV RES METH INS C, V36, P388, DOI 10.3758/BF03195586
   Li YQ, 2003, IEICE T FUND ELECTR, VE86A, P580
   Martiriggiano T, 2005, LECT NOTES ARTIF INT, V3533, P55
   PLUMBLEY MD, 1993, NEURAL NETWORKS, V6, P823, DOI 10.1016/S0893-6080(05)80127-3
   Schmidhuber Jurgen, 2011, Artificial General Intelligence. Proceedings 4th International Conference, AGI 2011, P243, DOI 10.1007/978-3-642-22887-2_25
   SCHMIDHUBER J, 1992, NEURAL COMPUT, V4, P863, DOI 10.1162/neco.1992.4.6.863
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Singliar T, 2006, J MACH LEARN RES, V7, P2189
   SLEPIAN D, 1973, IEEE T INFORM THEORY, V19, P471, DOI 10.1109/TIT.1973.1055037
   Streich A. P., 2009, P 26 ANN INT C MACH, P969
   Szpankowski W, 2012, IEEE T INFORM THEORY, V58, P4094, DOI 10.1109/TIT.2012.2195769
   WATANABE S, 1960, IBM J RES DEV, V4, P66, DOI 10.1147/rd.41.0066
   Wood F., 2012, NONPARAMETRIC BAYESI
   Yeredor A, 2007, LECT NOTES COMPUT SC, V4666, P827
   Yeredor A, 2011, IEEE T INFORM THEORY, V57, P5342, DOI 10.1109/TIT.2011.2145090
NR 35
TC 6
Z9 6
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 0018-9448
EI 1557-9654
J9 IEEE T INFORM THEORY
JI IEEE Trans. Inf. Theory
PD FEB
PY 2016
VL 62
IS 2
BP 1038
EP 1053
DI 10.1109/TIT.2015.2510657
PG 16
WC Computer Science, Information Systems; Engineering, Electrical &
   Electronic
SC Computer Science; Engineering
GA DC6DL
UT WOS:000369309900028
DA 2020-02-19
ER

PT J
AU Yu, Z
   Wang, HX
   Lin, XM
   Wang, M
AF Yu, Zheng
   Wang, Haixun
   Lin, Xuemin
   Wang, Min
TI Understanding Short Texts through Semantic Enrichment and Hashing
SO IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING
LA English
DT Article; Proceedings Paper
CT IEEE 30th International Conference on Data Engineering (ICDE)
CY MAR 31-APR 04, 2014
CL Chicago, IL
SP IEEE, Microsoft, Qatar Comp Res Inst, HERE Nokia, Purdue Univ, Cyber Ctr, NW Univ, McCormick Sch Engn, Google
DE Short text; semantic enrichment; semantic hashing; deep neural network
AB Clustering short texts (such as news titles) by their meaning is a challenging task. The semantic hashing approach encodes the meaning of a text into a compact binary code. Thus, to tell if two texts have similar meanings, we only need to check if they have similar codes. The encoding is created by a deep neural network, which is trained on texts represented by word-count vectors (bag-of-word representation). Unfortunately, for short texts such as search queries, tweets, or news titles, such representations are insufficient to capture the underlying semantics. To cluster short texts by their meanings, we propose to add more semantic signals to short texts. Specifically, for each term in a short text, we obtain its concepts and co-occurring terms from a probabilistic knowledge base to enrich the short text. Furthermore, we introduce a simplified deep learning network consisting of a 3-layer stacked auto-encoders for semantic hashing. Comprehensive experiments show that, with more semantic signals, our simplified deep learning model is able to capture the semantics of short texts, which enables a variety of applications including short text retrieval, classification, and general purpose text processing.
C1 [Yu, Zheng; Lin, Xuemin] E China Normal Univ, Shanghai 200062, Peoples R China.
   [Wang, Haixun; Wang, Min] Google Res, Mountain View, CA 94043 USA.
   [Lin, Xuemin] Univ New S Wales, CSE, Sydney, NSW, Australia.
RP Yu, Z; Lin, XM (reprint author), E China Normal Univ, Shanghai 200062, Peoples R China.; Wang, HX; Wang, M (reprint author), Google Res, Mountain View, CA 94043 USA.; Lin, XM (reprint author), Univ New S Wales, CSE, Sydney, NSW, Australia.
EM zyu.0910@gmail.com; haixun@google.com; lxue@cse.unsw.edu.au;
   minwang@google.com
OI Lin, Xuemin/0000-0003-2396-7225
FU ARCAustralian Research Council [DP120104168, DP140103578, DP150102728]; 
   [NSFC61232006]
FX This work is supported by NSFC61232006, ARC DP120104168, ARC
   DP140103578, and ARC DP150102728. The authors thank the reviewers for
   their detailed comments.
CR Anderson J. A., 1995, INTRO NEURAL NETWORK
   Banerjee Somnath, 2007, 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P787, DOI 10.1145/1277741.1277909
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Bottou L., 1991, P NEUR EC2, V2
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411
   DEERWESTER S, 1990, J AM SOC INFORM SCI, V41, P391, DOI 10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9
   Egozi O., 2008, P 23 AAAI C ART INT, P1132
   Fellbaum C., 1998, WORDNET ELECT LEXICA
   Gabrilovich E, 2005, 19TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-05), P1048
   Gabrilovich E, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1606
   Gehler Peter V, 2006, P 23 INT C MACH LEAR, P337, DOI DOI 10.1145/1143844.1143887
   Han J, 1995, LECT NOTES COMPUT SC, V930, P195
   Harris ZS, 1954, WORD, V10, P146, DOI 10.1080/00437956.1954.11659520
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hofmann T, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P289
   Horiguchi S., 2008, P 17 INT C WORLD WID, P91, DOI DOI 10.1145/1367497.1367510
   Hu X., 2009, P 18 ACM C INF KNOWL, P919, DOI DOI 10.1145/1645953.1646071
   Kim D., 2013, IJCAI, P2654
   Li P., 2013, P 22 ACM INT C C INF, P1401
   Manning C. D, 2008, INTRO INFORM RETRIEV
   Meyer D, 2003, NEUROCOMPUTING, V55, P169, DOI 10.1016/S0925-2312(03)00431-4
   Mitchell T., 1997, MACHINE LEARNING
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Sahami M., 2006, P 15 INT C WORLD WID, P377, DOI DOI 10.1145/1135777.1135834
   Salakhutdinov R, 2009, INT J APPROX REASON, V50, P969, DOI 10.1016/j.ijar.2008.11.006
   SALTON G, 1988, INFORM PROCESS MANAG, V24, P513, DOI 10.1016/0306-4573(88)90021-0
   SALTON G, 1991, SCIENCE, V253, P974, DOI 10.1126/science.253.5023.974
   Shen D, 2006, ACM T INFORM SYST, V24, P320, DOI 10.1145/1165774.1165776
   Song Y., 2011, P 22 INT JOINT C ART, VThree, P2330
   Stein Benno, 2007, 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P527, DOI 10.1145/1277741.1277832
   Wu W., 2012, P 2012 ACM SIGMOD IN, P481, DOI DOI 10.1145/2213836.2213891
   Yih W., 2007, P 22 NAT C ART INT, P1489
NR 34
TC 11
Z9 16
U1 2
U2 25
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1041-4347
EI 1558-2191
J9 IEEE T KNOWL DATA EN
JI IEEE Trans. Knowl. Data Eng.
PD FEB 1
PY 2016
VL 28
IS 2
BP 566
EP 579
DI 10.1109/TKDE.2015.2485224
PG 14
WC Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Engineering, Electrical & Electronic
SC Computer Science; Engineering
GA DC1VV
UT WOS:000369006800021
DA 2020-02-19
ER

PT J
AU Zhang, J
   Ding, SF
   Zhang, N
   Shi, ZZ
AF Zhang, Jian
   Ding, Shifei
   Zhang, Nan
   Shi, Zhongzhi
TI Incremental extreme learning machine based on deep feature embedded
SO INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS
LA English
DT Article
DE RBM; SRBM; Manifold Regularization; ELM; Incremental feature mapping
ID NETWORKS; ERROR
AB Extreme learning machine (ELM) algorithm is used to train Single-hidden Layer Feed forward Neural Networks. And Deep Belief Network (DBN) is based on Restricted Boltzmann Machine (RBM). The conventional DBN algorithm has some insufficiencies, i.e., Contrastive Divergence (CD) Algorithm is not an ideal approximation method to Maximum Likelihood Estimation. And bad parameters selected in RBM algorithm will produce a bad initialization in DBN model so that we will spend more training time and get a low classification accuracy. To solve the problems above, we summarize the features of extreme learning machine and deep belief networks, and then propose Incremental extreme learning machine based on Deep Feature Embedded algorithm which combines the deep feature extracting ability of Deep Learning Networks with the feature mapping ability of extreme learning machine. Firstly, we introduce Manifold Regularization to our model to attenuate the complexity of probability distribution. Secondly, we introduce the semi-restricted Boltzmann machine (SRBM) to our algorithm, and build a deep belief network based on SRBM. Thirdly, we introduce the thought of incremental feature mapping in ELM to the classifier of DBN model. Finally, we show validity of the algorithm by experiments.
C1 [Zhang, Jian; Ding, Shifei; Zhang, Nan] China Univ Min & Technol, Sch Comp Sci & Technol, Xuzhou 221116, Peoples R China.
   [Zhang, Jian; Ding, Shifei; Zhang, Nan; Shi, Zhongzhi] Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
RP Ding, SF (reprint author), China Univ Min & Technol, Sch Comp Sci & Technol, Xuzhou 221116, Peoples R China.
EM dingsf@cumt.edu.cn
OI Zhang, Nan/0000-0001-9620-5665
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61379101]; National Key Basic Research Program of
   ChinaNational Basic Research Program of China [2013CB329502]
FX This work is supported by the National Natural Science Foundation of
   China (No. 61379101), and the National Key Basic Research Program of
   China (No. 2013CB329502).
CR Belkin M, 2006, J MACH LEARN RES, V7, P2399
   Ding SF, 2014, NEURAL COMPUT APPL, V24, P1487, DOI 10.1007/s00521-013-1385-z
   Erhan D, 2010, J MACH LEARN RES, V11, P625
   Fu AM, 2014, NEUROCOMPUTING, V146, P75, DOI 10.1016/j.neucom.2014.04.067
   He Q, 2014, NEUROCOMPUTING, V128, P88, DOI 10.1016/j.neucom.2012.12.063
   Hinton G. E., 1983, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, P448
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Huang G.-B., 2004, P INT JOINT C NEUR N, P25
   Huang G, 2014, IEEE T CYBERNETICS, V44, P2405, DOI 10.1109/TCYB.2014.2307349
   Huang GB, 2006, NEUROCOMPUTING, V70, P489, DOI 10.1016/j.neucom.2005.12.126
   Huang GB, 2006, IEEE T NEURAL NETWOR, V17, P879, DOI 10.1109/TNN.2006.875977
   Lee H, 2011, COMMUN ACM, V54, P95, DOI 10.1145/2001269.2001295
   [吕启 Lu Qi], 2014, [计算机研究与发展, Journal of Computer Research and Development], V51, P1911
   Norouzi Mohammad, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2735, DOI 10.1109/CVPRW.2009.5206577
   OSINDERO S, 2008, [No title captured], V20, P1121
   Salakhutdinov R, 2008, LEARNING EVALUATING
   Salakhutdinov R, 2015, ANNU REV STAT APPL, V2, P361, DOI 10.1146/annurev-statistics-010814-020120
   Tieleman T., 2008, P 25 INT C MACH LEAR, P1064, DOI DOI 10.1145/1390156.1390290
   Tieleman T., 2009, P 26 ANN INT C MACH, P1033, DOI DOI 10.1145/1553374.1553506
   Wang XZ, 2013, NEUROCOMPUTING, V102, P3, DOI [10.1016/j.neucom.2011.12.053, 10.1016/j.neucom.2011.11053]
   Wang XZ, 2011, NEUROCOMPUTING, V74, P2520, DOI 10.1016/j.neucom.2010.12.034
   Zhang N, NEUROCOMPUT IN PRESS
NR 23
TC 20
Z9 23
U1 6
U2 64
PU SPRINGER HEIDELBERG
PI HEIDELBERG
PA TIERGARTENSTRASSE 17, D-69121 HEIDELBERG, GERMANY
SN 1868-8071
EI 1868-808X
J9 INT J MACH LEARN CYB
JI Int. J. Mach. Learn. Cybern.
PD FEB
PY 2016
VL 7
IS 1
BP 111
EP 120
DI 10.1007/s13042-015-0419-5
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA DA9ZD
UT WOS:000368167400008
DA 2020-02-19
ER

PT J
AU Wu, F
   Lu, XY
   Song, J
   Yan, SC
   Zhang, Z
   Rui, Y
   Zhuang, YT
AF Wu, Fei
   Lu, Xinyan
   Song, Jun
   Yan, Shuicheng
   Zhang, Zhongfei (Mark)
   Rui, Yong
   Zhuang, Yueting
TI Learning of Multimodal Representations With Random Walks on the Click
   Graph
SO IEEE TRANSACTIONS ON IMAGE PROCESSING
LA English
DT Article
DE Cross-media search; click log; latent representation; deep learning
ID IMAGE; RANK
AB In multimedia information retrieval, most classic approaches tend to represent different modalities of media in the same feature space. With the click data collected from the users' searching behavior, existing approaches take either one-to-one paired data (text-image pairs) or ranking examples (text-query-image and/or image-query-text ranking lists) as training examples, which do not make full use of the click data, particularly the implicit connections among the data objects. In this paper, we treat the click data as a large click graph, in which vertices are images/text queries and edges indicate the clicks between an image and a query. We consider learning a multimodal representation from the perspective of encoding the explicit/implicit relevance relationship between the vertices in the click graph. By minimizing both the truncated random walk loss as well as the distance between the learned representation of vertices and their corresponding deep neural network output, the proposed model which is named multimodal random walk neural network (MRW-NN) can be applied to not only learn robust representation of the existing multimodal data in the click graph, but also deal with the unseen queries and images to support cross-modal retrieval. We evaluate the latent representation learned by MRW-NN on a public large-scale click log data set Clickture and further show that MRW-NN achieves much better cross-modal retrieval performance on the unseen queries/images than the other state-of-the-art methods.
C1 [Wu, Fei; Lu, Xinyan; Song, Jun; Zhuang, Yueting] Zhejiang Univ, Sch Comp Sci & Technol, Hangzhou 310027, Peoples R China.
   [Yan, Shuicheng] Natl Univ Singapore, Dept Elect Engn, Singapore 119077, Singapore.
   [Zhang, Zhongfei (Mark)] Zhejiang Univ, Dept Informat Sci & Elect Engn, Hangzhou 310027, Peoples R China.
   [Rui, Yong] Microsoft Res Asia, Beijing 100080, Peoples R China.
RP Wu, F (reprint author), Zhejiang Univ, Sch Comp Sci & Technol, Hangzhou 310027, Peoples R China.
EM wufei@cs.zju.edu.cn; xinyanlu@zju.edu.cn; eleyans@nus.edu.sg;
   zhongfei@zju.edu.cn; yongrui@microsoft.com; yzhuang@cs.zju.edu.cn
FU National Basic Research Program of ChinaNational Basic Research Program
   of China [2015CB352300]; China Knowledge Centre for Engineering Sciences
   and Technology; Fundamental Research Funds for the Central
   UniversitiesFundamental Research Funds for the Central Universities;
   Zhejiang Provincial Engineering Center on Media Data Cloud Processing
   and Analysis
FX This work was supported in part by the National Basic Research Program
   of China under Grant 2015CB352300, in part by the China Knowledge Centre
   for Engineering Sciences and Technology, and in part by the Fundamental
   Research Funds for the Central Universities and the Zhejiang Provincial
   Engineering Center on Media Data Cloud Processing and Analysis.
CR Andrew G., 2013, P 30 INT C MACH LEAR, P1247
   Baeza-Yates R, 1999, MODERN INFORM RETRIE
   Bai B., 2009, ADV NEURAL INFORM PR, V22, P64
   Bai YL, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P229, DOI 10.1145/2647868.2656402
   Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317
   Bishop CM, 2006, PATTERN RECOGN, V4, P4
   Craswell Nick, 2007, 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P239, DOI 10.1145/1277741.1277784
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Frome A., 2013, ADV NEURAL INFORM PR, P2121
   Grangier D, 2008, IEEE T PATTERN ANAL, V30, P1371, DOI 10.1109/TPAMI.2007.70791
   Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814
   Hua Xian-Sheng, 2013, P 21 ACM INT C MULT, P243, DOI DOI 10.1145/2502081.2502283
   Jarvelin K, 2002, ACM T INFORM SYST, V20, P422, DOI 10.1145/582415.582418
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Jing Y, 2008, IEEE T PATTERN ANAL, V30, P1877, DOI 10.1109/TPAMI.2008.121
   Joachims T., 2002, P 8 ACM SIGKDD INT C, P133, DOI DOI 10.1145/775047.775067
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lu XY, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P985, DOI 10.1145/2647868.2655001
   Lu XY, 2013, SIGIR'13: THE PROCEEDINGS OF THE 36TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH & DEVELOPMENT IN INFORMATION RETRIEVAL, P433
   Mikolov T, 2013, ADV NEURAL INFORM PR, V27, P3111, DOI DOI 10.1162/JMLR.2003.3.4-5.951
   Mnih A., 2009, ADV NEURAL INFORM PR, P1081
   Ngiam J, 2011, IEEE INT C MACH LEAR, V28, P689, DOI DOI 10.1145/2647868
   Pan YW, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P233, DOI 10.1145/2647868.2656404
   Pan YW, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P717, DOI 10.1145/2600428.2609568
   Perozzi B, 2014, KDD, V20, P701, DOI DOI 10.1145/2623330.2623732
   Rasiwasia N., 2010, P ACM MULT 2010, P251, DOI DOI 10.1145/1873951.1873987
   Sharma A, 2012, PROC CVPR IEEE, P2160, DOI 10.1109/CVPR.2012.6247923
   Simonyan K., 2014, VERY DEEP CONVOLUTIO
   Smith G, 2012, J AM SOC INF SCI TEC, V63, P2451, DOI 10.1002/asi.22742
   Snoek C. G. M., 2005, 13th Annual ACM International Conference on Multimedia, P399
   Srivastava N., 2012, ADV NEURAL INFORM PR, V25, P2231
   Wang M, 2015, IEEE T CYBERNETICS, V45, P1561, DOI 10.1109/TCYB.2014.2356136
   Wang M, 2012, IEEE T IMAGE PROCESS, V21, P4649, DOI 10.1109/TIP.2012.2207397
   Wang M, 2009, IEEE T CIRC SYST VID, V19, P733, DOI 10.1109/TCSVT.2009.2017400
   Wu C.-C., 2013, P 21 ACM INT C MULT, P393, DOI DOI 10.1145/2502081.2508127
   Wu F, 2015, IEEE T IMAGE PROCESS, V24, P1497, DOI 10.1109/TIP.2015.2403240
   Wu F, 2014, IEEE T MULTIMEDIA, V16, P427, DOI 10.1109/TMM.2013.2291214
   Wu Fei, 2013, P 21 ACM INT C MULT, P877
   Yang Y, 2008, IEEE T MULTIMEDIA, V10, P437, DOI 10.1109/TMM.2008.917359
NR 39
TC 18
Z9 21
U1 4
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1057-7149
EI 1941-0042
J9 IEEE T IMAGE PROCESS
JI IEEE Trans. Image Process.
PD FEB
PY 2016
VL 25
IS 2
BP 630
EP 642
DI 10.1109/TIP.2015.2507401
PG 13
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
SC Computer Science; Engineering
GA DW8KY
UT WOS:000383905800011
PM 26672038
DA 2020-02-19
ER

PT J
AU Dong, C
   Loy, CC
   He, KM
   Tang, XO
AF Dong, Chao
   Loy, Chen Change
   He, Kaiming
   Tang, Xiaoou
TI Image Super-Resolution Using Deep Convolutional Networks
SO IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
LA English
DT Article
DE Super-resolution; deep convolutional neural networks; sparse coding
ID QUALITY ASSESSMENT
AB We propose a deep learning method for single image super-resolution (SR). Our method directly learns an end-to-end mapping between the low/high-resolution images. The mapping is represented as a deep convolutional neural network (CNN) that takes the low-resolution image as the input and outputs the high-resolution one. We further show that traditional sparse-coding-based SR methods can also be viewed as a deep convolutional network. But unlike traditional methods that handle each component separately, our method jointly optimizes all layers. Our deep CNN has a lightweight structure, yet demonstrates state-of-the-art restoration quality, and achieves fast speed for practical on-line usage. We explore different network structures and parameter settings to achieve trade-offs between performance and speed. Moreover, we extend our network to cope with three color channels simultaneously, and show better overall reconstruction quality.
C1 [Dong, Chao; Loy, Chen Change; Tang, Xiaoou] Chinese Univ Hong Kong, Dept Informat Engn, Hong Kong, Hong Kong, Peoples R China.
   [He, Kaiming] Microsoft Res Asia, Visual Comp Grp, Beijing 100080, Peoples R China.
RP Dong, C; Loy, CC; Tang, XO (reprint author), Chinese Univ Hong Kong, Dept Informat Engn, Hong Kong, Hong Kong, Peoples R China.; He, KM (reprint author), Microsoft Res Asia, Visual Comp Grp, Beijing 100080, Peoples R China.
EM dc012@ie.cuhk.edu.hk; ccloy@ie.cuhk.edu.hk; kahe@microsoft.com;
   xtang@ie.cuhk.edu.hk
CR Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Burger HC, 2012, PROC CVPR IEEE, P2392, DOI 10.1109/CVPR.2012.6247952
   Chang H., 2004, IEEE C COMP VIS PATT
   Cui Z, 2014, LECT NOTES COMPUT SC, V8693, P49, DOI 10.1007/978-3-319-10602-1_4
   Dai D., 2015, EUROGRAPHICS, V7, P8
   Dai SY, 2009, IEEE T IMAGE PROCESS, V18, P969, DOI 10.1109/TIP.2009.2012908
   Damera-Venkata N, 2000, IEEE T IMAGE PROCESS, V9, P636, DOI 10.1109/83.841940
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Denton E. L., 2014, ADV NEURAL INFORM PR, P1269
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Eigen D, 2013, IEEE I CONF COMP VIS, P633, DOI 10.1109/ICCV.2013.84
   Freedman G, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1944846.1944852
   Freeman WT, 2000, INT J COMPUT VISION, V40, P25, DOI 10.1023/A:1026501619075
   Freeman WT, 2002, IEEE COMPUT GRAPH, V22, P56, DOI 10.1109/38.988747
   Glasner D, 2009, IEEE I CONF COMP VIS, P349, DOI 10.1109/ICCV.2009.5459271
   He K., 2015, P IEEE C COMP VIS PA, P3791
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI 10.1007/978-3-319-10578-9_23
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   IRANI M, 1991, CVGIP-GRAPH MODEL IM, V53, P231, DOI 10.1016/1049-9652(91)90045-L
   Jain V., 2008, P ADV NEUR INF PROC, P769
   Jia K, 2013, IEEE T PATTERN ANAL, V35, P367, DOI 10.1109/TPAMI.2012.95
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Kim KI, 2010, IEEE T PATTERN ANAL, V32, P1127, DOI 10.1109/TPAMI.2010.25
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee H., 2006, ADV NEURAL INF PROCE, P801
   Liu C, 2007, INT J COMPUT VISION, V75, P115, DOI 10.1007/s11263-006-0029-5
   Mamalet Franck, 2012, Artificial Neural Networks and Machine Learning - ICANN 2012. 22nd International Conference on Artificial Neural Networks, P58, DOI 10.1007/978-3-642-33266-1_8
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Nair V, 2010, ICML, V27, P807, DOI DOI 10.0RG/PAPERS/432.PDF
   Ouyang WL, 2015, PROC CVPR IEEE, P2403, DOI 10.1109/CVPR.2015.7298854
   Ouyang WL, 2013, IEEE I CONF COMP VIS, P2056, DOI 10.1109/ICCV.2013.257
   Schuler CJ, 2013, PROC CVPR IEEE, P1067, DOI 10.1109/CVPR.2013.142
   Schulter S, 2015, PROC CVPR IEEE, P3791, DOI 10.1109/CVPR.2015.7299003
   Sheikh HR, 2005, IEEE T IMAGE PROCESS, V14, P2117, DOI 10.1109/TIP.2005.859389
   Sun Y, 2014, ADV NEURAL INFORM PR, V60, P1988
   Timofte R, 2015, LECT NOTES COMPUT SC, V9006, P111, DOI 10.1007/978-3-319-16817-3_8
   Timofte R, 2013, IEEE I CONF COMP VIS, P1920, DOI 10.1109/ICCV.2013.241
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Yang CY, 2014, LECT NOTES COMPUT SC, V8692, P372, DOI 10.1007/978-3-319-10593-2_25
   Yang CY, 2011, LECT NOTES COMPUT SC, V6494, P497, DOI 10.1007/978-3-642-19318-7_39
   Yang J, 2008, PROC CVPR IEEE, P173
   Yang JC, 2013, PROC CVPR IEEE, P1059, DOI 10.1109/CVPR.2013.141
   Yang JC, 2012, IEEE T IMAGE PROCESS, V21, P3467, DOI 10.1109/TIP.2012.2192127
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Zeyde R, 2012, CURVES SURFACES, P711, DOI [DOI 10.1007/978-3-642-27413-8_47, 10.1007/978-3-642-27413-8_47]
   Zhang N, 2014, LECT NOTES COMPUT SC, V8689, P834, DOI 10.1007/978-3-319-10590-1_54
NR 50
TC 1338
Z9 1475
U1 192
U2 1045
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 0162-8828
EI 1939-3539
J9 IEEE T PATTERN ANAL
JI IEEE Trans. Pattern Anal. Mach. Intell.
PD FEB
PY 2016
VL 38
IS 2
BP 295
EP 307
DI 10.1109/TPAMI.2015.2439281
PG 13
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
SC Computer Science; Engineering
GA DD5UI
UT WOS:000369989600008
PM 26761735
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Gao, YB
   Lee, HJ
AF Gao, Yongbin
   Lee, Hyo Jong
TI Local Tiled Deep Networks for Recognition of Vehicle Make and Model
SO SENSORS
LA English
DT Article
DE vehicle-model recognition; deep learning; moving-vehicle detection; HOG
ID REPRESENTATION
AB Vehicle analysis involves license-plate recognition (LPR), vehicle-type classification (VTC), and vehicle make and model recognition (MMR). Among these tasks, MMR plays an important complementary role in respect to LPR. In this paper, we propose a novel framework for MMR using local tiled deep networks. The frontal views of vehicle images are first extracted and fed into the local tiled deep networks for training and testing. A local tiled convolutional neural network (LTCNN) is proposed to alter the weight sharing scheme of CNN with local tiled structure. The LTCNN unties the weights of adjacent units and then ties the units k steps from each other within a local map. This architecture provides the translational, rotational, and scale invariance as well as locality. In addition, to further deal with the colour and illumination variation, we applied the histogram oriented gradient (HOG) to the frontal view of images prior to the LTCNN. The experimental results show that our LTCNN framework achieved a 98% accuracy rate in terms of vehicle MMR.
C1 [Gao, Yongbin; Lee, Hyo Jong] Chonbuk Natl Univ, Div Comp Sci & Engn, 567 Baekje Daero, Jeonju 54596, South Korea.
   [Lee, Hyo Jong] Chonbuk Natl Univ, Ctr Adv Image & Informat Technol, 567 Baekje Daero, Jeonju 54596, South Korea.
RP Lee, HJ (reprint author), Chonbuk Natl Univ, Div Comp Sci & Engn, 567 Baekje Daero, Jeonju 54596, South Korea.; Lee, HJ (reprint author), Chonbuk Natl Univ, Ctr Adv Image & Informat Technol, 567 Baekje Daero, Jeonju 54596, South Korea.
EM gaoyongbin.sam@gmail.com; hlee@chonbuk.ac.kr
RI Lee, Hyo Jong/B-7565-2017
FU Brain Korea 21 PLUS Project, National Research Foundation of Korea;
   Business for Academic-Industrial Cooperative establishments - Korea
   Small and Medium Business Administration [C0221114]; MSIP (Ministry of
   Science, ICT and Future Planning), Korea, under the ITRC (Information
   Technology Research Center) support program [IITP-2015-R0992-15-1023]
FX This work was supported by the Brain Korea 21 PLUS Project, National
   Research Foundation of Korea. This work was also supported by the
   Business for Academic-Industrial Cooperative establishments that were
   funded by the Korea Small and Medium Business Administration in 2014
   (Grants No. C0221114). This research was also supported by the MSIP
   (Ministry of Science, ICT and Future Planning), Korea, under the ITRC
   (Information Technology Research Center) support program
   (IITP-2015-R0992-15-1023) supervised by the IITP (Institute for
   Information & communications Technology Promotion).
CR AbdelMaseeh M, 2012, INT C PATT RECOG, P910
   Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   Alex K., 2012, P ADV NEURAL INFORM
   Chen ZZ, 2011, IEEE INT C INTELL TR, P74, DOI 10.1109/ITSC.2011.6083075
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Faro A, 2011, IEEE T INTELL TRANSP, V12, P1398, DOI 10.1109/TITS.2011.2159266
   Foresti GL, 1999, IEEE T VEH TECHNOL, V48, P301, DOI 10.1109/25.740109
   Gao Y., 2015, P INT C INF SCI SEC
   He Kaiming, DEEP RESIDUAL LEARNI
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hsieh JW, 2014, IEEE T INTELL TRANSP, V15, P6, DOI 10.1109/TITS.2013.2294646
   Hyvarinen A, 2001, NEURAL COMPUT, V13, P1527, DOI 10.1162/089976601750264992
   Jazayeri A, 2011, IEEE T INTELL TRANSP, V12, P583, DOI 10.1109/TITS.2011.2113340
   Le Q.V., 2010, ADV NEURAL INFORM PR, V23, P1279
   Lecun Yann, 1998, P IEEE
   Lee H, 2009, P 26 ANN INT C MACH
   Lee HJ, 2006, LECT NOTES COMPUT SC, V3973, P66
   Llorca DF, 2014, 2014 IEEE 17TH INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS (ITSC), P3094, DOI 10.1109/ITSC.2014.6958187
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Ma XX, 2005, IEEE I CONF COMP VIS, P1185
   Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2
   Pearce G., 2011, Proceedings of the 2011 8th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS 2011), P373, DOI 10.1109/AVSS.2011.6027353
   Petrovic V.S., 2004, BRIT MACH VIS C, P587
   Proka J, 2009, P WORKSH APPL COMP V, V11, P1
   Ramnath K., 2014, P IEEE WINT C APPL C
   Ranzato M., 2010, P NEUR INF PROC SYST
   Ranzato M, 2010, PROC CVPR IEEE, P2551, DOI 10.1109/CVPR.2010.5539962
   Ratan AL, 2000, INT J COMPUT VISION, V36, P131, DOI 10.1023/A:1008147915077
   Rifai S, 2011, P 28 INT C MACH LEAR
   Sermanet P, OVERFEAT INTEGRATED
   Szegedy Christian, GOING DEEPER CONVOLU
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Tzomakas C., 1998, 9806 RUHR U I NUER
   Unno Hiroshi, 2007, Proceedings of the 2007 IEEE Intelligent Vehicles Symposium, P1127
   Varjas V., 2013, P 8 INT S IM SIGN PR, P819
   Vincent P., 2008, P 25 INT C MACH LEAR
   Vincent P, 2011, NEURAL COMPUT, V23, P1661, DOI 10.1162/NECO_a_00142
   Wu JW, 2001, 2001 IEEE INTELLIGENT TRANSPORTATION SYSTEMS - PROCEEDINGS, P740, DOI 10.1109/ITSC.2001.948752
   Zafar I., 2009, P SPIE IMAGE PROCESS
   Zhang WC, 2005, IEEE I CONF COMP VIS, P786
NR 41
TC 17
Z9 17
U1 3
U2 31
PU MDPI
PI BASEL
PA ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
SN 1424-8220
J9 SENSORS-BASEL
JI Sensors
PD FEB
PY 2016
VL 16
IS 2
DI 10.3390/s16020226
PG 13
WC Chemistry, Analytical; Engineering, Electrical & Electronic; Instruments
   & Instrumentation
SC Chemistry; Engineering; Instruments & Instrumentation
GA DG0WX
UT WOS:000371787800103
PM 26875983
OA DOAJ Gold, Green Published
DA 2020-02-19
ER

PT J
AU Yuan, ZL
   Lu, YQ
   Xue, YB
AF Yuan, Zhenlong
   Lu, Yongqiang
   Xue, Yibo
TI DroidDetector: Android Malware Characterization and Detection Using Deep
   Learning
SO TSINGHUA SCIENCE AND TECHNOLOGY
LA English
DT Article
DE Android security; malware detection; characterization; deep learning;
   association rules mining
AB Smartphones and mobile tablets are rapidly becoming indispensable in daily life. Android has been the most popular mobile operating system since 2012. However, owing to the open nature of Android, countless malwares are hidden in a large number of benign apps in Android markets that seriously threaten Android security. Deep learning is a new area of machine learning research that has gained increasing attention in artificial intelligence. In this study, we propose to associate the features from the static analysis with features from dynamic analysis of Android apps and characterize malware using deep learning techniques. We implement an online deep-learning-based Android malware detection engine (DroidDetector) that can automatically detect whether an app is a malware or not. With thousands of Android apps, we thoroughly test DroidDetector and perform an indepth analysis on the features that deep learning essentially exploits to characterize malware. The results show that deep learning is suitable for characterizing Android malware and especially effective with the availability of more training data. DroidDetector can achieve 96.76% detection accuracy, which outperforms traditional machine learning techniques. An evaluation of ten popular anti-virus softwares demonstrates the urgency of advancing our capabilities in Android malware detection.
C1 [Yuan, Zhenlong] Tsinghua Univ, Dept Automat, Beijing 100084, Peoples R China.
   [Yuan, Zhenlong; Xue, Yibo] Tsinghua Univ, RIIT, Beijing 100084, Peoples R China.
   [Lu, Yongqiang] Baidu Inc, Dept Antivirus, Beijing 100085, Peoples R China.
   [Xue, Yibo] Tsinghua Univ, Tsinghua Natl Lab Informat Sci & Technol, Beijing 100084, Peoples R China.
RP Xue, YB (reprint author), Tsinghua Univ, RIIT, Beijing 100084, Peoples R China.; Xue, YB (reprint author), Tsinghua Univ, Tsinghua Natl Lab Informat Sci & Technol, Beijing 100084, Peoples R China.
EM yuanzl11@mails.tsinghua.edu.cn; luyongqiang@baidu.com;
   yiboxue@tsinghua.edu.cn
CR Aafer Y, 2013, L N INST COMP SCI SO, V127, P86
   [Anonymous], 2015, ANDR APPL SANDB DYN
   Arp D., 2014, P 21 ANN S NETW DIST
   Barrera D, 2010, PROCEEDINGS OF THE 17TH ACM CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY (CCS'10), P73, DOI 10.1145/1866307.1866317
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Burguera I., 2011, P 1 ACM WORKSH SEC P, P15, DOI DOI 10.1145/2046614.2046619
   DroidDetector, 2015, DROIDDETECTOR DEEP L
   Elish KO, 2015, COMPUT SECUR, V49, P255, DOI 10.1016/j.cose.2014.11.001
   Enck W., 2010, P 9 USENIX S OP SYST
   Fang ZR, 2014, COMPUT SECUR, V43, P205, DOI 10.1016/j.cose.2014.02.007
   Felt A. P., 2011, P 1 ACM WORKSH SEC P, P3, DOI DOI 10.1145/2046614.2046618
   Friedman JH, 1999, STAT COMPUT, V9, P123, DOI 10.1023/A:1008894516817
   Geneiatakis D, 2015, COMPUT SECUR, V49, P192, DOI 10.1016/j.cose.2014.10.005
   Grace M., 2012, P 10 INT C MOB SYST, V12, P281, DOI DOI 10.1145/2307636.2307663
   Grace M. C., 2012, P 5 ACM C SEC PRIV W, P101, DOI DOI 10.1145/2185448.2185464
   Jones N, 2014, NATURE, V505, P146, DOI 10.1038/505146a
   McAfee, 2015, MCAFEE LABS THREATS
   Mylonas A, 2013, COMPUT SECUR, V34, P47, DOI 10.1016/j.cose.2012.11.004
   Pandita Rahul, 2013, USENIX SECURITY, P527
   Poeplau S., 2014, P 21 ANN S NETW DIST
   Qu ZY, 2014, CCS'14: PROCEEDINGS OF THE 21ST ACM CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1354, DOI 10.1145/2660267.2660287
   Rastogi V., 2013, P 8 ACM SIGSAC S INF, P329, DOI [10.1145/2484313.2484355, DOI 10.1145/2484313.2484355]
   Vidas T, 2011, WOOT, P81
   Wei XT, 2012, MOBICOM 12: PROCEEDINGS OF THE 18TH ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING, P137
   Xu JL, 2013, TSINGHUA SCI TECHNOL, V18, P418, DOI 10.1109/TST.2013.6574680
   Yan L.K., 2012, USENIX SEC S, P569
   Yuan ZL, 2014, ACM SIGCOMM COMP COM, V44, P371, DOI [10.1145/2619239.2631434, 10.1145/2740070.2631434]
   Zhang M, 2014, CCS'14: PROCEEDINGS OF THE 21ST ACM CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1105, DOI 10.1145/2660267.2660359
   Zhou Y., 2012, P 19 ANN S NETW DIST
   Zhou YJ, 2012, P IEEE S SECUR PRIV, P95, DOI 10.1109/SP.2012.16
NR 30
TC 66
Z9 66
U1 3
U2 63
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 1007-0214
EI 1878-7606
J9 TSINGHUA SCI TECHNOL
JI Tsinghua Sci. Technol.
PD FEB
PY 2016
VL 21
IS 1
SI SI
BP 114
EP 123
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Engineering, Electrical & Electronic
SC Computer Science; Engineering
GA DP4BV
UT WOS:000378441300010
DA 2020-02-19
ER

PT J
AU Wu, GX
   Lu, WJ
   Gao, GW
   Zhao, CX
   Liu, JY
AF Wu, Guoxing
   Lu, Wenjie
   Gao, Guangwei
   Zhao, Chunxia
   Liu, Jiayin
TI Regional deep learning model for visual tracking
SO NEUROCOMPUTING
LA English
DT Article
DE Deep learning; Particle filter; Visual tracking
ID OBJECT TRACKING
AB Deep learning has been successfully applied to visual tracking due to its powerful feature learning characteristic. However, existing deep learning trackers rely on single observation model and focus on the holistic representation of the tracking object. When occlusion occurs, the trackers suffer from the contaminated features obtained in occluded areas. In this paper, we propose a regional deep learning tracker that observes the target by multiple sub-regions and each region is observed by a deep learning model. In particular, we devise a stable factor, modeled as a hidden variable of the Factorial Hidden Markov Model, to characterize the stability of these sub-models. The stability indicator not only provides a confidence degree for the response score of each model during inference stage, but also determines the online training criteria for each deep learning model. This online training strategy enables the tracker to achieve more accurate local features compared with those fixed training trackers. In addition, to improve the computational efficiency, we exploit the structurized response property of the customized deep learning model to approximate the final tracking results by the weighted Gaussian Mixture Model under the particle filter framework. Qualitative and quantitative evaluations on the recent public benchmark dataset show that our approach outperforms most state-of-the-art trackers. (C) 2015 Elsevier B.V. All rights reserved.
C1 [Wu, Guoxing; Zhao, Chunxia; Liu, Jiayin] NUST, Sch Comp Sci & Engn, Nanjing 210096, Jiangsu, Peoples R China.
   [Lu, Wenjie] Minist Publ Secur Rd Traff Safety, Key Lab, Wuxi 214151, Peoples R China.
   [Gao, Guangwei] NUPT, Inst Adv Technol, Nanjing 210096, Jiangsu, Peoples R China.
RP Wu, GX (reprint author), NUST, Sch Comp Sci & Engn, Nanjing 210096, Jiangsu, Peoples R China.
EM leonidwoo@gmail.com; luwenjie0122@msn.cn; zhaochx@mail.nust.edu.cn;
   liu.smiton@gmail.com
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61272220, 61401212]; Fundamental Research Funds for
   the Central UniversitiesFundamental Research Funds for the Central
   Universities [30915011321]
FX This project is supported by the National Natural Science Foundation of
   China (Grant nos. 61272220 and 61401212) and the Fundamental Research
   Funds for the Central Universities (Grant no. 30915011321).
CR Amit Adam, 2006, IEEE C COMP VIS PATT, P798, DOI DOI 10.1109/CVPR.2006.256
   Avidan S, 2004, IEEE T PATTERN ANAL, V26, P1064, DOI 10.1109/TPAMI.2004.53
   Babenko B, 2011, IEEE T PATTERN ANAL, V33, P1619, DOI 10.1109/TPAMI.2010.226
   Bao CL, 2012, PROC CVPR IEEE, P1830, DOI 10.1109/CVPR.2012.6247881
   Dalal N, 2005, PROC CVPR IEEE, P886
   Dinh TB, 2011, PROC CVPR IEEE, P1177, DOI 10.1109/CVPR.2011.5995733
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Ghahramani Z, 1997, MACH LEARN, V29, P245, DOI 10.1023/A:1007425814087
   Girshick R., ARXIV14095403
   Gordon N, 2001, SEQUENTIAL MONTE CAR
   Grabner Helmut, 2006, BMVC, V1, P6
   Hare S, 2011, IEEE I CONF COMP VIS, P263, DOI 10.1109/ICCV.2011.6126251
   He SF, 2013, PROC CVPR IEEE, P2427, DOI 10.1109/CVPR.2013.314
   Henriques JF, 2012, LECT NOTES COMPUT SC, V7575, P702, DOI 10.1007/978-3-642-33765-9_50
   Jia X, 2012, PROC CVPR IEEE, P1822, DOI 10.1109/CVPR.2012.6247880
   Kalal Z, 2010, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2010.5540231
   Kwon J, 2013, PROC CVPR IEEE, P2355, DOI 10.1109/CVPR.2013.305
   Kwon J, 2013, IEEE T PATTERN ANAL, V35, P2427, DOI 10.1109/TPAMI.2013.32
   Kwon J, 2013, IEEE T PATTERN ANAL, V35, P1011, DOI 10.1109/TPAMI.2012.161
   Kwon J, 2011, IEEE I CONF COMP VIS, P1195, DOI 10.1109/ICCV.2011.6126369
   Kwon J, 2010, PROC CVPR IEEE, P1269, DOI 10.1109/CVPR.2010.5539821
   Lee H. S., 2011, P ICC 11 KYOT JAP JU, P1, DOI DOI 10.1109/ICC.2011.5963055
   Li HX, 2011, PROC CVPR IEEE, P1305, DOI 10.1109/CVPR.2011.5995483
   Liu BY, 2011, PROC CVPR IEEE, P1313, DOI 10.1109/CVPR.2011.5995730
   Mei X, 2011, PROC CVPR IEEE, P1257, DOI 10.1109/CVPR.2011.5995421
   Mei X, 2009, IEEE I CONF COMP VIS, P1436, DOI 10.1109/ICCV.2009.5459292
   Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7
   Sevilla-Lara L, 2012, PROC CVPR IEEE, P1910, DOI 10.1109/CVPR.2012.6247891
   Smeulders AWM, 2014, IEEE T PATTERN ANAL, V36, P1442, DOI 10.1109/TPAMI.2013.230
   Stenger Bjorn, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2647, DOI 10.1109/CVPRW.2009.5206634
   Wang N., 2013, ADV NEURAL INFORM PR, P809
   Wang NY, 2013, IEEE I CONF COMP VIS, P657, DOI 10.1109/ICCV.2013.87
   WILLIAMS O, 2003, 2003 P 9 IEEE INT C, P353
   Wu GX, 2015, INT J MACH LEARN CYB, V6, P581, DOI 10.1007/s13042-015-0334-9
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Yang CJ, 2005, IEEE I CONF COMP VIS, P212
   Zhang K., 2014, IEEE T PATTERN ANAL, V36, P1, DOI DOI 10.1038/KI.2014.274
   Zhang KH, 2012, LECT NOTES COMPUT SC, V7574, P864, DOI 10.1007/978-3-642-33712-3_62
   Zhang TZ, 2012, LECT NOTES COMPUT SC, V7577, P470, DOI 10.1007/978-3-642-33783-3_34
   Zhang TZ, 2012, PROC CVPR IEEE, P2042, DOI 10.1109/CVPR.2012.6247908
   Zhong W, 2012, PROC CVPR IEEE, P1838, DOI 10.1109/CVPR.2012.6247882
   Zhuang BH, 2014, IEEE T IMAGE PROCESS, V23, P1872, DOI 10.1109/TIP.2014.2308414
NR 42
TC 26
Z9 28
U1 0
U2 76
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0925-2312
EI 1872-8286
J9 NEUROCOMPUTING
JI Neurocomputing
PD JAN 29
PY 2016
VL 175
BP 310
EP 323
DI 10.1016/j.neucom.2015.10.064
PN A
PG 14
WC Computer Science, Artificial Intelligence
SC Computer Science
GA DA4FX
UT WOS:000367756600030
DA 2020-02-19
ER

PT J
AU Ren, ZQ
   Deng, Y
   Dai, QH
AF Ren, Zhiquan
   Deng, Yue
   Dai, Qionghai
TI Local visual feature fusion via maximum margin multimodal deep neural
   network
SO NEUROCOMPUTING
LA English
DT Article
DE Image categorization; Deep learning; Feature fusion; Discriminative
   learning
AB In this letter, we consider improving the image categorization performance by exploiting multiple local descriptors on the image. To achieve this goal, a novel deep learning configuration called maximum margin multimodal deep neural network (3mDNN) is proposed to learn joint feature from different data views. The local feature representations encoded by 3mDNN exhibit two significant advantages: (1) involving the information of multiple descriptors and (2) exhibiting discriminative ability. The whole deep architecture is well solved by the typical back propagation (BP) method and its performances are verified on three benchmark image datasets. (C) 2015 Elsevier B.V. All rights reserved.
C1 [Ren, Zhiquan; Deng, Yue; Dai, Qionghai] Tsinghua Univ, Beijing 100084, Peoples R China.
RP Dai, QH (reprint author), Tsinghua Univ, Beijing 100084, Peoples R China.
FU NSFCNational Natural Science Foundation of China [61327902, 61120106003]
FX This work was supported by Project of the NSFC under Grants 61327902 and
   61120106003.
CR Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153
   Bengio Y., 2007, LARGE SCALE KERNEL M, V34, P1
   Chen K, 2011, IEEE T NEURAL NETWOR, V22, P1744, DOI 10.1109/TNN.2011.2167240
   Deng Y, 2015, IEEE T IND INFORM, V11, P467, DOI 10.1109/TII.2015.2404299
   Deng Y, 2014, IEEE T CYBERNETICS, V44, P1924, DOI 10.1109/TCYB.2014.2300192
   Deng Y, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0063385
   Deng Y, 2013, IEEE T NEUR NET LEAR, V24, P383, DOI 10.1109/TNNLS.2012.2235082
   Deng Y, 2011, IEEE T IMAGE PROCESS, V20, P2329, DOI 10.1109/TIP.2011.2109729
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fei-Fei L, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1134, DOI 10.1109/ICCV.2003.1238476
   Fu Y., 2008, P 2008 INT C CONT BA, P127
   Gehler P, 2009, IEEE I CONF COMP VIS, P221, DOI 10.1109/ICCV.2009.5459169
   Harada T, 2011, PROC CVPR IEEE, P1617, DOI 10.1109/CVPR.2011.5995691
   Jiang ZL, 2011, PROC CVPR IEEE, P1697, DOI 10.1109/CVPR.2011.5995354
   Jing X.-Y., 2014, 28 AAAI C ART INT
   Lazebnik S., 2006, P IEEE INT C COMP VI, V2, P2169, DOI DOI 10.1109/CVPR.2006.68
   Lee H., 2009, P 26 ANN INT C MACH, P609, DOI DOI 10.1145/1553374.1553453
   Ngiam J, 2011, IEEE INT C MACH LEAR, V28, P689, DOI DOI 10.1145/2647868
   Quattoni A, 2009, PROC CVPR IEEE, P413, DOI 10.1109/CVPRW.2009.5206537
   van Gemert JC, 2010, IEEE T PATTERN ANAL, V32, P1271, DOI 10.1109/TPAMI.2009.132
   Wang H., 2013, P 30 INT C MACH LEAR, P352
   Wang JJ, 2010, PROC CVPR IEEE, P3360, DOI 10.1109/CVPR.2010.5540018
   Wang L., 2007, P IEEE COMP SOC C CO, P1
   Wang Xinggang, 2013, P 30 INT C MACH LEAR, P846
   Wenchao Yu, 2013, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2013. Proceedings: LNCS 8190, P208, DOI 10.1007/978-3-642-40994-3_14
   Yang JC, 2009, PROC CVPR IEEE, P1794, DOI 10.1109/CVPRW.2009.5206757
   Zhang R., 2011, ACM MULTIMEDIA, P1513
   Zhu ZT, 2014, 2014 INTERNATIONAL CONFERENCE ON SECURITY, PATTERN ANALYSIS, AND CYBERNETICS (SPAC), P279, DOI 10.1109/SPAC.2014.6982699
NR 28
TC 11
Z9 11
U1 0
U2 29
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0925-2312
EI 1872-8286
J9 NEUROCOMPUTING
JI Neurocomputing
PD JAN 29
PY 2016
VL 175
BP 427
EP 432
DI 10.1016/j.neucom.2015.10.076
PN A
PG 6
WC Computer Science, Artificial Intelligence
SC Computer Science
GA DA4FX
UT WOS:000367756600041
DA 2020-02-19
ER

PT J
AU Cao, LL
   Huang, WB
   Sun, FC
AF Cao, Le-le
   Huang, Wen-bing
   Sun, Fu-chun
TI Building feature space of extreme learning machine with sparse denoising
   stacked-autoencoder
SO NEUROCOMPUTING
LA English
DT Article
DE Extreme learning machine (ELM); Ridge regression; Feature space; Stacked
   autoencoder (SAE); Classification; Regression
ID FACE RECOGNITION; BELIEF NETWORKS; DEEP; REGRESSION
AB The random-hidden-node extreme learning machine (ELM) is a much more generalized cluster of single-hidden-layer feed-forward neural networks (SLFNs) which has three parts: random projection, nonlinear transformation, and ridge regression (RR) model. Networks with deep architectures have demonstrated state-of-the-art performance in a variety of settings, especially with computer vision tasks. Deep learning algorithms such as stacked autoencoder (SAE) and deep belief network (DEN) are built on learning several levels of representation of the input. Beyond simply learning features by stacking autoencoders (AE), there is a need for increasing its robustness to noise and reinforcing the sparsity of weights to make it easier to discover interesting and prominent features. The sparse AE and denoising AE was hence developed for this purpose. This paper proposes an approach: SSDAE-RR (stacked sparse denoising autoencoder - ridge regression) that effectively integrates the advantages in SAE, sparse AE, denoising AE, and the RR implementation in ELM algorithm. We conducted experimental study on real-world classification (binary and multiclass) and regression problems with different scales among several relevant approaches: SSDAE-RR, ELM, DBN, neural network (NN), and SAE. The performance analysis shows that the SSDAE-RR tends to achieve a better generalization ability on relatively large datasets (large sample size and high dimension) that were not pre-processed for feature abstraction. For 16 out of 18 tested datasets, the performance of SSDAE-RR is more stable than other tested approaches. We also note that the sparsity regularization and denoising mechanism seem to be mandatory for constructing interpretable feature representations. The fact that a SSDAE-RR approach often has a comparable training time to ELM makes it useful in some real applications. (C) 2015 Elsevier B.V. All rights reserved.
C1 [Cao, Le-le; Huang, Wen-bing; Sun, Fu-chun] Tsinghua Univ, Dept Comp Sci & Technol, Tsinghua Natl Lab Informat Sci & Technol TNList, Beijing 100084, Peoples R China.
RP Cao, LL (reprint author), Tsinghua Univ, Dept Comp Sci & Technol, Tsinghua Natl Lab Informat Sci & Technol TNList, Beijing 100084, Peoples R China.
EM caoll12@mails.tsinghua.edu.cn; huangwb12@mails.tsinghua.edu.cn;
   fcsun@mail.tsinghua.edu.cn
FU China National Natural Science FoundationNational Natural Science
   Foundation of China [613278050, 61210013]
FX This work was supported by grants from China National Natural Science
   Foundation under Project 613278050 and 61210013. The authors would like
   to appreciate Huang G.B. together with his research team from Nanyang
   Technological University, Singapore, for kindly providing newest
   research papers and materials on ELM. The authors would also like to
   thank the reviewers for their constructive and inspiring comments and
   suggestions on our research.
CR Bache K., 2013, UCI REPOSITORY MACHI
   Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Bondarenko A., 2013, INF TECHNOL MANAG SC, V16, P60
   Broomhead D. S., 1988, Complex Systems, V2, P321
   Cai D., 2007, IEEE C COMP VIS ICCV, P1, DOI DOI 10.1109/ICCV.2007.4408856
   Cheng MY, 2013, J AM STAT ASSOC, V108, P1421, DOI 10.1080/01621459.2013.827984
   Erhan D, 2010, J MACH LEARN RES, V11, P625
   Feng GR, 2009, IEEE T NEURAL NETWOR, V20, P1352, DOI 10.1109/TNN.2009.2024147
   Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464
   Golub TR, 1999, SCIENCE, V286, P531, DOI 10.1126/science.286.5439.531
   Goodfellow I., 2009, ADV NEURAL INFORM PR, V22, P646
   Guyon I., 2004, ADV NEURAL INFORM PR, V17, P545
   HARRISON D, 1978, J ENVIRON ECON MANAG, V5, P81, DOI 10.1016/0095-0696(78)90006-2
   Hinton G., 2010, MOMENTUM, V9, P926
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2007, TRENDS COGN SCI, V11, P428, DOI 10.1016/j.tics.2007.09.004
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   HOERL AE, 1970, TECHNOMETRICS, V12, P55
   Hse H, 2004, INT C PATT RECOG, P367, DOI 10.1109/ICPR.2004.1334128
   Huang GB, 2004, IEEE IJCNN, P985
   Huang GB, 2008, NEUROCOMPUTING, V71, P3460, DOI 10.1016/j.neucom.2007.10.008
   Huang GB, 2007, NEUROCOMPUTING, V70, P3056, DOI 10.1016/j.neucom.2007.02.009
   Huang GB, 2006, NEUROCOMPUTING, V70, P489, DOI 10.1016/j.neucom.2005.12.126
   Huang GB, 2006, IEEE T NEURAL NETWOR, V17, P879, DOI 10.1109/TNN.2006.875977
   Huang GB, 2014, COGN COMPUT, V6, P376, DOI 10.1007/s12559-014-9255-2
   Huang GB, 2012, IEEE T SYST MAN CY B, V42, P513, DOI 10.1109/TSMCB.2011.2168604
   Huang GB, 2011, INT J MACH LEARN CYB, V2, P107, DOI 10.1007/s13042-011-0019-y
   Huang GB, 2010, NEUROCOMPUTING, V74, P155, DOI 10.1016/j.neucom.2010.02.019
   Kasun LLC, 2013, IEEE INTELL SYST, V28, P31
   Lan Y, 2010, NEUROCOMPUTING, V73, P3191, DOI 10.1016/j.neucom.2010.05.022
   Larochelle H., 2007, P 24 INT C MACH LEAR, P473, DOI DOI 10.1145/1273496.1273556
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee H., 2008, ADV NEURAL INFORM PR, V20, P873
   Lee H., 2009, P 26 ANN INT C MACH, P609, DOI DOI 10.1145/1553374.1553453
   Lee KC, 2005, IEEE T PATTERN ANAL, V27, P684, DOI 10.1109/TPAMI.2005.92
   Li FF, 2007, COMPUT VIS IMAGE UND, V106, P59, DOI 10.1016/j.cviu.2005.09.012
   Lopes Noel, 2011, International Journal of Computer Information Systems and Industrial Management Applications, V3, P355
   Marlin B. M., 2010, P 13 INT C ART INT S, P509
   Masci J, 2011, LECT NOTES COMPUT SC, V6791, P52, DOI 10.1007/978-3-642-21735-7_7
   Miche Y., 2008, P EUR S ART NEUR NET, P247
   Miche Y, 2008, LECT NOTES COMPUT SC, V5163, P145, DOI 10.1007/978-3-540-87536-9_16
   Miche Y, 2011, NEUROCOMPUTING, V74, P2413, DOI 10.1016/j.neucom.2010.12.042
   Mohamed AR, 2012, INT CONF ACOUST SPEE, P4273, DOI 10.1109/ICASSP.2012.6288863
   NASH WJ, 1994, POPULATION BIOL ABAL
   Palm R B, 2012, THESIS
   Ranzato M., 2006, ADV NEURAL INFORM PR, V19, P1137
   Rao CR, 1971, GEN INVERSE MATRICES, V7
   Ribeiro B., 2013, PROGR PATTERN RECOGN, V8258, P182
   Salakhutdinov R., 2008, ADV NEURAL INFORM PR, V20, P1249
   Schlimmer J., MUSHROOM RECORDS DRA
   SCHMIDT WF, 1992, 11TH IAPR INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION, PROCEEDINGS, VOL II, P1, DOI 10.1109/ICPR.1992.201708
   Sutskever I, 2008, NEURAL COMPUT, V20, P2629, DOI 10.1162/neco.2008.12-07-661
   Vincent P, 2008, P 25 INT C MACH LEAR, P1096, DOI DOI 10.1145/1390156.1390294
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   YUAN L, 2010, EUR S ART NEUR NETW, P327, DOI DOI 10.1109/TNN.2009.2024147
   Zhou SHK, 2005, IEEE I CONF COMP VIS, P541
NR 58
TC 22
Z9 23
U1 3
U2 64
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0925-2312
EI 1872-8286
J9 NEUROCOMPUTING
JI Neurocomputing
PD JAN 22
PY 2016
VL 174
SI SI
BP 60
EP 71
DI 10.1016/j.neucom.2015.02.096
PN A
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA CZ7JW
UT WOS:000367276700007
DA 2020-02-19
ER

PT J
AU Shah, SAA
   Bennamoun, M
   Boussaid, F
AF Shah, Syed Afaq Ali
   Bennamoun, Mohammed
   Boussaid, Farid
TI Iterative deep learning for image set based face and object recognition
SO NEUROCOMPUTING
LA English
DT Article
DE Face/object recognition; Image set classification
ID EFFICIENT APPROACH; ALGORITHM
AB We present a novel technique for image set based face/object recognition, where each gallery and query example contains a face/object image set captured from different viewpoints, background, facial expressions, resolution and illumination levels. While several linage set classification approaches have been proposed in recent years, most of them represent each image set as a single linear subspace, mixture of linear subspaces or Lie group of Riemannian manifold. These techniques make prior assumptions in regards to the specific category of the geometric surface on which images of the set are believed to lie. This could result in a loss of discriminative information for classification. This paper alleviates these limitations by proposing an Iterative Deep Learning Model (IDLM) that automatically and hierarchically learns discriminative representations from raw face and object images. In the proposed approach, low level translationally invariant features are learnt by the Pooled Convolutional Layer (PCL). The latter is followed by Artificial Neural Networks (ANNs) applied iteratively in a hierarchical fashion to learn a discriminative non-linear feature representation of the input image sets. The proposed technique was extensively evaluated for the task of image set based face and object recognition on YouTube Celebrities, Honda/UCSD, CMU Mobo and ETH-80 (object) dataset, respectively. Experimental results and comparisons with state-of-the-art methods show that our technique achieves the best performance on all these datasets. (C) 2015 Elsevier B.V. All rights reserved.
C1 [Shah, Syed Afaq Ali; Bennamoun, Mohammed] Univ Western Australia, Sch Comp Sci & Software Engn, Perth, WA 6009, Australia.
   [Boussaid, Farid] Univ Western Australia, Sch Elect Elect & Comp Engn, Perth, WA 6009, Australia.
RP Shah, SAA (reprint author), Univ Western Australia, Sch Comp Sci & Software Engn, Perth, WA 6009, Australia.
EM syed.shah@research.uwa.edu.au
RI Bennamoun, Mohammed/C-2789-2013
OI Bennamoun, Mohammed/0000-0002-6603-3257; Shah, Syed Afaq
   Ali/0000-0003-2181-8445
FU RTS scholarship from the University of Western Australia (UWA);
   Australian Research Council (ARC) grantAustralian Research Council
   [DP110102166]
FX This research is supported by RTS scholarship from the University of
   Western Australia (UWA) and Australian Research Council (ARC) grant
   DP110102166.
CR Ali Shah Syed Afaq, 2010, 2010 6th International Conference on Emerging Technologies (ICET), P175, DOI 10.1109/ICET.2010.5638495
   Arandjelovic O, 2005, PROC CVPR IEEE, P581
   Belhumeur P. N., 2005, FRONTIERS ENG REPORT, P5
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Cevikalp H, 2010, PROC CVPR IEEE, P2567, DOI 10.1109/CVPR.2010.5539965
   Coates Adam, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P561, DOI 10.1007/978-3-642-35289-8_30
   Coates A., INT C ART INT STAT, P215
   Gross R, 2001, CMU MOTION BODY MOBO
   Hamm J., 2008, P 25 INT C MACH LEAR, P376
   Harandi M. T., 2012, 2012 IEEE Workshop on Applications of Computer Vision (WACV), P433, DOI 10.1109/WACV.2012.6163005
   Harandi M. T., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2705, DOI 10.1109/CVPR.2011.5995564
   Hayat M., 2014, 2014 IEEE C COMP VIS
   Hayat M, 2014, LECT NOTES COMPUT SC, V8694, P784, DOI 10.1007/978-3-319-10599-4_50
   Heide F, 2015, PROC CVPR IEEE, P5135, DOI 10.1109/CVPR.2015.7299149
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hu YQ, 2012, IEEE T PATTERN ANAL, V34, P1992, DOI 10.1109/TPAMI.2011.283
   Huang YZ, 2014, NEUROCOMPUTING, V129, P225, DOI 10.1016/j.neucom.2013.09.037
   Hyvarinen A, 2000, NEURAL NETWORKS, V13, P411, DOI 10.1016/S0893-6080(00)00026-5
   Jarrett K, 2009, IEEE I CONF COMP VIS, P2146, DOI 10.1109/ICCV.2009.5459469
   Kavukcuoglu K., 2010, ADV NEURAL INFORM PR, V23, P1090
   Kim TK, 2007, IEEE T PATTERN ANAL, V29, P1005, DOI 10.1109/TPAMI.2007.1037
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee K.-C., 2003, 2003 IEEE COMP SOC C, V1, P1, DOI DOI 10.1186/1471-2156-4-S1-S94.[
   Leibe B., 2003, COMPUTER VISION PATT, V2, P11
   Li JW, 2015, NEUROCOMPUTING, V151, P1187, DOI 10.1016/j.neucom.2014.10.035
   Lu YF, 2014, NEUROCOMPUTING, V139, P189, DOI 10.1016/j.neucom.2014.02.046
   Lu YY, 2014, NEUROCOMPUTING, V126, P132, DOI 10.1016/j.neucom.2012.08.071
   Mei KZ, 2014, NEUROCOMPUTING, V144, P304, DOI 10.1016/j.neucom.2014.04.042
   Minyoung K., 2008, IEEE C COMP VIS PATT, V1, P1, DOI DOI 10.1109/CVPR.2008.4587572
   Nishiyama M., 2007, P IEEE C COMP VIS PA, P1
   Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0
   Ortiz EG, 2013, PROC CVPR IEEE, P3531, DOI 10.1109/CVPR.2013.453
   Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7
   Shah S.A.A., 2013, 1 INT C COMM SIGN PR, P1
   Shah SAA, 2015, PATTERN RECOGN, V48, P2859, DOI 10.1016/j.patcog.2015.03.014
   Shah SAA, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P638, DOI 10.1109/ICCVW.2013.88
   Shah SAA, 2013, IEEE IMAGE PROC, P2934, DOI 10.1109/ICIP.2013.6738604
   Shahdoost S., 2014, DIG IM COMP TECHN AP, P1
   Tang X, 2014, NEUROCOMPUTING, V145, P402, DOI 10.1016/j.neucom.2014.05.012
   Turaga P, 2011, IEEE T PATTERN ANAL, V33, P2273, DOI 10.1109/TPAMI.2011.52
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Wang R, 2008, IEEE C COMP VIS PATT, P1, DOI DOI 10.1109/CVPR.2008.4587719
   Wang RP, 2012, PROC CVPR IEEE, P2496, DOI 10.1109/CVPR.2012.6247965
   Wang RP, 2009, PROC CVPR IEEE, P429, DOI 10.1109/CVPRW.2009.5206850
   Yamaguchi O, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P318, DOI 10.1109/AFGR.1998.670968
   Yan Y, 2013, NEUROCOMPUTING, V119, P201, DOI 10.1016/j.neucom.2013.03.039
   Yang D, 2013, IEEE INT WORKSH COMP, P13, DOI 10.1109/CAMAD.2013.6708080
   Yang JC, 2009, PROC CVPR IEEE, P1794, DOI 10.1109/CVPRW.2009.5206757
   Zhou Y, 2014, PROC CVPR IEEE, P3081, DOI 10.1109/CVPR.2014.394
   Zhu P., 2013, IEEE C INT C COMP VI
   Zou FH, 2015, NEUROCOMPUTING, V151, P603, DOI 10.1016/j.neucom.2014.06.089
NR 51
TC 29
Z9 33
U1 3
U2 91
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0925-2312
EI 1872-8286
J9 NEUROCOMPUTING
JI Neurocomputing
PD JAN 22
PY 2016
VL 174
BP 866
EP 874
DI 10.1016/j.neucom.2015.10.004
PN B
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA CZ7JY
UT WOS:000367276900031
DA 2020-02-19
ER

PT J
AU Wang, YQ
   Xie, ZG
   Xu, K
   Dou, Y
   Lei, YW
AF Wang, Yueqing
   Xie, Zhige
   Xu, Kai
   Dou, Yong
   Lei, Yuanwu
TI An efficient and effective convolutional auto-encoder extreme learning
   machine network for 3d feature learning
SO NEUROCOMPUTING
LA English
DT Article
DE Convolutional; Extreme learning machine; Auto-encoder; Feature learning
ID REGRESSION; ALGORITHM; MECHANISM; ELMS
AB 3D shape features play a crucial role in graphics applications, such as 3D shape matching, recognition, and retrieval. Various 3D shape descriptors have been developed over the last two decades; however, existing descriptors are handcrafted features that are labor-intensively designed and cannot extract discriminative information for a large set of data. In this paper, we propose a rapid 3D feature learning method, namely, a convolutional auto-encoder extreme learning machine (CAE-ELM) that combines the advantages of the convolutional neuron network, auto-encoder, and extreme learning machine (ELM). This method performs better and faster than other methods. In addition, we define a novel architecture based on CAE-ELM. The architecture accepts two types of 3D shape representation, namely, voxel data and signed distance field data (SDF), as inputs to extract the global and local features of 3D shapes. Voxel data describe structural information, whereas SDF data contain details on 3D shapes. Moreover, the proposed CAE-ELM can be used in practical graphics applications, such as 3D shape completion. Experiments show that the features extracted by CAE-ELM are superior to existing hand-crafted features and other deep learning methods or ELM models. Moreover, the classification accuracy of the proposed architecture is superior to that of other methods on ModelNet10 (91.4%) and ModelNet40 (8435%). The training process also runs faster than existing deep learning methods by approximately two orders of magnitude. (C) 2015 Elsevier B.V. All rights reserved.
C1 [Wang, Yueqing; Dou, Yong] Natl Univ Def Technol, Natl Lab Parallel & Distributed Proc, Changsha, Hunan, Peoples R China.
   [Wang, Yueqing; Xie, Zhige; Xu, Kai; Dou, Yong; Lei, Yuanwu] Natl Univ Def Technol, Coll Comp, Changsha, Hunan, Peoples R China.
RP Wang, YQ (reprint author), Natl Univ Def Technol, Natl Lab Parallel & Distributed Proc, Changsha, Hunan, Peoples R China.
EM yqwang2013@163.com
OI WANG, Yueqing/0000-0002-8734-8346
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61125201, 61402499, 61379103, U1435219]
FX The authors would like to thank the anonymous reviewers for their
   helpful comments. This work was supported by the National Natural
   Science Foundation of China (No. 61125201, 61402499, 61379103 and
   U1435219).
CR Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   Bengio Y., CORR
   Bronstein AM, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1899404.1899405
   Bu SH, 2015, COMPUT GRAPH-UK, V46, P117, DOI 10.1016/j.cag.2014.09.007
   Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669
   Funkhouser T, 2003, ACM T GRAPHIC, V22, P83, DOI 10.1145/588272.588279
   He Q, 2013, NEUROCOMPUTING, V102, P52, DOI 10.1016/j.neucom.2012.01.040
   Heider P., 2011, P 4 EUR C 3D OBJ RET, P49, DOI DOI 10.2312/3DOR/3DOR11/049-056
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Huang G.-B., 2015, IEEE COMPUT INTELL M, V10
   Huang GB, 2007, NEUROCOMPUTING, V70, P3056, DOI 10.1016/j.neucom.2007.02.009
   Huang GB, 2006, NEUROCOMPUTING, V70, P489, DOI 10.1016/j.neucom.2005.12.126
   Huang GB, 2006, IEEE T NEURAL NETWOR, V17, P879, DOI 10.1109/TNN.2006.875977
   Huang GB, 2012, IEEE T SYST MAN CY B, V42, P513, DOI 10.1109/TSMCB.2011.2168604
   Kasun LLC, 2013, IEEE INTELL SYST, V28, P31
   Kazhdan M., 2003, Symposium on Geometry Processing, P156
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y., 1995, HDB BRAIN THEORY NEU, P3361
   Leng B, 2015, SIGNAL PROCESS, V112, P119, DOI 10.1016/j.sigpro.2014.09.005
   Leng B, 2015, NEUROCOMPUTING, V151, P593, DOI 10.1016/j.neucom.2014.06.084
   Liang NY, 2006, IEEE T NEURAL NETWOR, V17, P1411, DOI 10.1109/TNN.2006.880583
   Lin JR, 2013, IEEE INTELL SYST, V28, P35
   Qin FW, 2014, J ZHEJIANG U-SCI C, V15, P91, DOI 10.1631/jzus.C1300185
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Shapira L, 2010, INT J COMPUT VISION, V89, P309, DOI 10.1007/s11263-009-0279-0
   Tang JX, 2016, IEEE T NEUR NET LEAR, V27, P809, DOI 10.1109/TNNLS.2015.2424995
   Wang BT, 2015, NEUROCOMPUTING, V149, P224, DOI 10.1016/j.neucom.2014.03.076
   Wang Y., NEUROCOMPUTING
   Woodbury M A, 1950, MEMORANDUM REPORT, V42, P106
   Wu Z., 2014, ARXIV E PRINTS
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Xie ZG, 2015, COMPUT GRAPH FORUM, V34, P1, DOI 10.1111/cgf.12740
   Zeiler M. D., ARXIV13112901
   Zhang X., 2015, ACM T GRAPH P SIGGRA
   Zhu Z., CORR
   Zhu Z., DEEP LEARNING REPRES
NR 38
TC 34
Z9 40
U1 5
U2 66
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0925-2312
EI 1872-8286
J9 NEUROCOMPUTING
JI Neurocomputing
PD JAN 22
PY 2016
VL 174
BP 988
EP 998
DI 10.1016/j.neucom.2015.10.035
PN B
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA CZ7JY
UT WOS:000367276900043
DA 2020-02-19
ER

PT J
AU Deng, L
AF Deng, Li
TI Deep learning: from speech recognition to language and multimodal
   processing
SO APSIPA TRANSACTIONS ON SIGNAL AND INFORMATION PROCESSING
LA English
DT Article
DE Deep learning; Multimodal; Speech recognition; Language processing; Deep
   neural networks
ID HIDDEN MARKOV MODEL; NEURAL-NETWORKS; STATE; ALGORITHM; EFFICIENT; TIME
AB While artificial neural networks have been in existence for over half a century, it was not until year 2010 that they had made a significant impact on speech recognition with a deep form of such networks. This invited paper, based on my keynote talk given at Interspeech conference in Singapore in September 2014, will first reflect on the historical path to this transformative success, after providing brief reviews of earlier studies on (shallow) neural networks and on (deep) generative models relevant to the introduction of deep neural networks (DNN) to speech recognition several years ago. The role of well-timed academic-industrial collaboration is highlighted, so are the advances of big data, big compute, and the seamless integration between the application-domain knowledge of speech and general principles of deep learning. Then, an overview is given on sweeping achievements of deep learning in speech recognition since its initial success. Such achievements, summarized into six major areas in this article, have resulted in across-the-board, industry-wide deployment of deep learning in speech recognition systems. Next, more challenging applications of deep learning, natural language and multimodal processing, are selectively reviewed and analyzed. Examples include machine translation, knowledgebase completion, information retrieval, and automatic image captioning, where fresh ideas from deep learning, continuous-space embedding in particular, are shown to be revolutionizing these application areas albeit with less rapid pace than for speech and image recognition. Finally, a number of key issues in deep learning are discussed, and future directions are analyzed for perceptual tasks such as speech, image, and video, as well as for cognitive tasks involving natural language.
C1 [Deng, Li] Microsoft Res, One MicrosoftWay, Redmond, WA 98052 USA.
RP Deng, L (reprint author), Microsoft Res, One MicrosoftWay, Redmond, WA 98052 USA.
EM deng@microsoft.com
CR Abdel-Hamid O., 2012, ICASSP
   Abdel-Hamid O, 2014, IEEE-ACM T AUDIO SPE, V22, P1533, DOI 10.1109/TASLP.2014.2339736
   Abdel-Hamid Ossama, 2013, INTERSPEECH
   Abdelaziz A. H., 2015, P INTERSPEECH
   Acero A., 2000, P INTERSPEECH
   Bahdanau D, 2015, ICLR
   Baker JM, 2009, IEEE SIGNAL PROC MAG, V26, P75, DOI 10.1109/MSP.2009.932166
   BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181
   Bengio Y, 2003, J MACH LEARN RES, V3, P1137, DOI 10.1162/153244303322533223
   Bengio Yoshua, 1991, THESIS
   Bergstra J, 2012, J MACH LEARN RES, V13, P281
   Biem A, 2001, IEEE T SPEECH AUDI P, V9, P96, DOI 10.1109/89.902277
   Bilmes J, 2010, IEEE SIGNAL PROC MAG, V27, P29, DOI 10.1109/MSP.2010.938078
   Bishop CM, 2006, PATTERN RECOGNITION
   Bishop CM, 1995, NEURAL NETWORKS PATT
   Bordes Antoine, 2013, P NIPS
   Bourlard H.A., 1993, CONNECTIONIST SPEECH
   BRIDLE J, 1998, 1998 WORKSH LANG ENG
   Chen J., 2014, P INT C LEARN REPR A
   Chen Z., 2015, P INTERSPEECH
   Chengalvarayan R, 1998, IEEE T SPEECH AUDI P, V6, P505, DOI 10.1109/89.725317
   Chengalvarayan R, 1997, IEEE T SPEECH AUDI P, V5, P243, DOI 10.1109/89.568731
   Chengalvarayan R, 1997, IEEE T SPEECH AUDI P, V5, P232, DOI 10.1109/89.568730
   Collobert R, 2011, J MACH LEARN RES, V12, P2493
   Dahl George, 2011, P ICASSP
   Dahl GE, 2012, IEEE T AUDIO SPEECH, V20, P30, DOI 10.1109/TASL.2011.2134090
   Dean J., 2012, P NIPS
   Deng L, 1997, SPEECH COMMUN, V22, P93, DOI 10.1016/S0167-6393(97)00018-6
   Deng L, 2000, J ACOUST SOC AM, V108, P3036, DOI 10.1121/1.1315288
   Deng L, 2006, IEEE T AUDIO SPEECH, V14, P256, DOI 10.1109/TSA.2005.854107
   Deng L, 2006, IEEE T AUDIO SPEECH, V14, P425, DOI 10.1109/TSA.2005.855841
   Deng L, 2005, IEEE T SPEECH AUDI P, V13, P412, DOI 10.1109/TSA.2005.845814
   DENG L, 1992, SIGNAL PROCESS, V27, P65, DOI 10.1016/0165-1684(92)90112-A
   Deng L, 1998, SPEECH COMMUN, V24, P299, DOI 10.1016/S0167-6393(98)00023-5
   Deng L, 1997, IEEE T SPEECH AUDI P, V5, P319, DOI 10.1109/89.593305
   DENG L, 1992, J ACOUST SOC AM, V92, P3058, DOI 10.1121/1.404202
   Deng L, 2002, IEEE T SPEECH AUDI P, V10, P605, DOI 10.1109/TSA.2002.804538
   DENG L, 1994, NEURAL NETWORKS, V7, P331, DOI 10.1016/0893-6080(94)90027-2
   Deng L., 2012, P IEEE WORKSH SPOK L
   DENG L, 2006, [No title captured]
   Deng L., 2013, NIPS WORKSH LEARN OU
   Deng  L., 2013, P ICASSP
   Deng L., 2014, BOOK SPEECH AUDIO PR, P153
   Deng L., 2012, P ICASSP
   DENG L, 2006, P ICASSP
   Deng L., 2001, P ICASSP
   Deng L., 2007, P ICASSP
   Deng L., 2014, P ICASSP
   Deng L., 2003, SPEECH PROCESSING DY
   Deng L., 2003, MATH FDN SPEECH LANG, P115
   Deng L., 2010, P INTERSPEECH
   Deng L., 1999, COMPUTATIONAL MODELS, P199
   Deng L, 2007, IEEE T AUDIO SPEECH, V15, P13, DOI 10.1109/TASL.2006.876724
   Deng L, 2006, IEEE T AUDIO SPEECH, V14, P1492, DOI 10.1109/TASL.2006.878265
   Fang H., 2014, ARXIV14114952
   Fang Hao, 2015, P CVPR
   Frome A., 2013, P NIPS
   Gales MJF, 2011, ROBUST SPEECH RECOGNITION OF UNCERTAIN OR MISSING DATA: THEORY AND APPLICATIONS, P101, DOI 10.1007/978-3-642-21317-5_5
   Gao J., 2014, P EMNLP
   Gao J., 2013, P NIPS WORKSH DEEP L
   Gao Jianfeng, 2014, P ACL
   Giri Ritwik, 2015, P ICASSP
   Graves Alan, 2013, P ICASSP
   Hannun A., 2014, ARXIV14125567
   He XD, 2008, IEEE SIGNAL PROC MAG, V25, P14, DOI 10.1109/MSP.2008.926652
   HEIGOLD G., 2013, P ICASSP
   Hermansky H., 2000, P ICASSP
   Hershey J. R., 2014, TR2014117 MERL
   HINTON GE, 1995, SCIENCE, V268, P1158, DOI 10.1126/science.7761831
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Huang  J.-T., 2013, P ICASSP
   Huang Po-Sen, 2013, CIKM
   Huang X., 2001, P ICASSP
   Huang Y., 2014, INTERSPEECH
   Hutchinson B., 2012, P ICASSP
   Hutchinson B, 2013, IEEE T PATTERN ANAL, V35, P1944, DOI 10.1109/TPAMI.2012.268
   Jaitly N., 2011, P ICASSP
   Jaitly N., 2012, P INTERSPEECH
   Jordan MI, 1996, ACM COMPUT SURV, V28, P73, DOI 10.1145/234313.234348
   Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178
   Karpathy A., 2014, DEEP VISUAL SEMANTIC
   Karpathy Andrej, 2015, P CVPR
   Kashiwagi Y., 2013, P ASRU
   Kingsbury B., 2012, P INTERSPEECH
   Kirchhoff K, 1996, P ICSLP
   Kiros R., 2014, ARXIV14112539V1
   Krizhevsky A., 2012, P NIPS
   Lee L., 2003, P ICASSP
   Lee L., 2004, P ICASSP
   Li B., 2013, P ICASSP
   Li J., 2015, ROBUST AUTOMATIC SPE
   Li J., 2012, P IEEE SLT
   Li JY, 2014, IEEE-ACM T AUDIO SPE, V22, P745, DOI 10.1109/TASLP.2014.2304637
   Lin Hui, 2009, P ICASSP
   Ling ZH, 2015, IEEE SIGNAL PROC MAG, V32, P35, DOI 10.1109/MSP.2014.2359987
   Ling ZH, 2013, INT CONF ACOUST SPEE, P7825, DOI 10.1109/ICASSP.2013.6639187
   Ling ZH, 2013, IEEE T AUDIO SPEECH, V21, P2129, DOI 10.1109/TASL.2013.2269291
   Liu  X., 2015, P NAACL MAY
   Ma J., 2000, COMPUTER SPEECH LANG, P101
   Ma JZ, 2004, IEEE T SPEECH AUDI P, V12, P47, DOI 10.1109/TSA.2003.818074
   Ma JZ, 2003, IEEE T SPEECH AUDI P, V11, P590, DOI 10.1109/TSA.2003.818075
   Mao J., 2014, ARXIV14101090V1
   Mesnil G, 2013, P INTERSPEECH
   Mesnil G, 2015, IEEE-ACM T AUDIO SPE, V23, P530, DOI 10.1109/TASLP.2014.2383614
   MIKOLOV T., 2013, P NIPS
   Mohamed A., 2012, T AUDIO SPEECH LANG, V20, P1
   Mohamed A., 2012, P ICASSP
   Ostendorf M, 1996, IEEE T SPEECH AUDI P, V4, P360, DOI 10.1109/89.536930
   Ostendorf M., 1999, P ASRU
   Picone P., 1999, P ICASSP
   Rathinavalu C., 1997, SIG PROCESS, V55, P149
   ROBINSON AJ, 1994, IEEE T NEURAL NETWOR, V5, P298, DOI 10.1109/72.279192
   Sainath T. N., 2011, P ASRU
   Sainath T. N., 2013, P ASRU
   Sainath T.N., 2015, P INTERSPEECH
   Sainath T. N., 2013, P ICASSP
   Sainath TN, 2013, IEEE T AUDIO SPEECH, V21, P2267, DOI 10.1109/TASL.2013.2284378
   Sainath Tara N., 2015, P ICASSP
   Sak  H., 2015, P ICASSP
   Sak H., 2014, P INTERSPEECH
   Sak Hasim, 2015, INTERSPEECH
   Saon G., 2013, P ASRU
   Seide F., 2011, P INTERSPEECH, P437
   Seide F., 2003, P ICASSP
   Seltzer M., 2013, P ICASSP
   Sheikhzadeh H, 1994, IEEE T SPEECH AUDI P, V2, P80, DOI 10.1109/89.260337
   Shen Y., 2014, CIKM
   Shen Y., 2014, LEARNING SEMANTIC RE
   Socher Richard, 2013, P NIPS
   Su  H., 2013, P ICASSP
   Subramanya A., 2005, P ICME
   Sun JP, 2002, J ACOUST SOC AM, V111, P1086, DOI 10.1121/1.1420380
   Sundermeyer M., 2015, P ICASSP
   SUTSKEVER I., 2014, P NIPS
   Tai K., 2015, ARXIV150300075V2
   Togneri R, 2003, IEEE T SIGNAL PROCES, V51, P3061, DOI 10.1109/TSP.2003.819013
   Toshiteru H., 1988, P NIPS
   Tur G., 2012, P ICASSP
   Tuske Z., 2014, P INTERSPEECH
   Variani E., 2015, P ICASSP
   Vesely K., 2013, P INTERSPEECH
   Vilnis Luke, 2015, ICLR
   Vinyals O., 2012, P NIPS
   Vinyals Oriol, 2014, ARXIV14114555
   WAIBEL A, 1989, IEEE T ACOUST SPEECH, V37, P328, DOI 10.1109/29.21701
   Weston J., 2014, ARXIV14103916
   Yang Bishan, 2015, ICLR
   Yao K., 2012, P ICASSP
   Yu D, 2015, SIGNALS COMMUN TECHN, P1, DOI 10.1007/978-1-4471-5779-3
   Yu D., 2009, NIPS WORKSH DEEP LEA
   Yu D., 2009, P ICASSP
   Yu D., 2010, P INTERSPEECH
   Yu D., 2007, P ICASSP
   Yu D., 2012, INT WORKSH STAT MACH
   Yu D., 2012, P ICASSP
   Yu D., 2009, NIPS 2009 WORKSH DEE
   Yu D., 2008, IEEE T AUDIO SPEECH, V16
   Yu D, 2013, IEEE T AUDIO SPEECH, V21, P388, DOI 10.1109/TASL.2012.2227738
   Yu D, 2012, PATTERN RECOGN LETT, V33, P554, DOI 10.1016/j.patrec.2011.12.002
   Yu D, 2011, IEEE SIGNAL PROC MAG, V28, P145, DOI 10.1109/MSP.2010.939038
   Yu D, 2009, IEEE T AUDIO SPEECH, V17, P1348, DOI 10.1109/TASL.2009.2020890
   Yu D, 2008, COMPUT SPEECH LANG, V22, P415, DOI 10.1016/j.csl.2008.03.002
   Yu Dong, 2010, NIPS 2010 WORKSH DEE
   Zhang Z., 2004, P ICASSP
   Zhou J., 2003, P ICASSP
NR 166
TC 8
Z9 11
U1 0
U2 32
PU CAMBRIDGE UNIV PRESS
PI NEW YORK
PA 32 AVENUE OF THE AMERICAS, NEW YORK, NY 10013-2473 USA
SN 2048-7703
J9 APSIPA TRANS SIGNAL
JI APSIPA Trans. Signal Inf. Proc.
PD JAN 19
PY 2016
VL 5
AR e1
DI 10.1017/ATSIP.2015.22
PG 15
WC Engineering, Electrical & Electronic
SC Engineering
GA DO2OG
UT WOS:000377618800001
OA DOAJ Gold
DA 2020-02-19
ER

PT J
AU Mansanet, J
   Albiol, A
   Paredes, R
AF Mansanet, Jordi
   Albiol, Alberto
   Paredes, Roberto
TI Local Deep Neural Networks for gender recognition
SO PATTERN RECOGNITION LETTERS
LA English
DT Article
DE Gender recognition; Face analysis; Deep learning; Local Deep Neural
   Network
ID CLASSIFICATION; REPRESENTATION; FEATURES
AB Deep learning methods are able to automatically discover better representations of the data to improve the performance of the classifiers. However, in computer vision tasks, such as the gender recognition problem, sometimes it is difficult to directly learn from the entire image. In this work we propose a new model called Local Deep Neural Network (Local-DNN), which is based on two key concepts: local features and deep architectures. The model learns from small overlapping regions in the visual field using discriminative feed forward networks with several layers. We evaluate our approach on two well-known gender benchmarks, showing that our Local-DNN outperforms other deep learning methods also evaluated and obtains state-of-the-art results in both benchmarks. (C) 2015 Elsevier B.V. All rights reserved.
C1 [Mansanet, Jordi; Albiol, Alberto] Univ Politecn Valencia, ITEAM, E-46022 Valencia, Spain.
   [Paredes, Roberto] Univ Politecn Valencia, PRHLT Res Ctr, E-46022 Valencia, Spain.
RP Albiol, A (reprint author), Univ Politecn Valencia, ITEAM, E-46022 Valencia, Spain.
EM alalbiol@iteam.upv.es
RI Paredes, Roberto/T-6152-2017; Albiol, Alberto/H-5273-2015
OI Paredes, Roberto/0000-0002-5192-0021; Albiol,
   Alberto/0000-0002-1970-3289
FU Ministerio de Ciencia e Innovacin (Spain), Plan Nacional de I-D+i
   [TEC2009-09146]; FPI grant [BES-2010-032945]
FX This work was financially supported by the Ministerio de Ciencia e
   Innovacin (Spain), Plan Nacional de I-D+i, TEC2009-09146, and the FPI
   grant BES-2010-032945.
CR Baluja S, 2007, INT J COMPUT VISION, V71, P111, DOI 10.1007/s11263-006-8910-9
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Buchala S, 2004, 2004 2ND INTERNATIONAL IEEE CONFERENCE INTELLIGENT SYSTEMS, VOLS 1 AND 2, PROCEEDINGS, P88, DOI 10.1109/IS.2004.1344642
   Dago-Casas P., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P2152, DOI 10.1109/ICCVW.2011.6130514
   Eidinger E, 2014, IEEE T INF FOREN SEC, V9, P2170, DOI 10.1109/TIFS.2014.2359646
   Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231
   Fazl-Ersi E., 2014, IEEE ICIP 2014
   FIPA, 2011, FOLDS GEND EV LFW
   FIPA, 2011, FOLDS GEND EV GALL D
   Gallagher A., 2009, P CVPR
   Graf ABA, 2002, LECT NOTES COMPUT SC, V2525, P491
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Huang G., 2012, ADV NEURAL INFORM PR, V25, P773
   Huang Gary B., 2007, 0749 U MASS
   Krizhevsky A., 2009, TECHNICAL REPORT
   Krizhevsky A., 2012, ADV NEURAL INFORM PR
   Kumar N, 2009, IEEE I CONF COMP VIS, P365, DOI 10.1109/ICCV.2009.5459250
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Leng XM, 2008, IEEE IMAGE PROC, P1656, DOI 10.1109/ICIP.2008.4712090
   Levi G, 2015, IEEE COMPUT SOC CONF
   Mansanet J, 2014, LECT NOTES COMPUT SC, V8814, P274, DOI 10.1007/978-3-319-11758-4_30
   Moeini A, 2015, ELECTRON LETT, V51, P760, DOI 10.1049/el.2015.0520
   Moghaddam B, 2002, IEEE T PATTERN ANAL, V24, P707, DOI 10.1109/34.1000244
   NG C. B., 2012, CORR
   Ng C. B., 2012, LECT NOTES COMPUTER, P335, DOI DOI 10.1007/978-3-642-32695-0
   PAREDES R, 2001, P WORKSH PATT REC IN, P71
   Ren HY, 2014, INT C PATT RECOG, P2389, DOI 10.1109/ICPR.2014.414
   Schroff Florian, 2015, CORR
   Serre T, 2007, PROG BRAIN RES, V165, P33, DOI 10.1016/S0079-6123(06)65004-8
   Shakhnarovich G, 2002, FIFTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P16, DOI 10.1109/AFGR.2002.1004124
   Shan CF, 2012, PATTERN RECOGN LETT, V33, P431, DOI 10.1016/j.patrec.2011.05.016
   Shan CF, 2010, LECT NOTES COMPUT SC, V6475, P323, DOI 10.1007/978-3-642-17691-3_30
   Simonyan K., 2014, CORR
   Smolensky P., PARALLEL DISTRIBUTED, V1, P194
   Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244
   Taigman Y., 2014, C CVPR
   Tapia JE, 2013, IEEE T INF FOREN SEC, V8, P488, DOI 10.1109/TIFS.2013.2242063
   Villegas M, 2008, 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P1, DOI 10.1109/CVPRW.2008.4563123
   Villegas M, 2011, PATTERN RECOGN LETT, V32, P633, DOI 10.1016/j.patrec.2010.12.002
NR 39
TC 56
Z9 60
U1 1
U2 41
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0167-8655
EI 1872-7344
J9 PATTERN RECOGN LETT
JI Pattern Recognit. Lett.
PD JAN 15
PY 2016
VL 70
BP 80
EP 86
DI 10.1016/j.patrec.2015.11.015
PG 7
WC Computer Science, Artificial Intelligence
SC Computer Science
GA DC6ZW
UT WOS:000369369100012
OA Green Published
DA 2020-02-19
ER

PT B
AU Goodfellow, I
   Bengio, Y
   Courville, A
AF Goodfellow, Ian
   Bengio, Yoshua
   Courville, Aaron
BA Goodfellow, I
   Bengio, Y
   Courville, A
BF Goodfellow, I
   Bengio, Y
   Courville, A
TI Regularization for Deep Learning
SO DEEP LEARNING
SE Adaptive Computation and Machine Learning
LA English
DT Article; Book Chapter
NR 0
TC 11
Z9 11
U1 0
U2 0
PU MIT PRESS
PI CAMBRIDGE
PA ONE ROGERS ST, CAMBRIDGE, MA 02142 USA
BN 978-0-262-03561-3
J9 ADAPT COMPUT MACH LE
PY 2016
BP 221
EP 265
D2 10.1007/978-3-319-50890-0
PG 45
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
SC Computer Science
GA BI5JK
UT WOS:000412476200007
DA 2020-02-19
ER

PT B
AU Goodfellow, I
   Bengio, Y
   Courville, A
AF Goodfellow, Ian
   Bengio, Yoshua
   Courville, Aaron
BA Goodfellow, I
   Bengio, Y
   Courville, A
BF Goodfellow, I
   Bengio, Y
   Courville, A
TI Structured Probabilistic Models for Deep Learning
SO DEEP LEARNING
SE Adaptive Computation and Machine Learning
LA English
DT Article; Book Chapter
NR 0
TC 2
Z9 2
U1 0
U2 0
PU MIT PRESS
PI CAMBRIDGE
PA ONE ROGERS ST, CAMBRIDGE, MA 02142 USA
BN 978-0-262-03561-3
J9 ADAPT COMPUT MACH LE
PY 2016
BP 549
EP 579
D2 10.1007/978-3-319-50890-0
PG 31
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
SC Computer Science
GA BI5JK
UT WOS:000412476200016
DA 2020-02-19
ER

PT S
AU Crandall, DJ
   Li, YP
   Lee, S
   Huttenlocher, DP
AF Crandall, David J.
   Li, Yunpeng
   Lee, Stefan
   Huttenlocher, Daniel P.
BE Zamir, AR
   Hakeem, A
   VanGool, L
   Shah, M
   Szeliski, R
TI Recognizing Landmarks in Large-Scale Social Image Collections
SO LARGE-SCALE VISUAL GEO-LOCALIZATION
SE Advances in Computer Vision and Pattern Recognition
LA English
DT Article; Book Chapter
AB The dramatic growth of social media websites over the last few years has created huge collections of online images and raised new challenges in organizing them effectively. One particularly intuitive way of browsing and searching images is by the geo-spatial location of where on Earth they were taken, but most online images do not have GPS metadata associated with them. We consider the problem of recognizing popular landmarks in large-scale datasets of unconstrained consumer images by formulating a classification problem involving nearly 2 million images and 500 categories. The dataset and categories are formed automatically from geo-tagged photos from Flickr by looking for peaks in the spatial geo-tag distribution corresponding to frequently photographed landmarks. We learn models for these landmarks with a multiclass support vector machine, using classic vector-quantized interest point descriptors as features. We also incorporate the nonvisual metadata available on modern photo-sharing sites, showing that textual tags and temporal constraints lead to significant improvements in classification rate. Finally, we apply recent breakthroughs in deep learning with Convolutional Neural Networks, finding that these models can dramatically outperform the traditional recognition approaches to this problem, and even beat human observers in some cases. (This is an expanded and updated version of an earlier conference paper [ 23]).
C1 [Crandall, David J.; Lee, Stefan] Indiana Univ, Bloomington, IN 47405 USA.
   [Li, Yunpeng] Ecole Polytech Fed Lausanne, CH-1015 Lausanne, Switzerland.
   [Huttenlocher, Daniel P.] Cornell Univ, Ithaca, NY USA.
RP Crandall, DJ (reprint author), Indiana Univ, Bloomington, IN 47405 USA.
EM djcran@indiana.edu; yunpeng.li@epfl.ch; steflee@indiana.edu;
   dph@cs.cornell.edu
CR Arya S., 1993, ACM SIAM S DISCR ALG
   B. Collins, 2008, EUR C COMP VIS
   Bort J., 2013, BUSINESS INSIDER
   Comaniciu D., 2002, IEEE T PATTERN ANAL
   Crammer K, 2001, J MACH LEARN RES
   Crandall D. J., 2009, INT WORLD WID WEB C
   Crandall DJ, 2013, IEEE T PATTERN ANAL, V35, P2841, DOI 10.1109/TPAMI.2012.218
   Csurka G., 2004, ECCV WORKSH STAT LEA
   Deng J., 2009, IEEE C COMP VIS PATT
   Everingham M, 2008, THE PASCAL VOC
   Girshick R., 2013, ARXIV13112524
   Grauman K., 2011, VISUAL OBJECT RECOGN
   Griffin G., 2007, CALTECH 256 OBJECT C
   Hao Q, 2012, IEEE C COMP VIS PATT
   Hauff C, 2013, INT ACM SIGIR C
   Hays J., 2008, IEEE C COMP VIS PATT
   Jia Y., 2013, CAFFE OPEN SOURCE CO
   Joachims T., 1999, ADV KERNEL METHODS S
   Kalogerakis E, 2009, IEEE I CONF COMP VIS, P253, DOI 10.1109/ICCV.2009.5459259
   Krizhevsky A., 2012, ADV NEURAL INFORM PR
   Lee S., 2015, IEEE WINT C APPL COM
   Li X., 2008, EUR C COMP VIS
   Li Y., 2012, EUR C COMP VIS
   Li Y, 2009, IEEE INT C COMP VIS
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Luo JB, 2011, MULTIMED TOOLS APPL, V51, P187, DOI 10.1007/s11042-010-0623-y
   McAuley JJ, 2012, EUR C COMP VIS
   MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748
   Oquab M., 2014, IEEE C COMP VIS PATT
   Philbin J, 2007, IEEE C COMP VIS PATT, P2007
   Raguram R, 2012, BRIT MACH VIS C
   Razavian A- S., 2014, CORR, V1403, P6382
   Schroff F, 2007, IEEE I CONF COMP VIS, P2120
   Sermanet P, 2013, OVERFEAT INTEGRATED
   Snavely N, 2008, INT J COMPUT, V80
   Stone Z., 2008, 1 IEEE WORKSH INT VI
   Taigman Y, 2013, IEEE C COMP VIS PATT
   Torralba Antonio, 2011, IEEE C COMP VIS PATT
   Toshev A., 2013, ARXIV13124659
   Tsochantaridis Ioannis, 2004, INT C MACH LEARN
   Zeiler Matthew D., 2014, EUR C COMP VIS
   Zheng Y. -T., 2009, IEEE C COMP VIS PATT
NR 42
TC 4
Z9 5
U1 0
U2 1
PU SPRINGER INTERNATIONAL PUBLISHING AG
PI CHAM
PA GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN 2191-6586
BN 978-3-319-25781-5; 978-3-319-25779-2
J9 ADV COMPUT VIS PATT
PY 2016
BP 121
EP 144
DI 10.1007/978-3-319-25781-5_7
D2 10.1007/978-3-319-25781-5
PG 24
WC Computer Science, Artificial Intelligence; Robotics
SC Computer Science; Robotics
GA BH1KU
UT WOS:000398104400008
DA 2020-02-19
ER

PT B
AU Brosch, T
   Yoo, Y
   Tang, LYW
   Tam, R
AF Brosch, T.
   Yoo, Y.
   Tang, L. Y. W.
   Tam, R.
BA Wu, G
   Shen, D
   Sabuncu, MR
BF Wu, G
   Shen, D
   Sabuncu, MR
TI Deep learning of brain images and its application to multiple sclerosis
SO MACHINE LEARNING AND MEDICAL IMAGING
SE Elsevier and MICCAI Society Book Series
LA English
DT Article; Book Chapter
ID FUNCTIONAL ARCHITECTURE; RECEPTIVE-FIELDS; REPRESENTATIONS; SCALE;
   SEGMENTATION; IMPAIRMENT; NETWORKS; LESIONS; MODEL; MRI
C1 [Brosch, T.; Yoo, Y.; Tang, L. Y. W.; Tam, R.] Univ British Columbia, Vancouver, BC, Canada.
RP Brosch, T (reprint author), Univ British Columbia, Vancouver, BC, Canada.
CR Alonso-Ortiz E, 2015, MAGN RESON MED, V73, P70, DOI 10.1002/mrm.25198
   Barkhof F, 2002, CURR OPIN NEUROL, V15, P239, DOI 10.1097/00019052-200206000-00003
   Breiman L, 2001, STAT SCI, V16, P199, DOI 10.1214/ss/1009213726
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Brosch T., 2016, IEEE T MED IN PRESS
   Brosch T., 2014, P MED IM COMP COMP 2, P463
   Brosch T, 2015, LECT NOTES COMPUT SC, V9351, P3, DOI 10.1007/978-3-319-24574-4_1
   Brosch T, 2015, NEURAL COMPUT, V27, P211, DOI 10.1162/NECO_a_00682
   Chiaravalloti ND, 2008, LANCET NEUROL, V7, P1139, DOI 10.1016/S1474-4422(08)70259-X
   Cho K, 2011, LECT NOTES COMPUT SC, V6791, P10, DOI 10.1007/978-3-642-21735-7_2
   Ciresan D., 2012, ADV NEURAL INFORM PR, P1
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411
   Dalal N, 2005, PROC CVPR IEEE, P886
   Dauphin YN, 2015, ARXIV150204390V1
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   FARLEY BG, 1954, IRE T INFORM THEOR, P76, DOI 10.1109/TIT.1954.1057468
   Freund Y., 1992, ADV NEURAL INFORM PR, P912
   FUKUSHIMA K, 1980, BIOL CYBERN, V36, P193, DOI 10.1007/BF00344251
   Garcia-Lorenzo D, 2013, MED IMAGE ANAL, V17, P1, DOI 10.1016/j.media.2012.09.004
   Ghafoorian M., 2015, P IEEE INT S BIOM IM
   Gonen M, 2011, J MACH LEARN RES, V12, P2211
   Guo YR, 2014, LECT NOTES COMPUT SC, V8674, P308, DOI 10.1007/978-3-319-10470-6_39
   Haacke EM, 2004, MAGN RESON MED, V52, P612, DOI 10.1002/mrm.20198
   Havaei M., 2015, ARXIV150503540
   Hinton G., 2012, NEURAL NETWORKS TRIC, V9, P599, DOI DOI 10.1007/978-3-642-35289-8_32
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hjelm RD, 2014, NEUROIMAGE, V96, P245, DOI 10.1016/j.neuroimage.2014.03.048
   Hu H, 2014, J NUCL MED, V55
   HUBEL DH, 1968, J PHYSIOL-LONDON, V195, P215, DOI 10.1113/jphysiol.1968.sp008455
   HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837
   JAIN AK, 1990, 1990 IEEE INTERNATIONAL CONFERENCE ON SYSTEMS, MAN, AND CYBERNETICS, P14, DOI 10.1109/ICSMC.1990.142050
   Kamnitsas K., 2015, ISCHEMIC STROKE LESI, V13, P13, DOI DOI 10.1016/J.MEDIA.2016.10.004
   Kim M, 2013, LECT NOTES COMPUT SC, V8184, P1, DOI 10.1007/978-3-319-02267-3_1
   King DB, 2015, ACS SYM SER, V1214, P1
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   KURTZKE JF, 1983, NEUROLOGY, V33, P1444, DOI 10.1212/WNL.33.11.1444
   Laule C, 2004, J NEUROL, V251, P284, DOI 10.1007/s00415-004-0306-6
   Le Bihan D, 2001, J MAGN RESON IMAGING, V13, P534, DOI 10.1002/jmri.1076
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lee H., 2009, P 26 ANN INT C MACH, P609, DOI DOI 10.1145/1553374.1553453
   Lee H, 2011, COMMUN ACM, V54, P95, DOI 10.1145/2001269.2001295
   Li H., 2014, ARXIV14124526
   Liu SQ, 2015, IEEE T BIO-MED ENG, V62, P1132, DOI 10.1109/TBME.2014.2372011
   Llado X, 2012, INFORM SCIENCES, V186, P164, DOI 10.1016/j.ins.2011.10.011
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lowe D., 1999, P INT C COMP VIS, V2, P1150, DOI [10.1109/ICCV.1999.790410, DOI 10.1109/ICCV.1999.790410]
   MALLAT SG, 1989, IEEE T PATTERN ANAL, V11, P674, DOI 10.1109/34.192463
   Nair V, 2010, ICML, V27, P807, DOI DOI 10.0RG/PAPERS/432.PDF
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Plis SM, 2014, FRONT NEUROSCI-SWITZ, V8, DOI 10.3389/fnins.2014.00229
   POLYAK BT, 1992, SIAM J CONTROL OPTIM, V30, P838, DOI 10.1137/0330046
   Raina R, 2009, P 26 ANN INT C MACH, V382, P873, DOI DOI 10.1145/1553374.1553486
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Sainath TN, 2013, INT CONF ACOUST SPEE, P8614, DOI 10.1109/ICASSP.2013.6639347
   Salakhutdinov R, 2012, NEURAL COMPUT, V24, P1967, DOI 10.1162/NECO_a_00311
   Scherer D, 2010, LECT NOTES COMPUT SC, V6354, P92, DOI 10.1007/978-3-642-15825-4_10
   Shen L, 2011, LECT NOTES COMPUT SC, V7012, P27, DOI 10.1007/978-3-642-24446-9_4
   Song A. W., 2006, HDB FUNCTIONAL NEURO, P21
   Suk HI, 2015, BRAIN STRUCT FUNCT, V220, P841, DOI 10.1007/s00429-013-0687-3
   Suk HI, 2014, NEUROIMAGE, V101, P569, DOI 10.1016/j.neuroimage.2014.06.077
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
   Tieleman T., 2008, P 25 INT C MACH LEAR, P1064, DOI DOI 10.1145/1390156.1390290
   Tomassini V, 2012, NAT REV NEUROL, V8, P635, DOI 10.1038/nrneurol.2012.179
   Traboulsee A, 2008, NEUROIMAG CLIN N AM, V18, P651, DOI 10.1016/j.nic.2008.07.001
   Vaidya S., 2015, P IEEE INT S BIOM IM
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Werbos P., 1974, THESIS
   Wu GR, 2013, LECT NOTES COMPUT SC, V8150, P649, DOI 10.1007/978-3-642-40763-5_80
   Yoo Y, 2014, LECT NOTES COMPUT SC, V8679, P117, DOI 10.1007/978-3-319-10581-9_15
   Yuan M, 2006, J ROY STAT SOC B, V68, P49, DOI 10.1111/j.1467-9868.2005.00532.x
   Zeiler Matthew D, 2012, ARXIV12125701
   Zhang WL, 2015, NEUROIMAGE, V108, P214, DOI 10.1016/j.neuroimage.2014.12.061
NR 77
TC 1
Z9 1
U1 0
U2 0
PU ACADEMIC PRESS LTD-ELSEVIER SCIENCE LTD
PI LONDON
PA 125 LONDON WALL, LONDON EC2Y 5AS, ENGLAND
BN 978-0-12-804114-7; 978-0-12-804076-8
J9 ELS MIC SOC BOOK SER
JI Elsevier and MICCAI Society Book Series
PY 2016
BP 69
EP 96
DI 10.1016/B978-0-12-804076-8.00003-7
PG 28
WC Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Imaging Science & Photographic Technology; Radiology, Nuclear
   Medicine & Medical Imaging
SC Computer Science; Imaging Science & Photographic Technology; Radiology,
   Nuclear Medicine & Medical Imaging
GA BI0GW
UT WOS:000404775100004
DA 2020-02-19
ER

PT S
AU Havaei, M
   Guizard, N
   Larochelle, H
   Jodoin, PM
AF Havaei, Mohammad
   Guizard, Nicolas
   Larochelle, Hugo
   Jodoin, Pierre-Marc
BE Holzinger, A
TI Deep Learning Trends for Focal Brain Pathology Segmentation in MRI
SO MACHINE LEARNING FOR HEALTH INFORMATICS: STATE-OF-THE-ART AND FUTURE
   CHALLENGES
SE Lecture Notes in Computer Science
LA English
DT Article; Book Chapter
DE Brain tumor segmentation; Brain lesion segmentation; Deep learning;
   Convolutional Neural Network
ID TUMOR SEGMENTATION
AB Segmentation of focal (localized) brain pathologies such as brain tumors and brain lesions caused by multiple sclerosis and ischemic strokes are necessary for medical diagnosis, surgical planning and disease development as well as other applications such as tractography. Over the years, attempts have been made to automate this process for both clinical and research reasons. In this regard, machine learning methods have long been a focus of attention. Over the past two years, the medical imaging field has seen a rise in the use of a particular branch of machine learning commonly known as deep learning. In the non-medical computer vision world, deep learning based methods have obtained state-of-the-art results on many datasets. Recent studies in computer aided diagnostics have shown deep learning methods (and especially convolutional neural networks - CNN) to yield promising results. In this chapter, we provide a survey of CNN methods applied to medical imaging with a focus on brain pathology segmentation. In particular, we discuss their characteristic peculiarities and their specific configuration and adjustments that are best suited to segment medical images. We also underline the intrinsic differences deep learning methods have with other machine learning methods.
C1 [Havaei, Mohammad; Larochelle, Hugo; Jodoin, Pierre-Marc] Univ Sherbrooke, Sherbrooke, PQ, Canada.
   [Guizard, Nicolas] Imagia Inc, Montreal, PQ, Canada.
   [Jodoin, Pierre-Marc] Imeka Inc, Sherbrooke, PQ, Canada.
   [Larochelle, Hugo] Twitter Inc, Cambridge, MA USA.
RP Havaei, M (reprint author), Univ Sherbrooke, Sherbrooke, PQ, Canada.
EM seyed.mohammad.havaei@usherbrooke.ca
CR Ali H, 2015, ARAB J SCI ENG, V40, P3173, DOI 10.1007/s13369-015-1791-x
   Alvarez JM, 2012, LECT NOTES COMPUT SC, V7578, P376, DOI 10.1007/978-3-642-33786-4_28
   Arevalo J, 2015, IEEE ENG MED BIO, P797, DOI 10.1109/EMBC.2015.7318482
   Bakas S., 2015, P MULT BRAIN TUM IM, P5
   BAR Y, 2015, SPIE MED IMAGING, V9414, DOI DOI 10.1117/12.2083124
   Bauer S., 2012, P BRATS MICCAI
   Bauer S, 2013, PHYS MED BIOL, V58, pR97, DOI 10.1088/0031-9155/58/13/R97
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Brosch  T., 2016, IEEE T MED IMAGING
   Brosch T, 2015, LECT NOTES COMPUT SC, V9351, P3, DOI 10.1007/978-3-319-24574-4_1
   Carneiro G, 2015, LECT NOTES COMPUT SC, V9351, P652, DOI 10.1007/978-3-319-24574-4_78
   Chen H, 2015, IEEE J BIOMED HEALTH, V19, P1627, DOI 10.1109/JBHI.2015.2425041
   Ciresan DC, 2013, LECT NOTES COMPUT SC, V8150, P411, DOI 10.1007/978-3-642-40763-5_51
   Corso JJ, 2008, IEEE T MED IMAGING, V27, P629, DOI 10.1109/TMI.2007.912817
   Corso JJ, 2006, LECT NOTES COMPUT SC, V4191, P790
   Dollar P, 2013, IEEE I CONF COMP VIS, P1841, DOI 10.1109/ICCV.2013.231
   Dvorak P., 2015, P MULT BRAIN TUM IM, P13
   Erhan D, 2010, J MACH LEARN RES, V11, P625
   Farahani K, 2014, BRATS 2014 CHALLENGE
   Farahani K., 2015, BRATS 2015 CHALLENGE
   Festa J., 2013, P WORKSH BRAIN TUM S
   Gai D., 2016, T1 WEIGHTED IMAGES
   Gao M, HOLISTIC CLASSIFICAT
   Girardi Dominic, 2016, Brain Inform, V3, P133, DOI 10.1007/s40708-016-0038-2
   Goodfellow Ian J, 2013, ICML
   Gotz M., 2014, P BRATS CHALL MICCAI
   Guizard N., 2015, ROTATION INVARIANT M
   Hariharan B, 2014, ARXIV14071808
   Hariharan B, 2015, PROC CVPR IEEE, P447, DOI 10.1109/CVPR.2015.7298642
   Havaei Mohammad, 2016, Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries. First International Workshop, Brainles 2015, held in conjunction with MICCAI 2015. Revised Selected Papers: LNCS 9556, P195, DOI 10.1007/978-3-319-30858-6_17
   Havaei M., 2016, MED IMAGE ANAL
   Havaei M., 2015, INT J COMPUT ASSIST, P1
   Havaei M, 2014, INT C PATT RECOG, P556, DOI 10.1109/ICPR.2014.106
   Ho S, 2002, INT C PATT RECOG, P532, DOI 10.1109/ICPR.2002.1044788
   Holzinger Andreas, 2016, Brain Inform, V3, P119, DOI 10.1007/s40708-016-0042-6
   Holzinger A, 2014, BMC BIOINFORMATICS, V15, DOI 10.1186/1471-2105-15-S6-I1
   Jiang CY, 2004, 2004 IEEE SYMPOSIUM ON VIRTUAL ENVIRONMENTS, HUMAN-COMPUTRE INTERFACES AND MEASUREMENT SYSTEMS, P61, DOI 10.1109/VECIMS.2004.1397188
   Kamnitsas K., 2015, ISCHEMIC STROKE LESI, V13, P13, DOI DOI 10.1016/J.MEDIA.2016.10.004
   Kaus M, 1999, BILDVERARBEITUNG MED, P102
   Khotanlou H, 2007, PROCEEDINGS OF THE SIXTH INTERNATIONAL CONFERENCE ON ADVANCES IN PATTERN RECOGNITION, P198
   Kleesiek J., 2014, P BRATS MICCAI
   Klein T, 2015, DISTRIBUTED DEEP LEA, P18
   Krizhevsky A., 2012, NIPS
   Kwon D., 2014, P BRATS CHALL MICCAI
   Larochelle H., 2007, P 24 INT C MACH LEAR, P473, DOI DOI 10.1145/1273496.1273556
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee CH, 2005, LECT NOTES ARTIF INT, V3721, P121
   Li RJ, 2014, LECT NOTES COMPUT SC, V8675, P305, DOI 10.1007/978-3-319-10443-0_39
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Maas A. L., 2013, P ICML, V30, P1
   Margeta J, 2015, COMPUT METHOD BIOMEC, P1, DOI DOI 10.1080/21681163.2015
   Masci J, 2011, LECT NOTES COMPUT SC, V6791, P52, DOI 10.1007/978-3-642-21735-7_7
   Meier R., 2013, P NCI MICCAI BRATS, P31
   Menze BH, 2015, IEEE T MED IMAGING, V34, P1993, DOI 10.1109/TMI.2014.2377694
   Mostajahi M, 2015, PROC CVPR IEEE, P3376, DOI 10.1109/CVPR.2015.7298959
   Pereira Sergio, 2016, Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries. First International Workshop, Brainles 2015, held in conjunction with MICCAI 2015. Revised Selected Papers: LNCS 9556, P131, DOI 10.1007/978-3-319-30858-6_12
   Pinheiro P., 2014, P 31 INT C MACH LEAR, V32, P82, DOI DOI 10.1016/J.VETPAR.2009.02.011
   Prastawa M, 2003, LECT NOTES COMPUT SC, V2879, P530
   Putaala J, 2009, NEUROLOGY, V72, P1823, DOI 10.1212/WNL.0b013e3181a711df
   Rao V., 2014, MICCAI BRATS BRAIN, P31
   Rexilius J, 2007, PROC SPIE, V6514, DOI 10.1117/12.709410
   Reza S., 2013, LNCS
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Roth HR, 2014, LECT NOTES COMPUT SC, V8673, P520, DOI 10.1007/978-3-319-10404-1_65
   Roth HR, 2015, LECT NOTES COMPUT SC, V9349, P556, DOI 10.1007/978-3-319-24553-9_68
   ROTH HR, 2015, SPIE MED IMAGING, V9413
   Schlegl T, 2014, LECT NOTES COMPUT SC, V8848, P82, DOI 10.1007/978-3-319-13972-2_8
   Shin H-C, 2016, DEEP CONVOLUTIONAL N
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Stollenga M. F., 2015, ADV NEURAL INFORM PR, V28, P2980
   Styner M, 2008, MIDAS J, V2008, P1
   Tajbakhsh Nima, 2016, CONVOLUTIONAL NEURAL
   Tustison NJ, 2015, NEUROINFORMATICS, V13, P209, DOI 10.1007/s12021-014-9245-2
   Urban G, 2014, MICCAI BRATS BRAIN T, P31
   Vaidhya Kiran, 2016, Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries. First International Workshop, Brainles 2015, held in conjunction with MICCAI 2015. Revised Selected Papers: LNCS 9556, P181, DOI 10.1007/978-3-319-30858-6_16
   Vaidya S., LNCS
   van Ginneken B, 2015, I S BIOMED IMAGING, P286, DOI 10.1109/ISBI.2015.7163869
   van Tulder G, 2015, LECT NOTES COMPUT SC, V9349, P531, DOI 10.1007/978-3-319-24553-9_65
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Wang T, 2009, IEEE T BIO-MED ENG, V56, P781, DOI 10.1109/TBME.2009.2012423
   YOSINSKI J, 2014, ADV NEURAL INFORM PR, P3320, DOI DOI 10.1109/IJCNN.2016.7727519
   Zhang J, 2004, INT WORKSH ADV IM TE, P207
   Zhao L., 2012, BRATS MICCAI, P19
   Zikic D., 2014, P MICCAI BRATS, P36
NR 84
TC 19
Z9 18
U1 0
U2 2
PU SPRINGER INTERNATIONAL PUBLISHING AG
PI CHAM
PA GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN 0302-9743
EI 1611-3349
BN 978-3-319-50478-0; 978-3-319-50477-3
J9 LECT NOTES COMPUT SC
PY 2016
VL 9605
BP 125
EP 148
DI 10.1007/978-3-319-50478-0_6
D2 10.1007/978-3-319-50478-0
PG 24
WC Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Medical Informatics
SC Computer Science; Medical Informatics
GA BI2EL
UT WOS:000408904100007
DA 2020-02-19
ER

PT S
AU Supratak, A
   Wu, C
   Dong, H
   Sun, K
   Guo, Y
AF Supratak, Akara
   Wu, Chao
   Dong, Hao
   Sun, Kai
   Guo, Yike
BE Holzinger, A
TI Survey on Feature Extraction and Applications of Biosignals
SO MACHINE LEARNING FOR HEALTH INFORMATICS: STATE-OF-THE-ART AND FUTURE
   CHALLENGES
SE Lecture Notes in Computer Science
LA English
DT Article; Book Chapter
DE Feature extraction; Deep learning; Biosignals; Analytical systems
ID HEART-RATE-VARIABILITY; EPILEPTIC SEIZURE DETECTION; NEURAL-NETWORK;
   STAGES CLASSIFICATION; SPECTRAL POWER; RATE INCREASE; SLEEP STAGES; EEG
   SIGNALS; PREDICTION; ECG
AB Biosignals have become an important indicator not only for medical diagnosis and subsequent therapy, but also passive health monitoring. Extracting meaningful features from biosignals can help people understand the human functional state, so that upcoming harmful symptoms or diseases can be alleviated or avoided. There are two main approaches commonly used to derive useful features from biosignals, which are hand-engineering and deep learning. The majority of the research in this field focuses on hand-engineering features, which require domain-specific experts to design algorithms to extract meaningful features. In the last years, several studies have employed deep learning to automatically learn features from raw biosignals to make feature extraction algorithms less dependent on humans. These studies have also demonstrated promising results in a variety of biosignal applications. In this survey, we review different types of biosignals and the main approaches to extract features from the signal in the context of biomedical applications. We also discuss challenges and limitations of the existing approaches, and possible future research.
C1 [Supratak, Akara; Wu, Chao; Dong, Hao; Sun, Kai; Guo, Yike] Imperial Coll London, William Penney Lab, Data Sci Inst, South Kensington Campus, London SW7 2AZ, England.
RP Guo, Y (reprint author), Imperial Coll London, William Penney Lab, Data Sci Inst, South Kensington Campus, London SW7 2AZ, England.
EM as12212@ic.ac.uk; chao.wu@ic.ac.uk; dong11@ic.ac.uk; kai.sun09@ic.ac.uk;
   y.guo@ic.ac.uk
RI Supratak, Akara/J-8916-2019
OI Supratak, Akara/0000-0002-6739-7642
CR Adeli H, 2007, IEEE T BIO-MED ENG, V54, P205, DOI 10.1109/TBME.2006.886855
   Adnane M, 2012, EXPERT SYST APPL, V39, P1401, DOI 10.1016/j.eswa.2011.08.022
   Aggarwal JK, 1999, COMPUT VIS IMAGE UND, V73, P428, DOI 10.1006/cviu.1998.0744
   Ahammad Nabeel, 2014, Biomed Res Int, V2014, P450573, DOI 10.1155/2014/450573
   AHMED N, 1974, IEEE T COMPUT, VC 23, P90, DOI 10.1109/T-C.1974.223784
   Azarbayejani A., 1996, IMAGECOM
   Bachler Martin, 2013, Pervasive Computing and the Networked World. Joint International Conference, ICPCA/SWS 2012. Revised Selected Papers, P1, DOI 10.1007/978-3-642-37015-1_1
   Bandarabadi M, 2015, CLIN NEUROPHYSIOL, V126, P237, DOI 10.1016/j.clinph.2014.05.022
   Bengio Y., 2009, LEARNING DEEP ARCHIT, V2
   Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Berthomier C, 2007, SLEEP, V30, P1587, DOI 10.1093/sleep/30.11.1587
   Black MJ, 1997, INT J COMPUT VISION, V25, P23, DOI 10.1023/A:1007977618277
   Braithwaite J. J., PSYCHOPHYSIOLOGY, V49, P1017
   Caridakis G., 2011, P 17 INT C DIG SIGN, P1
   Chen LS, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P366, DOI 10.1109/AFGR.1998.670976
   Chi Yu Mike, 2010, IEEE Rev Biomed Eng, V3, P106, DOI 10.1109/RBME.2010.2084078
   CLANCY RR, 1988, EPILEPSIA, V29, P256, DOI 10.1111/j.1528-1157.1988.tb03715.x
   Cona F, 2014, MED ENG PHYS, V36, P954, DOI 10.1016/j.medengphy.2014.03.019
   Cowie R, 2001, IEEE SIGNAL PROC MAG, V18, P32, DOI 10.1109/79.911197
   Critchley HD, 2002, NEUROSCIENTIST, V8, P132, DOI 10.1177/107385840200800209
   De Silva LC, 1997, ICICS - PROCEEDINGS OF 1997 INTERNATIONAL CONFERENCE ON INFORMATION, COMMUNICATIONS AND SIGNAL PROCESSING, VOLS 1-3, P397, DOI 10.1109/ICICS.1997.647126
   Di Gennaro G, 2004, CLIN NEUROPHYSIOL, V115, P1169, DOI 10.1016/j.clinph.2003.12.016
   Eftekhar A, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0096235
   Ekman P, 1977, FACIAL ACTION CODING
   ELMAN JL, 1990, COGNITIVE SCI, V14, P179, DOI 10.1016/0364-0213(90)90002-E
   Essa IA, 1997, IEEE T PATTERN ANAL, V19, P757, DOI 10.1109/34.598232
   ETCOFF NL, 1992, COGNITION, V44, P227, DOI 10.1016/0010-0277(92)90002-Y
   Fisher RS, 2005, EPILEPSIA, V46, P470, DOI 10.1111/j.0013-9580.2005.66104.x
   Fraiwan L, 2012, COMPUT METH PROG BIO, V108, P10, DOI 10.1016/j.cmpb.2011.11.005
   Gadhoumi K, 2013, CLIN NEUROPHYSIOL, V124, P1745, DOI 10.1016/j.clinph.2013.04.006
   Gandhi T, 2010, EXPERT SYST APPL, V37, P3513, DOI 10.1016/j.eswa.2009.10.036
   Gavrila DM, 1999, COMPUT VIS IMAGE UND, V73, P82, DOI 10.1006/cviu.1998.0716
   Ghosh-Dastidar S, 2008, IEEE T BIO-MED ENG, V55, P512, DOI 10.1109/TBME.2007.905490
   Ghosh-Dastidar S, 2007, IEEE T BIO-MED ENG, V54, P1545, DOI 10.1109/TBME.2007.891945
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   Giakoumis D, 2011, IEEE T AFFECT COMPUT, V2, P119, DOI 10.1109/T-AFFC.2011.4
   Glorot X., 2011, P 14 INT C ART INT S, V15, P315
   Gomez P, 2004, INT J PSYCHOPHYSIOL, V53, P91, DOI 10.1016/j.ijpsycho.2004.02.002
   Greene BR, 2007, CLIN NEUROPHYSIOL, V118, P1348, DOI 10.1016/j.clinph.2007.02.015
   Guler NF, 2005, EXPERT SYST APPL, V29, P506, DOI 10.1016/j.eswa.2005.04.011
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Holzinger Andreas, 2012, Active Media Technology. 8th International Conference, AMT 2012. Proceedings, P646, DOI 10.1007/978-3-642-35236-2_64
   Holzinger A, 2016, BRAIN INFORM, P1
   Hush D., 1993, PROGR SUPERVISED NEU
   Jahankhani P, 2006, IEEE JOHN VINCENT ATANASOFF 2006 INTERNATIONAL SYMPOSIUM ON MODERN COMPUTING, PROCEEDINGS, P120, DOI 10.1109/JVA.2006.17
   Kaniusas E., 2012, BIOMEDICAL ENG
   Kapur A, 2005, LECT NOTES COMPUT SC, V3784, P1
   Kiranyaz S., 2015, REAL TIME PATIENT SP
   Kiymik MK, 2005, COMPUT BIOL MED, V35, P603, DOI 10.1016/j.compbiomed.2004.05.001
   Kleinsmith A, 2013, IEEE T AFFECT COMPUT, V4, P15, DOI 10.1109/T-AFFC.2012.16
   Kuhlmann L, 2009, ANN BIOMED ENG, V37, P2129, DOI 10.1007/s10439-009-9755-5
   Lajnef T., 2014, J NEUROSCI METH, P1
   LANG PJ, 1993, PSYCHOPHYSIOLOGY, V30, P261, DOI 10.1111/j.1469-8986.1993.tb03352.x
   Langkvist M., 2012, ADV ARTIFICIAL NEURA, V2012, P5, DOI DOI 10.1155/2012/107046
   Le Van Quyen M, 2001, LANCET, V357, P183, DOI 10.1016/S0140-6736(00)03591-1
   LeCun Yann, 1995, HDB BRAIN THEORY NEU, P255, DOI DOI 10.1109/IJCNN.2004.1381049
   LeCun Yann, 1995, HDB BRAIN THEORY NEU, V3361, P10
   Lee CM, 2005, IEEE T SPEECH AUDI P, V13, P293, DOI 10.1109/TSA.2004.838534
   Lehnertz K, 2005, CLIN NEUROPHYSIOL, V116, P493, DOI 10.1016/j.clinph.2004.08.020
   Lesh N., 1999, P 5 ACM SIGKDD INT C, P342
   Leutmezer F, 2003, EPILEPSIA, V44, P348, DOI 10.1046/j.1528-1157.2003.34702.x
   LI CW, 1995, IEEE T BIO-MED ENG, V42, P21, DOI 10.1109/10.362922
   Li SF, 2013, IEEE T NEUR SYS REH, V21, P880, DOI 10.1109/TNSRE.2013.2282153
   Liang SF, 2012, IEEE T INSTRUM MEAS, V61, P1649, DOI 10.1109/TIM.2012.2187242
   Litt B, 2001, NEURON, V30, P51, DOI 10.1016/S0896-6273(01)00262-8
   Litt B, 2002, CURR OPIN NEUROL, V15, P173, DOI 10.1097/00019052-200204000-00008
   Lockerd A., 2001, CHI 01 HUM FACT COMP, P279, DOI DOI 10.1145/634067.634233
   Logesparan L, 2013, IEEE ENG MED BIO, P1692, DOI 10.1109/EMBC.2013.6609844
   Looney D, 2012, IEEE PULSE, V3, P32, DOI 10.1109/MPUL.2012.2216717
   Marshall H., 2013, POLYSOMNOGRAPHY SLEE
   Martinez H. P., 2010, P 3 INT WORKSH AFF I, P15
   Martinez HP, 2013, IEEE COMPUT INTELL M, V8, P20, DOI 10.1109/MCI.2013.2247823
   Martis RJ, 2009, ANNU IEEE IND CONF, P422
   Matsugu M, 2003, NEURAL NETWORKS, V16, P555, DOI 10.1016/S0893-6080(03)00115-1
   Mayer C, 2016, ENTROPY-SWITZ, V18, DOI 10.3390/e18040129
   Mirowski P, 2009, CLIN NEUROPHYSIOL, V120, P1927, DOI 10.1016/j.clinph.2009.09.002
   Mormann F, 2005, CLIN NEUROPHYSIOL, V116, P569, DOI 10.1016/j.clinph.2004.08.025
   Mormann F, 2007, BRAIN, V130, P314, DOI 10.1093/brain/awl241
   Murray DM, 2008, ARCH DIS CHILD-FETAL, V93, pF187, DOI 10.1136/adc.2005.086314
   O'Brien J. F., 1999, AUTOMATIC JOINT PARA
   Ohayon MM, 2002, SLEEP MED REV, V6, P97, DOI 10.1053/smrv.2002.0186
   Opherk C, 2002, EPILEPSY RES, V52, P117, DOI 10.1016/S0920-1211(02)00215-2
   OPPENHEIMER SM, 1992, NEUROLOGY, V42, P1727, DOI 10.1212/WNL.42.9.1727
   Park Y, 2011, EPILEPSIA, V52, P1761, DOI 10.1111/j.1528-1167.2011.03138.x
   Pavlovic VI, 1997, IEEE T PATTERN ANAL, V19, P677, DOI 10.1109/34.598226
   Penzel T, 2003, NEUROPSYCHOPHARMACOL, V28, pS48, DOI 10.1038/sj.npp.1300146
   Petrosian A, 2000, NEUROCOMPUTING, V30, P201, DOI 10.1016/S0925-2312(99)00126-5
   Petrushin V., 2000, STUDIES, V3, P4
   Phomsiricharoenphant W., 2015, BMEICON 2014 7 BIOM, P1
   Picard Rosalind W., 1997, AFFECTIVE COMPUTING
   Picard RW, 2003, INT J HUM-COMPUT ST, V59, P55, DOI 10.1016/S1071-5819(03)00052-1
   Picard RW, 2001, IEEE T PATTERN ANAL, V23, P1175, DOI 10.1109/34.954607
   Piper D, 2014, BIOMED ENG-BIOMED TE, V59, P343, DOI 10.1515/bmt-2013-0139
   Polat K, 2007, APPL MATH COMPUT, V187, P1017, DOI 10.1016/j.amc.2006.09.022
   Ramgopal S, 2014, EPILEPSY BEHAV, V37, P291, DOI 10.1016/j.yebeh.2014.06.023
   Rechtschaffen A., 1968, MANUAL STANDARDIZED
   Rifai S, 2012, LECT NOTES COMPUT SC, V7577, P808, DOI 10.1007/978-3-642-33783-3_58
   Saab ME, 2005, CLIN NEUROPHYSIOL, V116, P427, DOI 10.1016/j.clinph.2004.08.004
   Sackellares JC, 2006, J CLIN NEUROPHYSIOL, V23, P509, DOI 10.1097/00004691-200612000-00003
   Sang-TaeLee B., 2000, P 6 INT C SPOK LANG
   Saritha C, 2008, Bulgarian Journal of Physics, V35, P68
   Saxena SC, 2002, INT J SYST SCI, V33, P1073, DOI 10.1080/00207720210167159
   Scheirer J, 2002, INTERACT COMPUT, V14, P93, DOI 10.1016/S0953-5438(01)00059-5
   SCHERER KR, 1986, PSYCHOL BULL, V99, P143, DOI 10.1037/0033-2909.99.2.143
   Schiano D. J., 2000, CHI 2000 Conference Proceedings. Conference on Human Factors in Computing Systems. CHI 2000. The Future is Here, P193
   Schulz H, 2007, AASM MANUAL SCORING
   Shenzhen Y.Y., 2015, IEEE INT S BROADB MU, P1, DOI DOI 10.1109/BSN.2015.7299399
   Shoeb A, 2010, P 27 INT C MACH LEAR, P975, DOI DOI 10.0RG/PAPERS/
   Song MH, 2005, INT J CONTROL AUTOM, V3, P571
   Stickel C, 2009, LECT NOTES COMPUT SC, V5614, P615, DOI 10.1007/978-3-642-02707-9_70
   Stuhlsatz A., 2010, P INT JOINT C NEUR N, P1
   Stuhlsatz A, 2011, INT CONF ACOUST SPEE, P5688
   Subasi A, 2007, EXPERT SYST APPL, V32, P1084, DOI 10.1016/j.eswa.2006.02.005
   Subasi A, 2007, COMPUT BIOL MED, V37, P227, DOI 10.1016/j.compbiomed.2005.12.003
   Supratak A, 2014, IEEE ENG MED BIO, P4184, DOI 10.1109/EMBC.2014.6944546
   Tao JH, 2005, LECT NOTES COMPUT SC, V3784, P981
   Teixeira CA, 2011, J NEUROSCI METH, V200, P257, DOI 10.1016/j.jneumeth.2011.07.002
   Togo F, 2001, AM J PHYSIOL-HEART C, V280, pH17
   Tsinalis O., 2015, ANN BIOMED ENG
   TURKER KS, 1993, PHYS THER, V73, P698
   Tzallas AT, 2009, IEEE T INF TECHNOL B, V13, P703, DOI 10.1109/TITB.2009.2017939
   Ubeyli ED, 2007, DIGIT SIGNAL PROCESS, V17, P675, DOI 10.1016/j.dsp.2006.11.009
   Valderrama M, 2012, BIOMED SIGNAL PROCES, V7, P237, DOI 10.1016/j.bspc.2011.05.005
   Van Quyen ML, 2003, EPILEPSIA, V44, P30
   Ververidis Dimitrios, 2004, 2004 12th European Signal Processing Conference (EUSIPCO), P341
   Vyzas E., 1998, Emotional and Intelligent: Tangled Knot of Cognition. Papers from the 1998 AAAI Fall Symposium, P176
   Wagner J, 2005, 2005 IEEE International Conference on Multimedia and Expo (ICME), Vols 1 and 2, P941
   Weil S, 2005, EPILEPTIC DISORD, V7, P199
   Witte H, 2003, IEEE T BIO-MED ENG, V50, P537, DOI 10.1109/TBME.2003.810708
   Xiao M, 2013, BIOMED SIGNAL PROCES, V8, P624, DOI 10.1016/j.bspc.2013.06.001
   Yang JL, 2015, BIO-MED MATER ENG, V26, pS1549, DOI 10.3233/BME-151454
   Yannakakis GN, 2008, INT J HUM-COMPUT ST, V66, P741, DOI 10.1016/j.ijhcs.2008.06.004
   Yannakakis GN, 2010, USER MODEL USER-ADAP, V20, P313, DOI 10.1007/s11257-010-9078-0
   Yao H., 2012, 2012 IEEE 25th International Conference on Micro Electro Mechanical Systems (MEMS), P769, DOI 10.1109/MEMSYS.2012.6170299
   Yetton BD, 2016, J NEUROSCI METH, V259, P72, DOI 10.1016/j.jneumeth.2015.11.015
   Yoshitomi Y, 2000, IEEE RO-MAN 2000: 9TH IEEE INTERNATIONAL WORKSHOP ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION, PROCEEDINGS, P178, DOI 10.1109/ROMAN.2000.892491
   Yu SN, 2007, PATTERN RECOGN LETT, V28, P1142, DOI 10.1016/j.patrec.2007.01.017
   Yu SN, 2009, EXPERT SYST APPL, V36, P2088, DOI 10.1016/j.eswa.2007.12.016
   Zandi AS, 2010, IEEE T BIO-MED ENG, V57, P1639, DOI 10.1109/TBME.2010.2046417
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhao QB, 2005, PROCEEDINGS OF THE 2005 INTERNATIONAL CONFERENCE ON NEURAL NETWORKS AND BRAIN, VOLS 1-3, P1089
   Zheng Y, 2014, CLIN NEUROPHYSIOL, V125, P1104, DOI 10.1016/j.clinph.2013.09.047
   Zimmermann Philippe, 2003, Int J Occup Saf Ergon, V9, P539
NR 144
TC 5
Z9 5
U1 0
U2 0
PU SPRINGER INTERNATIONAL PUBLISHING AG
PI CHAM
PA GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN 0302-9743
EI 1611-3349
BN 978-3-319-50478-0; 978-3-319-50477-3
J9 LECT NOTES COMPUT SC
PY 2016
VL 9605
BP 161
EP 182
DI 10.1007/978-3-319-50478-0_8
D2 10.1007/978-3-319-50478-0
PG 22
WC Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Medical Informatics
SC Computer Science; Medical Informatics
GA BI2EL
UT WOS:000408904100009
DA 2020-02-19
ER

PT S
AU Pereira, CR
   Pereira, DR
   Papa, JP
   Rosa, GH
   Yang, XS
AF Pereira, Clayton R.
   Pereira, Danillo R.
   Papa, Joao P.
   Rosa, Gustavo H.
   Yang, Xin-She
BE Holzinger, A
TI Convolutional Neural Networks Applied for Parkinson's Disease
   Identification
SO MACHINE LEARNING FOR HEALTH INFORMATICS: STATE-OF-THE-ART AND FUTURE
   CHALLENGES
SE Lecture Notes in Computer Science
LA English
DT Article; Book Chapter
DE Convolutional Neural Networks; Parkinson's Disease; Machine learning;
   Meta-heuristics
ID ALGORITHM; CLASSIFICATION; MODEL
AB Parkinson's Disease (PD) is a chronic and progressive illness that affects hundreds of thousands of people worldwide. Although it is quite easy to identify someone affected by PD when the illness shows itself (e.g. tremors, slowness of movement and freezing-of-gait), most works have focused on studying the working mechanism of the disease in its very early stages. In such cases, drugs can be administered in order to increase the quality of life of the patients. Since the beginning, it is well-known that PD patients feature the micrography, which is related to muscle rigidity and tremors. As such, most exams to detect Parkinson's Disease make use of handwritten assessment tools, where the individual is asked to perform some predefined tasks, such as drawing spirals and meanders on a template paper. Later, an expert analyses the drawings in order to classify the progressive of the disease. In this work, we are interested into aiding physicians in such task by means of machine learning techniques, which can learn proper information from digitized versions of the exams, and them recommending a probability of a given individual being affected by PD depending on its handwritten skills. Particularly, we are interested in deep learning techniques (i.e. Convolutional Neural Networks) due to their ability into learning features without human interaction. Additionally, we propose to fine-tune hyper-arameters of such techniques by means of meta-heuristic-based techniques, such as Bat Algorithm, Firefly Algorithm and Particle Swarm Optimization.
C1 [Pereira, Clayton R.] Univ Fed Sao Carlos, Dept Comp, Sao Carlos, SP, Brazil.
   [Pereira, Danillo R.; Papa, Joao P.; Rosa, Gustavo H.] Sao Paulo State Univ, Dept Comp, Bauru, Brazil.
   [Yang, Xin-She] Middlesex Univ, Sch Sci & Technol, London, England.
RP Papa, JP (reprint author), Sao Paulo State Univ, Dept Comp, Bauru, Brazil.
EM claytontey@gmail.com; dpereira@ic.unicamp.br; papa@fc.unesp.br;
   gth.rosa@uol.com.br; x.yang@mdx.ac.uk
RI Yang, Xin-She/A-5769-2012; Yang, Xin-She/I-5662-2019
OI Yang, Xin-She/0000-0001-8231-5556; Yang, Xin-She/0000-0001-8231-5556
CR Das R, 2010, EXPERT SYST APPL, V37, P1568, DOI 10.1016/j.eswa.2009.06.040
   Drotar P, 2014, COMPUT METH PROG BIO, V117, P405, DOI 10.1016/j.cmpb.2014.08.007
   Eichhorn TE, 1996, MOVEMENT DISORD, V11, P289, DOI 10.1002/mds.870110313
   Fedorovici Lucian-Ovidiu, 2012, Proceedings of the 2012 7th IEEE International Symposium on Applied Computational Intelligence and Informatics (SACI), P125, DOI 10.1109/SACI.2012.6249989
   Fundation P. D., 2016, STAT PARKINSONS WHO
   Geldenhuys WJ, 2015, PEERJ, V3, DOI 10.7717/peerj.1175
   Harel B, 2004, BRAIN COGNITION, V56, P24, DOI 10.1016/j.bandc.2004.05.002
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Holzinger A, BRAIN INF, V3
   Holzinger A, 2016, LECT NOTES COMPUT SC, V9817, P81, DOI 10.1007/978-3-319-45507-5_6
   Holzinger Katharina, 2014, Interactive Knowledge Discovery and Data Mining in Biomedical Informatics. State-of-the-Art and Future Challenges: LNCS 8401, P35, DOI 10.1007/978-3-662-43968-5_3
   Jia Yangqing, 2014, ARXIV14085093
   Kennedy J, 2001, SWARM INTELLIGENCE
   Khobragade N, 2015, IEEE ENG MED BIO, P2616, DOI 10.1109/EMBC.2015.7318928
   Kim H, 2015, IEEE ENG MED BIO, P3751, DOI 10.1109/EMBC.2015.7319209
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Palacios-Navarro G, 2015, J MED SYST, V39, DOI 10.1007/s10916-015-0289-0
   Papa JP, 2009, INT J IMAG SYST TECH, V19, P120, DOI 10.1002/ima.20188
   Papa J. P., 2015, P GEN EV COMP C GECC, P1449
   Papa JP, 2015, J COMPUT SCI-NETH, V9, P14, DOI 10.1016/j.jocs.2015.04.014
   Papa JP, 2012, PATTERN RECOGN, V45, P512, DOI 10.1016/j.patcog.2011.07.013
   Papa JP, 2016, APPL SOFT COMPUT, V46, P875, DOI 10.1016/j.asoc.2015.08.043
   Parkinson J., 1817, J NEUROPSYCHIATRY CL, V14, P223
   Pasluosta CF, 2015, IEEE J BIOMED HEALTH, V19, P1873, DOI 10.1109/JBHI.2015.2461555
   Pereira CR, 2015, COMP MED SY, P171, DOI 10.1109/CBMS.2015.34
   Rere L. M. R., 2016, COMPUT INTEL NEUROSC, V2016, P1
   Rosa G, 2015, LECT NOTES COMPUT SC, V9423, P683, DOI 10.1007/978-3-319-25751-8_82
   Rosenblum S, 2013, J NEUROL, V260, P2357, DOI 10.1007/s00415-013-6996-x
   Salakhutdinov R, 2012, NEURAL COMPUT, V24, P1967, DOI 10.1162/NECO_a_00311
   Shunan Zhao, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P4813, DOI 10.1109/ICASSP.2014.6854516
   Spadoto AA, 2011, IEEE ENG MED BIO, P7857, DOI 10.1109/IEMBS.2011.6091936
   Spadoto AA, 2010, IEEE ENG MED BIO, P6087, DOI 10.1109/IEMBS.2010.5627634
   Tsanas A, 2012, IEEE T BIO-MED ENG, V59, P1264, DOI 10.1109/TBME.2012.2183367
   Weber Silke Anna Theresa, 2014, GLOBAL ADV RES J MED, V3, P362
   WILCOXON F, 1945, BIOMETRICS BULL, V1, P80, DOI 10.2307/3001968
   Yang X.-S., 2008, NATURE INSPIRED META
   Yang XS, 2012, ENG COMPUTATION, V29, P464, DOI 10.1108/02644401211235834
   Yang XS, 2010, INT J BIO-INSPIR COM, V2, P78, DOI 10.1504/IJBIC.2010.032124
   Zhao Y, 2015, J PARKINSON DIS, V5, P369, DOI 10.3233/JPD-150568
NR 41
TC 7
Z9 7
U1 0
U2 2
PU SPRINGER INTERNATIONAL PUBLISHING AG
PI CHAM
PA GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN 0302-9743
EI 1611-3349
BN 978-3-319-50478-0; 978-3-319-50477-3
J9 LECT NOTES COMPUT SC
PY 2016
VL 9605
BP 377
EP 390
DI 10.1007/978-3-319-50478-0_19
D2 10.1007/978-3-319-50478-0
PG 14
WC Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Medical Informatics
SC Computer Science; Medical Informatics
GA BI2EL
UT WOS:000408904100020
DA 2020-02-19
ER

PT J
AU Sun, B
   Feng, H
   Chen, KF
   Zhu, XS
AF Sun, Biao
   Feng, Hui
   Chen, Kefan
   Zhu, Xinshan
TI A Deep Learning Framework of Quantized Compressed Sensing for Wireless
   Neural Recording
SO IEEE ACCESS
LA English
DT Article
DE Wireless neural recording; quantized compressive sensing; non-uniform
   quantization; deep learning
ID SIGNAL RECOVERY; HIGH-RESOLUTION; EFFICIENT; ALGORITHM; MATRICES
AB In low-power wireless neural recording tasks, signals must be compressed before transmission to extend battery life. Recently, compressed sensing (CS) theory has successfully demonstrated its potential in neural recording applications. In this paper, a deep learning framework of quantized CS, termed BW-NQ-DNN, is proposed, which consists of a binary measurement matrix, a non-uniform quantizer, and a non-iterative recovery solver. By training the BW-NQ-DNN, the three parts are jointly optimized. Experimental results on synthetic and real datasets reveal that BW-NQ-DNN not only drastically reduce the transmission bits but also outperforms the state-of-the-art CS-based methods. On the challenging high compression ratio task, the proposed approach still achieves high recovery performance and spike classification accuracy. This framework is of great values to wireless neural recoding devices, and many variants can be straightforwardly derived for low-power wireless telemonitoring applications.
C1 [Sun, Biao; Feng, Hui; Chen, Kefan; Zhu, Xinshan] Tianjin Univ, Sch Elect Engn & Automat, Tianjin 300072, Peoples R China.
RP Zhu, XS (reprint author), Tianjin Univ, Sch Elect Engn & Automat, Tianjin 300072, Peoples R China.
EM xszhu@tju.edu.cn
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61401303, 51578189]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61401303 and Grant 51578189.
CR Bach V, 2009, IEEE T INFORM THEORY, V55, P1683, DOI 10.1109/TIT.2009.2013020
   Bengio Y., 2013, ESTIMATING PROPAGATI
   BENNETT WR, 1948, AT&T TECH J, V27, P446, DOI 10.1002/j.1538-7305.1948.tb01340.x
   Blumensath T, 2009, APPL COMPUT HARMON A, V27, P265, DOI 10.1016/j.acha.2009.04.002
   Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16
   Candes EJ, 2008, CR MATH, V346, P589, DOI 10.1016/j.crma.2008.03.014
   Candes EJ, 2006, IEEE T INFORM THEORY, V52, P5406, DOI 10.1109/TIT.2006.885507
   Chae M., 2008, P IEEE INT SOL STAT, P146
   Chen F, 2012, IEEE J SOLID-ST CIRC, V47, P744, DOI 10.1109/JSSC.2011.2179451
   Chen SSB, 2001, SIAM REV, V43, P129, DOI 10.1137/S003614450037906X
   Donoho DL, 2009, P NATL ACAD SCI USA, V106, P18914, DOI 10.1073/pnas.0909892106
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Gersho A., 1992, VECTOR QUANTIZATION, V159
   Graf S., 2000, FDN QUANTIZATION PRO
   Gray RM, 1998, IEEE T INFORM THEORY, V44, P2325, DOI 10.1109/18.720541
   Haboba J, 2012, IEEE J EM SEL TOP C, V2, P443, DOI 10.1109/JETCAS.2012.2220392
   Haykin S, 2009, NEURAL NETWORKS LEAR, V3
   Henze DA, 2000, J NEUROPHYSIOL, V84, P390
   Hinton G., 2013, COURSERA NEURAL NETW
   Hui D, 2001, IEEE T INFORM THEORY, V47, P957, DOI 10.1109/18.915652
   Iliadis M., 2016, DEEP FULLY CONNECTED
   Jacques L., 2013, QUANTIZED ITERATIVE
   Jacques L, 2011, IEEE T INFORM THEORY, V57, P559, DOI 10.1109/TIT.2010.2093310
   Kamilov U, 2011, 2011 IEEE INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY PROCEEDINGS (ISIT), P459, DOI 10.1109/ISIT.2011.6034168
   Kamilov US, 2012, IEEE T SIGNAL PROCES, V60, P6270, DOI 10.1109/TSP.2012.2217334
   Kulkarni K, 2016, PROC CVPR IEEE, P449, DOI 10.1109/CVPR.2016.55
   Laska JN, 2011, APPL COMPUT HARMON A, V31, P429, DOI 10.1016/j.acha.2011.02.002
   Li SX, 2014, IEEE T SIGNAL PROCES, V62, P2850, DOI 10.1109/TSP.2014.2318139
   Liu BY, 2016, IEEE SENS J, V16, P8206, DOI 10.1109/JSEN.2016.2550602
   Misra V, 2011, IEEE T INFORM THEORY, V57, P5298, DOI 10.1109/TIT.2011.2158882
   Monajemi H, 2013, P NATL ACAD SCI USA, V110, P1181, DOI 10.1073/pnas.1219540110
   Mousavi A, 2015, ANN ALLERTON CONF, P1336, DOI 10.1109/ALLERTON.2015.7447163
   Quiroga RQ, 2004, NEURAL COMPUT, V16, P1661, DOI 10.1162/089976604774201631
   Ruck D. W., 1990, J NEURAL NETWORK COM, V2, P40
   Rumelhart D. E., 1985, ICS8506 U CAL
   Schwarz DA, 2014, NAT METHODS, V11, P670, DOI [10.1038/NMETH.2936, 10.1038/nmeth.2936]
   Sun JZ, 2009, 2009 IEEE INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY, VOLS 1- 4, P6, DOI 10.1109/ISIT.2009.5205695
   Suo YM, 2014, IEEE T BIOMED CIRC S, V8, P648, DOI 10.1109/TBCAS.2014.2359180
   Tipping ME, 2001, J MACH LEARN RES, V1, P211, DOI 10.1162/15324430152748236
   Tropp JA, 2007, IEEE T INFORM THEORY, V53, P4655, DOI 10.1109/TIT.2007.909108
   Unser M, 1999, IEEE SIGNAL PROC MAG, V16, P22, DOI 10.1109/79.799930
   Unser M, 2000, P IEEE, V88, P569, DOI 10.1109/5.843002
   Wang A., 2015, P 52 ANN DES AUT C, P173
   Wang AS, 2015, IEEE T CIRCUITS-II, V62, P104, DOI 10.1109/TCSII.2014.2387677
   Wu T, 2014, I C CONT AUTOMAT ROB, P7, DOI 10.1109/ICARCV.2014.7064270
   Xiong T, 2015, IEEE INT SYMP CIRC S, P1010, DOI 10.1109/ISCAS.2015.7168807
   Xiong T, 2014, BIOMED CIRC SYST C, P9, DOI 10.1109/BioCAS.2014.6981632
   Yang Z, 2013, IEEE T SIGNAL PROCES, V61, P2815, DOI 10.1109/TSP.2013.2256901
   Yin M, 2014, NEURON, V84, P1170, DOI 10.1016/j.neuron.2014.11.010
   Zhang J, 2014, IEEE T BIOMED CIRC S, V8, P485, DOI 10.1109/TBCAS.2013.2284254
NR 50
TC 16
Z9 16
U1 1
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 2169-3536
J9 IEEE ACCESS
JI IEEE Access
PY 2016
VL 4
DI 10.1109/ACCESS.2016.2604397
PG 10
WC Computer Science, Information Systems; Engineering, Electrical &
   Electronic; Telecommunications
SC Computer Science; Engineering; Telecommunications
GA EV3IN
UT WOS:000401652400001
OA DOAJ Gold
DA 2020-02-19
ER

PT J
AU Tariyal, S
   Majumdar, A
   Singh, R
   Vatsa, M
AF Tariyal, Snigdha
   Majumdar, Angshul
   Singh, Richa
   Vatsa, Mayank
TI Deep Dictionary Learning
SO IEEE ACCESS
LA English
DT Article
DE Deep learning; dictionary learning; feature representation
ID K-SVD; SPARSE REPRESENTATION; FACE RECOGNITION; ALGORITHM;
   CLASSIFICATION
AB Two popular representation learning paradigms are dictionary learning and deep learning. While dictionary learning focuses on learning "basis'' and "features'' by matrix factorization, deep learning focuses on extracting features via learning "weights'' or filter'' in a greedy layer by layer fashion. This paper focuses on combining the concepts of these two paradigms by proposing deep dictionary learning and show how deeper architectures can be built using the layers of dictionary learning. The proposed technique is compared with other deep learning approaches, such as stacked autoencoder, deep belief network, and convolutional neural network. Experiments on benchmark data sets show that the proposed technique achieves higher classification and clustering accuracies. On a real-world problem of electrical appliance classification, we show that deep dictionary learning excels where others do not yield at-par performance. We postulate that the proposed formulation can pave the path for a new class of deep learning tools.
C1 [Tariyal, Snigdha; Majumdar, Angshul; Singh, Richa; Vatsa, Mayank] IIIT Delhi, New Delhi 110020, India.
RP Vatsa, M (reprint author), IIIT Delhi, New Delhi 110020, India.
EM mayank@iiitd.ac.in
RI Singh, Richa/M-9961-2017; Vatsa, Mayank/I-5050-2013
OI Singh, Richa/0000-0003-4060-4573; Vatsa, Mayank/0000-0001-5952-2274
CR Agarwal A., 2014, INT C FIELD PROGR LO, P1
   Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   Arora S., 2014, MORE ALGORITHMS PROV
   Bar L, 2010, INT CONF ACOUST SPEE, P3578, DOI 10.1109/ICASSP.2010.5495916
   Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Boureau YL, 2010, PROC CVPR IEEE, P2559, DOI 10.1109/CVPR.2010.5539963
   BOURLARD H, 1988, BIOL CYBERN, V59, P291, DOI 10.1007/BF00332918
   Brown G, 2012, J MACH LEARN RES, V13, P27
   Caballero J, 2014, IEEE T MED IMAGING, V33, P979, DOI 10.1109/TMI.2014.2301271
   Candes EJ, 2006, IEEE T INFORM THEORY, V52, P5406, DOI 10.1109/TIT.2006.885507
   Chan TH, 2015, IEEE T IMAGE PROCESS, V24, P5017, DOI 10.1109/TIP.2015.2475625
   Chen Y, 2013, IEEE T GEOSCI REMOTE, V51, P217, DOI 10.1109/TGRS.2012.2201730
   Cho K., 2013, P INT C MACH LEARN, P1469
   Cho K. H., 2013, INT JOINT C NEUR NET, P1, DOI [10.1109/IJCNN.2013.6706831, DOI 10.1109/IJCNN.2013.6706831]
   Cui ZY, 2015, J INTELL ROBOT SYST, V80, pS121, DOI 10.1007/s10846-015-0213-3
   Dabbaghchian S, 2010, PATTERN RECOGN, V43, P1431, DOI 10.1016/j.patcog.2009.11.001
   Daubechies I, 2004, COMMUN PUR APPL MATH, V57, P1413, DOI 10.1002/cpa.20042
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Eggert J, 2004, IEEE IJCNN, P2529
   Elad M., 2006, IEEE COMP SOC C COMP, P895, DOI DOI 10.1109/CVPR.2006.142
   Elad M, 2006, IEEE T IMAGE PROCESS, V15, P3736, DOI 10.1109/TIP.2006.881969
   Engan K, 1999, INT CONF ACOUST SPEE, P2443, DOI 10.1109/ICASSP.1999.760624
   Fawzi A., 2014, DICT LEARNING FAST C
   Gulati M., 2014, P 1 ACM C EMB SYST E, P70, DOI DOI 10.1145/2674061.2674070
   Gulati M., DEPTH STUDY USING EM
   Gupta S, 2010, UBICOMP 2010: PROCEEDINGS OF THE 2010 ACM CONFERENCE ON UBIQUITOUS COMPUTING, P139
   Hillar C., 2011, CAN DICT LEARNING UN
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Huang K., 2007, ADV NEURAL INFORM PR, V19, P609
   Jadhav DV, 2009, NEUROCOMPUTING, V72, P1951, DOI 10.1016/j.neucom.2008.05.001
   Jain P, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P665
   Jiang ZL, 2013, IEEE T PATTERN ANAL, V35, P2651, DOI 10.1109/TPAMI.2013.88
   Jin W, 2014, PATTERN RECOGN LETT, V49, P193, DOI 10.1016/j.patrec.2014.07.015
   Khan N, 2012, INT C PATT RECOG, P3224
   Koh MS, 2009, IEEE INT WORKSH MULT, P74
   Kong S, 2015, IEEE T CONSUM ELECTR, V61, P24, DOI 10.1109/TCE.2015.7064107
   Larochelle H., 2008, P 25 INT C MACH LEAR, P536, DOI [10.1145/1390156.1390224, DOI 10.1145/1390156.1390224]
   Larochelle H., 2007, P 24 INT C MACH LEAR, P473, DOI DOI 10.1145/1273496.1273556
   Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565
   Luo H, 2010, SPARSE GROUP RESTRIC
   Mairal J., 2008, P IEEE C COMP VIS PA, P1, DOI [10.1109/CVPR.2008.4587652, DOI 10.1109/CVPR.2008.4587652]
   Mairal J., 2008, P ADV NEUR INF PROC, P1033
   Mairal J, 2008, LECT NOTES COMPUT SC, V5304, P43, DOI 10.1007/978-3-540-88690-7_4
   Majumdar A., 2009, RECENT ADV FACE RECO, P79
   Majumdar A, 2010, IEEE T SYST MAN CY B, V40, P1359, DOI 10.1109/TSMCB.2009.2038493
   Majumdar A, 2009, CAN J ELECT COMPUT E, V34, P136, DOI 10.1109/CJECE.2009.5599420
   Majumdar A, 2010, PATTERN RECOGN LETT, V31, P1959, DOI 10.1016/j.patrec.2010.06.014
   Makhzani A., 2013, K SPARSE AUTOENCODER
   NATARAJAN BK, 1995, SIAM J COMPUT, V24, P227, DOI 10.1137/S0097539792240406
   Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7
   PATI YC, 1993, CONFERENCE RECORD OF THE TWENTY-SEVENTH ASILOMAR CONFERENCE ON SIGNALS, SYSTEMS & COMPUTERS, VOLS 1 AND 2, P40, DOI 10.1109/ACSSC.1993.342465
   Pham D., 2008, P IEEE C COMP VIS PA, P1
   Protter M, 2009, IEEE T IMAGE PROCESS, V18, P27, DOI 10.1109/TIP.2008.2008065
   Rakotomamonjy A, 2013, NEUROCOMPUTING, V106, P126, DOI 10.1016/j.neucom.2012.10.024
   Ramirez I, 2010, PROC CVPR IEEE, P3501, DOI 10.1109/CVPR.2010.5539964
   Rubinstein R, 2013, IEEE T SIGNAL PROCES, V61, DOI 10.1109/TSP.2012.2226445
   Rubinstein R, 2010, P IEEE, V98, P1045, DOI 10.1109/JPROC.2010.2040551
   Rubinstein R, 2010, IEEE T SIGNAL PROCES, V58, P1553, DOI 10.1109/TSP.2009.2036477
   Salah R, 2011, P 28 INT C MACH LEAR, P833, DOI 10.1016/j.wasman.2010.10.006
   Salakhutdinov R., 2009, P INT C ART INT STAT, P448
   Son CH, 2014, IEEE T IMAGE PROCESS, V23, P2542, DOI 10.1109/TIP.2014.2319732
   Spielman D. A., 2012, P INT C LEARN THEOR
   Suo Y., 2014, STRUCTURED DICT LEAR
   Tian F, 2014, PROCEEDINGS OF THE TWENTY-EIGHTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1293
   Tzanetakis G, 2002, IEEE T SPEECH AUDI P, V10, P293, DOI 10.1109/TSA.2002.800560
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Yaghoobi M, 2009, IEEE T SIGNAL PROCES, V57, P2178, DOI 10.1109/TSP.2009.2016257
   Yang JC, 2010, PROC CVPR IEEE, P3517, DOI 10.1109/CVPR.2010.5539958
   Yang L., 2008, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/DYSPAN.2008.47
   Yang M, 2011, IEEE I CONF COMP VIS, P543, DOI 10.1109/ICCV.2011.6126286
   Yang M, 2010, IEEE IMAGE PROC, P1601, DOI 10.1109/ICIP.2010.5652363
   Yin J, 2012, NEUROCOMPUTING, V77, P120, DOI 10.1016/j.neucom.2011.08.018
   Zhang L, 2012, IEEE T SIGNAL PROCES, V60, P1684, DOI 10.1109/TSP.2011.2179539
   Zhang QA, 2010, PROC CVPR IEEE, P2691, DOI 10.1109/CVPR.2010.5539989
NR 77
TC 28
Z9 28
U1 0
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 2169-3536
J9 IEEE ACCESS
JI IEEE Access
PY 2016
VL 4
BP 10096
EP 10109
DI 10.1109/ACCESS.2016.2611583
PG 14
WC Computer Science, Information Systems; Engineering, Electrical &
   Electronic; Telecommunications
SC Computer Science; Engineering; Telecommunications
GA EU8TS
UT WOS:000401312000001
OA DOAJ Gold
DA 2020-02-19
ER

PT J
AU Baek, J
   Sohn, K
AF Baek, Junghan
   Sohn, Keemin
TI Deep-Learning Architectures to Forecast Bus Ridership at the Stop and
   Stop-To-Stop Levels for Dense and Crowded Bus Networks
SO APPLIED ARTIFICIAL INTELLIGENCE
LA English
DT Article
ID TRAFFIC VOLUME; NEURAL-NETWORK; TRANSIT RIDERSHIP; STATION BOARDINGS;
   DEMAND; PREDICTION; MODELS
AB The conventional transit assignment models that depend on either probabilistic or deterministic theory have failed to accurately estimate rider demand for dense and crowded bus transit networks. It is well known that the existing models are so blunt that they cannot accommodate the impact of miscellaneous changes in activity and transportation systems on bus demand. Recently, artificial neural networks (ANNs) have been refocused after two monumental breakthroughs: Big-data and a novel pre-training method. A deep-learning model, which simply represents an ANN with multiple hidden layers, has had a great success in recognizing images, human voices, and handwritten texts. The present study adopted a deep-learning model to forecast bus ridership at the stop and stop-to-stop levels. While the stop-level model, which had insufficient training data, suffered from an overfitting of the data, the stop-to-stop-level model showed good performance both in training and testing. The success of the latter model is owed to a larger sample size compared with the former model. This represents the first meaningful attempt to apply a data-driven approach to forecasting transportation demand.
C1 [Baek, Junghan; Sohn, Keemin] Chung Ang Univ, Dept Urban Engn, 221,Heukseok Dong, Seoul 156756, South Korea.
RP Sohn, K (reprint author), Chung Ang Univ, Dept Urban Engn, 221,Heukseok Dong, Seoul 156756, South Korea.
EM kmsohn@cau.ac.kr
OI Sohn, Keemin/0000-0002-7270-7094
FU Chung-Ang University Research Grants; National Research Foundation of
   Korea (NRF) grant - Korean government (MSIP) [2015005843]
FX This research was supported by the Chung-Ang University Research Grants
   in 2016, and also supported by the National Research Foundation of Korea
   (NRF) grant funded by the Korean government (MSIP) (2015005843).
CR Boyle D. K, 2006, TRANSPORTATION RES B, V66
   Cervero R, 2006, J AM PLANN ASSOC, V72, P285, DOI 10.1080/01944360608976751
   Choi J, 2012, TRANSPORTATION, V39, P705, DOI 10.1007/s11116-011-9368-3
   Chu X., 2004, RIDERSHIP MODELS STO
   Dargay JM, 2002, J TRANSP ECON POLICY, V36, P73
   Dean J., 2012, ADV NEURAL INFORM PR, V25, P1223
   Deng L, 2013, FOUND TRENDS SIGNAL, V7, pI, DOI 10.1561/2000000039
   Dill J., 2013, 92 ANN M TRANSP RES
   Dougherty MS, 1997, INT J FORECASTING, V13, P21, DOI 10.1016/S0169-2070(96)00697-8
   Estupinan N, 2008, TRANSPORT RES A-POL, V42, P296, DOI 10.1016/j.tra.2007.10.006
   Fang H, 2012, 26 ANN C NEUR INF PR
   Fischer Asja, 2012, Progress in Pattern Recognition, Image Analysis, ComputerVision, and Applications. Proceedings 17th Iberoamerican Congress, CIARP 2012, P14, DOI 10.1007/978-3-642-33275-3_2
   Friedrich M., 2007, TRANSPORT RES REC, V1752, P100
   HASHEMI RR, 1995, EXPERT SYST APPL, V9, P247, DOI 10.1016/0957-4174(95)00002-Q
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hoogendoorn-Lanser S, 2005, TRANSPORT RES REC, P27
   Kuby M, 2004, TRANSPORT RES A-POL, V38, P223, DOI 10.1016/j.tra.2003.10.006
   Lam SH, 2002, TRANSPORT RES REC, P58
   Ledoux C, 1997, TRANSPORT RES C-EMER, V5, P287, DOI 10.1016/S0968-090X(97)00015-6
   Lemieux J., 2015, 2015 IEEE VEH POW PR, P1, DOI DOI 10.1109/VPPC.2015.7353037
   Lv YS, 2015, IEEE T INTELL TRANSP, V16, P865, DOI 10.1109/TITS.2014.2345663
   Ma XL, 2015, TRANSPORT RES C-EMER, V54, P187, DOI 10.1016/j.trc.2015.03.014
   Moniruzzaman M, 2016, TRANSPORT RES C-EMER, V63, P182, DOI 10.1016/j.trc.2015.12.004
   Ng A., 2011, LECT NOTES STANFORD
   Nokel K, 2009, TRANSP RES RECORD, P60, DOI 10.3141/2111-08
   Park B, 1998, TRANSPORT RES REC, P39, DOI 10.3141/1651-06
   Parsons Brinckerhoff Quade and Douglas Inc., 1996, TRANS URB FORM
   Pulugurtha SS, 2012, J PUBLIC TRANSPORT, V15, P33, DOI 10.5038/2375-0901.15.1.3
   Ramos-Santiago LE, 2016, URBAN STUD, V53, P915, DOI 10.1177/0042098015571057
   Ritchie S. G., 1993, Transportation Research Part C (Emerging Technologies), V1C, P203
   Smith B.L., 1994, TRANSPORT RES REC, V1453, P98
   Sohn K, 2010, CITIES, V27, P358, DOI 10.1016/j.cities.2010.05.001
   Wenhao Huang, 2013, Advanced Data Mining and Applications. 9th International Conference, ADMA 2013. Proceedings: LNCS 8347, P165, DOI 10.1007/978-3-642-53917-6_15
   Yai T, 1997, TRANSPORT RES B-METH, V31, P195, DOI 10.1016/S0191-2615(96)00025-2
   Yao X, 2007, COMPUT ENVIRON URBAN, V31, P535, DOI 10.1016/j.compenvurbsys.2007.08.005
   Zhao JB, 2014, TRANSPORTATION, V41, P133, DOI 10.1007/s11116-013-9492-3
   Zhao JB, 2013, CITIES, V35, P114, DOI 10.1016/j.cities.2013.07.002
   Zhu JZ, 2014, TRANSPORT RES C-EMER, V47, P139, DOI 10.1016/j.trc.2014.06.011
NR 38
TC 4
Z9 5
U1 5
U2 18
PU TAYLOR & FRANCIS INC
PI PHILADELPHIA
PA 530 WALNUT STREET, STE 850, PHILADELPHIA, PA 19106 USA
SN 0883-9514
EI 1087-6545
J9 APPL ARTIF INTELL
JI Appl. Artif. Intell.
PY 2016
VL 30
IS 9
BP 861
EP 885
DI 10.1080/08839514.2016.1277291
PG 25
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
SC Computer Science; Engineering
GA EP4FB
UT WOS:000397335000003
DA 2020-02-19
ER

PT J
AU Hoffman, J
   Pathak, D
   Tzeng, E
   Long, J
   Guadarrama, S
   Darrell, T
   Saenko, K
AF Hoffman, Judy
   Pathak, Deepak
   Tzeng, Eric
   Long, Jonathan
   Guadarrama, Sergio
   Darrell, Trevor
   Saenko, Kate
TI Large Scale Visual Recognition through Adaptation using Joint
   Representation and Multiple Instance Learning
SO JOURNAL OF MACHINE LEARNING RESEARCH
LA English
DT Article
DE Computer Vision; Deep Learning; Transfer Learning; Large Scale Learning
AB A major barrier towards scaling visual recognition systems is the difficulty of obtaining labeled images for large numbers of categories. Recently, deep convolutional neural networks (CNNs) trained used 1.2M+ labeled images have emerged as clear winners on object classification benchmarks. Unfortunately, only a small fraction of those labels are available with bounding box localization for training the detection task and even fewer pixel level annotations are available for semantic segmentation. It is much cheaper and easier to collect large quantities of image-level labels from search engines than it is to collect scene-centric images with precisely localized labels. We develop methods for learning large scale recognition models which exploit joint training over both weak (image-level) and strong (bounding box) labels and which transfer learned perceptual representations from strongly-labeled auxiliary tasks. We provide a novel formulation of a joint multiple instance learning method that includes examples from object-centric data with image-level labels when available, and also performs domain transfer learning to improve the underlying detector representation. We then show how to use our large scale detectors to produce pixel level annotations. Using our method, we produce a >7.6K category detector and release code and models at lsda.berkeleyvision.org.
C1 [Hoffman, Judy; Pathak, Deepak; Tzeng, Eric; Long, Jonathan; Guadarrama, Sergio; Darrell, Trevor] Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA.
   [Saenko, Kate] Univ Massachusetts, Dept Comp Sci, Lowell, MA 01854 USA.
   [Guadarrama, Sergio] Google Res, Mountain View, CA USA.
RP Hoffman, J (reprint author), Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA.
EM JHOFFMAN@EECS.BERKELEY.EDU; PATHAK@BERKELEY.EDU;
   ETZENG@EECS.BERKELEY.EDU; JONLONG@BERKELEY.EDU;
   SGUADA@EECS.BERKELEY.EDU; TREVOR@EECS.BERKELEY.EDU; SAENKO@CS.UML.EDU
CR Alexe B., 2010, P CVPR
   Ali K., 2014, IEEE C COMP VIS PATT
   Andrews S, 2002, ADV NEURAL INFORM PR, V15, P561
   Aytar Y., 2012, BRIT MACH VIS C
   Aytar Y., 2011, ICCV
   Bengio Yoshua, 2009, P ICML
   Berg A., 2012, IMAGENET LARGE SCALE
   Bilen H, 2014, INT J COMPUT VISION, V106, P237, DOI 10.1007/s11263-013-0646-8
   Borth D., 2013, ACM MULT C
   Cinbis R.G., 2014, CVPR
   Dalai N., 2005, P CVPR
   Deselaers T., 2012, IJCV
   Dietterich T. G., 1997, ARTIFICIAL INTELLIGE
   Donahue J., 2013, COMPUTER VISION PATT
   Donahue J., 2014, P ICML
   Duan L., 2012, P ICML
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Fernando B., 2013, P ICCV
   Fischer P., 2014, ABS14055769 ARXIV
   Galleguillos C., 2008, ECCV
   Ganin Yaroslav, 2015, ICML
   Ghifary M., 2014, CORR
   Girshick R., 2014, P CVPR
   Goehring D., 2014, ICRA
   Gong B., 2012, P CVPR
   Guillaumin M, 2014, INT J COMPUT VISION, V110, P328, DOI 10.1007/s11263-014-0713-9
   Guillaumin M, 2012, PROC CVPR IEEE, P3202, DOI 10.1109/CVPR.2012.6248055
   He K., 2014, P ECCV
   Hoeim D., 2012, P ECCV
   Hoffman J., 2015, CVPR
   Hoffman J., 2013, P ICLR
   Hoffman J., 2014, NEURAL INFORM PROCES
   Hoffman Judy, 2013, CORR
   Jia Yangqing, 2014, ARXIV14085093
   Krahenbuhl P., 2014, P ECCV
   Krizhevsky A., 2012, P NIPS
   Kulis B., 2011, P CVPR
   Kumar M. P., 2010, P NIPS
   LeCun Yann, 1989, NEURAL COMPUTATION
   Long Jonathan, 2015, CVPR
   Long M., 2015, ICML
   LOWE D, 2004, IJCV
   Matan O., 1992, NEURAL INFORM PROCES, V4, P488
   Mrowca D., 2015, ICCV
   Ning F, 2005, IEEE T IMAGE PROCESS, V14, P1360, DOI 10.1109/TIP.2005.852470
   Oh Song Hyun, 2014, ICML
   Pandey M., 2011, P ICCV
   Papandreou G., 2015, CORR
   Pathak D., 2015, ICCV
   Pathak D., 2014, CORR
   Russakovsky O., 2014, ARXIV14090575
   Saenko K., 2010, P ECCV
   Sermanet P., 2013, CORR
   Simonyan K., 2014, 14091556 ARXIV, DOI DOI 10.1016/J.INFSOF.2008.09.005
   Singh S., 2012, ECCV
   Siva P., 2013, P CVPR
   Siva Parthipan, 2012, ECCV
   Song H.O., 2014, P NIPS
   Tzeng Eric, 2014, CORR
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Vezhnevets A., 2014, CVPR
   Wang C., 2014, EUR C COMP VIS ECCV
   Wolf R., 1994, ADV NEURAL INFORM PR, P745
   Xu J., 2014, IEEE T PATT IN PRESS
   Yang J., 2007, ACM MULTIMEDIA
   Yang J., 2007, ICDM WORKSH
   Yu C. N. J., 2009, P 26 ANN INT C MACH, P1169, DOI DOI 10.1145/1553374.1553523
   Yuille AL, 2003, NEURAL COMPUT, V15, P915, DOI 10.1162/08997660360581958
   Zhang C., 2005, ADV NEURAL INFORM PR
NR 70
TC 0
Z9 0
U1 1
U2 2
PU MICROTOME PUBL
PI BROOKLINE
PA 31 GIBBS ST, BROOKLINE, MA 02446 USA
SN 1532-4435
J9 J MACH LEARN RES
JI J. Mach. Learn. Res.
PY 2016
VL 17
AR 142
PG 31
WC Automation & Control Systems; Computer Science, Artificial Intelligence
SC Automation & Control Systems; Computer Science
GA EH3GT
UT WOS:000391660800001
DA 2020-02-19
ER

PT J
AU Shin, HC
   Lu, L
   Kim, L
   Seff, A
   Yao, JH
   Summers, RM
AF Shin, Hoo-Chang
   Lu, Le
   Kim, Lauren
   Seff, Ari
   Yao, Jainhua
   Summers, Ronald M.
TI Interleaved Text/Image Deep Mining on a Large-Scale Radiology Database
   for Automated Image Interpretation
SO JOURNAL OF MACHINE LEARNING RESEARCH
LA English
DT Article
DE Deep learning; Convolutional Neural Networks; Topic Models; Natural
   Language Processing; Medical Imaging
AB Despite tremendous progress in computer vision, there has not been an attempt to apply machine learning on very large-scale medical image databases. We present an interleaved text/image deep learning system to extract and mine the semantic interactions of radiology images and reports from a national research hospital's Picture Archiving and Communication System. With natural language processing, we mine a collection of similar to 216K representative two-dimensional images selected by clinicians for diagnostic reference and match the images with their descriptions in an automated manner. We then employ a weakly supervised approach using all of our available data to build models for generating approximate interpretations of patient images. Finally, we demonstrate a more strictly supervised approach to detect the presence and absence of a number of frequent disease types, providing more specific interpretations of patient scans. A relatively small amount of data is used for this part, due to the challenge in gathering quality labels from large raw text data. Our work shows the feasibility of large-scale learning and prediction in electronic patient records available in most modern clinical institutions. It also demonstrates the trade-offs to consider in designing machine learning systems for analyzing large medical data.
C1 [Shin, Hoo-Chang; Lu, Le; Kim, Lauren; Seff, Ari; Yao, Jainhua; Summers, Ronald M.] NIH, Imaging Biomarkers & Comp Aided Diag Lab Radiol &, Ctr Clin, Bldg 10, Bethesda, MD 20892 USA.
RP Shin, HC (reprint author), NIH, Imaging Biomarkers & Comp Aided Diag Lab Radiol &, Ctr Clin, Bldg 10, Bethesda, MD 20892 USA.
EM HOOCHANG.SHIN@NIH.GOV; LE.LU@NIH.GOV; LAUREN.KIM2@NIH.GOV;
   ARI.SEFF@NIH.GOV; JYAO@CC.NIH.GOV; RMS@NIH.GOV
RI Lu, Le/AAD-7619-2020
OI Lu, Le/0000-0002-6799-9416
FU Intramural Research Program of the National Institutes of Health
   Clinical CenterUnited States Department of Health & Human
   ServicesNational Institutes of Health (NIH) - USA; KRIBB Research
   Initiative Program (Korean Biomedical Scientist Fellowship Program),
   Korea Research Institute of Bioscience and Biotechnology, Republic of
   Korea
FX This work was supported in part by the Intramural Research Program of
   the National Institutes of Health Clinical Center, and in part by a
   grant from the KRIBB Research Initiative Program (Korean Biomedical
   Scientist Fellowship Program), Korea Research Institute of Bioscience
   and Biotechnology, Republic of Korea. This study utilized the
   high-performance computational capabilities of the Biowulf Linux cluster
   at the National Institutes of Health, Bethesda, MD
   (http://biowulf.nih.gov). We thank NVIDIA for the K40 GPU donation.
CR [Anonymous], OPEN OP ACC BIOM IM
   Barnard K, 2003, J MACH LEARN RES, V3, P1107, DOI 10.1162/153244303322533214
   Berg TL, 2010, LECT NOTES COMPUT SC, V6311, P663, DOI 10.1007/978-3-642-15549-9_48
   Bird S., 2009, NATURAL LANGUAGE PRO
   Blei D. M., 2003, P 26 ANN INT ACM SIG, P127, DOI DOI 10.1145/860435.860460
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Carrivick Luke, 2005, 2005 COMP SOC C COP, V2, P854
   Chapman WW, 2013, STUD HEALTH TECHNOL, V192, P677, DOI 10.3233/978-1-61499-289-9-677
   Chapman WW, 2001, J BIOMED INFORM, V34, P301, DOI 10.1006/jbin.2001.1029
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deselaers T, 2008, PATTERN RECOGN LETT, V29, P2003, DOI 10.1016/j.patrec.2008.03.013
   Ding C., 2006, P NAT C ART INT, V21, P342
   Donahue Jeffrey, 2015, P IEEE C COMP VIS PA
   Frome A., 2013, ADV NEURAL INFORM PR, P2121
   Gaussier E., 2005, SIGIR 2005. Proceedings of the Twenty-Eighth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P601
   Gupta A., 2013, P INT C MACH LEARN
   Gupta S, 2014, LECT NOTES COMPUT SC, V8695, P345, DOI 10.1007/978-3-319-10584-0_23
   Hodosh M, 2013, J ARTIF INTELL RES, V47, P853, DOI 10.1613/jair.3994
   Hofmann T, 1999, SIGIR'99: PROCEEDINGS OF 22ND INTERNATIONAL CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P50, DOI 10.1145/312624.312649
   Humphreys BL, 1998, J AM MED INFORM ASSN, V5, P1
   Jaderberg M, 2014, LECT NOTES COMPUT SC, V8692, P512, DOI 10.1007/978-3-319-10593-2_34
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Kaban A, 2003, P 26 ANN INT ACM SIG, P433, DOI DOI 10.1145/860435.860537
   Karpathy Andrej, 2014, ADV NEURAL INFORM PR, P1889
   Kelvin Xu, 2015, P 31 INT C MACH LEAR
   Kiapour MH, 2014, LECT NOTES COMPUT SC, V8689, P472, DOI 10.1007/978-3-319-10590-1_31
   Kim G, 2014, PROC CVPR IEEE, P4225, DOI 10.1109/CVPR.2014.538
   Kim Gunhee, 2015, IEEE C COMP VIS PATT
   Krizhevsky A., 2009, TECH REP
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kulkarni G, 2013, IEEE T PATTERN ANAL, V35, P2891, DOI 10.1109/TPAMI.2012.162
   Lampert CH, 2014, IEEE T PATTERN ANAL, V36, P453, DOI 10.1109/TPAMI.2013.140
   Langlotz CP, 2006, RADIOGRAPHICS, V26, P1595, DOI 10.1148/rg.266065168
   Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565
   Li S., 2011, P 15 C COMP NAT LANG, P220
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   LINDBERG DAB, 1993, METHOD INFORM MED, V32, P281
   Mao Junhua, 2015, P INT C LEARN REPR I
   Mikolov T., 2013, P NAACL 2013, P746
   Mikolov T, 2013, ADV NEURAL INFORM PR, V27, P3111, DOI DOI 10.1162/JMLR.2003.3.4-5.951
   MIKOLOV T., 2013, ARXIV13013781
   MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748
   Ordonez V, 2014, LECT NOTES COMPUT SC, V8694, P494, DOI 10.1007/978-3-319-10599-4_32
   Ordonez Vicente, 2011, ADV NEURAL INFORM PR, P1143
   Rashtchian C., 2010, P NAACL HLT 2010 WOR, P139
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Scheirer WJ, 2012, PROC CVPR IEEE, P2933, DOI 10.1109/CVPR.2012.6248021
   Schriml LM, 2012, NUCLEIC ACIDS RES, V40, pD940, DOI 10.1093/nar/gkr972
   SCHUYLER PL, 1993, B MED LIBR ASSOC, V81, P217
   Shin HC, 2013, IEEE T PATTERN ANAL, V35, P1930, DOI 10.1109/TPAMI.2012.277
   Simonyan K., 2015, P INT C LEARN REPR I
   Socher R., 2013, ADV NEURAL INFORM PR, V26, P935
   Stevens K., 2012, P 2012 JOINT C EMP M, V2012, P952
   Szegedy C., 2015, P IEEE C COMP VIS PA
   Turian J, 2010, ACL 2010: 48TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, P384
   Vincent P, 2008, P 25 INT C MACH LEAR, P1096, DOI DOI 10.1145/1390156.1390294
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Vinyals O., 2015, P IEEE C COMP VIS PA
   Young P., 2014, P TACL, V2, P67, DOI DOI 10.1162/tacl_a_00166
NR 59
TC 19
Z9 19
U1 0
U2 12
PU MICROTOME PUBL
PI BROOKLINE
PA 31 GIBBS ST, BROOKLINE, MA 02446 USA
SN 1532-4435
J9 J MACH LEARN RES
JI J. Mach. Learn. Res.
PY 2016
VL 17
AR 107
PG 31
WC Automation & Control Systems; Computer Science, Artificial Intelligence
SC Automation & Control Systems; Computer Science
GA EH1SD
UT WOS:000391546500001
DA 2020-02-19
ER

PT J
AU Cai, YF
   Sun, XQ
   Wang, H
   Chen, L
   Jiang, HB
AF Cai, Yingfeng
   Sun, Xiaoqiang
   Wang, Hai
   Chen, Long
   Jiang, Haobin
TI Night-Time Vehicle Detection Algorithm Based on Visual Saliency and Deep
   Learning
SO JOURNAL OF SENSORS
LA English
DT Article
AB Night vision systems get more and more attention in the field of automotive active safety field. In this area, a number of researchers have proposed far-infrared sensor based night-time vehicle detection algorithm. However, existing algorithms have low performance in some indicators such as the detection rate and processing time. To solve this problem, we propose a far-infrared image vehicle detection algorithm based on visual saliency and deep learning. Firstly, most of the nonvehicle pixels will be removed with visual saliency computation. Then, vehicle candidate will be generated by using prior information such as camera parameters and vehicle size. Finally, classifier trained with deep belief networks will be applied to verify the candidates generated in last step. The proposed algorithm is tested in around 6000 images and achieves detection rate of 92.3% and processing time of 25 Hz which is better than existing methods.
C1 [Cai, Yingfeng; Chen, Long] Jiangsu Univ, Sch Automot & Traff Engn, Zhenjiang 212013, Peoples R China.
   [Sun, Xiaoqiang; Wang, Hai; Jiang, Haobin] Jiangsu Univ, Automot Engn Res Inst, Zhenjiang 212013, Peoples R China.
RP Wang, H (reprint author), Jiangsu Univ, Automot Engn Res Inst, Zhenjiang 212013, Peoples R China.
EM wanghai1019@163.com
OI Wang, Hai/0000-0002-9136-8091
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [U1564201, 61601203, 61573171, 61403172]; China
   Postdoctoral Science FoundationChina Postdoctoral Science Foundation
   [2014M561592, 2015T80511]; Key Research and Development Program of
   Jiangsu Province [BE2016149]; Natural Science Foundation of Jiangsu
   ProvinceJiangsu Planned Projects for Postdoctoral Research FundsNatural
   Science Foundation of Jiangsu Province [BK20140555]; Six Talent Peaks
   Project of Jiangsu Province [2015-JXQC-012, 2015-XNYQC-004,
   2014-DZXX-040]
FX This work is supported by The National Natural Science Foundation of
   China (U1564201, 61601203, 61573171, and 61403172), China Postdoctoral
   Science Foundation (2014M561592 and 2015T80511), Key Research and
   Development Program of Jiangsu Province (BE2016149), Natural Science
   Foundation of Jiangsu Province (BK20140555), and Six Talent Peaks
   Project of Jiangsu Province (2015-JXQC-012, 2015-XNYQC-004, and
   2014-DZXX-040).
CR Andreone L, 2002, IEEE 5TH INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS, PROCEEDINGS, P141, DOI 10.1109/ITSC.2002.1041203
   Arrospide J, 2008, IEEE IMAGE PROC, P2008, DOI 10.1109/ICIP.2008.4712178
   Baran R, 2015, MULTIMED TOOLS APPL, V74, P4269, DOI 10.1007/s11042-013-1545-2
   Besbes Bassem, 2010, 2010 13th International IEEE Conference on Intelligent Transportation Systems (ITSC 2010), P1869, DOI 10.1109/ITSC.2010.5625285
   Dorj B, 2016, J SENSORS, DOI 10.1155/2016/4058093
   Hermes C, 2010, IEEE INT VEH SYM, P26, DOI 10.1109/IVS.2010.5548014
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hurney P, 2015, IET INTELL TRANSP SY, V9, P824, DOI 10.1049/iet-its.2014.0236
   Hurney P, 2015, IET INTELL TRANSP SY, V9, P75, DOI 10.1049/iet-its.2013.0163
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jin L, 2014, J INFRARED MILLIM W, V33, P465, DOI 10.3724/SP.J.1010.2014.00465
   Montabone S, 2010, IMAGE VISION COMPUT, V28, P391, DOI 10.1016/j.imavis.2009.06.006
   Noh S, 2016, IEEE T INTELL TRANSP, V17, P323, DOI 10.1109/TITS.2015.2466652
   Qiong Liu, 2012, Proceedings of the 2012 IEEE International Conference on Imaging Systems and Techniques (IST), P398, DOI 10.1109/IST.2012.6295596
   Ramirez A, 2014, IEEE INT VEH SYM, P96, DOI 10.1109/IVS.2014.6856598
   Sun ZH, 2006, IEEE T PATTERN ANAL, V28, P694, DOI 10.1109/TPAMI.2006.104
   Wang H., 2016, J SENSORS, V2016
   Wood F., 2012, TECH REP
NR 18
TC 2
Z9 2
U1 2
U2 23
PU HINDAWI LTD
PI LONDON
PA ADAM HOUSE, 3RD FLR, 1 FITZROY SQ, LONDON, W1T 5HF, ENGLAND
SN 1687-725X
EI 1687-7268
J9 J SENSORS
JI J. Sens.
PY 2016
AR 8046529
DI 10.1155/2016/8046529
PG 7
WC Engineering, Electrical & Electronic; Instruments & Instrumentation
SC Engineering; Instruments & Instrumentation
GA EE4NB
UT WOS:000389578300001
OA DOAJ Gold
DA 2020-02-19
ER

PT S
AU Yan, ZG
   Xu, Z
   Du, H
   Mei, L
AF Yan, Zhiguo
   Xu, Zheng
   Du, Huan
   Mei, Lin
BE Hung, JC
   Yen, NY
   Li, KC
TI The Scheme of the Cooperative Gun-Dome Face Image Acquisition in
   Surveillance Sensors
SO FRONTIER COMPUTING: THEORY, TECHNOLOGIES AND APPLICATIONS
SE Lecture Notes in Electrical Engineering
LA English
DT Article; Book Chapter
DE Gun-dome camera cooperation; Calibration; Deformable part model;
   Pedestrian detection; LBP
AB How to automatically realize acquisition, refining and fast retrieval of the pedestrian face image in surveillance video is of great importance in public security visual surveillance field. This paper proposes a new gun-dome camera cooperative system which solves the above problem partly. The system adopts static panorama-variable view dual-camera cooperative video-monitoring system. As respect to the face detection, the deep learning architecture is exploited and proves it effectiveness. The experimental results show the effectiveness and efficiency of the dual-camera system in close-up face image acquisition.
C1 [Yan, Zhiguo; Xu, Zheng; Du, Huan; Mei, Lin] Minist Publ Secur, Res Inst 3, Beijing, Peoples R China.
   [Xu, Zheng] Tsinghua Univ, Beijing, Peoples R China.
RP Xu, Z (reprint author), Minist Publ Secur, Res Inst 3, Beijing, Peoples R China.
EM xuzheng@shu.edu.cn
CR Beriault S, 2008, SCH INFORM TECHNOLOG, P146
   Cho HG, 2012, 2012 IEEE INTELLIGENT VEHICLES SYMPOSIUM (IV), P1035, DOI 10.1109/IVS.2012.6232264
   Di Xie, 2012, 2012 9th International Conference on Fuzzy Systems and Knowledge Discovery, P2832, DOI 10.1109/FSKD.2012.6234136
   Felzenszwalb Pedro, 2008, P IEEE C COMP VIS PA, V08, P1, DOI DOI 10.1109/CVPR.2008.4587597
   Hao Z, 2010, 2ND IEEE INTERNATIONAL CONFERENCE ON ADVANCED COMPUTER CONTROL (ICACC 2010), VOL. 3, P232, DOI 10.1109/ICACC.2010.5486857
   Hu CP, 2014, IEEE T EMERG TOP COM, V2, P376, DOI 10.1109/TETC.2014.2316525
   Jong-Min Kim, 2009, Proceedings of the 2009 Fifth International Conference on Natural Computation (ICNC 2009), P171, DOI 10.1109/ICNC.2009.382
   Li H, 2006, INT C ADV VID SIGN B
   Liu X, 2013, ACM T SOFTW ENG METH
   Liu YH, 2012, IEEE J SEL AREA COMM, V30, P1780, DOI 10.1109/JSAC.2012.121023
   Liu YH, 2011, IEEE T PARALL DISTR, V22, P2100, DOI 10.1109/TPDS.2011.113
   Liu YH, 2010, IEEE T PARALL DISTR, V21, P405, DOI 10.1109/TPDS.2009.57
   Luo XF, 2011, IEEE T AUTOM SCI ENG, V8, P482, DOI 10.1109/TASE.2010.2094608
   Rong Dong, 2009, 2009 WRI World Congress on Computer Science and Information Engineering, CSIE, P636, DOI 10.1109/CSIE.2009.763
   Wang LZ, 2013, FUTURE GENER COMP SY, V29, P739, DOI 10.1016/j.future.2012.09.001
   Wang Y., 2013, DISTRIBUTED MULTI OB, P183
   Xu Z, 2015, FUTURE GENER COMP SY, V43-44, P40, DOI 10.1016/j.future.2014.04.002
   Xu Z, 2014, FUTURE GENER COMP SY, V37, P468, DOI 10.1016/j.future.2013.09.027
NR 18
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER
PI NEW YORK
PA 233 SPRING STREET, NEW YORK, NY 10013, UNITED STATES
SN 1876-1100
EI 1876-1119
BN 978-981-10-0539-8; 978-981-10-0538-1
J9 LECT NOTES ELECTR EN
PY 2016
VL 375
BP 1159
EP 1165
DI 10.1007/978-981-10-0539-8_117
D2 10.1007/978-981-10-0539-8
PG 7
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Engineering, Electrical & Electronic
SC Computer Science; Engineering
GA BG0IT
UT WOS:000386254700118
DA 2020-02-19
ER

PT J
AU Yue, Q
   Ma, CW
AF Yue, Qi
   Ma, Caiwen
TI Deep Learning for Hyperspectral Data Classification through Exponential
   Momentum Deep Convolution Neural Networks
SO JOURNAL OF SENSORS
LA English
DT Article
AB Classification is a hot topic in hyperspectral remote sensing community. In the last decades, numerous efforts have been concentrated on the classification problem. Most of the existing studies and research efforts are following the conventional pattern recognition paradigm, which is based on complex handcrafted features. However, it is rarely known which features are important for the problem. In this paper, a new classification skeleton based on deep machine learning is proposed for hyperspectral data. The proposed classification framework, which is composed of exponential momentum deep convolution neural network and support vector machine (SVM), can hierarchically construct high-level spectral-spatial features in an automated way. Experimental results and quantitative validation on widely used datasets showcase the potential of the developed approach for accurate hyperspectral data classification.
C1 [Yue, Qi; Ma, Caiwen] Chinese Acad Sci, Xian Inst Opt & Precis Mech, Xian 710119, Peoples R China.
   [Yue, Qi] Univ Chinese Acad Sci, Beijing 100039, Peoples R China.
   [Yue, Qi] Xian Univ Posts & Telecommun, Xian 710121, Peoples R China.
RP Yue, Q (reprint author), Chinese Acad Sci, Xian Inst Opt & Precis Mech, Xian 710119, Peoples R China.; Yue, Q (reprint author), Univ Chinese Acad Sci, Beijing 100039, Peoples R China.; Yue, Q (reprint author), Xian Univ Posts & Telecommun, Xian 710121, Peoples R China.
EM yueqi6@163.com
FU National 863 High Tech Research and Development ProgramNational High
   Technology Research and Development Program of China [2010AA7080302]
FX This work is supported by the National 863 High Tech Research and
   Development Program (2010AA7080302).
CR Agrawal SS, 2013, MATER MANUF PROCESS, V28, P381, DOI 10.1080/10426914.2013.763678
   BALDI P, 1989, NEURAL NETWORKS, V2, P53, DOI 10.1016/0893-6080(89)90014-2
   Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153
   Bengio Y., 2011, P WORKSH UNS TRANSF
   Bengio Y, 2006, ADV NEURAL INFORM PR, P153
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bioucas-Dias JM, 2013, IEEE GEOSC REM SEN M, V1, P6, DOI 10.1109/MGRS.2013.2244672
   Chen YS, 2015, IEEE J-STARS, V8, P2381, DOI 10.1109/JSTARS.2015.2388577
   Chen YS, 2014, IEEE J-STARS, V7, P2094, DOI 10.1109/JSTARS.2014.2329330
   Eismann MT, 2009, P IEEE, V97, P1031, DOI 10.1109/JPROC.2009.2013561
   Foody GM, 2004, IEEE T GEOSCI REMOTE, V42, P1335, DOI 10.1109/TGRS.2004.827257
   Hege E. K., 2004, P SOC PHOTO-OPT INS, V5159, P380, DOI DOI 10.1117/12.506426
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton G. E., 2010, TR2010003 UTML DEP C
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Karayiannis NB, 1999, IEEE T NEURAL NETWOR, V10, P657, DOI 10.1109/72.761725
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lacar FM, 2001, INT GEOSCI REMOTE SE, P2875, DOI 10.1109/IGARSS.2001.978191
   Larochelle H., 2007, P 24 INT C MACH LEAR, P473, DOI DOI 10.1145/1273496.1273556
   Lu Q, 2012, PROCEDIA ENVIRON SCI, V12, P1172, DOI 10.1016/j.proenv.2012.01.404
   Malthus TJ, 2003, INT J REMOTE SENS, V24, P2805, DOI 10.1080/0143116031000066954
   Mohamed AR, 2011, INT CONF ACOUST SPEE, P5060
   Naigong Yu, 2015, 2015 27th Chinese Control and Decision Conference (CCDC), P4871, DOI 10.1109/CCDC.2015.7162796
   Ouyang WL, 2013, IEEE I CONF COMP VIS, P2056, DOI 10.1109/ICCV.2013.257
   Rajan S, 2008, IEEE T GEOSCI REMOTE, V46, P1231, DOI 10.1109/TGRS.2007.910220
   Sadeghi BHM, 2000, J MATER PROCESS TECH, V103, P411, DOI 10.1016/S0924-0136(00)00498-2
   Salakhutdinov R, 2009, ARTIF INTELL, P448, DOI DOI 10.1109/CVPRW.2009.5206577
   Shukla D, 1999, IEEE T NEURAL NETWOR, V10, P1494, DOI 10.1109/72.809095
   TAN W X, 2015, NONGYE JIXIE XUEBAO, V46, P20
   Van der Meer F., 2004, INT J APPL EARTH OBS, V5, P55, DOI DOI 10.1016/J.JAG.2003.09.001
   Yu D., 2009, P NIPS WORKSH, P1
   Yuen PWT, 2010, IMAGING SCI J, V58, P241, DOI 10.1179/174313110X12771950995716
   Zeiler M. D., STOCHASTIC POOLING R
   Zhu Z, 2012, REMOTE SENS ENVIRON, V117, P72, DOI 10.1016/j.rse.2011.07.020
NR 34
TC 5
Z9 5
U1 3
U2 32
PU HINDAWI LTD
PI LONDON
PA ADAM HOUSE, 3RD FLR, 1 FITZROY SQ, LONDON, W1T 5HF, ENGLAND
SN 1687-725X
EI 1687-7268
J9 J SENSORS
JI J. Sens.
PY 2016
AR 3150632
DI 10.1155/2016/3150632
PG 8
WC Engineering, Electrical & Electronic; Instruments & Instrumentation
SC Engineering; Instruments & Instrumentation
GA EB4YY
UT WOS:000387381100001
OA DOAJ Gold
DA 2020-02-19
ER

PT J
AU Hsu, WY
AF Hsu, Wei-Yen
TI Automatic atrium contour tracking in ultrasound imaging
SO INTEGRATED COMPUTER-AIDED ENGINEERING
LA English
DT Article
DE Cardiac ultrasound image; contour tracking; active contour model; scale
   invariant feature transform
ID DEEP LEARNING ARCHITECTURES; BRAIN-COMPUTER INTERFACE; LEFT-VENTRICLE;
   SEGMENTATION; SYSTEM; TELEMEDICINE; ALGORITHM; SELECTION; MODELS; IMAGES
AB Cardiac ultrasound imaging tracking is an important issue in medical image analysis. The results of tracking greatly influence the diagnoses and judgment of physicians. However, the conventional active contour model is inappropriate for use in cardiac imaging tracking since the mitral and tricuspid valves rise and fall, leading to poor tracking and excessive convergence in overall contour during systoles and diastoles. In this study, a novel tracking approach, combining the active contour model and scale invariant feature transform, is proposed for use in cardiac ultrasound imaging tracking. In addition to preprocessing that removes some noise and detects initial cardiac edges automatically, the active contour model is used to segment and track cardiac regions. The scale invariant feature transform is proposed to determine and track the correct positions of heart valves. The performance of the proposed method is evaluated by testing with cardiac ultrasound image sequence and in comparison with five other tracking techniques in terms of several metrics, such as segmentation accuracy, AUC, the Dice coefficient, and modified Hausdorff distance. The results indicate that the proposed method is more accurate and effective in the tracking of cardiac imaging.
C1 [Hsu, Wei-Yen] Natl Chung Cheng Univ, Dept Informat Management, Minhsiung, Chiayi, Taiwan.
   [Hsu, Wei-Yen] Natl Chung Cheng Univ, Adv Inst Mfg High Tech Innovat, Minhsiung, Chiayi, Taiwan.
RP Hsu, WY (reprint author), Natl Chung Cheng Univ, Dept Informat Management, Minhsiung, Chiayi, Taiwan.; Hsu, WY (reprint author), Natl Chung Cheng Univ, Adv Inst Mfg High Tech Innovat, Minhsiung, Chiayi, Taiwan.
EM shenswy@gmail.com
FU Ministry of Science and Technology, TaiwanMinistry of Science and
   Technology, Taiwan [MOST103-2410-H-194-070-MY2,
   MOST104-2622-E-194-003-CC2]
FX The author would like to express his sincere appreciation for grants
   partially from MOST103-2410-H-194-070-MY2 and
   MOST104-2622-E-194-003-CC2, Ministry of Science and Technology, Taiwan.
   He also wants to thanks to Dr. Hung and Dr. Yun, Taipei Mackay Memorial
   Hospital, Taiwan for their assistance in the segmentation of cardiac
   image sequence.
CR Adeli H., 1993, INTEGRATED COMPUTER, V1, P43
   Almehio Y, 2014, INTEGR COMPUT-AID E, V21, P101, DOI 10.3233/ICA-130436
   Almeida G., 2016, COMPUTER AIDED CIVIL, V31
   Neto ENA, 2014, INTEGR COMPUT-AID E, V21, P281, DOI 10.3233/ICA-140462
   Belles C, 2015, COMPUT-AIDED CIV INF, V30, P906, DOI 10.1111/mice.12107
   Carneiro G, 2013, IEEE T PATTERN ANAL, V35, P2592, DOI 10.1109/TPAMI.2013.96
   Carneiro G, 2012, IEEE T IMAGE PROCESS, V21, P968, DOI 10.1109/TIP.2011.2169273
   Charpiat G, 2005, IEEE I CONF COMP VIS, P1403
   Chon KH, 2009, IEEE ENG MED BIOL, V28, P18, DOI 10.1109/MEMB.2009.934629
   Corsi C, 2002, IEEE T MED IMAGING, V21, P1202, DOI 10.1109/TMI.2002.804418
   Cremers D, 2005, INT J COMPUT VISION, V62, P249, DOI 10.1007/s11263-005-4882-4
   Cremers D, 2003, LECT NOTES COMPUT SC, V2695, P388
   DICE LR, 1945, ECOLOGY, V26, P297, DOI 10.2307/1932409
   Dimitrov A., 2016, COMPUTER AIDED CIVIL, V31
   DUBUISSON MP, 1994, INT C PATT RECOG, P566, DOI 10.1109/ICPR.1994.576361
   Fawcett T, 2006, PATTERN RECOGN LETT, V27, P861, DOI 10.1016/j.patrec.2005.10.010
   Garson CD, 2008, COMPUT MED IMAG GRAP, V32, P321, DOI 10.1016/j.compmedimag.2008.02.004
   Ghuffar S, 2014, INTEGR COMPUT-AID E, V21, P203, DOI 10.3233/ICA-130456
   Goldenberg R, 2001, IEEE T IMAGE PROCESS, V10, P1467, DOI 10.1109/83.951533
   Goncalves N, 2014, INT J NEURAL SYST, V24, DOI 10.1142/S012906571450004X
   Hao XH, 2000, ULTRASON, P1717, DOI 10.1109/ULTSYM.2000.921653
   Hsu WL, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0041779
   Hsu WY, 2015, INT J NEURAL SYST, V25, DOI 10.1142/S0129065715500379
   Hsu WY, 2015, J MED BIOL ENG, V35, P580, DOI 10.1007/s40846-015-0078-8
   Hsu WY, 2015, COMPUT-AIDED CIV INF, V30, P802, DOI 10.1111/mice.12156
   Hsu WY, 2015, TELEMAT INFORM, V32, P475, DOI 10.1016/j.tele.2014.11.003
   Hsu WY, 2015, TELEMAT INFORM, V32, P180, DOI 10.1016/j.tele.2014.07.001
   Hsu WY, 2013, INT J NEURAL SYST, V23, DOI 10.1142/S0129065713500263
   Hsu WY, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0066656
   Hsu WY, 2013, INT J NEURAL SYST, V23, DOI 10.1142/S012906571350007X
   Hsu WY, 2012, CLIN EEG NEUROSCI, V43, P87, DOI 10.1177/1550059412445051
   Hsu WY, 2012, INT J NEURAL SYST, V22, P51, DOI 10.1142/S0129065712002979
   Hsu WY, 2011, INT J NEURAL SYST, V21, P335, DOI 10.1142/S0129065711002870
   Huo J, 2014, INT J NEURAL SYST, V24, DOI 10.1142/S0129065714300101
   [纪建鹏 Ji Jianpeng], 2011, [中国组织工程研究与临床康复, Journal of Clinical Rehabilitative Tissue Engineering Research], V15, P7351
   Justice RK, 1997, P SOC PHOTO-OPT INS, V3034, P900, DOI 10.1117/12.274179
   KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570
   Lai YH, 2013, ACTA CARDIOL SIN, V29, P64
   Lankton S, 2008, IEEE T IMAGE PROCESS, V17, P2029, DOI 10.1109/TIP.2008.2004611
   Li B, 2007, IEEE T IMAGE PROCESS, V16, P2096, DOI 10.1109/TIP.2007.899601
   Li H, 2007, IEEE T PATTERN ANAL, V29, P1, DOI 10.1109/TPAMI.2007.250595
   Lin N, 2003, MED IMAGE ANAL, V7, P529, DOI 10.1016/S1361-8415(03)00035-5
   Liu K, 2015, INT J NEURAL SYST, V25, DOI 10.1142/S0129065715500161
   Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Marani R., 2016, COMPUTER AIDED CIVIL, V31
   Mari JF, 2015, INT J NEURAL SYST, V25, DOI 10.1142/S0129065715500331
   Martis RJ, 2014, BIOMED SIGNAL PROCES, V13, P295, DOI 10.1016/j.bspc.2014.04.001
   Martis RJ, 2014, COMPUT BIOL MED, V48, P133, DOI 10.1016/j.compbiomed.2014.02.012
   Melo SA, 2010, BIOMED ENG ONLINE, V9, DOI 10.1186/1475-925X-9-5
   Mesquita RG, 2014, INTEGR COMPUT-AID E, V21, P133, DOI 10.3233/ICA-130453
   Mille J, 2009, LECT NOTES COMPUT SC, V5681, P168, DOI 10.1007/978-3-642-03641-5_13
   Mishra AK, 2011, IEEE T PATTERN ANAL, V33, P310, DOI 10.1109/TPAMI.2010.83
   Mondillo S, 2011, J ULTRAS MED, V30, P71, DOI 10.7863/jum.2011.30.1.71
   Nascimento JC, 2008, IEEE T IMAGE PROCESS, V17, P392, DOI 10.1109/TIP.2007.915552
   Noble JA, 2006, IEEE T MED IMAGING, V25, P987, DOI 10.1109/TMI.2006.877092
   Olmeda D, 2013, INTEGR COMPUT-AID E, V20, P347, DOI 10.3233/ICA-130441
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Pan KL, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0108357
   Pedrino EC, 2013, INTEGR COMPUT-AID E, V20, P275, DOI 10.3233/ICA-130429
   Perez G, 2014, INTEGR COMPUT-AID E, V21, P163, DOI 10.3233/ICA-130457
   Piaggi P, 2014, INT J NEURAL SYST, V24, DOI 10.1142/S0129065714500105
   PINCUS SM, 1991, P NATL ACAD SCI USA, V88, P2297, DOI 10.1073/pnas.88.6.2297
   Revell J., 2009, ULTRASOUND SPECKLE T
   Rousson M., 2003, IEEE COMPUTER SOC C, V2, P11
   Sankari Z, 2011, COMPUT BIOL MED, V41, P211, DOI 10.1016/j.compbiomed.2011.02.002
   Shih FY, 2007, COMPUT VIS IMAGE UND, V105, P93, DOI 10.1016/j.cviu.2006.08.007
   Skalski A, 2010, PROCEDIA COMPUT SCI, V1, P2717, DOI 10.1016/j.procs.2010.04.306
   Sonka M, 1995, IEEE T MED IMAGING, V14, P719, DOI 10.1109/42.476113
   Sundaramoorthi G, 2008, IEEE T PATTERN ANAL, V30, P851, DOI 10.1109/TPAMI.2007.70751
   Sundaramoorthi G, 2007, INT J COMPUT VISION, V73, P345, DOI 10.1007/s11263-006-0635-2
   Sundaramoorthi G, 2010, PROC CVPR IEEE, P2855, DOI 10.1109/CVPR.2010.5540020
   VINCENT L, 1991, IEEE T PATTERN ANAL, V13, P583, DOI 10.1109/34.87344
   Xie XH, 2008, IEEE T PATTERN ANAL, V30, P632, DOI 10.1109/TPAMI.2007.70737
   Yang YB, 2014, INT J NEURAL SYST, V24, DOI 10.1142/S0129065714500245
   Yuan Q, 2011, EPILEPSY RES, V96, P29, DOI 10.1016/j.eplepsyres.2011.04.013
   Zhang L., 2014, INTEGRATED COMPUTER, V21
   Zhu S., 1995, INT C COMP VIS, V416, P423
NR 78
TC 9
Z9 9
U1 0
U2 16
PU IOS PRESS
PI AMSTERDAM
PA NIEUWE HEMWEG 6B, 1013 BG AMSTERDAM, NETHERLANDS
SN 1069-2509
EI 1875-8835
J9 INTEGR COMPUT-AID E
JI Integr. Comput.-Aided Eng.
PY 2016
VL 23
IS 4
BP 401
EP 411
DI 10.3233/ICA-160517
PG 11
WC Computer Science, Artificial Intelligence; Computer Science,
   Interdisciplinary Applications; Engineering, Multidisciplinary
SC Computer Science; Engineering
GA EA3VY
UT WOS:000386538600006
DA 2020-02-19
ER

PT S
AU Jamshidi, M
   Tannahill, B
   Ezell, M
   Yetis, Y
   Kaplan, H
AF Jamshidi, Mo
   Tannahill, Barney
   Ezell, Maryam
   Yetis, Yunus
   Kaplan, Halid
BE Emrouznejad, A
TI Applications of Big Data Analytics Tools for Data Management
SO BIG DATA OPTIMIZATION: RECENT DEVELOPMENTS AND CHALLENGES
SE Studies in Big Data
LA English
DT Article; Book Chapter
DE Machine learning; Data analytics; Computational intelligence;
   Statistical techniques; Finances; Solar energy; Biology
AB Data, at a very large scale, has been accumulating in all aspects of our lives for a long time. Advances in sensor technology, the Internet, social networks, wireless communication, and inexpensive memory have all contributed to an explosion of "Big Data". Our interconnected world of today and the advent of cyber-physical or system of systems (SoS) are also a key source of data accumulation- be it numerical, image, text or texture, etc. SoS is basically defined as an integration of independently operating, non-homogeneous systems for certain duration to achieve a higher goal than the sum of the parts. Recent efforts have developed a promising approach, called "Data Analytics", which uses statistical and computational intelligence (CI) tools such as principal component analysis (PCA), clustering, fuzzy logic, neuro-computing, evolutionary computation, Bayesian networks, data mining, pattern recognition, deep learning, etc. to reduce the size of "Big Data" to a manageable size and apply these tools to (a) extract information, (b) build a knowledge base using the derived data, (c) optimize validation of clustered knowledge through evolutionary computing and eventually develop a non-parametric model for the "Big Data", and (d) Test and verify the model. This chapter attempts to construct a bridge between SoS and Data Analytics to develop reliable models for such systems. Four applications of big data analytics will be presented, i. e. solar, wind, financial and biological data.
C1 [Jamshidi, Mo; Tannahill, Barney; Ezell, Maryam; Yetis, Yunus; Kaplan, Halid] Univ Texas San Antonio, ACE Lab, San Antonio, TX 78249 USA.
RP Jamshidi, M (reprint author), Univ Texas San Antonio, ACE Lab, San Antonio, TX 78249 USA.
EM moj@wacong.org
CR Ball R., 2006, DEMONSTRATION ARTIFI
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Jamshidi M., 2014, P AAAI WORKSH BIG DA
   JAMSHIDI M, 2009, [No title captured]
   Jamshidi M., 2008, SYSTEMS SYSTEMS ENG
   Jamshidi Mo, 2015, FRONTIERS HIGHER ORD, P229
   Kimoto T., 1990, IJCNN International Joint Conference on Neural Networks (Cat. No.90CH2879-5), P1, DOI 10.1109/IJCNN.1990.137535
   Langdell S., EXAMPLES USE DATA MI
   Manjili Y. S., 2012, P 2012 WORLD AUT C W, P1
   Mizono H., 1998, STUDIES INFORM CONTR, V7, P111
   Moussavi A., 2015, NEURAL COMPUT UNPUB
   Neenwi S., 2013, EUROPEAN J COMPUTER, V1, P30
   Philip M.T., 2007, J ENG APPL ARTIFICIA, V20, P453
   Rumelhart D.E., 1986, PARALLEL DISTRIBUTED, V1, P318, DOI DOI 10.1016/B978-1-4832-1446-7.50035-2
   Sergiu C., FINANCIAL PREDICTOR
   Sergiu Ciumac, 2011, FINANCIAL PREDICTOR
   Sexton S. R., 1998, DECIS SUPPORT SYST, V22, P117
   Shlens J, 2009, TUTORIAL PRINCIPAL C
   Smith L., 2002, TUTORIAL PRINCIPAL C
   Tannahill B., 2013, THESIS U TEXAS SAN A
   Tannahill B. K., 2013, P 8 IEEE SOSE MAUI H
   The Royal Academy of Engineering, 2014, WIND TURB POW CALC
   Wu X., 2001, FORECASTING STOCK PE, P447
   Zilouchian A., 2001, INTELLIGENT CONTROL
   Zilouchian A., 2001, INTELLIGENT CONTROL
NR 25
TC 5
Z9 5
U1 0
U2 4
PU SPRINGER-VERLAG BERLIN
PI BERLIN
PA HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY
SN 2197-6503
BN 978-3-319-30265-2; 978-3-319-30263-8
J9 STUD BIG DATA
PY 2016
VL 18
BP 177
EP 199
DI 10.1007/978-3-319-30265-2_8
D2 10.1007/978-3-319-30265-2
PG 23
WC Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Computer Science, Theory & Methods
SC Computer Science
GA BF5DL
UT WOS:000381890400009
DA 2020-02-19
ER

PT J
AU Donadello, I
   Serafini, L
AF Donadello, Ivan
   Serafini, Luciano
TI Integration of numeric and symbolic information for semantic image
   interpretation
SO INTELLIGENZA ARTIFICIALE
LA English
DT Article
DE Information extraction; computer vision; semantic image interpretation;
   ontologies; clustering
ID KNOWLEDGE
AB Semantic image interpretation (SII) is the process of generating meaningful descriptions of the content of images. Background knowledge (BK), in the form of logical theories, is extremely useful for SII. State-of-the-art algorithms for SII mainly adopt a bottom-up approach, which generates semantic interpretations of images starting from their low-level features. In these approaches BK is used only at a late stage for both enriching the semantic descriptions and improving image retrieval. In this paper, we show how BK plays an important role also during the early phase of SII. To this aim, we propose: (i) a reference framework where a semantic image description is a partial model of the BK. The elements of the partial model are grounded (linked) to a (set of) image segment(s). (ii) A loss function that evaluates how well this partial model fits the picture; (iii) a clustering-based optimization process that searches the partial model that better fits a picture. BK is used to prune branches of the search space that correspond to partial models which are inconsistent with BK. To evaluate our approach, we built a gold standard dataset of 203 pictures annotated with complex objects and their parts. We also evaluated our method on a reference dataset in Computer Vision, namely, the PASCAL-Part dataset. The results are positive. The evaluation assumes a perfect detection of parts. To understand the impact of a realistic (and noisy) part detection on our algorithm, we did a preliminary evaluation by implementing the entire SII pipeline. Part detection is performed by a recent deep learning architecture trained for detecting parts. From a qualitative analysis, it emerges that recognizing complex objects starting from parts in some cases gets better results than detecting complex objects directly.
C1 [Donadello, Ivan; Serafini, Luciano] Fdn Bruno Kessler, Via Sommarive, Trento, Italy.
   [Donadello, Ivan] Univ Trento, Dept Informat Engn & Comp Sci, Via Sommarive, Trento, Italy.
RP Donadello, I (reprint author), Fdn Bruno Kessler, Via Sommarive, Trento, Italy.; Donadello, I (reprint author), Univ Trento, Dept Informat Engn & Comp Sci, Via Sommarive, Trento, Italy.
EM donadello@fbk.eu
RI donadello, ivan/Y-5533-2019
OI Donadello, Ivan/0000-0002-0701-5729
CR Atif J, 2014, IEEE T SYST MAN CY-S, V44, P552, DOI 10.1109/TSMC.2013.2280440
   Bannour H., 2011, 2011 9th International Workshop on Content-Based Multimedia Indexing (CBMI), P211, DOI 10.1109/CBMI.2011.5972547
   Chen N., 2012, P 21 INT C WORLD WID, P291
   Chen Xianjie, 2014, IEEE C COMP VIS PATT
   Cimiano P., 2009, HDB ONTOLOGIES, P245, DOI DOI 10.1007/978-3-540-92673-3_11
   Dasiopoulou S, 2009, LECT NOTES COMPUT SC, V5850, P105, DOI 10.1007/978-3-642-10562-3_4
   Deng J, 2014, LECT NOTES COMPUT SC, V8689, P48, DOI 10.1007/978-3-319-10590-1_4
   Donadello I, 2015, LECT NOTES COMPUT SC, V8926, P283, DOI 10.1007/978-3-319-16181-5_20
   Espinosa Peraldi I. S., 2009, P 22 INT WORKSH DESC
   Espinosa S, 2011, LECT NOTES ARTIF INT, V6050, P110, DOI 10.1007/978-3-642-20795-2_5
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   F Baader, 2003, DESCRIPTION LOGIC HD
   Fei-Fei L., 2014, CORR
   Fellbaum C., 1998, WORDNET ELECT LEXICA
   Forestier G, 2013, COMPUT GEOSCI-UK, V54, P88, DOI 10.1016/j.cageo.2012.11.023
   Girshick R., 2015, INT C COMP VIS ICCV
   Gould S, 2008, INT J COMPUT VISION, V80, P300, DOI 10.1007/s11263-008-0140-x
   Hirst G, 1998, LANG SPEECH & COMMUN, P305
   Hudelot C., 2005, P 10 IEEE INT C COMP
   Hudelot C, 2008, FUZZY SET SYST, V159, P1929, DOI 10.1016/j.fss.2008.02.011
   Jung J, 2003, J GLOBAL OPTIM, V25, P91
   KOHONEN T, 1990, P IEEE, V78, P1464, DOI 10.1109/5.58325
   Liu Y, 2007, PATTERN RECOGN, V40, P262, DOI 10.1016/j.patcog.2006.04.045
   Long J., 2014, ARXIV14114038
   Mahdisoltani Farzaneh, 2015, P C INN DAT SYST RES
   Neumann B, 2008, IMAGE VISION COMPUT, V26, P82, DOI 10.1016/j.imavis.2007.08.013
   Nyga D., 2014, ROB AUT ICRA 2014 IE, P3916
   Oliva A., 2010, FRONTIERS COMPUTER V
   Petrucci G, 2015, LECT NOTES COMPUT SC, V9088, P740, DOI 10.1007/978-3-319-18818-8_47
   REITER R, 1989, ARTIF INTELL, V41, P125, DOI 10.1016/0004-3702(89)90008-8
   Russell BC, 2008, INT J COMPUT VISION, V77, P157, DOI 10.1007/s11263-007-0090-8
   Schmid C., 2007, COMPUTER VISION PATT
   Schroder C, 1996, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, PROCEEDINGS - VOL II, P785, DOI 10.1109/ICIP.1996.561022
   Sirin E, 2007, J WEB SEMANT, V5, P51, DOI 10.1016/j.websem.2007.03.004
   Smith B., 1988, FDN GESTALT THEORY
   Town C, 2006, MACH VISION APPL, V17, P94, DOI 10.1007/s00138-006-0017-3
   Xu K., 2015, ABS150203044 CORR
   Zhu YK, 2014, LECT NOTES COMPUT SC, V8690, P408, DOI 10.1007/978-3-319-10605-2_27
NR 38
TC 0
Z9 0
U1 1
U2 2
PU IOS PRESS
PI AMSTERDAM
PA NIEUWE HEMWEG 6B, 1013 BG AMSTERDAM, NETHERLANDS
SN 1724-8035
EI 2211-0097
J9 INTELL ARTIF
JI Intell. Artif.
PY 2016
VL 10
IS 1
BP 33
EP 47
DI 10.3233/IA-160093
PG 15
WC Computer Science, Artificial Intelligence
SC Computer Science
GA DN7FH
UT WOS:000377240200003
DA 2020-02-19
ER

PT S
AU Havens, TC
   Anderson, DT
   Stone, K
   Becker, J
   Pinar, AJ
AF Havens, Timothy C.
   Anderson, Derek T.
   Stone, Kevin
   Becker, John
   Pinar, Anthony J.
BE Abielmona, R
   Falcon, R
   ZincirHeywood, N
   Abbass, HA
TI Computational Intelligence Methods in Forward-Looking Explosive Hazard
   Detection
SO RECENT ADVANCES IN COMPUTATIONAL INTELLIGENCE IN DEFENSE AND SECURITY
SE Studies in Computational Intelligence
LA English
DT Article; Book Chapter
DE Sensor fusion; Explosive hazard detection; Aggregation; Multiple kernel
   learning; Deep learning; Fuzzy integral
ID RECOGNITION; TEXTURE; SENSOR
AB This chapter discusses several methods for forward-looking (FL) explosive hazard detection (EHD) using FL infrared (FLIR) and FL ground penetrating radar (FLGPR). The challenge in detecting explosive hazards with FL sensors is that there are multiple types of targets buried at different depths in a highly-cluttered environment. A wide array of target and clutter signatures exist, which makes detection algorithm design difficult. Recent work in this application has focused on fusion methods, including fusion of multiple modalities of sensors (e.g., GPR and IR), fusion of multiple frequency sub-band images in FLGPR, and feature-level fusion using multiple kernel and iECO learning. For this chapter, we will demonstrate several types of EHD techniques, including kernel methods such as support vector machines (SVMs), multiple kernel learning MKL, and feature learning methods, including deep learners and iECO learning. We demonstrate the performance of several algorithms using FLGPR and FLIR data collected at a US Army test site. The summary of this work is that deep belief networks and evolutionary approaches to feature learning were shown to be very effective both for FLGPR and FLIR based EHD.
C1 [Havens, Timothy C.; Becker, John; Pinar, Anthony J.] Michigan Technol Univ, Dept Comp Sci, Dept Elect & Comp Engn, Houghton, MI 49931 USA.
   [Anderson, Derek T.] Mississippi State Univ, Dept Elect & Comp Engn, Mississippi State, MS 39762 USA.
   [Stone, Kevin] Univ Missouri, Dept Elect & Comp Engn, Columbia, MO USA.
RP Havens, TC (reprint author), Michigan Technol Univ, Dept Comp Sci, Dept Elect & Comp Engn, Houghton, MI 49931 USA.
EM thavens@mtu.edu; anderson@ece.msstate.edu; kes25c@mail.missouri.edu;
   jtbecker@mtu.edu
CR Anderson DT, 2014, IEEE T FUZZY SYST, V22, P1625, DOI 10.1109/TFUZZ.2014.2302479
   Anderson DT, 2012, PROC SPIE, V8357, DOI 10.1117/12.920346
   Anderson DT, 2012, IEEE J-STARS, V5, P313, DOI 10.1109/JSTARS.2011.2178119
   Becker J., 2015, P SPIE
   Boser B. E., 1992, Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, P144, DOI 10.1145/130385.130401
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Collins LM, 2002, PROC SPIE, V4742, P709, DOI 10.1117/12.479144
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411
   Costley RD, 2001, P SOC PHOTO-OPT INS, V4394, P617, DOI 10.1117/12.445514
   Cremer F, 2003, P SOC PHOTO-OPT INS, V5089, P517, DOI 10.1117/12.487823
   Dalal N, 2005, PROC CVPR IEEE, P886
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Donahue J., 2013, CORR
   Gader PD, 2004, P SOC PHOTO-OPT INS, V5415, P953, DOI 10.1117/12.544320
   Havens T. C., 2009, P SOC PHOTO-OPT INS, V7303
   Havens T. C., 2010, P SOC PHOTO-OPT INS, V7664
   Havens T. C., 2010, P SOC PHOTO-OPT INS, V7664
   Havens TC, 2014, PROC SPIE, V9072, DOI 10.1117/12.2051034
   Havens TC, 2012, PROC SPIE, V8357, DOI 10.1117/12.920482
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hu LQ, 2014, COMM COM INF SC, V442, P206
   Hu L, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON SERVICE OPERATIONS AND LOGISTICS, AND INFORMATICS (SOLI), P59, DOI 10.1109/SOLI.2013.6611382
   Jarrett K, 2009, IEEE I CONF COMP VIS, P2146, DOI 10.1109/ICCV.2009.5459469
   JIEDDO COIC MID, 2012, GLOB IED MONTHL SUMM
   Jin R., 2010, P 27 INT C MACH LEAR, P1175
   Kandel ER, 2000, PRINCIPLES NEURAL SC
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   Lanckriet GRG, 2004, J MACH LEARN RES, V5, P27
   Lawrence S, 1997, IEEE T NEURAL NETWOR, V8, P98, DOI 10.1109/72.554195
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y., 1995, INT C ART NEUR NERW, V60
   Lowe D., 1999, P INT C COMP VIS, V2, P1150, DOI [10.1109/ICCV.1999.790410, DOI 10.1109/ICCV.1999.790410]
   Matas J, 2004, IMAGE VISION COMPUT, V22, P761, DOI 10.1016/j.imavis.2004.02.006
   Mercer J, 1909, PHILOS T R SOC LOND, V209, P415, DOI 10.1098/rsta.1909.0016
   Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4
   Palm R. B., 2012, THESIS TU DENMARK
   Playle N, 2002, P SOC PHOTO-OPT INS, V4742, P11, DOI 10.1117/12.479086
   Price S. R., 2014, IEEE S SERIES COMPUT
   Sarikaya R, 2011, INT CONF ACOUST SPEE, P5680
   Scott GJ, 2012, INT GEOSCI REMOTE SE, P79, DOI 10.1109/IGARSS.2012.6351632
   Stauffer C., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P246, DOI 10.1109/CVPR.1999.784637
   Stone K, 2010, PROC SPIE, V7664, DOI 10.1117/12.851370
   Stone K., 2014, P SPIE
   Stone K., 2013, P SPIE
   Stone K, 2008, P SPIE, V6953
   Stone K. E., 2012, P SPIE C DET SENS MI
NR 47
TC 1
Z9 1
U1 0
U2 2
PU SPRINGER-VERLAG BERLIN
PI BERLIN
PA HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY
SN 1860-949X
EI 1860-9503
BN 978-3-319-26450-9; 978-3-319-26448-6
J9 STUD COMPUT INTELL
PY 2016
VL 621
BP 13
EP 44
DI 10.1007/978-3-319-26450-9_2
D2 10.1007/978-3-319-26450-9
PG 32
WC Computer Science, Artificial Intelligence; Robotics
SC Computer Science; Robotics
GA BE2FK
UT WOS:000369153300003
DA 2020-02-19
ER

PT J
AU Wolff, JG
AF Wolff, James Gerard
TI The SP Theory of Intelligence: Distinctive Features and Advantages
SO IEEE ACCESS
LA English
DT Article
DE Artificial intelligence; information compression; multiple alignment;
   perception; cognition; neural networks; deep learning; unsupervised
   learning; reasoning; mathematics
ID NEURAL-NETWORKS; INFORMATION
AB This paper aims to highlight distinctive features of the SP theory of intelligence, realized in the SP computer model, and its apparent advantages compared with some AI-related alternatives. Perhaps most importantly, the theory simplifies and integrates observations and concepts in AI-related areas, and has potential to simplify and integrate of structures and processes in computing systems. Unlike most other AI-related theories, the SP theory is itself a theory of computing, which can be the basis for new architectures for computers. Fundamental in the theory is information compression via the matching and unification of patterns and, more specifically, via a concept of multiple alignment. The theory promotes transparency in the representation and processing of knowledge, and unsupervised learning of natural structures via information compression. It provides an interpretation of aspects of mathematics and an interpretation of phenomena in human perception and cognition. Abstract concepts in the theory may be realized in terms of neurons and their inter-connections (SP-neural). These features and advantages of the SP system are discussed in relation to AI-related alternatives: the concept of minimum length encoding and related concepts, how computational and energy efficiency in computing may be achieved, deep learning in neural networks, unified theories of cognition and related research, universal search, Bayesian networks and some other models for AI, IBM's Watson, solving problems associated with big data and in the development of intelligence in autonomous robots, pattern recognition and vision, the learning and processing of natural language, exact and inexact forms of reasoning, representation and processing of diverse forms of knowledge, and software engineering. In conclusion, the SP system can provide a firm foundation for the long-term development of AI and related areas, and at the same time, it may deliver useful results on relatively short timescales.
C1 [Wolff, James Gerard] CognitionRes Org, Menai Bridge LL59 5LR, Gwynedd, Wales.
RP Wolff, JG (reprint author), CognitionRes Org, Menai Bridge LL59 5LR, Gwynedd, Wales.
EM jgw@cognitionresearch.org
CR Nguyen A, 2015, PROC CVPR IEEE, P427, DOI 10.1109/CVPR.2015.7298640
   Barrow J., 1992, PI SKY COUNTING THIN
   Bauer J., 2013, P 51 ANN M ASS COMP, P455
   Bengio Y., 2013, BIG NEURAL NETWORKS
   CHURCH A, 1941, ANN MATH STUDIES, V6
   Clocksin W. F., 2003, PROGRAMMING PROLOG U
   Dahl GE, 2012, IEEE T AUDIO SPEECH, V20, P30, DOI 10.1109/TASL.2011.2134090
   Davis E, 2015, COMMUN ACM, V58, P92, DOI 10.1145/2701413
   Dreyfus H. L., 1992, WHAT COMPUTERS STILL
   Edwards C, 2015, COMMUN ACM, V58, P14, DOI 10.1145/2771283
   Ferrucci DA, 2012, IBM J RES DEV, V56, DOI 10.1147/JRD.2012.2184356
   Gagliolo M., 2007, SCHOLARPEDIA, V2, P2575
   Ganin Y., 2014, 12 AS C COMP VIS SIN
   Garcia A. S., 2015, POWER SYSTEMS IEEE T, P1
   Gazzaniga M. S., 2000, COGNITIVE NEUROSCIEN, P58
   Goertzel B., 2012, 20121002 OP COGN PRO
   Graves A., 2014, NEURAL TURING MACHIN
   GRIMSON WEL, 1981, PHILOS T ROY SOC B, V292, P217, DOI 10.1098/rstb.1981.0031
   Gross CG, 2002, NEUROSCIENTIST, V8, P512, DOI 10.1177/107385802237175
   Hebb D. O, 1949, ORG BEHAV NEUROPSYCH
   Herculano-Houzel S, 2012, P NATL ACAD SCI USA, V109, P10661, DOI 10.1073/pnas.1201895109
   Horrocks I, 2008, COMMUN ACM, V51, P58, DOI 10.1145/1409360.1409377
   Hutter M., 2002, International Journal of Foundations of Computer Science, V13, P431, DOI 10.1142/S0129054102001199
   Isaacson W., 2007, EINSTEIN HIS LIFE UN
   Ivakhnenko A. G., 1995, Pattern Recognition and Image Analysis, V5, P527
   Jean S, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P1
   Julesz B., 1971, FDN CYCLOPEAN PERCEP
   Kelly III J., 2013, SMART MACHINES IBMS
   Klinov P., 2013, LNCS, V7123, P59, DOI DOI 10.1007/978-3-642-35975-0-4
   Laird JE, 2012, SOAR COGNITIVE ARCHITECTURE, P1
   Le QV, 2013, INT CONF ACOUST SPEE, P8595, DOI 10.1109/ICASSP.2013.6639343
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Levesque H. J., 2012, P 13 INT C PRINC KNO, P1
   LEVIN LA, 1984, INFORM CONTROL, V61, P15, DOI 10.1016/S0019-9958(84)80060-1
   Levin LA, 1973, PROBL PEREDACHI INF, V9, P115
   LEVINE D, 1994, NEURAL NETWORKS KNOW
   Li Ming, 2014, INTRO KOLMOGOROV COM
   Marcus Gary, 2008, KLUGE HAPHAZARD EVOL
   MARR D, 1979, PROC R SOC SER B-BIO, V204, P301, DOI 10.1098/rspb.1979.0029
   McCorduck P., 2004, MACHINES WHO THINK P
   Milner PM, 1996, J COGNITIVE NEUROSCI, V8, P69, DOI 10.1162/jocn.1996.8.1.69
   MINSKY M, 1986, SOC MIND
   Mordvintsev A., 2015, 20150713 GOOGL INC
   Neisser U, 1967, COGNITIVE PSYCHOL
   Newell A., 1990, UNIFIED THEORIES COG
   Newell A., 1973, VISUAL INFORM PROCES, P283
   Palade V., 2015, 20151116 COGNITIONRE
   Post EL, 1943, AM J MATH, V65, P197, DOI 10.2307/2371809
   RISSANEN J, 1978, AUTOMATICA, V14, P465, DOI 10.1016/0005-1098(78)90005-5
   Russell S, 2015, COMMUN ACM, V58, P88, DOI 10.1145/2699411
   Samsonovich AV, 2010, FRONT ARTIF INTEL AP, V221, P195, DOI 10.3233/978-1-60750-661-4-195
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Schmidhuber J, 2009, COGN COMPUT, V1, P177, DOI 10.1007/s12559-009-9014-y
   Schwenk Holger, 2012, P 24 INT C COMP LING, P1071
   Simon H. A., 1972, HUMAN PROBLEM SOLVIN
   Simonite T, 2015, TECHNOL REV, V118, P70
   Solomonoff R. J., 1986, UNCERTAINTY ARTIFICI, P473
   SOLOMONOFF RJ, 1964, INFORM CONTROL, V7, P1, DOI 10.1016/S0019-9958(64)90223-2
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Sutskever I., 2014, ADV NEURAL INFORM PR, P3104, DOI DOI 10.1007/S10107-014-0839-0
   Szegedy C., 2014, INTRIGUING PROPERTIE
   Turing A.M., 1950, MIND, VLIX, P433, DOI [10.1093/mind/LIX.236.433, DOI 10.1093/MIND/LIX.236.433]
   Turing AM, 1937, P LOND MATH SOC, V42, P230
   TURING AM, 1992, COLLECTED WORKS AM T
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   WALLACE CS, 1968, COMPUT J, V11, P185, DOI 10.1093/comjnl/11.2.185
   Wiesler Simon, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P180, DOI 10.1109/ICASSP.2014.6853582
   WINOGRAD T, 1972, COGNITIVE PSYCHOL, V3, P1, DOI 10.1016/0010-0285(72)90002-3
   Wolff J.G., 1988, CATEGORIES PROCESSES, P179
   Wolff J. G., 2006, UNIFYING COMPUTING C
   Wolff J. G., 2015, 20150604 COGNITIONRE
   Wolff JG, 2007, DATA KNOWL ENG, V60, P596, DOI 10.1016/j.datak.2006.04.003
   Wolff JG, 2006, DECIS SUPPORT SYST, V42, P608, DOI 10.1016/j.dss.2005.02.005
   Wolff JG, 2014, SPRINGERPLUS, V3, DOI 10.1186/2193-1801-3-552
   Wolff JG, 2014, INFORMATION, V5, P1, DOI 10.3390/info5010001
   Wolff JG, 2014, IEEE ACCESS, V2, P301, DOI 10.1109/ACCESS.2014.2315297
   Wolff JG, 2013, INFORMATION, V4, P283, DOI 10.3390/info4030283
   Wolff JG, 2014, IEEE ACCESS, V2, P1629, DOI 10.1109/ACCESS.2014.2382753
   Yu Q., 2015, SKETCH A NET BEATS H
   ZADEH LA, 1965, INFORM CONTROL, V8, P338, DOI 10.1016/S0019-9958(65)90241-X
   Zeng X., 2013, P IEEE INT C COMP VI, P180
   Zhang X., 2015, TEXT UNDERSTANDING S
   Zhu Y., 2015, SEMANTIC AMODAL SEGM
NR 83
TC 4
Z9 4
U1 0
U2 29
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 2169-3536
J9 IEEE ACCESS
JI IEEE Access
PY 2016
VL 4
BP 216
EP 246
DI 10.1109/ACCESS.2015.2513822
PG 31
WC Computer Science, Information Systems; Engineering, Electrical &
   Electronic; Telecommunications
SC Computer Science; Engineering; Telecommunications
GA DH2LP
UT WOS:000372617600015
OA DOAJ Gold
DA 2020-02-19
ER

PT J
AU Xing, C
   Ma, L
   Yang, XQ
AF Xing, Chen
   Ma, Li
   Yang, Xiaoquan
TI Stacked Denoise Autoencoder Based Feature Extraction and Classification
   for Hyperspectral Images
SO JOURNAL OF SENSORS
LA English
DT Article
AB Deep learning methods have been successfully applied to learn feature representations for high-dimensional data, where the learned features are able to reveal the nonlinear properties exhibited in the data. In this paper, deep learning method is exploited for feature extraction of hyperspectral data, and the extracted features can provide good discriminability for classification task. Training a deep network for feature extraction and classification includes unsupervised pretraining and supervised fine-tuning. We utilized stacked denoise autoencoder (SDAE) method to pretrain the network, which is robust to noise. In the top layer of the network, logistic regression (LR) approach is utilized to perform supervised fine-tuning and classification. Since sparsity of features might improve the separation capability, we utilized rectified linear unit (ReLU) as activation function in SDAE to extract high level and sparse features. Experimental results using Hyperion, AVIRIS, and ROSIS hyperspectral data demonstrated that the SDAE pretraining in conjunction with the LR fine-tuning and classification (SDAE LR) can achieve higher accuracies than the popular support vector machine (SVM) classifier.
C1 [Xing, Chen; Ma, Li] China Univ Geosci, Fac Mech & Elect Informat, Wuhan 430074, Hubei, Peoples R China.
   [Yang, Xiaoquan] Huazhong Univ Sci & Technol, Wuhan Natl Lab Optoelect, Wuhan 430074, Hubei, Peoples R China.
RP Ma, L (reprint author), China Univ Geosci, Fac Mech & Elect Informat, Wuhan 430074, Hubei, Peoples R China.
EM maryparisster@gmail.com
FU National Natural Science Foundations of ChinaNational Natural Science
   Foundation of China [61102104, 81201067]; Fundamental Research Funds for
   National University, China University of Geosciences (Wuhan) [CUG120408,
   CUG120119]; Institute of Automation, Chinese Academy of SciencesChinese
   Academy of Sciences
FX This work was supported by National Natural Science Foundations of China
   (61102104, 81201067), the Fundamental Research Funds for National
   University, China University of Geosciences (Wuhan) (CUG120408,
   CUG120119), and Institute of Automation, Chinese Academy of Sciences.
CR Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153
   Bottou Lon, 1998, ONLINE LEARNING NEUR, V17, P9
   Chen YS, 2014, IEEE J-STARS, V7, P2094, DOI 10.1109/JSTARS.2014.2329330
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Huang X, 2013, IEEE T GEOSCI REMOTE, V51, P257, DOI 10.1109/TGRS.2012.2202912
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Ma JY, 2015, IEEE T SIGNAL PROCES, V63, P1115, DOI 10.1109/TSP.2014.2388434
   Ma JY, 2015, PATTERN RECOGN, V48, P772, DOI 10.1016/j.patcog.2014.09.005
   Ma L, 2015, IEEE T GEOSCI REMOTE, V53, P2832, DOI 10.1109/TGRS.2014.2365676
   Ma L, 2010, IEEE T GEOSCI REMOTE, V48, P4099, DOI 10.1109/TGRS.2010.2055876
   Norouzi Mohammad, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2735, DOI 10.1109/CVPRW.2009.5206577
   Tan CC, 2008, CAN CON EL COMP EN, P442
   van der Maaten L., 2008, DIMENSIONALITY REDUC
   Vincent P, 2008, P 25 INT C MACH LEAR, P1096, DOI DOI 10.1145/1390156.1390294
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Xavier Glorot, 2011, P 14 INT C ART INT S, P315, DOI DOI 10.1177/1753193410395357
   Zhang F, 2015, IEEE T GEOSCI REMOTE, V53, P2175, DOI 10.1109/TGRS.2014.2357078
   Zhang LF, 2015, PATTERN RECOGN, V48, P3102, DOI 10.1016/j.patcog.2014.12.016
   Zhang LF, 2012, IEEE T GEOSCI REMOTE, V50, P879, DOI 10.1109/TGRS.2011.2162339
NR 19
TC 50
Z9 55
U1 12
U2 70
PU HINDAWI LTD
PI LONDON
PA ADAM HOUSE, 3RD FLR, 1 FITZROY SQ, LONDON, W1T 5HF, ENGLAND
SN 1687-725X
EI 1687-7268
J9 J SENSORS
JI J. Sens.
PY 2016
AR UNSP 3632943
DI 10.1155/2016/3632943
PG 10
WC Engineering, Electrical & Electronic; Instruments & Instrumentation
SC Engineering; Instruments & Instrumentation
GA DF7KP
UT WOS:000371536900001
OA DOAJ Gold
DA 2020-02-19
ER

PT J
AU Lian, ZF
   Jing, XJ
   Wang, XH
   Huang, H
   Tan, YH
   Cui, YH
AF Lian Zifeng
   Jing Xiaojun
   Wang Xiaohan
   Huang Hai
   Tan Youheng
   Cui Yuanhao
TI DropConnect Regularization Method with Sparsity Constraint for Neural
   Networks
SO CHINESE JOURNAL OF ELECTRONICS
LA English
DT Article
DE DropConnect; Sparse regularization; Deep learning; Neural networks
AB DropConnect is a recently introduced algorithm to prevent the co-adaptation of feature detectors. Compared to Dropout, DropConnect gains state-of-the-art results on several image recognition benchmarks. Motivated by the success of DropConnect, we extended this algorithm with the ability of sparse feature selection. In DropConnect algorithm, the dropping masks of weights are generated using Bernoulli gating variables that are independent of the weights and activations. We introduce a new strategy to generate masks depending on the outputs of previous layer. Using this method, neurons which are promising to produce sparser features will be assigned a bigger possibility to keep active in the forward and backward propagations. We then evaluate such sparsity constrained DropConnect on MNIST and CIFAR datasets in comparison with ordinary DropConnect and Dropout method. The results show that our new method improves the sparsity of features significantly, while not degrading the precision.
C1 [Lian Zifeng; Jing Xiaojun; Huang Hai; Tan Youheng; Cui Yuanhao] Beijing Univ Posts & Telecommun, Minist Educ, Key Lab Trustworthy Distributed Comp & Serv BUPT, Beijing 100876, Peoples R China.
   [Wang Xiaohan] Peking Univ, Sch Software & Microelect, Beijing 102600, Peoples R China.
RP Lian, ZF (reprint author), Beijing Univ Posts & Telecommun, Minist Educ, Key Lab Trustworthy Distributed Comp & Serv BUPT, Beijing 100876, Peoples R China.
EM lianzf@bupt.edu.cn; jx-iaojun@bupt.edu.cn
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61143008, 61471066]; National High Technology
   Research and Development Program of ChinaNational High Technology
   Research and Development Program of China [2011AA01A204]
FX This work is supported by the National Natural Science Foundation of
   China (No.61143008, No.61471066), National High Technology Research and
   Development Program of China (No.2011AA01A204).
CR Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   FIELD DJ, 1994, NEURAL COMPUT, V6, P559, DOI 10.1162/neco.1994.6.4.559
   Hinton G. E, 2012, ARXIV12070580
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hoyer PO, 2004, J MACH LEARN RES, V5, P1457
   Krizhevsky A., 2015, CUDA CONVNET
   Krizhevsky A., 2009, THESIS, V1
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   MACKAY DJC, 1995, NETWORK-COMP NEURAL, V6, P469, DOI 10.1088/0954-898X/6/3/011
   Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128
   Vincent P, 2008, P 25 INT C MACH LEAR, P1096, DOI DOI 10.1145/1390156.1390294
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Wan L., 2013, P 30 INT C MACH LEAR, P1058
   Weigend A. S., 1991, NEURAL INFORM PROCES
   Zhao P, 2009, ANN STAT, V37, P3468, DOI 10.1214/07-AOS584
NR 16
TC 11
Z9 12
U1 2
U2 14
PU TECHNOLOGY EXCHANGE LIMITED HONG KONG
PI BEIJING
PA BLDG#13, PUHUINANLI, SOUTH YUYUANTAN RD, HAIDIAN DIST, BEIJING, 00000,
   PEOPLES R CHINA
SN 1022-4653
EI 2075-5597
J9 CHINESE J ELECTRON
JI Chin. J. Electron.
PD JAN
PY 2016
VL 25
IS 1
BP 152
EP 158
DI 10.1049/cje.2016.01.023
PG 7
WC Engineering, Electrical & Electronic
SC Engineering
GA DB2FD
UT WOS:000368322800023
DA 2020-02-19
ER

PT J
AU Wang, H
   Cai, YF
   Chen, XB
   Chen, L
AF Wang, Hai
   Cai, Yingfeng
   Chen, Xiaobo
   Chen, Long
TI Night-Time Vehicle Sensing in Far Infrared Image with Deep Learning
SO JOURNAL OF SENSORS
LA English
DT Article
ID ROAD; TRACKING
AB The use of night vision systems in vehicles is becoming increasingly common. Several approaches using infrared sensors have been proposed in the literature to detect vehicles in far infrared (FIR) images. However, these systems still have low vehicle detection rates and performance could be improved. This paper presents a novel method to detect vehicles using a far infrared automotive sensor. Firstly, vehicle candidates are generated using a constant threshold from the infrared frame. Contours are then generated by using a local adaptive threshold based on maximum distance, which decreases the number of processing regions for classification and reduces the false positive rate. Finally, vehicle candidates are verified using a deep belief network (DBN) based classifier. The detection rate is 93.9% which is achieved on a database of 5000 images and video streams. This result is approximately a 2.5% improvement on previously reported methods and the false detection rate is also the lowest among them.
C1 [Wang, Hai] Jiangsu Univ, Sch Automot & Traff Engn, Zhenjiang 212013, Peoples R China.
   [Cai, Yingfeng; Chen, Xiaobo; Chen, Long] Jiangsu Univ, Automot Engn Res Inst, Zhenjiang 212013, Peoples R China.
RP Cai, YF (reprint author), Jiangsu Univ, Automot Engn Res Inst, Zhenjiang 212013, Peoples R China.
EM caicaixiao0304@126.com
OI Chen, Xiaobo/0000-0001-9940-1637
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61403172, 61203244, 51305167]; China Postdoctoral
   Science FoundationChina Postdoctoral Science Foundation [2014M561592];
   China Postdoctoral Science Foundation Special Funding [2015T80511];
   Information Technology Research Program of Transport Ministry of China
   [2013364836900]; Natural Science Foundation of Jiangsu ProvinceJiangsu
   Planned Projects for Postdoctoral Research FundsNatural Science
   Foundation of Jiangsu Province [BK20140555]; Jiangsu University
   Scientific Research Foundation [12JDG010, 14JDG028]
FX This work has been supported by the National Natural Science Foundation
   of China under the Grants 61403172, 61203244, and 51305167, China
   Postdoctoral Science Foundation (2014M561592), China Postdoctoral
   Science Foundation Special Funding (2015T80511), Information Technology
   Research Program of Transport Ministry of China under the Grant
   2013364836900, Natural Science Foundation of Jiangsu Province
   (BK20140555), and Jiangsu University Scientific Research Foundation for
   Senior Professionals (12JDG010, 14JDG028).
CR Andreone L, 2002, IEEE 5TH INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS, PROCEEDINGS, P141, DOI 10.1109/ITSC.2002.1041203
   Arrospide J, 2008, IEEE IMAGE PROC, P2008, DOI 10.1109/ICIP.2008.4712178
   Bar Hillel A, 2014, MACH VISION APPL, V25, P727, DOI 10.1007/s00138-011-0404-2
   Benenson R, 2015, LECT NOTES COMPUT SC, V8926, P613, DOI 10.1007/978-3-319-16181-5_47
   Besbes Bassem, 2010, 2010 13th International IEEE Conference on Intelligent Transportation Systems (ITSC 2010), P1869, DOI 10.1109/ITSC.2010.5625285
   Cui JZ, 2010, IEEE INT VEH SYM, P871, DOI 10.1109/IVS.2010.5548101
   Hermes C, 2010, IEEE INT VEH SYM, P26, DOI 10.1109/IVS.2010.5548014
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hurney P, 2015, IET INTELL TRANSP SY, V9, P75, DOI 10.1049/iet-its.2013.0163
   Pauplin O, 2012, PATTERN RECOGN LETT, V33, P685, DOI 10.1016/j.patrec.2011.12.010
   Qiong Liu, 2012, Proceedings of the 2012 IEEE International Conference on Imaging Systems and Techniques (IST), P398, DOI 10.1109/IST.2012.6295596
   RAMIREZ A, 2014, P 25 IEEE INT VEH S, P96
   Sindoori R., 2013, INT J ENG TECHNOLOGY, V5, P765
   Sivaraman S, 2014, MACH VISION APPL, V25, P599, DOI 10.1007/s00138-011-0388-y
   Sivaraman S, 2013, IEEE T INTELL TRANSP, V14, P1773, DOI 10.1109/TITS.2013.2266661
   Sivaraman S, 2010, IEEE T INTELL TRANSP, V11, P267, DOI 10.1109/TITS.2010.2040177
   Sun ZH, 2006, IEEE T PATTERN ANAL, V28, P694, DOI 10.1109/TPAMI.2006.104
   Teoh SS, 2012, MACH VISION APPL, V23, P831, DOI 10.1007/s00138-011-0355-7
   Wang H, 2015, OPTIK, V126, P386, DOI 10.1016/j.ijleo.2014.09.010
   Wood F., 2012, TRAINING PRODUCTS EX
   Zaklouta F, 2014, ROBOT AUTON SYST, V62, P16, DOI 10.1016/j.robot.2012.07.019
   Zhong S., 2011, P 19 ACM INT C MULT, P343
NR 22
TC 2
Z9 2
U1 0
U2 27
PU HINDAWI LTD
PI LONDON
PA ADAM HOUSE, 3RD FLR, 1 FITZROY SQ, LONDON, W1T 5HF, ENGLAND
SN 1687-725X
EI 1687-7268
J9 J SENSORS
JI J. Sens.
PY 2016
AR 3403451
DI 10.1155/2016/3403451
PG 8
WC Engineering, Electrical & Electronic; Instruments & Instrumentation
SC Engineering; Instruments & Instrumentation
GA DB1PI
UT WOS:000368280500001
OA DOAJ Gold
DA 2020-02-19
ER

PT J
AU Li, QL
   Feng, BW
   Xie, LP
   Liang, P
   Zhang, HS
   Wang, TF
AF Li, Qiaoliang
   Feng, Bowei
   Xie, LinPei
   Liang, Ping
   Zhang, Huisheng
   Wang, Tianfu
TI A Cross-Modality Learning Approach for Vessel Segmentation in Retinal
   Images
SO IEEE TRANSACTIONS ON MEDICAL IMAGING
LA English
DT Article
DE Cross-modality learning; deep learning; retinal image; vessel
   segmentation
ID BLOOD-VESSELS; COLOR IMAGES; CLASSIFICATION; EXTRACTION; ALGORITHM;
   NETWORKS; FEATURES; DATABASE; FILTER; LEVEL
AB This paper presents a new supervised method for vessel segmentation in retinal images. This method remolds the task of segmentation as a problem of cross-modality data transformation from retinal image to vessel map. A wide and deep neural network with strong induction ability is proposed to model the transformation, and an efficient training strategy is presented. Instead of a single label of the center pixel, the network can output the label map of all pixels for a given image patch. Our approach outperforms reported state-of-the-art methods in terms of sensitivity, specificity and accuracy. The result of cross-training evaluation indicates its robustness to the training set. The approach needs no artificially designed feature and no preprocessing step, reducing the impact of subjective factors. The proposed method has the potential for application in image diagnosis of ophthalmologic diseases, and it may provide a new, general, high-performance computing framework for image segmentation.
C1 [Li, Qiaoliang; Feng, Bowei; Xie, LinPei; Liang, Ping; Zhang, Huisheng; Wang, Tianfu] Shenzhen Univ, Dept Biomed Engn, Guangdong Key Lab Biomed Measurements & Ultrasoun, Shenzhen 518060, Peoples R China.
RP Liang, P (reprint author), Shenzhen Univ, Dept Biomed Engn, Guangdong Key Lab Biomed Measurements & Ultrasoun, Shenzhen 518060, Peoples R China.
EM lql@szu.edu.cn; feng.bowei@163.com; xielinpei@email.szu.edu.cn;
   liangping@szu.edu.cn; isaac_zhs@126.com; tfwang@szu.edu.cn
OI Wang, Tianfu/0000-0002-1248-1214
FU National Science Foundation of ChinaNational Natural Science Foundation
   of China [61401285, 81401750]; Guangdong Science Plan [2014A010103035];
   Shenzhen Science Plan [CYJ20140418182819179, JSGG20130411160504896,
   JCYJ20130408173226864]
FX This work was supported by the Project of the National Science
   Foundation of China under Grants 61401285, 81401750, Guangdong Science
   Plan 2014A010103035, and Shenzhen Science Plan CYJ20140418182819179,
   JSGG20130411160504896, JCYJ20130408173226864. Asterisk indicates
   corresponding author.
CR Abramoff MD, 2013, JAMA OPHTHALMOL, V131, P351, DOI 10.1001/jamaophthalmol.2013.1743
   Al-Diri B, 2009, IEEE T MED IMAGING, V28, P1488, DOI 10.1109/TMI.2009.2017941
   Azzopardi G, 2015, MED IMAGE ANAL, V19, P46, DOI 10.1016/j.media.2014.08.002
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Cheng EK, 2014, MACH VISION APPL, V25, P1779, DOI 10.1007/s00138-014-0638-x
   Espona L, 2008, INT C PATT RECOG, P2128
   Fraz MM, 2012, COMPUT METH PROG BIO, V108, P600, DOI 10.1016/j.cmpb.2011.08.009
   Fraz MM, 2012, COMPUT METH PROG BIO, V108, P407, DOI 10.1016/j.cmpb.2012.03.009
   Fraz MM, 2012, IEEE T BIO-MED ENG, V59, P2538, DOI 10.1109/TBME.2012.2205687
   Gang L, 2002, IEEE T BIO-MED ENG, V49, P168, DOI 10.1109/10.979356
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hoover A, 2000, IEEE T MED IMAGING, V19, P203, DOI 10.1109/42.845178
   Jiang XY, 2003, IEEE T PATTERN ANAL, V25, P131, DOI 10.1109/TPAMI.2003.1159954
   Kirbas C, 2004, ACM COMPUT SURV, V36, P81, DOI 10.1145/1031120.1031121
   Lam BSY, 2010, IEEE T MED IMAGING, V29, P1369, DOI 10.1109/TMI.2010.2043259
   Lam BSY, 2008, IEEE T MED IMAGING, V27, P237, DOI 10.1109/TMI.2007.909827
   LIU IC, 1993, IEEE T MED IMAGING, V12, P334, DOI 10.1109/42.232264
   Lupascu CA, 2010, IEEE T INF TECHNOL B, V14, P1267, DOI 10.1109/TITB.2010.2052282
   Marin D, 2011, IEEE T MED IMAGING, V30, P146, DOI 10.1109/TMI.2010.2064333
   Marino C, 2006, PATTERN ANAL APPL, V9, P21, DOI 10.1007/s10044-005-0022-6
   Mendonca AM, 2006, IEEE T MED IMAGING, V25, P1200, DOI 10.1109/TMI.2006.879955
   Miri MS, 2011, IEEE T BIO-MED ENG, V58, P1183, DOI 10.1109/TBME.2010.2097599
   Niemeijer M, 2004, PROC SPIE, V5370, P648, DOI 10.1117/12.535349
   Odstrcilik J, 2013, IET IMAGE PROCESS, V7, P373, DOI 10.1049/iet-ipr.2012.0455
   Osareh A, 2009, IRAN J SCI TECHNOL B, V33, P191
   Ricci E, 2007, IEEE T MED IMAGING, V26, P1357, DOI 10.1109/TMI.2007.898551
   Rifai S, 2011, LECT NOTES ARTIF INT, V6912, P645, DOI 10.1007/978-3-642-23783-6_41
   Ruggiero C, 2007, IEEE T NANOBIOSCI, V6, P180, DOI 10.1109/TNB.2007.897454
   Soares JVB, 2006, IEEE T MED IMAGING, V25, P1214, DOI 10.1109/TMI.2006.879967
   Staal J, 2004, IEEE T MED IMAGING, V23, P501, DOI 10.1109/TMI.2004.825627
   Sum KW, 2008, IEEE T BIO-MED ENG, V55, P358, DOI 10.1109/TBME.2007.896587
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   You XG, 2011, PATTERN RECOGN, V44, P2314, DOI 10.1016/j.patcog.2011.01.007
   Zana F, 1999, IEEE T MED IMAGING, V18, P419, DOI 10.1109/42.774169
   Zana F, 2001, IEEE T IMAGE PROCESS, V10, P1010, DOI 10.1109/83.931095
NR 35
TC 130
Z9 136
U1 10
U2 52
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 0278-0062
EI 1558-254X
J9 IEEE T MED IMAGING
JI IEEE Trans. Med. Imaging
PD JAN
PY 2016
VL 35
IS 1
BP 109
EP 118
DI 10.1109/TMI.2015.2457891
PG 10
WC Computer Science, Interdisciplinary Applications; Engineering,
   Biomedical; Engineering, Electrical & Electronic; Imaging Science &
   Photographic Technology; Radiology, Nuclear Medicine & Medical Imaging
SC Computer Science; Engineering; Imaging Science & Photographic
   Technology; Radiology, Nuclear Medicine & Medical Imaging
GA DA2KX
UT WOS:000367624800010
PM 26208306
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Chen, Y
   Yang, XN
   Zhong, BN
   Pan, SN
   Chen, DS
   Zhang, HZ
AF Chen, Yan
   Yang, Xiangnan
   Zhong, Bineng
   Pan, Shengnan
   Chen, Duansheng
   Zhang, Huizhen
TI CNNTracker: Online discriminative object tracking via deep convolutional
   neural network
SO APPLIED SOFT COMPUTING
LA English
DT Article
DE Deep learning; Object tracking; Convolutional neural network; Object
   appearance model; Large scale training data
AB Object appearance model is a crucial module for object tracking and numerous schemes have been developed for object representation with impressive performance. Traditionally, the features used in such object appearance models are predefined in a handcrafted offline way but not tuned for the tracked object. In this paper, we propose a deep learning architecture to learn the most discriminative features dynamically via a convolutional neural network (CNN). In particular, we propose to enhance the discriminative ability of the appearance model in three-fold. First, we design a simple yet effective method to transfer the features learned from CNNs on the source tasks with large scale training data to the new tracking tasks with limited training data. Second, to alleviate the tracker drifting problem caused by model update, we exploit both the ground truth appearance information of the object labeled in the initial frames and the image observations obtained online. Finally, a heuristic schema is used to judge whether updating the object appearance models or not. Extensive experiments on challenging video sequences from the CVPR2013 tracking benchmark validate the robustness and effectiveness of the proposed tracking method. (C) 2015 Elsevier B.V. All rights reserved.
C1 [Chen, Yan; Yang, Xiangnan; Zhong, Bineng; Pan, Shengnan; Chen, Duansheng; Zhang, Huizhen] Huaqiao Univ, Dept Comp Sci & Technol, Xiamen 361021, Fujian, Peoples R China.
RP Chen, Y (reprint author), Huaqiao Univ, Xiamen 361021, Fujian, Peoples R China.
EM yannychen@hqu.edu.cn
FU Natural Science Foundation of ChinaNational Natural Science Foundation
   of China [61202299]; Natural Science Foundation of Fujian
   ProvinceNatural Science Foundation of Fujian Province [2015J01257];
   Promotion Program for Young and Middle-aged Teacher in Science and
   Technology Research of Huaqiao University [ZQN-PY210]; Outstanding Young
   Persons' Research Program for Higher Education of Fujian Province
   [JA13007]
FX This work is supported by Natural Science Foundation of China (No.
   61202299), Natural Science Foundation of Fujian Province (No.
   2015J01257), Promotion Program for Young and Middle-aged Teacher in
   Science and Technology Research of Huaqiao University (No. ZQN-PY210),
   and Outstanding Young Persons' Research Program for Higher Education of
   Fujian Province (No. JA13007).
CR Ahonen T., 2006, TPAMI
   Alex K., 2009, LEARNING MULTIPLE LA
   Avidan S., 2004, TPAMI
   Avidan S., 2007, TPAMI
   Babenko B., 2011, TPAMI
   Bai Y., 2012, CVPR
   Bao C, 2012, CVPR
   Bengio Y., 2013, TPAMI
   Carneiro G., 2013, TAPMI
   Cehovin L., 2013, TPAMI
   Chen X., 2014, IEEE ACCESS
   Collins R., 2005, TPAMI
   Comaniciu D., 2003, TPAMI
   Cui Z., 2014, ECCV
   Dalal N, 2005, PROC CVPR IEEE, P886
   Danelljan M., 2014, CVPR
   Deng L., 2012, ICASSP
   Denton E., 2014, NIPS
   Donahue J., 2014, ICML
   Duffner S., 2013, ICCV
   Fan J. L., 2012, TPAMI
   Fan J. L., 2010, TNN
   Fiaschi L., 2014, CVPR
   Gall J., 2011, TPAMI
   Girshick R. B., 2013, ARXIV13112524CSCV
   Godec M., 2011, ICCV
   Grabner H., 2008, ECCV
   Grabner H., 2006, CVPR
   Grabner H., 2010, CVPR
   Hare S., 2011, ICCV
   He W., 2009, ICCV
   Hinton G., 2006, SCIENCE
   Hinton G., 2006, NEURAL COMPUT, V18
   Hinton G., 2012, IEEE SIGNAL PROCESS
   Isard M., 1998, IJCV
   Jaderberg M., 2014, BMVC
   Jepson A.D., 2003, TPAMI
   Jia X., 2012, CVPR
   Kalal Z., 2012, TPAMI
   Krizhevsky A., 2012, NIPS
   Kwon J., 2014, TPAMI
   Kwon J., 2010, CVPR
   Laney D, 2012, IMPORTANCE BIG DATA
   Li X., 2013, TRUE TRIAXIAL TESTIN, P3
   Li Xi, 2013, ACM T INTELLIGENT SY
   Lu Y., 2014, CVPR
   Matthews I., 2004, TPAMI
   Mei X., 2009, ICCV
   NVIDIA Corporation, 2013, CUDA C PROGRAMMING G
   Porikli F., 2006, CVPR
   Raina R., 2009, ICML
   Ross D. A., 2008, IJCV
   Santner J., 2010, CVPR
   Sermanet P., 2013, CVPR
   Smeulders A., 2014, TPAMI
   Sun Y., 2014, TECHNICAL REPORT
   Szegedy C., 2015, CVPR
   Taigman Y., 2014, CVPR
   Takala V., 2007, MULTIOBJECT TRACKING
   Vanhoucke V., 2011, NIPS
   Viola P., 2004, IJCV
   Wang N, 2014, ICML
   Wang N., 2013, NIPS
   Wang Q., 2012, TIP
   Wu Y., 2012, TIP
   Wu Yi, 2013, CVPR
   Yadan O., ARXIV13125853V4
   Yang F., 2012, IET IMAGE PROCESS
   Yao R., 2013, CVPR
   Yilmaz A, 2006, ACM COMPUT SURV, V38, DOI 10.1145/1177352.1177355
   Zhang Jie, 2014, ECCV
   Zhang K., 2012, ECCV
   Zhang L., 2014, TPAMI
   Zhang T., 2014, CVPR
   Zhang T., 2012, ECCV
   Zhang Z., 2014, CVPR
   Zhong B. N., 2014, VISUAL TRACKING VIA
NR 77
TC 42
Z9 47
U1 1
U2 58
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 1568-4946
EI 1872-9681
J9 APPL SOFT COMPUT
JI Appl. Soft. Comput.
PD JAN
PY 2016
VL 38
BP 1088
EP 1098
DI 10.1016/j.asoc.2015.06.048
PG 11
WC Computer Science, Artificial Intelligence; Computer Science,
   Interdisciplinary Applications
SC Computer Science
GA CZ0OQ
UT WOS:000366805900080
DA 2020-02-19
ER

PT B
AU Goodfellow, I
   Bengio, Y
   Courville, A
AF Goodfellow, I
   Bengio, Y.
   Courville, A.
BA Goodfellow, I
   Bengio, Y
   Courville, A
BF Goodfellow, I
   Bengio, Y
   Courville, A
TI Deep Learning
SO DEEP LEARNING
SE Adaptive Computation and Machine Learning
LA English
DT Article; Book Chapter
ID MULTILAYER FEEDFORWARD NETWORKS; ARTIFICIAL NEURAL-NETWORKS; SLOW
   FEATURE ANALYSIS; WAKE-SLEEP ALGORITHM; ENERGY-BASED MODELS; COMPONENT
   ANALYSIS; RECEPTIVE-FIELDS; BELIEF NETWORKS; DIMENSIONALITY REDUCTION;
   UNIVERSAL APPROXIMATION
CR ABADI M., 2015, TENSORFLOW LARGE SCA
   ACKLEY DH, 1985, COGNITIVE SCI, V9, P147
   Alain G., 2015, ARXIV150305571 GSNS
   Alain G., 2013, ICLR 2013
   Allen R. B., 1987, IEEE 1 INT C NEUR NE, V2, P335
   Anderson E, 1935, B AM IRIS SOC, V59, P2
   [Anonymous], 1908, BIOMETRIKA, V6, P1
   Ba J., 2014, MULTIPLE OBJECT RECO, V1412, P7755
   Bachman P., 2015, P 32 INT C MACH LEAR, P1964
   Bacon P.-L., 2015, 2 MULT C REINF LEARN
   Bahdanau D., 2015, ICLR 2015
   Bahl L. R., 1987, Computer Speech and Language, V2, P219, DOI 10.1016/0885-2308(87)90010-6
   Baldi P, 1999, BIOINFORMATICS, V15, P937, DOI 10.1093/bioinformatics/15.11.937
   Baldi P, 2014, NAT COMMUN, V5, DOI 10.1038/ncomms5308
   BALDI P, 1989, NEURAL NETWORKS, V2, P53, DOI 10.1016/0893-6080(89)90014-2
   Ballard D. H., 1983, NATURE
   Barlow HB, 1989, NEURAL COMPUT, V1, P295, DOI 10.1162/neco.1989.1.3.295
   BARRON AR, 1993, IEEE T INFORM THEORY, V39, P930, DOI 10.1109/18.256500
   Bartholomew D.J., 1987, LATENT VARIABLE MODE
   Basilevsky A, 1994, STAT FACTOR ANAL REL
   Bastien F., 2012, DEEP LEARN UNS FEAT
   Basu S., 2013, AAAI 2013
   Baxter J., 1995, Proceedings of the Eighth Annual Conference on Computational Learning Theory, P311, DOI 10.1145/225298.225336
   Bayer J., 2014, ARXIV E PRINTS
   BECKER S, 1992, NATURE, V355, P161, DOI 10.1038/355161a0
   Behnke S., 2001, International Journal of Computational Intelligence and Applications, V1, P427, DOI 10.1142/S1469026801000342
   Beiu V, 2003, IEEE T NEURAL NETWOR, V14, P1217, DOI 10.1109/TNN.2003.816365
   Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317
   BELKIN M, 2002, ADV NEURAL INFORM PR, V14
   Bengio E., 2015, ARXIV1511
   Bengio S, 2000, IEEE T NEURAL NETWOR, V11, P550, DOI 10.1109/72.846725
   Bengio Samy, 2015, ARXIV150603099
   Bengio Yoshua, 2013, Statistical Language and Speech Processing. First International Conference, SLSP 2013. Proceedings: LNCS 7978, P1, DOI 10.1007/978-3-642-39593-2_1
   Bengio Y, 2003, J MACH LEARN RES, V3, P1137, DOI 10.1162/153244303322533223
   Bengio Y, 2001, ADV NEUR IN, V13, P932
   Bengio Y, 2000, ADV NEUR IN, V12, P400
   BENGIO Y, 1993, 1993 IEEE INTERNATIONAL CONFERENCE ON NEURAL NETWORKS, VOLS 1-3, P1183, DOI 10.1109/ICNN.1993.298725
   BENGIO Y, 1992, ADV NEUR IN, V4, P175
   Bengio Y, 2000, NEURAL COMPUT, V12, P1889, DOI 10.1162/089976600300015187
   Bengio Y., 2013, ICML 2013
   Bengio Y., 2009, ICML 09
   Bengio Y., 2009, LEARNING DEEP ARCHIT
   Bengio Y., 2006, ADV NEURAL INFORM PR, P123
   Bengio Y., 2014, ICML 2014
   Bengio Y., 2015, ARXIV151002777 U MON
   Bengio Y., 2013, ARXIV13083432
   BENGIO Y, 2004, ADV NEURAL INFORM PR, V16
   BENGIO Y, 2005, ADV NEURAL INFORM PR, P129
   Bengio Y., 2003, P AISTATS 2003
   Bengio Y., 1999, LEARN C SNOWB
   Bengio Y., 2007, NIPS 2006
   Bengio Y., 2006, NIPS 2005
   Bengio Y., 1994, IEEE T NEURAL NETS
   Bengio Y., 2013, NIPS 2013
   Bengio Y., 2007, LARGE SCALE KERNEL M
   Bengio Y., 1991, P EUROSPEECH 91
   Bengio Y., 2002, 1215 U MONTR DEP IRO
   BENGIO Y, 1991, THESIS MCGILL U MONT
   Bengio Y, 2008, IEEE T NEURAL NETWOR, V19, P713, DOI 10.1109/TNN.2007.912312
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bengio Y, 2009, NEURAL COMPUT, V21, P1601, DOI 10.1162/neco.2008.11-07-647
   BENNETT CH, 1976, J COMPUT PHYS, V22, P245, DOI 10.1016/0021-9991(76)90078-4
   Bennett James, 2007, THE NETFLIX PRIZE
   Berger AL, 1996, COMPUT LINGUIST, V22, P39
   Berglund M., 2013, ABS13126002 CORR
   Bergstra J., 2011, THESIS U MONTREAL
   Bergstra J., 2009, NIPS 2009
   Bergstra J., 2010, P SCIPY
   Bergstra J, 2012, J MACH LEARN RES, V13, P281
   Bergstra James, 2011, NIPS 2011
   Berkes P, 2005, J VISION, V5, P579, DOI 10.1167/5.6.9
   BERTSEKAS DP, 1996, NEURO DYNAMIC PROGRA
   BESAG J, 1975, STATISTICIAN, V24, P179, DOI 10.2307/2987782
   Bishop C. M., 1995, ICANN '95. International Conference on Artificial Neural Networks. Neuronimes '95 Scientific Conference, P141
   Bishop C.M., 1994, MIXTURE DENSITY NETW
   BISHOP CM, 1995, NEURAL COMPUT, V7, P108, DOI 10.1162/neco.1995.7.1.108
   Bishop CM, 2006, PATTERN RECOGNITION
   Blum A. L., 1992, TRAINING 3 NODE NEUR
   BLUMER A, 1989, J ACM, V36, P929, DOI 10.1145/76359.76371
   BONNET G, 1964, ANNALES TELECOMMUN, V19, P203
   Bordes A., 2013, ADV NEURAL INFORM PR, P2787
   Bordes A., 2012, AISTATS 2012
   Bordes Antoine, 2011, AAAI 2011
   Bordes Antoine, 2013, MACHINE LEARNING SPE
   Bornschein J., 2015, ARXIV150603877
   Bornschein J., 2015, ICLR 2015
   Boser B. E., 1992, Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, P144, DOI 10.1145/130385.130401
   Bottou L., 2011, ARXIV11021808
   Bottou L., 2015, MULTILAYER NEURAL NE
   Bottou L., 1998, ONLINE LEARNING NEUR
   Bottou L., 2008, NIPS 2008
   Boulanger-Lewandowski N., 2012, ICML 12
   Boureau Y., 2010, P INT C MACH
   Boureau Y., 2011, INT C COMPUTER LEARN
   BOURLARD H, 1988, BIOL CYBERN, V59, P291, DOI 10.1007/BF00332918
   Bourlard H., 1989, Computer Speech and Language, V3, P1, DOI 10.1016/0885-2308(89)90011-9
   Boyd S., 2004, CONVEX OPTIMIZATION
   Bradley D., 2009, ADV NEURAL INFORM PR, P113
   BRADY ML, 1989, IEEE T CIRCUITS SYST, V36, P665, DOI 10.1109/31.31314
   Brakel P, 2013, J MACH LEARN RES, V14, P2771
   Brand M., 2003, ADV NEURAL INFORM PR, P961, DOI DOI 10.1109/34.682189
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Breiman L, 1984, CLASSIFICATION REGRE
   BRIDLE JS, 1990, SPEECH COMMUN, V9, P83, DOI 10.1016/0167-6393(90)90049-F
   Brown P. F., 1992, Computational Linguistics, V18, P467
   Brown P. F., 1990, Computational Linguistics, V16, P79
   Bryson A. E., 1969, APPL OPTIMAL CONTROL
   Bryson Jr A. E., 1961, BR1303 RAYTH CO MISS
   Bucilu C., 2006, P 12 ACM SIGKDD INT, P535, DOI DOI 10.1145/1150402.1150464
   Burda Y., 2015, ARXIV150900519
   Cai M, 2013, 2013 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING (ASRU), P291, DOI 10.1109/ASRU.2013.6707745
   Caruana RA, 1993, P 1993 CONN MOD SUMM, P372
   Cauchy A., 1847, CR HEBD ACAD SCI, P536
   Cayton L., 2005, CS20080923 UCSD
   Chandola V, 2009, ACM COMPUT SURV, V41, DOI 10.1145/1541880.1541882
   Chapelle O., 2006, SEMISUPERVISED LEARN
   Chapelle O., 2003, ADV NEURAL INFORM PR, V15, P585
   Chellapilla Kumar, 2006, 10 INT WORKSH FRONT
   Chen B., 2010, NIPS 2010 DEEP FEAT
   Chen SF, 1999, COMPUT SPEECH LANG, V13, P359, DOI 10.1006/csla.1999.0128
   Chen Tianqi, 2015, ARXIV151201274
   Chen TS, 2014, ACM SIGPLAN NOTICES, V49, P269, DOI 10.1145/2541940.2541967
   Chen YJ, 2014, INT SYMP MICROARCH, P609, DOI 10.1109/MICRO.2014.58
   Chilimbi Trishul, 2014, 11 USENIX S OP SYST
   Cho K., 2011, P 28 INT C MACH LEAR, P105
   Cho K, 2014, P EMP METH NAT LANG
   Cho K., 2014, ABS14091259 ARXIV E
   Choromanska A, 2014, LOSS SURFACE MULTILA
   Chorowski Jan, 2014, ARXIV14121602
   Chrisman L., 1991, Connection Science, V3, P345, DOI 10.1080/09540099108946592
   CHRISTIANSON B, 1992, IMA J NUMER ANAL, V12, P135, DOI 10.1093/imanum/12.2.135
   Chrupala G., 2015, ARXIV150603694
   Chung J., 2015, NIPS 2015
   Chung J., 2014, CORR, DOI DOI 10.1109/IJCNN.2015.7280624
   Chung J., 2015, ICML 15
   Ciresan D, 2012, NEURAL NETWORKS, V32, P333, DOI 10.1016/j.neunet.2012.02.023
   Ciresan DC, 2010, NEURAL COMPUT, V22, P3207, DOI 10.1162/NECO_a_00052
   Coates A., 2013, P 30 INT C MACH LEAR, p1337~1345
   Coates A., 2011, P 13 INT C ART INT S
   Coates A., 2011, ICML 2011
   Cohen N., 2015, ARXIV150905009
   Collobert R, 2002, NEURAL COMPUT, V14, P1105, DOI 10.1162/089976602753633402
   Collobert R., 2011, BIGL NIPS WORKSH
   Collobert R., 2001, IDIAPRR0112
   Collobert R., 2008, ICML 2008
   Collobert R., 2011, AISTATS 2011
   Collobert R, 2011, J MACH LEARN RES, V12, P2493
   Collobert Ronan, 2004, LARGE SCALE MACHINE
   COMON P, 1994, SIGNAL PROCESS, V36, P287, DOI 10.1016/0165-1684(94)90029-9
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411
   Couprie C., 2013, INT C LEARN REPR ICL
   Courbariaux Matthieu, 2015, ICLR 2015 WORKSH
   Courville A., 2011, ICML 11
   Courville A, 2014, IEEE T PATTERN ANAL, V36, P1874, DOI 10.1109/TPAMI.2013.238
   Cover T., 2006, ELEMENTS INFORM THEO
   Cox D, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P8, DOI 10.1109/FG.2011.5771385
   Cramer H, 1946, MATH METHODS STAT
   CRICK F, 1983, NATURE, V304, P111, DOI 10.1038/304111a0
   Cybenko G., 1989, Mathematics of Control, Signals, and Systems, V2, P303, DOI 10.1007/BF02551274
   Dahl G. E., 2010, NIPS 2010
   Dahl G. E., 2013, ICASSP 2013
   Dahl GE, 2014, ARXIV14061231
   Dahl GE, 2012, IEEE T AUDIO SPEECH, V20, P30, DOI 10.1109/TASL.2011.2134090
   Dauphin Y., 2013, NIPS26 NIPS FDN
   Dauphin Y., 2011, ICML 2011
   Dauphin Yann, 2014, NIPS 2014
   Davis A, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601119
   DAYAN P, 1995, NEURAL COMPUT, V7, P889, DOI 10.1162/neco.1995.7.5.889
   Dayan P, 1996, NEURAL NETWORKS, V9, P1385, DOI 10.1016/S0893-6080(96)00009-3
   Dayan P., 1990, P 1990 CONN SUMM SCH
   Dean J., 2012, NIPS 2012
   Dean T., 1989, Computational Intelligence, V5, P142, DOI 10.1111/j.1467-8640.1989.tb00324.x
   DEERWESTER S, 1990, J AM SOC INFORM SCI, V41, P391, DOI 10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9
   Del Giudice M, 2009, DEVELOPMENTAL SCI, V12, P350, DOI 10.1111/j.1467-7687.2008.00783.x
   Delalleau O, 2011, NIPS
   Deng J., 2009, CVPR09
   Deng J, 2010, LECT NOTES COMPUT SC, V6315, P71, DOI 10.1007/978-3-642-15555-0_6
   Deng L., 2014, FDN TRENDS SIGNAL PR
   Deng L., 2010, INT 2010 MAK CHIB JA
   Denil M, 2012, NEURAL COMPUT, V24, P2151, DOI 10.1162/NECO_a_00312
   Denton E. L., 2015, DEEP GENERATIVE IMAG
   Desjardins G., 2008, 1327 U MONTR DEP INF
   Desjardins G., 2015, ADV NEURAL INFORM PR, P2062
   Desjardins G., 2010, P 13 INT C ART INT S, V9, P145
   Desjardins G., 2011, NIPS 2011
   Devlin J., 2014, P ACL 2014
   Devroye L., 2013, NONUNIFORM RANDOM VA
   DiCarlo J. J., 2013, MECHANISMS UNDERLYIN
   Dinh L., 2014, ARXIV14108516
   Donahue Jeff, 2014, ARXIV14114389
   Donoho D. L., 2003, 200308 STANF U DEP S
   Dosovitskiy A, 2015, PROC CVPR IEEE, P1538, DOI 10.1109/CVPR.2015.7298761
   Doya K, 1993, IEEE T NEURAL NETWOR, V1, P75
   DREYFUS SE, 1973, IEEE T AUTOMAT CONTR, VAC18, P383, DOI 10.1109/TAC.1973.1100330
   Dreyfus SE, 1962, J MATH ANAL APPL, V5, P30, DOI [10.1016/0022-247X(62)90004-5, DOI 10.1016/0022-247X(62)90004-5]
   DRUCKER H, 1992, IEEE T NEURAL NETWOR, V3, P991, DOI 10.1109/72.165600
   Duchi John, 2011, J MACHINE LEARNING R
   Dudik M., 2011, ADV NEURAL INFORM PR
   Dziugaite G. K., 2015, ARXIV150503906
   El Hihi S., 1996, NIPS 1995
   Elkahky A, 2015, PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW 2015), P278, DOI 10.1145/2736277.2741667
   ELMAN JL, 1993, COGNITION, V48, P71, DOI 10.1016/0010-0277(93)90058-4
   Erhan D., 2009, P AISTATS 2009
   Erhan D, 2010, J MACHINE LEARNING R
   Fahlman S. E., 1983, P NATL C ART INT AAA
   Fang H., 2015, ARXIV14114952
   Farabet C., 2011, SCALING MACHINE LEAR
   Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231
   Finn C, 2015, ARXIV150906113
   Fisher RA, 1936, ANN EUGENIC, V7, P179, DOI 10.1111/j.1469-1809.1936.tb02137.x
   Foldiak P., 1989, IJCNN: International Joint Conference on Neural Networks (Cat. No.89CH2765-6), P401, DOI 10.1109/IJCNN.1989.118615
   Forcada ML, 1997, LECT NOTES COMPUT SC, V1240, P453
   Franzius M, 2008, LECT NOTES COMPUT SC, V5163, P961, DOI 10.1007/978-3-540-87536-9_98
   Frasconi P, 1998, IEEE T NEURAL NETWOR, V9, P768, DOI 10.1109/72.712151
   Frasconi P., 1997, P INT JOINT C ART IN
   Freund Y., 1996, Machine Learning. Proceedings of the Thirteenth International Conference (ICML '96), P148
   Freund Y., 1996, Proceedings of the Ninth Annual Conference on Computational Learning Theory, P325, DOI 10.1145/238061.238163
   Frey BJ, 1996, ADV NEUR IN, V8, P661
   FREY BJ, 1998, ADAP COMP MACH LEARN, P1
   Frobenius G., 1908, B PREUSS AKAD WISS B
   FUKUSHIMA K, 1975, BIOL CYBERN, V20, P121, DOI 10.1007/BF00342633
   FUKUSHIMA K, 1980, BIOL CYBERN, V36, P193, DOI 10.1007/BF00344251
   Gal Y., 2015, ARXIV150602158
   Gallinari P., 1987, P COGNITIVA 87 PAR L
   Garcia-Duran A., 2015, ARXIV150600999
   Garofolo J. S., 1993, DARPA TIMIT ACOUSTIC, V93, P27403, DOI DOI 10.6028/NIST.IR.4930
   Garson J., 1900, J ANTHR I GREAT BRIT, P177
   Gers FA, 2000, NEURAL COMPUT, V12, P2451, DOI 10.1162/089976600300015015
   Ghahramani Z., 1996, CRGT961 U TOR DEP CO
   Gillick Dan, 2015, ARXIV151200103
   Girshick R, 2015, REGION BASED CONVOLU
   Glorot X., 2011, AISTATS 2011
   Glorot X, 2010, AISTATS 2010
   Glorot X, 2011, ICML 2011
   Goldberger J., 2005, ADV NEURAL INFORM PR
   Gong S., 2000, DYNAMIC VISION FROM
   Goodfellow I., 2009, ADV NEURAL INFORM PR, V22, P646
   Goodfellow I., 2010, P HUM ROB INT HRI OS
   Goodfellow I. J., 2011, NIPS WORKSH CHALL LE
   Goodfellow I. J., 2013, NIPS26
   Goodfellow I. J., 2014, ABS14126572 CORR
   Goodfellow I. J., 2013, JMLR W CP, P1319
   Goodfellow I. J., 2010, TECHNICAL REPORT MUL
   Goodfellow I. J., 2014, ICLR 2014
   Goodfellow I. J., 2014, NIPS 2014
   Goodfellow I. J., 2013, ARXIV13084214
   GOODFELLOW I. J., 2015, INT C LEARN REPR
   Goodfellow I. J., 2014, INT C LEAR REPR
   Goodfellow IJ, 2014, ADV NEUR IN, V27
   Goodfellow IJ, 2013, IEEE T PATTERN ANAL, V35, P1902, DOI 10.1109/TPAMI.2012.273
   Goodman J., 2001, INT C AC
   GORI M, 1992, IEEE T PATTERN ANAL, V14, P76, DOI 10.1109/34.107014
   Gouws Stephan, 2014, ARXIV14102455
   GRAF HP, 1989, IEEE CIRCUIT DEVIC, V5, P44, DOI 10.1109/101.29902
   Graves A, 2005, NEURAL NETWORKS, V18, P602, DOI 10.1016/j.neunet.2005.06.042
   Graves A., 2013, ARXIV13080850
   Graves A, 2012, STUDIES COMPUTATIONA
   Graves A., 2008, ADV NEURAL INFORM PR, P577
   Graves A., 2011, NIPS 2011
   Graves A., 2014, ICML 2014
   Graves A., 2009, ADV NEURAL INFORM PR, V21, P545
   Graves A., 2006, P 23 INT C MACH LEAR, P369, DOI DOI 10.1145/1143844.1143891
   Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947
   Graves A, 2009, IEEE T PATTERN ANAL, V31, P855, DOI 10.1109/TPAMI.2008.137
   Graves Alex, 2014, ARXIV14105401
   Grefenstette E., 2015, NIPS2015
   Greff K., 2015, ARXIV150304069
   Gregor K., 2010, P 27 2NT C MACH LEAR
   Gregor K., 2010, ARXIV10060448
   Gregor K, 2015, ARXIV150204623
   Gregor K., 2014, INT C MACH LEARN ICM
   Gretton A, 2012, J MACH LEARN RES, V13, P723
   Gulcehre C., 2013, INT C LEARN REPR ICL
   GUO H, 1992, IEEE T NEURAL NETWOR, V3, P923, DOI 10.1109/72.165594
   Gupta Suyog, 2015, ABS150202551 CORR
   Gutmann M., 2010, P 13 INT C ART INT S
   Hadsell R., 2007, P ROB SCI SYST ATL G
   HAJNAL A, 1993, J COMPUT SYST SCI, V46, P129, DOI 10.1016/0022-0000(93)90001-D
   Hastad J., 1991, Computational Complexity, V1, P113, DOI 10.1007/BF01272517
   HASTAD J, 1986, P 18 ANN ACM S THEOR, P6, DOI DOI 10.1145/12130.12132
   Hastie T, 2001, ELEMENTS STAT LEARNI
   He K., 2015, ARXIV150201852
   Hebb D.O., 1949, ORG BEHAV
   Henaff M., 2011, ISMIR 11
   Henderson J, 2003, HLT-NAACL 2003: HUMAN LANGUAGE TECHNOLOGY CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE MAIN CONFERENCE, P103
   Henderson James, 2004, P 42 M ASS COMP LING, P95
   Henniges M, 2010, LECT NOTES COMPUT SC, V6365, P450, DOI 10.1007/978-3-642-15995-4_56
   HERAULT J, 1984, CR ACAD SCI III-VIE, V299, P525
   Hinton G, 2005, AISTATS, V10, P33
   Hinton G., 2012, NEURAL NETWORKS MACH
   Hinton G., 2015, ARXIV150302531
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton G. E., 1995, Advances in Neural Information Processing Systems 7, P1015
   Hinton G. E, 2012, ARXIV12070580
   Hinton G. E., 2006, 2006003 UTML TR U TO
   Hinton G. E., 2014, INV TALK BAYL BAY AR
   Hinton G.E., 1986, PARALLEL DISTRIBUTED, V1, P282, DOI DOI 10.1234/12345678
   Hinton G. E., 2010, 2010003 UTML U TOR D
   Hinton G. E., 2000, 2000004 GCNU TR U CO
   Hinton G. E., 1994, NIPS 1993
   Hinton G. E., 2007, NIPS 2007 DEEP LEARN
   Hinton G. E., 2001, P 3 INT C IND COMP A, P746
   Hinton G. E., 1984, TCMUCS84119 CARN MEL
   Hinton G. E., 1999, ICANN 1999
   Hinton G. E., 1997, PHILOS T ROYAL SOC L
   HINTON GE, 1990, ARTIF INTELL, V46, P47, DOI 10.1016/0004-3702(90)90004-J
   Hinton GE, 1997, IEEE T NEURAL NETWOR, V8, P65, DOI 10.1109/72.554192
   HINTON GE, 1995, SCIENCE, V268, P1158, DOI 10.1126/science.7761831
   HINTON GE, 1989, ARTIF INTELL, V40, P185, DOI 10.1016/0004-3702(89)90049-0
   HINTON GE, 1991, PSYCHOL REV, V98, P74, DOI 10.1037/0033-295X.98.1.74
   Hinton GE, 2003, NIPS 2002
   HINTON GE, 1986, PARALLEL DISTRIBUTED, V1, P77
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Hinton GE, 2007, TRENDS COGN SCI, V11, P428, DOI 10.1016/j.tics.2007.09.004
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hinton Geoffrey E, 1988, NEURAL INFORMATION P, P358
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Hochreiter S., 1995, Advances in Neural Information Processing Systems 7, P529
   Hochreiter S., 1991, UNTERSUCHUNGEN DYNAM
   Hochreiter S, 2001, FIELD GUIDE DYNAMICA
   Holt J. L., 1991, IJCNN-91-Seattle: International Joint Conference on Neural Networks (Cat. No.91CH3049-4), P121, DOI 10.1109/IJCNN.1991.155324
   HOLT JL, 1993, IEEE T COMPUT, V42, P281, DOI 10.1109/12.210171
   HORNIK K, 1990, NEURAL NETWORKS, V3, P551, DOI 10.1016/0893-6080(90)90005-6
   HORNIK K, 1989, NEURAL NETWORKS, V2, P359, DOI 10.1016/0893-6080(89)90020-8
   Hsu F.-H., 2002, BEHIND DEEP BLUE COM
   Huang F, 2002, ANN I STAT MATH, V54, P1, DOI 10.1023/A:1016170102988
   Huang P.-S., 2013, P 22 ACM INT C INF K, V13, P2333, DOI [DOI 10.1145/2505515.2505665, 10.1145/2505515.2505665]
   HUBEL DH, 1968, J PHYSIOL-LONDON, V195, P215, DOI 10.1113/jphysiol.1968.sp008455
   HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837
   HUBEL DH, 1959, J PHYSIOL-LONDON, V148, P574, DOI 10.1113/jphysiol.1959.sp006308
   Huszar F., 2015, ARXIV151105101
   Hutter F., 2011, LION 5
   Hyotyniemi H., 1996, STeP '96 - Genes, Nets and Symbols. Finnish Artificial Intelligence Conference, P13
   Hyvarinen A, 2005, J MACH LEARN RES, V6, P695
   Hyvarinen A, 1999, NEURAL NETWORKS, V12, P429, DOI 10.1016/S0893-6080(98)00140-3
   Hyvarinen A, 2001, NEURAL COMPUT, V13, P1527, DOI 10.1162/089976601750264992
   Hyvarinen A., 1999, NIPS, P827
   Hyvarinen A., 1999, NEURAL COMPUTING SUR, V2, P94
   Hyvarinen A, 2007, COMPUT STAT DATA AN, V51, P2499, DOI 10.1016/j.csda.2006.09.003
   Hyvarinen A, 2007, IEEE T NEURAL NETWOR, V18, P1529, DOI 10.1109/TNN.2007.895819
   Hyvrinen A., 2009, NATURAL IMAGE STAT P
   HYVRINEN A, 2001, INDEPENDENT COMPONEN
   Iba Y, 2001, INT J MOD PHYS C, V12, P623, DOI 10.1142/S0129183101001912
   Inayoshi H, 2005, 2005 IEEE Workshop on Machine Learning for Signal Processing (MLSP), P141, DOI 10.1109/MLSP.2005.1532889
   Ioffe S, 2015, BATCH NORMALIZATION
   JACOBS RA, 1988, NEURAL NETWORKS, V1, P295, DOI 10.1016/0893-6080(88)90003-2
   Jacobs RA, 1991, NEURAL COMPUT, V3, P79, DOI 10.1162/neco.1991.3.1.79
   Jaeger H, 2004, SCIENCE, V304, P78, DOI 10.1126/science.1091277
   Jaeger H., 2007, DISCOVERING MULTISCA
   Jaeger H, 2007, SCHOLARPEDIA, V2, P2330, DOI DOI 10.4249/SCH0LARPEDIA.2330
   Jaeger H., 2003, ADV NEUR INF P SYST, V15
   Jaegera H, 2007, NEURAL NETWORKS, V20, P335, DOI 10.1016/j.neunet.2007.04.016
   Jaitly N., 2013, ICML
   Jaitly N, 2011, INT CONF ACOUST SPEE, P5884
   Jarrett Kevin, 2009, ICCV 09
   Jarzynski C, 1997, PHYS REV LETT, V78, P2690, DOI 10.1103/PhysRevLett.78.2690
   Jaynes E. T., 2003, PROBABILITY THEORY L
   Jean S., 2014, ARXIV14122007
   Jelinek F, 1980, PATTERN RECOGNITION
   Jia Y. Q. C., 2013, OPEN SOURCE CONVOLUT
   Jia YQ, 2012, PROC CVPR IEEE, P3370, DOI 10.1109/CVPR.2012.6248076
   Jim KC, 1996, IEEE T NEURAL NETWOR, V7, P1424, DOI 10.1109/72.548170
   Jordan M. I., 1998, ARXIV150301007
   Jozefowicz Rafal, 2015, ICML
   Judd J. S., 1989, NEURAL NETWORK DESIG
   JUTTEN C, 1991, SIGNAL PROCESS, V24, P1, DOI 10.1016/0165-1684(91)90079-X
   Kahou S., 2013, P 15 ACM INT C MULT
   Kalchbrenner N, 2015, ARXIV150701526
   Kalchbrenner Nal, 2013, EMNLP
   Kamyshanska H., 2015, IEEE T PATTERN ANAL
   Karpathy A., 2015, ARXIV14122306
   Karpathy Andrej, 2014, CVPR
   Karush W., 1939, THESIS
   KATZ SM, 1987, IEEE T ACOUST SPEECH, V35, P400, DOI 10.1109/TASSP.1987.1165125
   Kavukcuoglu K., 2009, LEARNING INVARIANT F
   Kavukcuoglu K., 2008, CBLLT20081201
   Kavukcuoglu  K., 2010, NIPS
   KELLEY HJ, 1960, ARSJ-AM ROCKET SOC J, V30, P947, DOI 10.2514/8.5282
   Khan F, 2011, ADV NEURAL INFORM PR, V24, P1449
   Kim SK, 2009, I C FIELD PROG LOGIC, P367, DOI 10.1109/FPL.2009.5272262
   KINDERMANN R, 1980, CONT MATH, V1
   Kingma D., 2014, 14126980 ARXIV, DOI DOI 10.1109/ICCE.2017.7889386
   Kingma D., 2010, NIPS
   Kingma D. P., 2014, ARXIV14020480
   Kingma D. P., 2013, ARXIV13060733
   Kingma D. P., 2014, NIPS
   Kingma Diederik P, 2014, P INT C LEARN REPR I
   KIRKPATRICK S, 1983, SCIENCE, V220, P671, DOI 10.1126/science.220.4598.671
   Kiros R., 2014, ICML
   Kiros Ryan, 2014, ARXIV14112539
   Klementiev A., 2012, P COLING 2012
   Knowles- Barley S., 2014, GPU TECHN C
   Kocisky Tomas, 2014, P ACL
   Koller D., 2009, PROBABILISTIC GRAPHI
   Konig Y., 1996, ADV NEURAL INFORM PR
   Koren Y., 2009, BELLKOR SOLUTION NET, P81
   Kotzias D., 2015, ACM SIGKDD
   Koutnik J., 2014, ICML
   Krause O., 2013, ICML
   Krizhevsky A., 2011, ESANN
   Krizhevsky A, 2009, LEARNING MULTIPLE LA
   Krizhevsky A., 2012, NIPS
   Krizhevsky A, 2010, CIFAR 10
   Krueger KA, 2009, COGNITION, V110, P380, DOI 10.1016/j.cognition.2008.11.014
   Kuhn H. W., 1951, P 2 BERK S MATH STAT, P481, DOI DOI 10.1007/BF01582292
   Kumar A., 2015, ARXIV150607285
   Kumar M. P., 2010, NIPS
   Lang K. J., 1988, CMUCS88152
   LANG KJ, 1990, NEURAL NETWORKS, V3, P23, DOI 10.1016/0893-6080(90)90044-L
   Langford John, 2008, NIPS, V20, P1096
   Lappalainen H., 2000, P ICA CIT
   Larochelle H., 2008, AAAI C ART INT
   Larochelle H., 2010, P ADV NEUR INF PROC, P1243
   Larochelle H., 2008, ICML
   Larochelle H, 2009, J MACH LEARN RES, V10, P1
   Lasserre J., 2006, IEEE COMPUTER SOCIET, P87, DOI DOI 10.1109/CVPR.2006.227
   Le Cun Yann, 1986, DISORDERED SYSTEMS B, P233
   Le Q., 2011, P ICML
   Le Q. V., 2012, ICML
   Le Q.V., 2010, ADV NEURAL INFORM PR, V23, P1279
   Le Roux N, 2008, NEURAL COMPUT, V20, P1631, DOI 10.1162/neco.2008.04-07-510
   Le Roux N, 2010, NEURAL COMPUT, V22, P2192, DOI 10.1162/neco.2010.08-09-1081
   LECUN Y, 1989, IEEE COMMUN MAG, V27, P41, DOI 10.1109/35.41400
   LeCun Y., 1989, CRGT894
   LeCun Y., 1998, LECT NOTES COMPUTER, V1524
   LeCun Y., 1987, THESIS U PAR
   LeCun Y., 1985, P COGNITIVA, V85, P599
   LeCun Y, 2010, IEEE INT SYMP CIRC S, P253, DOI 10.1109/ISCAS.2010.5537907
   Lecun Yann, 1998, P IEEE
   LECUYER P, 1994, 1994 WINTER SIMULATION CONFERENCE PROCEEDINGS, P122
   Lee C.-Y., 2014, ARXIV14095185
   Lee H., 2008, NIPS 07
   Lee H., 2009, 26 INT C MACH LEAR I
   Lee H., 2007, ADV NEURAL INF PROCE, P801
   Lee Y. J., 2011, CVPR
   Leibniz G. W., 1676, MEMOIR USING CHAIN R, V7, P321
   Lenat D. B., 1989, BUILDING LARGE KNOWL
   LESHNO M, 1993, NEURAL NETWORKS, V6, P861, DOI 10.1016/S0893-6080(05)80131-5
   Levenberg K., 1944, Quarterly of Applied Mathematics, V2, P164
   LHOPITAL GF, 1696, ANALYSE INFINIMENT P
   Li FF, 2006, IEEE T PATTERN ANAL, V28, P594, DOI 10.1109/TPAMI.2006.79
   Li Y., 2015, CORR ABS150202761
   Lin TN, 1996, IEEE T NEURAL NETWOR, V7, P1329, DOI 10.1109/72.548162
   Lin Y., 2015, P AAAI 15
   Linde Nancy, 1992, MACHINE CHANGED WORL
   Lindsey C., 1994, P 3 WORKSH NEUR NETW, V3, P195
   Linnainmaa S., 1976, BIT (Nordisk Tidskrift for Informationsbehandling), V16, P146, DOI 10.1007/BF01931367
   LISA, 2008, DEEP LEARN TUT REST
   Long P. M, 2010, P 27 INT C MACH LEAR
   Lotter W., 2015, ARXIV151106380
   Lovelace A., 1842, NOTES
   Lu L., 2015, P INT
   Lu T., 2010, P 13 INT C ART INT S, P485
   Luenberger D. G., 1984, LINEAR NONLINEAR PRO
   Lukosevicius M, 2009, COMPUT SCI REV, V3, P127, DOI 10.1016/j.cosrev.2009.03.005
   Luo H., 2013, AISTATS
   Luo H., 2011, INT C ART INT STAT, P470
   Lyu S., 2009, P 25 C UNC ART INT
   Ma J., 2015, J CHEM INFORM MODELI
   Maas AL, 2013, ICML WORKSH DEEP LEA
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Maass W., 1992, P 25 ACM S THEORY CO, P335
   Maass Wolfgang, 1994, NEURAL COMPUTATION L, P127
   MacKay D. J. C., 2003, INFORM THEORY INFERE
   Maclaurin Dougal, 2015, ARXIV150203492
   Mao J., 2015, ARXIV14101090
   Marcotte P., 1992, ZOR, Methods and Models of Operations Research, V36, P517, DOI 10.1007/BF01416243
   Marlin B, 2011, UAI
   Marlin B. M., 2010, P 13 INT C ART INT S, P509
   MARQUARDT DW, 1963, J SOC IND APPL MATH, V11, P431, DOI 10.1137/0111030
   Marr D., 1976, SCI, V194
   Martens J., 2011, LEARNING RECURRENT N
   Martens J., 2010, P 27 INT C MACH LEAR, P735, DOI DOI 10.1155/2011/176802
   Martens James, 2014, ARXIV14117717
   Mase S, 1995, ANN APPL PROBAB, V5, P603, DOI 10.1214/aoap/1177004697
   McClelland J., 1995, COMPUTATION INTELLIG, P305
   McCulloch W. S., 1943, B MATH BIOPHYS, V5, P115, DOI DOI 10.1007/BF02478259
   Mead C., 2012, ANALOG VLSI IMPLEMEN, V80
   Melchior J, 2013, ARXIV13111354
   Memisevic R., 2007, P COMP VIS PATT COMP
   Memisevic R, 2010, NEURAL COMPUT, V22, P1473, DOI 10.1162/neco.2010.01-09-953
   Mesnil G., 2012, LEARN WORKSH SNOWB
   Mesnil G., 2011, P U TRANSF LEAR, V7
   MIIKKULAINEN R, 1991, COGNITIVE SCI, V15, P343, DOI 10.1207/s15516709cog1503_2
   Mikolov T., 2011, P 12 ANN C INT SPEEC
   Mikolov T., 2013, ARXIV13094168
   Mikolov T., 2011, P ASRU 2011
   Mikolov T., 2013, INT C LEARN REP WORK
   Mikolov T A, 2012, STAT LANGUAGE MODELS
   Minka T., 2005, MSRTR2005173
   Minsky M, 1969, PERCEPTRONS
   Mirza M, 2014, ARXIV14111784
   Mishkin D., 2015, ARXIV151106422
   Misra J, 2010, NEUROCOMPUTING, V74, P239, DOI 10.1016/j.neucom.2010.03.021
   Murray I., 2011, AISTATS
   Ovid, 2004, METAMORPHOSES
   TERRENCE J, 1999, UNSUPERVISED LEARNIN
   Turaga S., 2009, ADV NEURAL INFORM PR, V22, P1865
   Wang S, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON NANOSCALE ARCHITECTURE, P1
   Wikipedia, 2015, LIST ANIMALS NUMBER
   Franzius M., 2007, SLOWNESS SPARSENESS
   Metz Luke, 2015, ARXIV151106434
   Mitchell T., 1997, MACHINE LEARNING
   Miyato T., 2015, ARXIV150700677
   Mnih A., 2007, P 24 INT C MACH LEAR, P641, DOI DOI 10.1145/1273496.1273577
   Mnih A., 2009, ADV NEURAL INFORM PR, V21, P1081
   Mnih A., 2013, ADV NEURAL INFORM PR, P2265
   Mnih A., 2012, P 29 INT C MACH LEAR, P1751
   Mnih Andriy, 2014, NEURAL VARIATIONAL I
   Mnih  V., 2010, P 11 EUR C COMP VIS
   Mnih V., 2011, P C UNC ART INT
   Mnih V., 2014, ADV NEURAL INFORM PR, P2204, DOI DOI 10.1017/S037346330300239X
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Mnih Volodymyr, 2013, ARXIV13125602
   Mobahi H., 2015, AAAI 2015
   Mobahi H., 2009, P 26 ANN INT C MACH, P737
   Mohamed A, 2009, DEEP BELIEF NETWORKS
   Mohamed AR, 2012, INT CONF ACOUST SPEE, P4273, DOI 10.1109/ICASSP.2012.6288863
   Mohamed AR, 2011, INT CONF ACOUST SPEE, P5060
   Mohamed AR, 2012, IEEE T AUDIO SPEECH, V20, P14, DOI 10.1109/TASL.2011.2109382
   MOLLER MF, 1993, NEURAL NETWORKS, V6, P525, DOI 10.1016/S0893-6080(05)80056-5
   Montavon Gregoire, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P621, DOI 10.1007/978-3-642-35289-8_33
   Montufar G., 2014, NEURAL COMPUTATION, V26
   Montufar G. F., 2014, NIPS 2014
   Montufar G, 2011, NEURAL COMPUT, V23, P1306, DOI 10.1162/NECO_a_00113
   More JJ, 1997, SIAM J OPTIMIZ, V7, P814, DOI 10.1137/S1052623495283024
   Morin F., 2005, AISTATS 2005
   MORYOSEF S, 1990, OBSTET GYNECOL, V75, P944
   MOZER MC, 1992, ADV NEUR IN, V4, P275
   Murphy KP., 2012, MACHINE LEARNING PRO
   Murray B. U. I., 2014, DEEP TRACTABLE DENSI
   Nair V, 2009, ADV NEURAL INF PROCE, V22, P1339
   Nair V., 2010, RECTIFIED LINEAR UNI
   Narayanan H., 2010, AMPLE COMPLEXITY TES
   Naumann U, 2008, MATH PROGRAM, V112, P427, DOI 10.1007/s10107-006-0042-z
   Navigli R, 2005, IEEE T PATTERN ANAL, V27, P1075, DOI 10.1109/TPAMI.2005.149
   Neal R. M., 1993, CRGT931
   Neal R. M., 1994, 9421 U TOR DEPT STAT
   Neal R. M., 1999, LEARNING GRAPHICAL M
   Neal Radford M., 1996, BAYESIAN LEARNING NE
   Neal Radford M., 1990, LEARNING STOCHASTIC
   Neal RM, 2001, STAT COMPUT, V11, P125, DOI 10.1023/A:1008923215028
   NEAL RM, 2005, ESTIMATING RATIOS NO
   Nesterov Y., 1983, SOV MATH DOKL, V27, P372
   Nesterov Y., 2004, INTRODUCTORY LECT CO
   Netzer Y., 2011, DEEP LEARNING UNSUPE
   Ney H., 1993, P EUR C SPEECH COMM, P973
   Ng A., 2015, ADVICE APPLYING MACH
   Niesler TR, 1998, INT CONF ACOUST SPEE, P177, DOI 10.1109/ICASSP.1998.674396
   Ning F, 2005, IEEE T IMAGE PROCESS, V14, P1360, DOI 10.1109/TIP.2005.852470
   Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5
   Norouzi M, 2011, MINIMAL LOSS HASHING
   Nowlan S. J., 1990, CRGT905
   NOWLAN SJ, 1992, NEURAL COMPUT, V4, P473, DOI 10.1162/neco.1992.4.4.473
   Olshausen BA, 2005, NEURAL COMPUT, V17, P1665, DOI 10.1162/0899766054026639
   Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0
   OLSHAUSEN BA, 1993, J NEUROSCI, V13, P4700
   Opper M, 2009, NEURAL COMPUT, V21, P786, DOI 10.1162/neco.2008.08-07-592
   Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222
   OSINDERO S, 2008, [No title captured], V20, P1121
   Paccanaro A., 2000, INT JOINT C NEUR NET
   Paine T. L., 2014, ARXIV14126597
   Palatucci M., 2009, ADV NEURAL INFORM PR, P1410, DOI DOI 10.1007/116694874
   Parker D. B., 1985, TR 47
   Pascanu Razvan, 2013, ICML
   Pascanu Razvan, 2014, ICLR
   Pati YC, 1993, SIGN SYST COMP 1993, P40, DOI DOI 10.1109/ACSSC.1993.342465
   Pearl J, 1988, PROBABILISTIC REASON
   Pearl J., 1985, P C COGNITIVE SCI SO, P329
   Perron O, 1907, MATH ANN, V64, P248, DOI 10.1007/BF01449896
   Petersen K. B., 2006, MATRIX COOKBOOK VISI
   Peterson GB, 2004, J EXP ANAL BEHAV, V82, P317, DOI 10.1901/jeab.2004.82-317
   Pham D. T., 1992, Signal Processing VI - Theories and Applications. Proceedings of EUSIPCO-92, Sixth European Signal Processing Conference, P771
   Pham PH, 2012, MIDWEST SYMP CIRCUIT, P1044, DOI 10.1109/MWSCAS.2012.6292202
   Pinheiro P. H. O., 2014, RECURRENT CONVOLUTIO
   Pinheiro P. H. O., 2015, C COMP VIS PATT REC
   Pinto N., 2011, 2011 IEEE COMP SOC C, P35, DOI DOI 10.1109/CVPRW.2011.5981788
   Pinto N, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.0040027
   POLLACK JB, 1990, ARTIF INTELL, V46, P77, DOI 10.1016/0004-3702(90)90005-K
   Polyak B., 1964, USSR COMP MATH MATH, V4, P1, DOI DOI 10.1016/0041-5553(64)90137-5
   POLYAK BT, 1992, SIAM J CONTROL OPTIM, V30, P838, DOI 10.1137/0330046
   Poole B., 2014, CORRABS14061831
   Poon H., 2011, P 27 C UNC ART INT
   PRESLEY RK, 1994, SOUTHEASTCON '94 - CREATIVE TECHNOLOGY TRANSFER - A GLOBAL AFFAIR, P136, DOI 10.1109/SECON.1994.324283
   PRICE R, 1958, IRE T INFORM THEOR, V4, P69, DOI 10.1109/TIT.1958.1057444
   Quiroga RQ, 2005, NATURE, V435, P1102, DOI 10.1038/nature03687
   Raiko T., 2014, ARXIV14061485
   Raina R, 2009, P 26 ANN INT C MACH, V382, P873, DOI DOI 10.1145/1553374.1553486
   Ramsey F.P., 1926, FDN MATH OTHER LOGIC, P156, DOI DOI 10.1007/978-3-319-20451-2_3
   Ranzato M., 2010, NIPS 2010
   Ranzato M., 2007, NIPS
   Ranzato M., 2007, P COMP VIS PATT RECO
   Ranzato M., 2010, P AISTATS 2010
   Ranzato M., 2008, NIPS
   Ranzato M, 2010, PROC CVPR IEEE, P2551, DOI 10.1109/CVPR.2010.5539962
   Rao C. R., 1945, B CALCUTTA MATH SOC, V37, P81
   Rasmus A., 2015, ARXIV150702672
   Recht B., 2011, NIPS 2011
   Reichert D, 2011, ADV NEURAL INFORM PR, V24, P2357
   Rezende D. J., 2014, ICML 2014
   Rifai S., 2011, ICML 2011
   Rifai S., 2011, ECML PKDD
   Rifai S., 2011, NIPS 2011
   Rifai S., 2012, ICML 2012
   Ringach D, 2004, COGNITIVE SCI, V28, P147, DOI 10.1016/j.cogsci.2003.11.003
   ROBERTS S, 2001, INDEPENDENT COMPONEN
   Robinson T., 1991, Computer Speech and Language, V5, P259, DOI 10.1016/0885-2308(91)90010-N
   Rockafellar R. T., 1997, CONVEX ANAL PRINCETO
   Romero A., 2015, ICLR 2015
   ROSEN JB, 1960, J SOC IND APPL MATH, V8, P181, DOI 10.1137/0108011
   ROSENBLATT F, 1958, PSYCHOL REV, V65, P386, DOI 10.1037/h0042519
   Rosenblatt F, 1962, PRINCIPLES NEURODYNA
   Roweis S., 2002, ADV NEURAL INFORM PR
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   RUBIN DB, 1984, ANN STAT, V12, P1151, DOI 10.1214/aos/1176346785
   Rumelhart D. E., 1986, PARALLEL DISTRIBUTED
   Rumelhart D.E., 1986, PARALLEL DISTRIBUTED, V1, P318, DOI DOI 10.1016/B978-1-4832-1446-7.50035-2
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Russakovsky O., 2014, IMAGENET LARGE SCALE
   Russakovsky O., 2014, ARXIV14090575
   Russel S., 2003, ARTIFICIAL INTELLIGE
   Rust NC, 2005, NEURON, V46, P945, DOI 10.1016/j.neuron.2005.05.021
   Sainath Tara, 2013, ICASSP 2013
   Salakhutdinov R., 2009, INT J APPROXIMATE RE
   Salakhutdinov R., 2007, ICML
   Salakhutdinov R., 2008, NIPS 2008
   Salakhutdinov R., 2008, ADV NEURAL INFORM PR, V20, P1249
   Salakhutdinov R., 2008, P 25 INT C MACH LEAR, P872, DOI [10.1145/1390156.1390266, DOI 10.1145/1390156.1390266]
   Salakhutdinov R, 2009, ARTIF INTELL, P448, DOI DOI 10.1109/CVPRW.2009.5206577
   Salakhutdinov R., 2010, ADV NEURAL INFORM PR
   Salakhutdinov R. R., 2007, P 11 INT C ART INT S
   Salakhutdinov Ruslan, 2010, INT C ART INT STAT, P693
   Sanger T. D., 1994, IEEE T ROBOTICS AUTO, V10
   Saul L. K., 1996, ADV NEURAL INFORM PR
   Saul LK, 1996, J ARTIF INTELL RES, V4, P61, DOI 10.1613/jair.251
   Savich AW, 2007, IEEE T NEURAL NETWOR, V18, P240, DOI 10.1109/TNN.2006.883002
   Saxe A. M., 2011, P ICML 2011 ACM
   Saxe Andrew M, 2013, ICLR
   Schaul T., 2014, INT C LEARN REPR
   SCHMIDHUBER J, 1992, NEURAL COMPUT, V4, P234, DOI 10.1162/neco.1992.4.2.234
   Schmidhuber J, 1996, IEEE T NEURAL NETWOR, V7, P142, DOI 10.1109/72.478398
   Schmidhuber  J., 2012, ARXIV12100118
   Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467
   Scholkopf B, 2012, P 29 INT C MACH LEAR, P1255
   Scholkopf B., 1999, ADV KERNEL METHODS S
   Scholkopf B., 2002, LEARNING KERNELS SUP
   Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093
   Schuster M., 1999, SUPERVISED LEARNING
   Schwenk Holger, 2010, Prague Bulletin of Mathematical Linguistics, P137, DOI 10.2478/v10108-010-0014-6
   Schwenk H, 2002, INT CONF ACOUST SPEE, P765
   Schwenk H, 1998, ADV NEUR IN, V10, P647
   Schwenk H., 2014, CLEANED SUBSET WMT 1
   SCHWENK H, 2006, [No title captured], P166
   Schwenk H, 2007, COMPUT SPEECH LANG, V21, P492, DOI 10.1016/j.csl.2006.09.003
   Seide F., 2011, P INTERSPEECH, P437
   Sejnowski TJ, 1987, AIP C P, P398
   Series P., 2010, ADV NEURAL INFORM PR, P2020
   Sermanet P., 2013, P INT C COMP VIS PAT
   Sermanet P., 2012, CONVOLUTIONAL NEURAL
   Shilov G., 1977, DOVER BOOKS MATH SER
   SIEGELMANN HT, 1995, SCIENCE, V268, P545, DOI 10.1126/science.268.5210.545
   SIEGELMANN HT, 1995, J COMPUT SYST SCI, V50, P132, DOI 10.1006/jcss.1995.1013
   SIEGELMANN HT, 1991, APPL MATH LETT, V4, P77, DOI 10.1016/0893-9659(91)90080-F
   SIETSMA J, 1991, NEURAL NETWORKS, V4, P67, DOI 10.1016/0893-6080(91)90033-2
   Simard D., 2003, ICDAR 2003
   Simard P., 1992, NIPS 1991
   Simard P., 1994, P ADV NEUR INF PROC, P232
   Simard P. Y., 1993, NIPS 92
   Simard P. Y., 1998, LECT NOTES COMPUTER, V1524
   Simons DJ, 1998, PSYCHON B REV, V5, P644, DOI 10.3758/BF03208840
   Simonyan K., 2015, ICLR
   Sivic J, 2008, PROC CVPR IEEE, P1, DOI 10.1109/CVPRW.2008.4562950
   Sjoberg J, 1995, INT J CONTROL, V62, P1391, DOI 10.1080/00207179508921605
   SKINNER BF, 1958, AM PSYCHOL, V13, P94, DOI 10.1037/h0049039
   Smolensky P, 1986, PARALLEL DISTRIBUTED, P194
   Snoek J., 2012, NIPS 2012
   Socher R, 2011, P 28 INT C MACH LEAR
   Socher R., 2011, EMNLP 2011
   Socher R., 2013, EMNLP 2013
   Socher R., 2013, 27 ANN C NEUR INF PR
   Socher R., 2011, NIPS 2011
   Sohl-Dickstein J., 2015, DEEP UNSUPERVISED LE
   Sohn K., 2013, ICML 2013
   Solomonoff R. J., 1989, SYSTEM INCREMENTAL L
   Sontag E. D., 1998, Neural Networks and Machine Learning. Proceedings, P69
   Sontag E. D., 1989, Complex Systems, V3, P91
   Sparkes B. A., 1996, RED BLACK STUDIES GR
   Spitkovsky V. I., 2010, HLT 10
   Squire W, 1998, SIAM REV, V40, P110, DOI 10.1137/S003614459631241X
   Srebro N, 2005, LECT NOTES COMPUT SC, V3559, P545, DOI 10.1007/11503415_37
   Srivastava N., 2013, THESIS U TORONTO
   Srivastava N., 2012, NIPS 2012
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Srivastava Nitish, 2013, ARXIV13096865
   Srivastava R.K., 2015, ARXIV150500387
   Steinkraus D, 2005, PROC INT CONF DOC, P1115, DOI 10.1109/ICDAR.2005.251
   Stoyanov V., 2011, P INT C ART INT STAT, P725
   Sukhbaatar S., 2015, ARXIV150308895
   Supancic J., 2013, CVPR 2013
   Sussillo D., 2014, RANDOM WALKS TRAININ
   Sutskever I., 2010, INT C ART INT STAT, P789
   Sutskever I, 2012, THESIS U TORONTO
   Sutskever I., 2009, NIPS 2008
   Sutskever I., 2011, P 28 INT C MACH LEAR, P1017
   Sutskever I., 2013, ICML
   Sutskever I, 2008, NEURAL COMPUT, V20, P2629, DOI 10.1162/neco.2008.12-07-661
   Sutskever Ilya, 2014, NIPS 2014
   Sutton R. S, 1998, REINFORCEMENT LEARNI
   Sutton RS, 2000, ADV NEUR IN, V12, P1057
   Swersky K., 2011, ICML 2011 ACM
   Swersky Kevin, 2014, ARXIV14063896
   Szegedy C., 2015, RETHINKING INCEPTION
   Szegedy C., 2014, INTRIGUING PROPERTIE
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Taigman Y., 2014, CVPR 2014
   Tandy D. W., 1997, WORKS DAYS TRANSLATI
   Tang Y., 2010, P 27 INT C MACH LEAR
   Tang Y., 2012, ARXIV12064635
   Taylor G.W., 2007, ADV NEURAL INFORM PR, V19, P1345
   Taylor GW, 2009, P 26 ANN INT C MACH, P1025, DOI DOI 10.1145/1553374.1553505
   Teh YW, 2004, J MACH LEARN RES, V4, P1235
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
   Theis Lucas, 2015, ARXIV151101844
   Thompson J., 2014, NIPS 2014
   Thrun S., 1995, NIPS 1994
   Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267
   Tieleman T., 2009, P 26 INT C MACH LEAR, P1033
   Tieleman T., 2008, P 25 INT C MACH LEAR, P1064, DOI DOI 10.1145/1390156.1390290
   Tipping ME, 1999, J ROY STAT SOC B, V61, P611, DOI 10.1111/1467-9868.00196
   Toscher A., 2009, IGCHAOS SOLUTION NET
   Touretzky D. S., 1985, P 9 INT JOINT C ART, V1, P238
   Tu K., 2011, IJCAI 2011
   Turaga SC, 2010, NEURAL COMPUT, V22, P511, DOI 10.1162/neco.2009.10-08-881
   Turian J, 2010, ACL 2010: 48TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, P384
   Uria B., 2013, NIPS 2013
   van den Oord A., 2013, NIPS 2013
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vanhoucke V., 2011, P DEEP LEARN UNS FEA
   Vapnik V. N., 1982, ESTIMATION DEPENDENC
   VAPNIK VN, 1971, THEOR PROBAB APPL+, V16, P264, DOI 10.1137/1116025
   Vapnik VN, 1995, NATURE STAT LEARNING
   Vincent P., 2011, NEURAL COMPUTATION, V23
   Vincent P., 2015, ADV NEURAL INFORM PR, P1108
   Vincent P., 2003, NIPS 2002
   Vincent P., 2008, ICML 2008
   Vincent P., 2010, J MACHINE LEARNING R, V11
   Vinyals O., 2015, ARXIV150603134
   Vinyals O., 2014, ARXIV14127449
   Vinyals O., 2015, CVPR 2015
   Vinyals Oriol, 2014, ARXIV14114555
   Viola P., 2001, INT J COMPUTER VISIO
   Visin F, 2015, ARXIV150500393
   von Melchner L, 2000, NATURE, V404, P871, DOI 10.1038/35009102
   Wager S., 2013, ADV NEURAL INFORM PR, V26, P351
   WAIBEL A, 1989, IEEE T ACOUST SPEECH, V37, P328, DOI 10.1109/29.21701
   Wan L., 2013, ICML 2013
   Wang S., 2013, ICML 2013
   Wang Z., 2014, P AAAI 2014
   Wang Z., 2014, P EMNLP 2014
   Warde- Farley D., 2014, ICLR 2014
   Wawrzynek J, 1996, COMPUTER, V29, P79, DOI 10.1109/2.485896
   Weaver L., 2001, P 17 C UNC ART INT, P538
   Weinberger KQ, 2004, PROC CVPR IEEE, P988
   Weiss P, 2008, INT CONF ACOUST SPEE, P1173, DOI 10.1109/ICASSP.2008.4517824
   Welling M, 2005, ADV NEURAL INFORM PR, V17
   Welling M., 2003, ADV NEURAL INFORM PR, P665
   Welling M, 2002, ADV NEURAL INFORM PR, P665
   Welling M., 2003, NIPS 2002
   Werbos P. J., 1981, P 10 IFIP C, P762
   Weston J., 2014, ARXIV14103916
   Weston J, 2010, MACH LEARN, V81, P21, DOI 10.1007/s10994-010-5198-3
   Widrow B., 1960, IRE Wescon Convention Record, V4, P96
   Williams CKI, 1996, ADV NEUR IN, V8, P514
   Williams CKI, 2002, NEURAL COMPUT, V14, P1169, DOI 10.1162/089976602753633439
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1023/A:1022672621406
   Williams RJ, 1989, NEURAL COMPUT, V1, P270, DOI 10.1162/neco.1989.1.2.270
   Wilson DR, 2003, NEURAL NETWORKS, V16, P1429, DOI 10.1016/S0893-6080(03)00138-2
   Wilson J. R., 1984, AM J MATH MANAGEMENT, V4, P277
   Wiskott L, 2002, NEURAL COMPUT, V14, P715, DOI 10.1162/089976602317318938
   Wolpert D. H., 1997, IEEE Transactions on Evolutionary Computation, V1, P67, DOI 10.1109/4235.585893
   Wolpert DH, 1996, NEURAL COMPUT, V8, P1341, DOI 10.1162/neco.1996.8.7.1341
   Wu R., 2015, ARXIV150102876
   Xiong HY, 2011, BIOINFORMATICS, V27, P2554, DOI 10.1093/bioinformatics/btr444
   Xu Kelvin, 2015, ICML 2015
   Yildiz IB, 2012, NEURAL NETWORKS, V35, P1, DOI 10.1016/j.neunet.2012.07.005
   Yosinski J., 2014, NIPS 2014
   Younes L, 1998, STOCHASTICS STOCHAST, P177
   Yu D., 2010, IEEE J SELECTED TOPI
   Zaremba W., 2014, ARXIV14104615
   Zaremba W., 2015, ARXIV 1505 00521
   Zaslavsky T., 1975, MEMOIRS AM MATH SOC, V154
   Zeiler M. D., 2013, ICASSP 2013
   Zeiler MD, 2014, ECCV 14
   Zhou B., 2015, ICLR 2015
   Zhou J., 2014, ICML 2014
   Zhou Y. T., 1988, IEEE International Conference on Neural Networks (IEEE Cat. No.88CH2632-8), P71, DOI 10.1109/ICNN.1988.23914
   Zohrer M., 2014, NIPS 2014
NR 797
TC 0
Z9 0
U1 1
U2 1
PU MIT PRESS
PI CAMBRIDGE
PA ONE ROGERS ST, CAMBRIDGE, MA 02142 USA
BN 978-0-262-03561-3
J9 ADAPT COMPUT MACH LE
PY 2016
BP 1
EP 775
PG 775
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
SC Computer Science
GA BI5JK
UT WOS:000412476200021
DA 2020-02-19
ER

PT B
AU Goodfellow, I
   Bengio, Y
   Courville, A
AF Goodfellow, Ian
   Bengio, Yoshua
   Courville, Aaron
BA Goodfellow, I
   Bengio, Y
   Courville, A
BF Goodfellow, I
   Bengio, Y
   Courville, A
TI Deep Learning Introduction
SO DEEP LEARNING
SE Adaptive Computation and Machine Learning
LA English
DT Article; Book Chapter
NR 0
TC 1699
Z9 1702
U1 13
U2 35
PU MIT PRESS
PI CAMBRIDGE
PA ONE ROGERS ST, CAMBRIDGE, MA 02142 USA
BN 978-0-262-03561-3
J9 ADAPT COMPUT MACH LE
PY 2016
BP 1
EP +
D2 10.1007/978-3-319-50890-0
PG 82
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
SC Computer Science
GA BI5JK
UT WOS:000412476200001
DA 2020-02-19
ER

PT J
AU Wang, G
AF Wang, Ge
TI A Perspective on Deep Imaging
SO IEEE ACCESS
LA English
DT Article
DE Tomographic imaging; medical imaging; data acquisition; image
   reconstruction; image analysis; big data; machine learning; deep
   learning
ID RECONSTRUCTION; CT; ALGORITHM
AB The combination of tomographic imaging and deep learning, or machine learning in general, promises to empower not only image analysis but also image reconstruction. The latter aspect is considered in this perspective article with an emphasis on medical imaging to develop a new generation of image reconstruction theories and techniques. This direction might lead to intelligent utilization of domain knowledge from big data, innovative approaches for image reconstruction, and superior performance in clinical and preclinical applications. To realize the full impact of machine learning for tomographic imaging, major theoretical, technical and translational efforts are immediately needed.
C1 [Wang, Ge] Rensselaer Polytech Inst, Ctr Biotechnol & Interdisciplinary Studies, Biomed Imaging Ctr, Dept Biomed Engn, Troy, NY 12180 USA.
RP Wang, G (reprint author), Rensselaer Polytech Inst, Ctr Biotechnol & Interdisciplinary Studies, Biomed Imaging Ctr, Dept Biomed Engn, Troy, NY 12180 USA.
EM ge-wang@ieee.org
CR Aberle DR, 2011, NEW ENGL J MED, V365, P395, DOI 10.1056/NEJMoa1102873
   Ackerman M J, 1991, J Biocommun, V18, P14
   Anthony M., 1999, NEURAL NETWORK LEARN
   Behrman E. C., 2002, QUANTUM NEURAL NETWO
   Byrne JH, 2014, MOL NETWORKS INTRO C
   Chen H., 2016, LOW DOSE CT DENOISIN
   Chen MY, 2016, J X-RAY SCI TECHNOL, V24, P241, DOI 10.3233/XST-160548
   Dabov K, 2006, PROC SPIE, V6064, DOI 10.1117/12.643267
   De Man B, 2007, PROC SPIE, V6510, DOI 10.1117/12.710713
   Estellers V, 2013, IEEE T IMAGE PROCESS, V22, P2611, DOI 10.1109/TIP.2013.2253484
   Geyer LL, 2015, RADIOLOGY, V276, P338, DOI 10.1148/radiol.2015132766
   Goodfellow I, 2016, DEEP LEARNING
   Greenspan H, 2016, IEEE T MED IMAGING, V35, P1153, DOI 10.1109/TMI.2016.2553401
   Hammernik K., 2016, 24 ANN M INT SOC MAG, P331
   Herman GT, 2008, INVERSE PROBL, V24, DOI 10.1088/0266-5611/24/4/045011
   Hey A. J. G., 4 PARADIGM DATA INTE
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hof R., 2013, 10 BREAKTHROUGH TECH
   HORNIK K, 1991, NEURAL NETWORKS, V4, P251, DOI 10.1016/0893-6080(91)90009-T
   HORNIK K, 1989, NEURAL NETWORKS, V2, P359, DOI 10.1016/0893-6080(89)90020-8
   Hsieh J., 2009, COMPUTED TOMOGRAPHY
   Huynh BQ, 2016, J MED IMAGING, V3, DOI 10.1117/1.JMI.3.3.034501
   Jaeger H, 2016, NATURE, V538, P467, DOI 10.1038/nature19477
   Katsevich A, 2004, ADV APPL MATH, V32, P681, DOI 10.1016/S0196-8858(03)00099-X
   KERR JP, 1995, J DIGIT IMAGING, V8, P116, DOI 10.1007/BF03168085
   Kuhn T, 1962, STRUCTURE SCI REVOLU
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   McCollough CH, 2012, RADIOLOGY, V264, P567, DOI 10.1148/radiol.12112265
   Mehta P., 2014, EXACT MAPPING VARIAT
   MUNLEY MT, 1994, MED PHYS, V21, P1889, DOI 10.1118/1.597167
   Natterer F., 2001, MATH METHODS IMAGE R
   Nguyen A., 2015, DEEP NEURAL NETWORKS
   Preston J., 2008, STRUCTURE SCI REVOLU
   Ravishankar S, 2011, IEEE T MED IMAGING, V30, P1028, DOI 10.1109/TMI.2010.2090538
   Shademan A, 2016, SCI TRANSL MED, V8, DOI 10.1126/scitranslmed.aad9398
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   SLANEY M, 2001, PRINCIPLES COMPUTERI
   Szegedy C., 2014, INTRIGUING PROPERTIE
   Tan SQ, 2015, PHYS MED BIOL, V60, P2803, DOI 10.1088/0031-9155/60/7/2803
   Wang G, 2015, MED PHYS, V42, P5879, DOI 10.1118/1.4929559
   Wang SS, 2016, I S BIOMED IMAGING, P514, DOI 10.1109/ISBI.2016.7493320
   Xu Q, 2012, IEEE T MED IMAGING, V31, P1682, DOI 10.1109/TMI.2012.2195669
   Xu XG, 2007, PHYS MED BIOL, V52, P7023, DOI 10.1088/0031-9155/52/23/017
   Yang Q., 2015, JSM BIOMED IMAG, V2
   Zhang H.M., 2016, IMAGE PREDICTION LTD
   Zhou S. K., 2015, 18 INT C MED IM COMP
NR 46
TC 93
Z9 94
U1 5
U2 30
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 2169-3536
J9 IEEE ACCESS
JI IEEE Access
PY 2016
VL 4
BP 8914
EP 8924
DI 10.1109/ACCESS.2016.2624938
PG 11
WC Computer Science, Information Systems; Engineering, Electrical &
   Electronic; Telecommunications
SC Computer Science; Engineering; Telecommunications
GA EM8DS
UT WOS:000395542100041
OA DOAJ Gold
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Sun, M
   Zhang, XW
   Van hamme, H
   Zheng, TF
AF Sun, Meng
   Zhang, Xiongwei
   Van hamme, Hugo
   Zheng, Thomas Fang
TI Unseen Noise Estimation Using Separable Deep Auto Encoder for Speech
   Enhancement
SO IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING
LA English
DT Article
DE Deep auto encoder; source separation; speech enhancement; unseen noise
   compensation
ID HMM
AB Unseen noise estimation is a key yet challenging step to make a speech enhancement algorithm work in adverse environments. At worst, the only prior knowledge we know about the encountered noise is that it is different from the involved speech. Therefore, by subtracting the components which cannot be adequately represented by a well defined speech model, the noises can be estimated and removed. Given the good performance of deep learning in signal representation, a deep auto encoder (DAE) is employed in this work for accurately modeling the clean speech spectrum. In the subsequent stage of speech enhancement, an extra DAE is introduced to represent the residual part obtained by subtracting the estimated clean speech spectrum (by using the pre-trained DAE) from the noisy speech spectrum. By adjusting the estimated clean speech spectrum and the unknown parameters of the noise DAE, one can reach a stationary point to minimize the total reconstruction error of the noisy speech spectrum. The enhanced speech signal is thus obtained by transforming the estimated clean speech spectrum back into time domain. The above proposed technique is called separable deep auto encoder (SDAE). Given the under-determined nature of the above optimization problem, the clean speech reconstruction is confined in the convex hull spanned by a pre-trained speech dictionary. New learning algorithms are investigated to respect the non-negativity of the parameters in the SDAE. Experimental results on TIMIT with 20 noise types at various noise levels demonstrate the superiority of the proposed method over the conventional baselines.
C1 [Sun, Meng; Zhang, Xiongwei] PLA Univ Sci & Technol, Lab Intelligent Informat Proc, Nanjing 210007, Jiangsu, Peoples R China.
   [Van hamme, Hugo] Katholieke Univ Leuven, Elect Engn Dept ESAT, Speech Proc Res Grp, B-3000 Louvain, Belgium.
   [Zheng, Thomas Fang] Tsinghua Univ, Res Inst Informat Technol, Beijing 100084, Peoples R China.
RP Sun, M (reprint author), PLA Univ Sci & Technol, Lab Intelligent Informat Proc, Nanjing 210007, Jiangsu, Peoples R China.
EM sunmengccjs@gmail.com
RI Van hamme, Hugo/D-6581-2012
OI Van hamme, Hugo/0000-0003-1331-5186
FU Natural Science Foundation of ChinaNational Natural Science Foundation
   of China [61471394, 61402519]; Natural Science Foundation of Jiangsu
   ProvinceJiangsu Planned Projects for Postdoctoral Research FundsNatural
   Science Foundation of Jiangsu Province [BK20140071, BK20140074,
   BK2012510]; KULeuven research grant [GOA/14/005]; National Natural
   Science Foundation of ChinaNational Natural Science Foundation of China
   [61271389]; National Basic Research Program (973 Program) of
   ChinaNational Basic Research Program of China [2013CB329302]
FX The work of M. Sun and X. Zhang was supported in part by the Natural
   Science Foundation of China under Grants 61471394 and 61402519 and in
   part by the Natural Science Foundation of Jiangsu Province under Grants
   BK20140071, BK20140074, and BK2012510. The work of H. Van hamme was
   supported by the KULeuven research grant GOA/14/005 (CAMETRON). The work
   of T. F. Zheng was supported in part by the National Natural Science
   Foundation of China under Grant 61271389 and in part by the National
   Basic Research Program (973 Program) of China under Grant 2013CB329302.
CR Bai J., 2011, P 19 EUR SIGN PROC C, P494
   Chen Z., 2013, P IEEE WORKSH APPL S, P1
   EPHRAIM Y, 1985, IEEE T ACOUST SPEECH, V33, P443, DOI 10.1109/TASSP.1985.1164550
   Fevotte C, 2013, INT CONF ACOUST SPEE, P3158, DOI 10.1109/ICASSP.2013.6638240
   Gerkmann T, 2012, IEEE T AUDIO SPEECH, V20, P1383, DOI 10.1109/TASL.2011.2180896
   Ghahramani Z, 1997, MACH LEARN, V29, P245, DOI 10.1023/A:1007425814087
   Ghosh PK, 2011, IEEE T AUDIO SPEECH, V19, P600, DOI 10.1109/TASL.2010.2052803
   Hershey JR, 2014, DEEP UNFOLDING MODEL
   Hinton G. E., 2014, IMPROVING NEURAL NET
   KAMATH S, 2002, ACOUST SPEECH SIG PR, P4164
   Loizou PC, 2005, IEEE T SPEECH AUDI P, V13, P857, DOI 10.1109/TSA.2005.851929
   Lu XG, 2013, INTERSPEECH, P436
   Mohammadiha N, 2013, IEEE T AUDIO SPEECH, V21, P2140, DOI 10.1109/TASL.2013.2270369
   Mohammadiha N, 2013, IEEE SIGNAL PROC LET, V20, P253, DOI 10.1109/LSP.2013.2242467
   Paliwal K, 2010, SPEECH COMMUN, V52, P450, DOI 10.1016/j.specom.2010.02.004
   Po-Sen Huang, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P1562, DOI 10.1109/ICASSP.2014.6853860
   Rix AW, 2001, INT CONF ACOUST SPEE, P749, DOI 10.1109/ICASSP.2001.941023
   Sameti H, 1998, IEEE T SPEECH AUDI P, V6, P445, DOI 10.1109/89.709670
   Smaragdis P, 2004, LECT NOTES COMPUT SC, V3195, P494
   Srinivasan S, 2006, IEEE T AUDIO SPEECH, V14, P163, DOI 10.1109/TSA.2005.854113
   Vincent E, 2006, IEEE T AUDIO SPEECH, V14, P1462, DOI 10.1109/TSA.2005.858005
   Virtanen T, 2007, IEEE T AUDIO SPEECH, V15, P1066, DOI 10.1109/TASL.2006.885253
   Xu Y, 2015, IEEE-ACM T AUDIO SPE, V23, P7, DOI 10.1109/TASLP.2014.2364452
   Xu Y, 2014, IEEE SIGNAL PROC LET, V21, P65, DOI 10.1109/LSP.2013.2291240
   Zeiler MD, 2013, INT CONF ACOUST SPEE, P3517, DOI 10.1109/ICASSP.2013.6638312
   Zhao DY, 2008, IEEE T AUDIO SPEECH, V16, P835, DOI 10.1109/TASL.2008.916055
NR 26
TC 29
Z9 35
U1 1
U2 27
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 2329-9290
J9 IEEE-ACM T AUDIO SPE
JI IEEE-ACM Trans. Audio Speech Lang.
PD JAN
PY 2016
VL 24
IS 1
BP 93
EP 104
DI 10.1109/TASLP.2015.2498101
PG 12
WC Acoustics; Engineering, Electrical & Electronic
SC Acoustics; Engineering
GA CW8AD
UT WOS:000365219900002
DA 2020-02-19
ER

PT J
AU Kim, Y
   Moon, T
AF Kim, Youngwook
   Moon, Taesup
TI Human Detection and Activity Classification Based on Micro-Doppler
   Signatures Using Deep Convolutional Neural Networks
SO IEEE GEOSCIENCE AND REMOTE SENSING LETTERS
LA English
DT Article
DE Convolutional neural network; deep learning; human activity
   classification; human detection; micro-Doppler
AB We propose the use of deep convolutional neural networks (DCNNs) for human detection and activity classification based on Doppler radar. Previously, proposed schemes for these problems remained in the conventional supervised learning paradigm that relies on the design of handcrafted features. Whereas these schemes attained high accuracy, the requirement for domain knowledge of each problem limits the scalability of the proposed schemes. In this letter, we present an alternative deep learning approach. We apply the DCNN, one of the most successful deep learning algorithms, directly to a raw micro-Doppler spectrogram for both human detection and activity classification problem. The DCNN can jointly learn the necessary features and classification boundaries using the measured data without employing any explicit features on the micro-Doppler signals. We show that the DCNN can achieve accuracy results of 97.6% for human detection and 90.9% for human activity classification.
C1 [Kim, Youngwook] Calif State Univ Fresno, Fresno State Lyles Coll Engn, Dept Elect & Comp Engn, Fresno, CA 93740 USA.
   [Moon, Taesup] Daegu Gyeongbuk Inst Sci & Technol, Dept Informat & Commun Engn, Daegu 711873, South Korea.
RP Kim, Y (reprint author), Calif State Univ Fresno, Fresno State Lyles Coll Engn, Dept Elect & Comp Engn, Fresno, CA 93740 USA.
EM youngkim@csufresno.edu; tsmoon@dgist.ac.kr
CR Bergstra J, 2012, J MACH LEARN RES, V13, P281
   Chen V. C., 2002, ARTECH HOUSE RADAR
   Chen VC, 2006, IEEE T AERO ELEC SYS, V42, P2, DOI 10.1109/TAES.2006.1603402
   Chetler S., 2014, CUDNN EFFICIEN UNPUB
   Fairchild DP, 2014, IET RADAR SONAR NAV, V8, P425, DOI 10.1049/iet-rsn.2013.0165
   Hinton G., 2012, P ADV NEUR INF PROC, V25, P1090
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Javier RJ, 2014, IEEE GEOSCI REMOTE S, V11, P1831, DOI 10.1109/LGRS.2014.2311819
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Kim YW, 2015, IEEE GEOSCI REMOTE S, V12, P289, DOI 10.1109/LGRS.2014.2336231
   Kim Y, 2009, IEEE T GEOSCI REMOTE, V47, P1328, DOI 10.1109/TGRS.2009.2012849
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   LeCun Yann, 1990, ADV NEURAL INFORM PR, P396, DOI DOI 10.1111/DSU.12130
   Li John Zenghong, 2012, ISRN Otolaryngol, V2012, P708974, DOI 10.5402/2012/708974
   Mikolov T., 2011, 2011 IEEE Workshop on Automatic Speech Recognition & Understanding (ASRU), P196, DOI 10.1109/ASRU.2011.6163930
   Nesterov Y., 1983, SOV MATH DOKL, V27, P372
   Palm R B, 2012, THESIS
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Stove AG, 2002, IEE CONF PUBL, P419, DOI 10.1109/RADAR.2002.1174739
   Tahmoush D., 2009, P IEEE 3 INT C BIOM, P1
   van Dorp P, 2003, IEE P-RADAR SON NAV, V150, P356, DOI 10.1049/ip-rsn:20030568
   Xavier Glorot, 2011, P 14 INT C ART INT S, P315, DOI DOI 10.1177/1753193410395357
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
NR 23
TC 131
Z9 131
U1 7
U2 64
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1545-598X
EI 1558-0571
J9 IEEE GEOSCI REMOTE S
JI IEEE Geosci. Remote Sens. Lett.
PD JAN
PY 2016
VL 13
IS 1
BP 8
EP 12
DI 10.1109/LGRS.2015.2491329
PG 5
WC Geochemistry & Geophysics; Engineering, Electrical & Electronic; Remote
   Sensing; Imaging Science & Photographic Technology
SC Geochemistry & Geophysics; Engineering; Remote Sensing; Imaging Science
   & Photographic Technology
GA DB0IY
UT WOS:000368193000002
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Marmanis, D
   Datcu, M
   Esch, T
   Stilla, U
AF Marmanis, Dimitrios
   Datcu, Mihai
   Esch, Thomas
   Stilla, Uwe
TI Deep Learning Earth Observation Classification Using ImageNet Pretrained
   Networks
SO IEEE GEOSCIENCE AND REMOTE SENSING LETTERS
LA English
DT Article
DE Convolutional neural networks (CNNs); deep learning (DL); feature
   extraction; land-use classification; pretrained network; remote sensing
   (RS)
AB Deep learning methods such as convolutional neural networks (CNNs) can deliver highly accurate classification results when provided with large enough data sets and respective labels. However, using CNNs along with limited labeled data can be problematic, as this leads to extensive overfitting. In this letter, we propose a novel method by considering a pretrained CNN designed for tackling an entirely different classification problem, namely, the ImageNet challenge, and exploit it to extract an initial set of representations. The derived representations are then transferred into a supervised CNN classifier, along with their class labels, effectively training the system. Through this two-stage framework, we successfully deal with the limited-data problem in an end-to-end processing scheme. Comparative results over the UC Merced Land Use benchmark prove that our method significantly outperforms the previously best stated results, improving the overall accuracy from 83.1% up to 92.4%. Apart from statistical improvements, our method introduces a novel feature fusion algorithm that effectively tackles the large data dimensionality by using a simple and computationally efficient approach.
C1 [Marmanis, Dimitrios; Esch, Thomas] German Aerosp Ctr, German Remote Sensing Ctr DFD, D-82234 Wessling, Germany.
   [Marmanis, Dimitrios; Stilla, Uwe] Tech Univ Munich, Dept Photogrammetry & Remote Sensing, D-80333 Munich, Germany.
   [Datcu, Mihai] German Aerosp Ctr, Remote Sensing Technol Inst IMF, D-82234 Wessling, Germany.
RP Marmanis, D (reprint author), German Aerosp Ctr, German Remote Sensing Ctr DFD, D-82234 Wessling, Germany.
EM Dimitrios.Marmanis@dlr.de; Mihai.Datcu@dlr.de; Thomas.Esch@dlr.de;
   Stilla@tum.de
RI Stilla, Uwe/H-1534-2011; DATCU, Mihai/G-1655-2016; Stilla,
   Uwe/AAG-4465-2019
OI Stilla, Uwe/0000-0002-1184-0924; Stilla, Uwe/0000-0002-1184-0924
CR Cheriyadat AM, 2014, IEEE T GEOSCI REMOTE, V52, P439, DOI 10.1109/TGRS.2013.2241444
   Donahue J., 2013, DECAF DEEP CON UNPUB
   Firat O, 2014, INT C PATT RECOG, P3708, DOI 10.1109/ICPR.2014.637
   Gueguen L, 2015, IEEE T GEOSCI REMOTE, V53, P1803, DOI 10.1109/TGRS.2014.2348864
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lazebnik S., 2006, P IEEE INT C COMP VI, V2, P2169, DOI DOI 10.1109/CVPR.2006.68
   Sermanet P., 2013, OVERFEAT INTEG UNPUB
   Sermanet P, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P2809, DOI 10.1109/IJCNN.2011.6033589
   Socher R., 2012, ADV NEURAL INFORM PR, V3, P665, DOI DOI 10.1002/2014GB005021
   Vincent P, 2008, P 25 INT C MACH LEAR, P1096, DOI DOI 10.1145/1390156.1390294
   Yang Y, 2011, IEEE I CONF COMP VIS, P1465, DOI 10.1109/ICCV.2011.6126403
   Zhang F, 2015, IEEE T GEOSCI REMOTE, V53, P2175, DOI 10.1109/TGRS.2014.2357078
NR 12
TC 161
Z9 168
U1 41
U2 206
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1545-598X
EI 1558-0571
J9 IEEE GEOSCI REMOTE S
JI IEEE Geosci. Remote Sens. Lett.
PD JAN
PY 2016
VL 13
IS 1
BP 105
EP 109
DI 10.1109/LGRS.2015.2499239
PG 5
WC Geochemistry & Geophysics; Engineering, Electrical & Electronic; Remote
   Sensing; Imaging Science & Photographic Technology
SC Geochemistry & Geophysics; Engineering; Remote Sensing; Imaging Science
   & Photographic Technology
GA DB0IY
UT WOS:000368193000022
OA Green Accepted
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Xu, J
   Xiang, L
   Liu, QS
   Gilmore, H
   Wu, JZ
   Tang, JH
   Madabhushi, A
AF Xu, Jun
   Xiang, Lei
   Liu, Qingshan
   Gilmore, Hannah
   Wu, Jianzhong
   Tang, Jinghai
   Madabhushi, Anant
TI Stacked Sparse Autoencoder (SSAE) for Nuclei Detection on Breast Cancer
   Histopathology Images
SO IEEE TRANSACTIONS ON MEDICAL IMAGING
LA English
DT Article
DE Terms-Automated nuclei detection; breast cancer histopathology; feature
   representation learning; stacked sparse autoencoder; digital pathology;
   deep learning
ID CELL DETECTION; CONTOUR
AB Automated nuclear detection is a critical step for a number of computer assisted pathology related image analysis algorithms such as for automated grading of breast cancer tissue specimens. The Nottingham Histologic Score system is highly correlated with the shape and appearance of breast cancer nuclei in histopathological images. However, automated nucleus detection is complicated by 1) the large number of nuclei and the size of high resolution digitized pathology images, and 2) the variability in size, shape, appearance, and texture of the individual nuclei. Recently there has been interest in the application of "Deep Learning" strategies for classification and analysis of big image data. Histopathology, given its size and complexity, represents an excellent use case for application of deep learning strategies. In this paper, a Stacked Sparse Autoencoder (SSAE), an instance of a deep learning strategy, is presented for efficient nuclei detection on high-resolution histopathological images of breast cancer. The SSAE learns high-level features from just pixel intensities alone in order to identify distinguishing features of nuclei. A sliding window operation is applied to each image in order to represent image patches via high-level features obtained via the auto-encoder, which are then subsequently fed to a classifier which categorizes each image patch as nuclear or non-nuclear. Across a cohort of 500 histopathological images (2200 2200) and approximately 3500 manually segmented individual nuclei serving as the groundtruth, SSAE was shown to have an improved F-measure 84.49% and an average area under Precision-Recall curve (AveP) 78.83%. The SSAE approach also out-performed nine other state of the art nuclear detection strategies.
C1 [Xu, Jun; Xiang, Lei; Liu, Qingshan] Nanjing Univ Informat Sci & Technol, Jiangsu Key Lab Big Data Anal Tech, Nanjing 210044, Jiangsu, Peoples R China.
   [Xu, Jun; Xiang, Lei; Liu, Qingshan] Nanjing Univ Informat Sci & Technol, CICAEET, Nanjing 210044, Jiangsu, Peoples R China.
   [Gilmore, Hannah] Case Western Reserve Univ, Univ Hosp, Case Med Ctr, Dept Pathol Anat, Cleveland, OH 44106 USA.
   [Wu, Jianzhong; Tang, Jinghai] Jiangsu Canc Hosp, Nanjing 210000, Jiangsu, Peoples R China.
   [Madabhushi, Anant] Case Western Reserve Univ, Dept Biomed Engn, Cleveland, OH 44106 USA.
RP Xu, J (reprint author), Nanjing Univ Informat Sci & Technol, Jiangsu Key Lab Big Data Anal Tech, Nanjing 210044, Jiangsu, Peoples R China.
EM xujung@gmail.com; axm788@case.edu
RI Madabhushi, Anant/AAG-2908-2019
OI Madabhushi, Anant/0000-0002-5741-0399
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61273259, 61272223]; Six Major Talents Summit of
   Jiangsu Province [2013-XXRJ-019]; Natural Science Foundation of Jiangsu
   Province of ChinaNatural Science Foundation of Jiangsu Province
   [BK20141482]; National Cancer Institute of the National Institutes of
   HealthUnited States Department of Health & Human ServicesNational
   Institutes of Health (NIH) - USANIH National Cancer Institute (NCI)
   [R01CA136535-01, R01CA140772-01, R21CA167811-01, R21CA179327-01];
   National Institute of Diabetes and Digestive and Kidney DiseasesUnited
   States Department of Health & Human ServicesNational Institutes of
   Health (NIH) - USANIH National Institute of Diabetes & Digestive &
   Kidney Diseases (NIDDK) [R01DK098503-02]; DOD Prostate Cancer
   Synergistic Idea Development Award [PC120857]; DOD Lung Cancer Idea
   Development New Investigator Award [LC130463]; Ohio Third Frontier
   Technology development Grant; CTSC Coulter Annual Pilot Grant; Wallace
   H. Coulter Foundation Program in the Department of Biomedical
   Engineering at Case Western Reserve University
FX This work is supported by the National Natural Science Foundation of
   China (61273259, 61272223); Six Major Talents Summit of Jiangsu Province
   (2013-XXRJ-019), and the Natural Science Foundation of Jiangsu Province
   of China (BK20141482); the National Cancer Institute of the National
   Institutes of Health under Awards R01CA136535-01, R01CA140772-01,
   R21CA167811-01, R21CA179327-01; the National Institute of Diabetes and
   Digestive and Kidney Diseases under Award R01DK098503-02, the DOD
   Prostate Cancer Synergistic Idea Development Award (PC120857); the DOD
   Lung Cancer Idea Development New Investigator Award (LC130463), the Ohio
   Third Frontier Technology development Grant, the CTSC Coulter Annual
   Pilot Grant, and the Wallace H. Coulter Foundation Program in the
   Department of Biomedical Engineering at Case Western Reserve University.
   The content is solely the responsibility of the authors and does not
   necessarily represent the official views of the National Institutes of
   Health. Asterisk indicates corresponding author.
CR Al-Kofahi Y, 2010, IEEE T BIO-MED ENG, V57, P841, DOI 10.1109/TBME.2009.2035102
   Cruz-Roa AA, 2013, LECT NOTES COMPUT SC, V8150, P403, DOI 10.1007/978-3-642-40763-5_50
   Ali S, 2012, IEEE T MED IMAGING, V31, P1448, DOI 10.1109/TMI.2012.2190089
   Andrew Ng, 2011, CS294A LECT NOTES, P72
   Basavanhally A., 2011, J PATHOL INF, V2
   Basavanhally AN, 2010, IEEE T BIO-MED ENG, V57, P642, DOI 10.1109/TBME.2009.2035305
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bernardis E, 2011, MED IMAGE ANAL, V15, P690, DOI 10.1016/j.media.2011.06.009
   Byun JY, 2006, MOL VIS, V12, P949
   Chang H, 2013, IEEE T MED IMAGING, V32, P670, DOI 10.1109/TMI.2012.2231420
   Di Cataldo S, 2010, COMPUT METH PROG BIO, V100, P1, DOI 10.1016/j.cmpb.2010.02.002
   Esmaeilsabzali H, 2012, MED BIOL ENG COMPUT, V50, P11, DOI 10.1007/s11517-011-0831-2
   Fatakdawala H, 2010, IEEE T BIO-MED ENG, V57, P1676, DOI 10.1109/TBME.2010.2041232
   Faustino GM, 2011, INTEGR COMPUT-AID E, V18, P91, DOI 10.3233/ICA-2011-0359
   Filipczuk P, 2013, IEEE T MED IMAGING, V32, P2169, DOI 10.1109/TMI.2013.2275151
   Gurcan Metin N, 2009, IEEE Rev Biomed Eng, V2, P147, DOI 10.1109/RBME.2009.2034865
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Irshad H., 2013, J PATHOL INF, V4
   Irshad Humayun, 2014, IEEE Rev Biomed Eng, V7, P97, DOI 10.1109/RBME.2013.2295804
   Jia Yangqing, 2014, ARXIV14085093
   Jung C, 2010, IEEE T BIO-MED ENG, V57, P2600, DOI 10.1109/TBME.2010.2060336
   Lewis JS, 2014, AM J SURG PATHOL, V38, P128, DOI 10.1097/PAS.0000000000000086
   Li G, 2007, CYTOM PART A, V71A, P835, DOI 10.1002/cyto.a.20436
   Liu TM, 2008, NEUROINFORMATICS, V6, P5, DOI 10.1007/s12021-007-9005-7
   Mahmoud SMA, 2011, J CLIN ONCOL, V29, P1949, DOI 10.1200/JCO.2010.30.5037
   MEYER F, 1994, SIGNAL PROCESS, V38, P113, DOI 10.1016/0165-1684(94)90060-4
   Nielsen B, 2012, CYTOM PART A, V81A, P588, DOI 10.1002/cyto.a.22068
   Oberlaender M, 2009, J NEUROSCI METH, V180, P147, DOI 10.1016/j.jneumeth.2009.03.008
   Parvin B, 2007, IEEE T IMAGE PROCESS, V16, P615, DOI 10.1109/TIP.2007.891154
   Petushi Sokol, 2006, BMC Med Imaging, V6, P14, DOI 10.1186/1471-2342-6-14
   Qi X, 2012, IEEE T BIO-MED ENG, V59, P754, DOI 10.1109/TBME.2011.2179298
   Ruifrok AC, 2001, ANAL QUANT CYTOL, V23, P291
   Schmitt O, 2008, PATTERN RECOGN, V41, P1905, DOI 10.1016/j.patcog.2007.11.006
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Veta M, 2014, IEEE T BIO-MED ENG, V61, P1400, DOI 10.1109/TBME.2014.2303852
   Vincent P, 2008, P 25 INT C MACH LEAR, P1096, DOI DOI 10.1145/1390156.1390294
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Vink JP, 2013, J MICROSC-OXFORD, V249, P124, DOI 10.1111/jmi.12001
   Wang HY, 2014, BMC BIOINFORMATICS, V15, DOI 10.1186/1471-2105-15-310
   Xing FY, 2014, IEEE T BIO-MED ENG, V61, P859, DOI 10.1109/TBME.2013.2291703
   Xu J, 2014, I S BIOMED IMAGING, P999, DOI 10.1109/ISBI.2014.6868041
   Yan C, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0062579
   Yan PK, 2008, IEEE T INF TECHNOL B, V12, P109, DOI 10.1109/TITB.2007.898006
NR 43
TC 221
Z9 242
U1 27
U2 169
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 0278-0062
EI 1558-254X
J9 IEEE T MED IMAGING
JI IEEE Trans. Med. Imaging
PD JAN
PY 2016
VL 35
IS 1
BP 119
EP 130
DI 10.1109/TMI.2015.2458702
PG 12
WC Computer Science, Interdisciplinary Applications; Engineering,
   Biomedical; Engineering, Electrical & Electronic; Imaging Science &
   Photographic Technology; Radiology, Nuclear Medicine & Medical Imaging
SC Computer Science; Engineering; Imaging Science & Photographic
   Technology; Radiology, Nuclear Medicine & Medical Imaging
GA DA2KX
UT WOS:000367624800011
PM 26208307
OA Green Accepted
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Gong, MG
   Zhao, JJ
   Liu, J
   Miao, QG
   Jiao, LC
AF Gong, Maoguo
   Zhao, Jiaojiao
   Liu, Jia
   Miao, Qiguang
   Jiao, Licheng
TI Change Detection in Synthetic Aperture Radar Images Based on Deep Neural
   Networks
SO IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
LA English
DT Article
DE Deep learning; image change detection; neural network; synthetic
   aperture radar (SAR)
ID UNSUPERVISED CHANGE DETECTION; MULTITEMPORAL SAR IMAGES; ALGORITHM;
   MODEL
AB This paper presents a novel change detection approach for synthetic aperture radar images based on deep learning. The approach accomplishes the detection of the changed and unchanged areas by designing a deep neural network. The main guideline is to produce a change detection map directly from two images with the trained deep neural network. The method can omit the process of generating a difference image (DI) that shows difference degrees between multitemporal synthetic aperture radar images. Thus, it can avoid the effect of the DI on the change detection results. The learning algorithm for deep architectures includes unsupervised feature learning and supervised fine-tuning to complete classification. The unsupervised feature learning aims at learning the representation of the relationships between the two images. In addition, the supervised fine-tuning aims at learning the concepts of the changed and unchanged pixels. Experiments on real data sets and theoretical analysis indicate the advantages, feasibility, and potential of the proposed method. Moreover, based on the results achieved by various traditional algorithms, respectively, deep learning can further improve the detection performance.
C1 [Gong, Maoguo; Zhao, Jiaojiao; Liu, Jia; Jiao, Licheng] Xidian Univ, Minist Educ, Key Lab Intelligent Percept & Image Understanding, Xian 710071, Peoples R China.
   [Miao, Qiguang] Xidian Univ, Sch Comp Sci & Technol, Xian 710071, Peoples R China.
RP Gong, MG (reprint author), Xidian Univ, Minist Educ, Key Lab Intelligent Percept & Image Understanding, Xian 710071, Peoples R China.
EM gong@ieee.org; 819708414@qq.com; 942407055@qq.com; qgmiao@xidian.edu.cn;
   lchjiao@ieee.org
RI Liu, Jia/P-9706-2018
FU National Nature Science Foundation of ChinaNational Natural Science
   Foundation of China [61273317, 61422209]; National Program for Support
   of Top-Notch Young Professionals of China; Specialized Research Fund for
   Doctoral Program of Higher EducationSpecialized Research Fund for the
   Doctoral Program of Higher Education (SRFDP) [20130203110011];
   Fundamental Research Funds for Central UniversitiesFundamental Research
   Funds for the Central Universities [K5051202053]
FX This work was supported in part by the National Nature Science
   Foundation of China under Grant 61273317 and Grant 61422209, in part by
   the National Program for Support of Top-Notch Young Professionals of
   China, in part by the Specialized Research Fund for the Doctoral Program
   of Higher Education under Grant 20130203110011, and in part by the
   Fundamental Research Funds for the Central Universities under Grant
   K5051202053.
CR Arel I, 2010, IEEE COMPUT INTELL M, V5, P13, DOI 10.1109/MCI.2010.938364
   Bazi Y, 2005, IEEE T GEOSCI REMOTE, V43, P874, DOI 10.1109/TGRS.2004.842441
   Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153
   Bengio Y., 2012, J MACHINE LEARNING R, P17
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Bruzzone L, 2002, IEEE T IMAGE PROCESS, V11, P452, DOI 10.1109/TIP.2002.999678
   Bujor F, 2004, IEEE T GEOSCI REMOTE, V42, P2073, DOI 10.1109/TGRS.2004.835304
   Chen B, 2013, IEEE T PATTERN ANAL, V35, P1887, DOI 10.1109/TPAMI.2013.19
   Cires D. C., 2011, P 22 INT JOINT C ART, V22, P1237, DOI DOI 10.5591/978-1-57735-516-8/IJCAI11-210
   Ciresan DC, 2010, NEURAL COMPUT, V22, P3207, DOI 10.1162/NECO_a_00052
   Collobert R., 2008, P 25 ICML, V25, P160, DOI [10.1145/1390156.1390177, DOI 10.1145/1390156.1390177]
   Dai XL, 1999, PHOTOGRAMM ENG REM S, V65, P1187
   Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231
   Ghosh S, 2007, IEEE T GEOSCI REMOTE, V45, P778, DOI 10.1109/TGRS.2006.888861
   Gong MG, 2014, INT J REMOTE SENS, V35, P4009, DOI 10.1080/01431161.2014.916054
   Gong MG, 2014, IEEE T FUZZY SYST, V22, P98, DOI 10.1109/TFUZZ.2013.2249072
   Gong MG, 2012, IEEE T IMAGE PROCESS, V21, P2141, DOI 10.1109/TIP.2011.2170702
   Goodfellow I. J., 2013, JMLR W CP, P1319
   Graves A, 2009, IEEE T PATTERN ANAL, V31, P855, DOI 10.1109/TPAMI.2008.137
   Haboudane D, 2007, INT GEOSCI REMOTE SE, P4327
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton G. E., 2010, 2010003 UTML TR DEP
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hinton GE, 2010, PHILOS T R SOC B, V365, P177, DOI 10.1098/rstb.2009.0200
   HUBEL DH, 1959, J PHYSIOL-LONDON, V148, P574, DOI 10.1113/jphysiol.1959.sp006308
   Inglada J, 2007, IEEE T GEOSCI REMOTE, V45, P1432, DOI 10.1109/TGRS.2007.893568
   KITTLER J, 1986, PATTERN RECOGN, V19, P41, DOI 10.1016/0031-3203(86)90030-0
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kuruoglu EE, 2004, IEEE T IMAGE PROCESS, V13, P527, DOI 10.1109/TIP.2003.818017
   Larochelle H., 2008, P 25 INT C MACH LEAR, P536, DOI [10.1145/1390156.1390224, DOI 10.1145/1390156.1390224]
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Liao PS, 2001, J INF SCI ENG, V17, P713
   Masci Jonathan, 2013, Mathematical Morphology and Its Applications to Signal and Image Processing. 11th International Symposium, ISMM 2013. Proceedings, P329, DOI 10.1007/978-3-642-38294-9_28
   Masci J, 2013, IEEE IMAGE PROC, P2713, DOI 10.1109/ICIP.2013.6738559
   Masci J, 2011, LECT NOTES COMPUT SC, V6791, P52, DOI 10.1007/978-3-642-21735-7_7
   Oliver C., 1998, UNDERSTANDING SYNTHE
   Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222
   Phung SL, 2007, IEEE T NEURAL NETWOR, V18, P329, DOI 10.1109/TNN.2006.884677
   Prokhorov D, 2010, IEEE T NEURAL NETWOR, V21, P858, DOI 10.1109/TNN.2010.2044802
   Radke RJ, 2005, IEEE T IMAGE PROCESS, V14, P294, DOI 10.1109/TIP.2004.838698
   ROSENFIELD GH, 1986, PHOTOGRAMM ENG REM S, V52, P223
   Rosin PL, 2003, PATTERN RECOGN LETT, V24, P2345, DOI 10.1016/S0167-8655(03)00060-6
   Sezgin M, 2004, J ELECTRON IMAGING, V13, P146, DOI 10.1117/1.1631315
   Srivastava Nitish, 2013, THESIS
   Srivastava R. K., 2013, ADV NEURAL INFORM PR, P2310
   Stuhlsatz A, 2012, IEEE T NEUR NET LEAR, V23, P596, DOI 10.1109/TNNLS.2012.2183645
   Yousif O, 2013, IEEE T GEOSCI REMOTE, V51, P2032, DOI 10.1109/TGRS.2013.2245900
NR 48
TC 146
Z9 154
U1 26
U2 30
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 2162-237X
EI 2162-2388
J9 IEEE T NEUR NET LEAR
JI IEEE Trans. Neural Netw. Learn. Syst.
PD JAN
PY 2016
VL 27
IS 1
BP 125
EP 138
DI 10.1109/TNNLS.2015.2435783
PG 14
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Computer Science, Theory & Methods; Engineering,
   Electrical & Electronic
SC Computer Science; Engineering
GA CZ7AV
UT WOS:000367253200011
PM 26068879
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Girshick, R
   Donahue, J
   Darrell, T
   Malik, J
AF Girshick, Ross
   Donahue, Jeff
   Darrell, Trevor
   Malik, Jitendra
TI Region-Based Convolutional Networks for Accurate Object Detection and
   Segmentation
SO IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
LA English
DT Article
DE Object recognition; detection; semantic segmentation; convolutional
   networks; deep learning; transfer learning
ID REPRESENTATION; HISTOGRAMS; GRADIENTS; FEATURES; SCENE
AB Object detection performance, as measured on the canonical PASCAL VOC Challenge datasets, plateaued in the final years of the competition. The best-performing methods were complex ensemble systems that typically combined multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 50 percent relative to the previous best result on VOC 2012-achieving a mAP of 62.4 percent. Our approach combines two ideas: (1) one can apply high-capacity convolutional networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data are scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, boosts performance significantly. Since we combine region proposals with CNNs, we call the resulting model an R-CNN or Region-based Convolutional Network. Source code for the complete system is available at http://www.cs.berkeley.edu/similar to rbg/rcnn.
C1 [Girshick, Ross] Microsoft Res, Redmond, WA 98052 USA.
   [Donahue, Jeff; Darrell, Trevor; Malik, Jitendra] Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA.
RP Girshick, R (reprint author), Microsoft Res, Redmond, WA 98052 USA.
EM rbg@eecs.berkeley.edu; jdonahue@eecs.berkeley.edu;
   malik@eecs.berkeley.edu
FU DARPAUnited States Department of DefenseDefense Advanced Research
   Projects Agency (DARPA); US National Science FoundationNational Science
   Foundation (NSF) [IIS-0905647, IIS-1134072, IIS-1212798, MURI
   N000014-10-1-0933]; Toyota
FX This research was supported in part by DARPA Mind's Eye and MSEE
   programs, by US National Science Foundation Awards IIS-0905647,
   IIS-1134072, and IIS-1212798, MURI N000014-10-1-0933, and by support
   from Toyota. The GPUs used in this research were generously donated by
   the NVI-DIA Corporation. R. Girshick is with Microsoft Research and was
   with the Department of Electrical Engineering and Computer Science, UC
   Berkeley during the majority of this work.
CR Agrawal P, 2014, LECT NOTES COMPUT SC, V8695, P329, DOI 10.1007/978-3-319-10584-0_22
   Alexe B, 2012, IEEE T PATTERN ANAL, V34, P2189, DOI 10.1109/TPAMI.2012.28
   Arbelaez P, 2014, PROC CVPR IEEE, P328, DOI 10.1109/CVPR.2014.49
   Arbelaez P, 2012, PROC CVPR IEEE, P3378, DOI 10.1109/CVPR.2012.6248077
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Carreira J, 2012, LECT NOTES COMPUT SC, V7578, P430, DOI 10.1007/978-3-642-33786-4_32
   Carreira J, 2012, IEEE T PATTERN ANAL, V34, P1312, DOI 10.1109/TPAMI.2011.231
   CARUNA R, 1993, P 10 INT C MACH LEAR, P41
   Chen L.-C., 2015, P INT C LEARN REPR
   Cheng MM, 2014, PROC CVPR IEEE, P3286, DOI 10.1109/CVPR.2014.414
   Ciresan DC, 2013, LECT NOTES COMPUT SC, V8150, P411, DOI 10.1007/978-3-642-40763-5_51
   Dalal N, 2005, PROC CVPR IEEE, P886
   Dean T, 2013, PROC CVPR IEEE, P1814, DOI 10.1109/CVPR.2013.237
   Deng J., 2012, IMAGENET LARGE SCALE
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng Jia, 2014, P SIGCHI C HUM FACT, P3099, DOI DOI 10.1145/2556288.2557011
   Donahue J., 2014, P INT C MACH LEARN, P647
   Douze M, 2009, P ACM INT C IM VID R, P19
   Endres I, 2010, LECT NOTES COMPUT SC, V6315, P575, DOI 10.1007/978-3-642-15555-0_42
   Erhan D, 2014, PROC CVPR IEEE, P2155, DOI 10.1109/CVPR.2014.276
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Fidler S, 2013, PROC CVPR IEEE, P3294, DOI 10.1109/CVPR.2013.423
   FUKUSHIMA K, 1980, BIOL CYBERN, V36, P193, DOI 10.1007/BF00344251
   Girshick R., 2015, ARXIV150408083V1CSCV
   Girshick R. B., DISCRIMINATIVELY TRA
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Gkioxari G., 2014, ARXIV14065212V1CSCV
   Gu CH, 2009, PROC CVPR IEEE, P1030, DOI 10.1109/CVPRW.2009.5206727
   Gupta S, 2014, LECT NOTES COMPUT SC, V8695, P345, DOI 10.1007/978-3-319-10584-0_23
   Hariharan B, 2015, PROC CVPR IEEE, P447, DOI 10.1109/CVPR.2015.7298642
   Hariharan B, 2014, LECT NOTES COMPUT SC, V8695, P297, DOI 10.1007/978-3-319-10584-0_20
   Hariharan B, 2011, IEEE I CONF COMP VIS, P991, DOI 10.1109/ICCV.2011.6126343
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI 10.1007/978-3-319-10578-9_23
   Hoffman Judy, 2014, ADV NEURAL INFORM PR, V27, P3536
   Hoiem D, 2005, IEEE I CONF COMP VIS, P654
   Hoiem D, 2012, LECT NOTES COMPUT SC, V7574, P340, DOI 10.1007/978-3-642-33712-3_25
   Hosang J., 2015, ARXIV150205082V1CSCV
   Humayun A, 2014, PROC CVPR IEEE, P336, DOI 10.1109/CVPR.2014.50
   Jia Y., 2013, CAFFE OPEN SOURCE CO
   Karpathy Andrej, 2014, ADV NEURAL INFORM PR, P1889
   Krahenbuhl P, 2014, LECT NOTES COMPUT SC, V8693, P725, DOI 10.1007/978-3-319-10602-1_47
   Krahenbuhl P., 2011, ADV NEURAL INFORM PR, P109
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lim JJ, 2013, PROC CVPR IEEE, P3158, DOI 10.1109/CVPR.2013.406
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Nowlan S. J., 1995, Advances in Neural Information Processing Systems 7, P901
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Ren XF, 2013, PROC CVPR IEEE, P3246, DOI 10.1109/CVPR.2013.417
   Rowley HA, 1998, IEEE T PATTERN ANAL, V20, P23, DOI 10.1109/34.655647
   Rumelhart D.E., 1986, PARALLEL DISTRIBUTED, V1, P318, DOI DOI 10.1016/B978-1-4832-1446-7.50035-2
   Russakovsky O., 2014, ARXIV14090575V1CSCV
   Russell B.C., 2006, COMP VIS PATT REC 20, P1605, DOI DOI 10.1109/CVPR.2006.326
   Sermanet P, 2013, PROC CVPR IEEE, P3626, DOI 10.1109/CVPR.2013.465
   Sermanet Pierre, 2013, P INT C LEARN REPR, P16
   Simonyan Karen, 2015, P INT C LEARN REPR
   Song Hyun Oh, 2014, P INT C MACH LEARN, P1611
   Su H., 2012, P AAAI 4 HUM COMP WO
   Sung K., 1994, 1521 AI MIT
   Szegedy C., 2015, ARXIV14121441V2CSCV
   Szegedy C., 2013, ADV NEURAL INFORM PR, V26, P2553
   Szegedy Christian, 2014, ARXIV14094842V1CSCV
   Thrun S, 1996, ADV NEUR IN, V8, P640
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   VAILLANT R, 1994, IEE P-VIS IMAGE SIGN, V141, P245, DOI 10.1049/ip-vis:19941301
   van de Sande KEA, 2014, PROC CVPR IEEE, P2377, DOI 10.1109/CVPR.2014.304
   Wang XY, 2013, IEEE I CONF COMP VIS, P17, DOI 10.1109/ICCV.2013.10
   Zeiler MD, 2011, IEEE I CONF COMP VIS, P2018, DOI 10.1109/ICCV.2011.6126474
   Zheng S., 2015, ARXIV150203240V2CSCV
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
NR 75
TC 518
Z9 566
U1 63
U2 438
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 0162-8828
EI 1939-3539
J9 IEEE T PATTERN ANAL
JI IEEE Trans. Pattern Anal. Mach. Intell.
PD JAN
PY 2016
VL 38
IS 1
BP 142
EP 158
DI 10.1109/TPAMI.2015.2437384
PG 17
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
SC Computer Science; Engineering
GA CY8OW
UT WOS:000366669200011
PM 26656583
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Jaderberg, M
   Simonyan, K
   Vedaldi, A
   Zisserman, A
AF Jaderberg, Max
   Simonyan, Karen
   Vedaldi, Andrea
   Zisserman, Andrew
TI Reading Text in the Wild with Convolutional Neural Networks
SO INTERNATIONAL JOURNAL OF COMPUTER VISION
LA English
DT Article
DE Text spotting; Text recognition; Text detection; Deep learning;
   Convolutional neural networks; Synthetic data; Text retrieval
ID SCENE TEXT; IMAGES
AB In this work we present an end-to-end system for text spotting-localising and recognising text in natural scene images-and text based image retrieval. This system is based on a region proposal mechanism for detection and deep convolutional neural networks for recognition. Our pipeline uses a novel combination of complementary proposal generation techniques to ensure high recall, and a fast subsequent filtering stage for improving precision. For the recognition and ranking of proposals, we train very large convolutional neural networks to perform word recognition on the whole proposal region at the same time, departing from the character classifier based systems of the past. These networks are trained solely on data produced by a synthetic text generation engine, requiring no human labelled data. Analysing the stages of our pipeline, we show state-of-the-art performance throughout. We perform rigorous experiments across a number of standard end-to-end text spotting benchmarks and text-based image retrieval datasets, showing a large improvement over all previous methods. Finally, we demonstrate a real-world application of our text spotting system to allow thousands of hours of news footage to be instantly searchable via a text query.
C1 [Jaderberg, Max; Simonyan, Karen; Vedaldi, Andrea; Zisserman, Andrew] Univ Oxford, Dept Engn Sci, Parks Rd, Oxford OX1 3PJ, England.
RP Jaderberg, M (reprint author), Univ Oxford, Dept Engn Sci, Parks Rd, Oxford OX1 3PJ, England.
EM max@robots.ox.ac.uk; karen@robots.ox.ac.uk; vedaldi@robots.ox.ac.uk;
   az@robots.ox.ac.uk
FU EPSRCEngineering & Physical Sciences Research Council (EPSRC);
   ERCEuropean Research Council (ERC) [228180]; NVIDIA Corporation
FX This work was supported by the EPSRC and ERC Grant VisRec No. 228180. We
   gratefully acknowledge the support of NVIDIA Corporation with the
   donation of the GPUs used for this research. We thank the BBC and in
   particular Rob Cooper for access to data and video processing resources.
CR Alexe B, 2012, IEEE T PATTERN ANAL, V34, P2189, DOI 10.1109/TPAMI.2012.28
   Almazan J, 2014, IEEE T PATTERN ANAL, V36, P2552, DOI 10.1109/TPAMI.2014.2339814
   Alsharif O., 2014, INT C LEARN REPR
   Anthimopoulos M, 2013, PATTERN ANAL APPL, V16, P431, DOI 10.1007/s10044-011-0237-7
   Bissacco A., 2013, P INT C COMP VIS
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Chen H., 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P2609, DOI 10.1109/ICIP.2011.6116200
   Chen XR, 2004, PROC CVPR IEEE, P366
   Cheng MM, 2014, PROC CVPR IEEE, P3286, DOI 10.1109/CVPR.2014.414
   de Campos TE, 2009, VISAPP 2009: PROCEEDINGS OF THE FOURTH INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS, VOL 2, P273
   Dollar P., 2014, ARXIV14065549
   Dollar P., 2010, P BRIT MACHINE VISIO, P1, DOI DOI 10.5244/C.24.68
   Dollar P, 2014, IEEE T PATTERN ANAL, V36, P1532, DOI 10.1109/TPAMI.2014.2300479
   Dollar P, 2013, IEEE I CONF COMP VIS, P1841, DOI 10.1109/ICCV.2013.231
   Epshtein B, 2010, PROC CVPR IEEE, P2963, DOI 10.1109/CVPR.2010.5540041
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Felzenszwalb PF, 2005, INT J COMPUT VISION, V61, P55, DOI 10.1023/B:VISI.0000042934.15159.49
   Fischer A, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P3416, DOI 10.1109/ICPR.2010.834
   Friedman J, 2000, ANN STAT, V28, P337, DOI 10.1214/aos/1016218223
   Frinken V, 2012, IEEE T PATTERN ANAL, V34, P211, DOI 10.1109/TPAMI.2011.113
   Girshick R, 2014, P IEEE C COMP VIS PA
   Goel V, 2013, PROC INT CONF DOC, P398, DOI 10.1109/ICDAR.2013.87
   Gomez L, 2014, ARXIV14077504
   Gomez L, 2013, PROC INT CONF DOC, P467, DOI 10.1109/ICDAR.2013.100
   Goodfellow I. J, 2013, ARXIV13126082
   Gordo A., 2014, CORR
   Hinton G.E., 2012, ARXIV12070580 CORR
   Huang WL, 2014, LECT NOTES COMPUT SC, V8692, P497, DOI 10.1007/978-3-319-10593-2_33
   Jaderberg M., 2014, ARXIV14062227
   Jaderberg M., 2014, EUR C COMP VIS
   Karatzas D, 2013, PROC INT CONF DOC, P1484, DOI 10.1109/ICDAR.2013.221
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lucas S. M., 2003, P ICDAR
   Lucas SM, 2005, PROC INT CONF DOC, P80, DOI 10.1109/ICDAR.2005.231
   Manmatha R, 1996, PROC CVPR IEEE, P631, DOI 10.1109/CVPR.1996.517139
   Matas J, 2004, IMAGE VISION COMPUT, V22, P761, DOI 10.1016/j.imavis.2004.02.006
   Mishra A, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.127
   Mishra A, 2013, IEEE I CONF COMP VIS, P3040, DOI 10.1109/ICCV.2013.378
   Neumann L., 2012, P IEEE C COMP VIS PA
   Neumann L, 2013, IEEE I CONF COMP VIS, P97, DOI 10.1109/ICCV.2013.19
   Neumann L, 2011, LECT NOTES COMPUT SC, V6494, P770, DOI 10.1007/978-3-642-19318-7_60
   Neumann L, 2011, PROC INT CONF DOC, P687, DOI 10.1109/ICDAR.2011.144
   Novikova T, 2012, LECT NOTES COMPUT SC, V7577, P752, DOI 10.1007/978-3-642-33783-3_54
   Ozuysal M., 2007, P IEEE C COMP VIS PA
   Perronnin F., 2010, P IEEE C COMP VIS PA
   Posner I, 2010, IEEE INT C INT ROBOT, P3181, DOI 10.1109/IROS.2010.5653151
   Quack T., 2009, THESIS ETH ZURICH
   Rath TM, 2007, INT J DOC ANAL RECOG, V9, P139, DOI 10.1007/s10032-006-0027-8
   Rodriguez-Serrano JA, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.5
   Shahab A, 2011, PROC INT CONF DOC, P1491, DOI 10.1109/ICDAR.2011.296
   Simard P. Y, 2003, BEST PRACTICES CONVO, P958
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Wang J., 2010, P IEEE C COMP VIS PA
   Wang K, 2011, IEEE I CONF COMP VIS, P1457, DOI 10.1109/ICCV.2011.6126402
   Wang T, 2012, INT C PATT RECOG, P3304
   Weinman JJ, 2014, IEEE T PATTERN ANAL, V36, P375, DOI 10.1109/TPAMI.2013.126
   Yao C, 2014, PROC CVPR IEEE, P4042, DOI 10.1109/CVPR.2014.515
   Yi CC, 2011, IEEE T IMAGE PROCESS, V20, P2594, DOI 10.1109/TIP.2011.2126586
   Yin X. C., 2013, ARXIV13012628 CORR
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
NR 61
TC 287
Z9 307
U1 21
U2 24
PU SPRINGER
PI DORDRECHT
PA VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN 0920-5691
EI 1573-1405
J9 INT J COMPUT VISION
JI Int. J. Comput. Vis.
PD JAN
PY 2016
VL 116
IS 1
BP 1
EP 20
DI 10.1007/s11263-015-0823-z
PG 20
WC Computer Science, Artificial Intelligence
SC Computer Science
GA DC7TE
UT WOS:000369422500001
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Zhou, MY
   Cong, YL
   Chen, B
AF Zhou, Mingyuan
   Cong, Yulai
   Chen, Bo
TI Augmentable Gamma Belief Networks
SO JOURNAL OF MACHINE LEARNING RESEARCH
LA English
DT Article
DE Bayesian nonparametrics; deep learning; multilayer representation;
   Poisson factor analysis; topic modeling; unsupervised learning
ID DEEP
AB To infer multilayer deep representations of high-dimensional discrete and nonnegative real vectors, we propose an augmentable gamma belief network (GBN) that factorizes each of its hidden layers into the product of a sparse connection weight matrix and the nonnegative real hidden units of the next layer. The GBN's hidden layers are jointly trained with an upward-downward Gibbs sampler that solves each layer with the same subroutine. The gamma-negative binomial process combined with a layer-wise training strategy allows inferring the width of each layer given a fixed budget on the width of the first layer. Example results illustrate interesting relationships between the width of the first layer and the inferred network structure, and demonstrate that the GBN can add more layers to improve its performance in both unsupervisedly extracting features and predicting heldout data. For exploratory data analysis, we extract trees and subnetworks from the learned deep network to visualize how the very specific factors discovered at the first hidden layer and the increasingly more general factors discovered at deeper hidden layers are related to each other, and we generate synthetic data by propagating random variables through the deep network from the top hidden layer back to the bottom data layer.
C1 [Zhou, Mingyuan] Univ Texas Austin, Dept Informat Risk & Operat Management, McCombs Sch Business, Austin, TX 78712 USA.
   [Cong, Yulai; Chen, Bo] Xidian Univ, Natl Lab Radar Signal Proc, Collaborat Innovat Ctr Informat Sensing & Underst, Xian 710071, Shaanxi, Peoples R China.
RP Zhou, MY (reprint author), Univ Texas Austin, Dept Informat Risk & Operat Management, McCombs Sch Business, Austin, TX 78712 USA.; Chen, B (reprint author), Xidian Univ, Natl Lab Radar Signal Proc, Collaborat Innovat Ctr Informat Sensing & Underst, Xian 710071, Shaanxi, Peoples R China.
EM MINGYUAN.ZHOU@MCCOMBS.UTEXAS.EDU; YULAI_CONG@163.COM;
   BCHEN@MAIL.XIDIAN.EDU.CN
FU Thousand Young Talent Program of China, NSFC [61372132, NCET-13-0945,
   NDPR-9140A07010115DZ01015]
FX The authors would like to thank the editor and two anonymous referees
   for their insightful and constructive comments and suggestions, which
   have helped us improve the paper substantially. M. Zhou thanks Texas
   Advanced Computing Center for computational support. B. Chen thanks the
   support of the Thousand Young Talent Program of China, NSFC (61372132),
   NCET-13-0945, and NDPR-9140A07010115DZ01015.
CR Acharya A, 2015, JMLR WORKSH CONF PRO, V38, P1
   Adams R., 2010, NIPS
   Aldous D., 1985, LECT NOTES MATH, V1117, P1, DOI DOI 10.1007/BFB0099421
   ANSCOMBE FJ, 1950, BIOMETRIKA, V37, P358, DOI 10.1093/biomet/37.3-4.358
   ANTONIAK CE, 1974, ANN STAT, V2, P1152, DOI 10.1214/aos/1176342871
   Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153
   Bengio Y., 2015, DEEP LEARNING
   Bengio Y., 2007, LARGE SCALE KERNEL M
   BLACKWELL D, 1973, ANN STAT, V1, P353, DOI 10.1214/aos/1176342372
   Blei D. M., 2006, NIPS
   Blei D. M., 2008, NIPS
   Blei DM, 2010, J ACM, V57, DOI 10.1145/1667053.1667056
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   BLISS CI, 1953, BIOMETRICS, V9, P176, DOI 10.2307/3001850
   BUNTINE WL, 2006, [No title captured]
   Canny J., 2004, SIGIR
   Chen J., 2013, NIPS
   CHOI SC, 1969, TECHNOMETRICS, V11, P683, DOI 10.2307/1266892
   Devroye L, 2002, STAT PROBABIL LETT, V57, P249, DOI 10.1016/S0167-7152(02)00055-X
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Fisher RA, 1943, J ANIM ECOL, V12, P42, DOI 10.2307/1411
   Frey B. J., 1997, CONTINUOUS SIGMOIDAL, P452
   Frey B. J., 1997, 6 INT WORKSH ART INT
   Frey BJ, 1999, NEURAL COMPUT, V11, P193, DOI 10.1162/089976699300016872
   Gan Z., 2015, AISTATS
   Gan Z., 2015, ICML
   Greenwood M., 1920, J R STAT SOC
   Griffiths T.L., 2004, PNAS
   Hinton GE, 1997, PHILOS T ROY SOC B, V352, P1177, DOI 10.1098/rstb.1997.0101
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hu C., 2015, UAI
   Huang Fu-Jie, 2007, CVPR
   JOHNSON N. L, 1997, DISCRETE MULTIVARIAT, V165
   Kingma D. P., 2014, ICLR
   Larochelle H., 2012, NIPS
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Lee D. D., 2001, NIPS
   Lee H., 2009, ICML
   Linderman Scott W, 2015, NIPS, P3438
   Nair V, 2010, ICML, V27, P807, DOI DOI 10.0RG/PAPERS/432.PDF
   NEAL RM, 1992, ARTIF INTELL, V56, P71, DOI 10.1016/0004-3702(92)90065-6
   Newman D., 2009, J MACH LEARN RES
   Paisley J., 2015, IEEE T PATTERN ANAL
   Paisley J., 2012, BAYESIAN ANAL
   Pitman J., 2006, LECT NOTES MATH
   Polson N. G, 2012, ARXIV12050310
   Ranganath R, 2014, DEEP EXPONENTIAL FAM
   Ranganath R., 2015, ARXIV150700720V1
   Ranganath Rajesh, 2014, AISTATS
   Rezende D.J., 2014, P 31 INT C MACH LEAR
   Salakhutdinov R., 2009, AISTATS
   Salakhutdinov R, 2013, IEEE T PATTERN ANAL, V35, P1958, DOI 10.1109/TPAMI.2012.269
   Saul LK, 1996, J ARTIF INTELL RES, V4, P61, DOI 10.1613/jair.251
   Srivastava N., 2013, UAI
   Sutskever I, 2008, NEURAL COMPUT, V20, P2629, DOI 10.1162/neco.2008.12-07-661
   TEH Y, 2006, [No title captured]
   van Dyk D.A., 2001, J COMP GRAPH STAT
   Wallach H., 2009, ICML
   Welling Max, 2004, ADV NEURAL INFORM PR, P1481
   Williamson S., 2010, ICML
   Xing E., 2005, UAI
   Yuan L, 2000, ANN I STAT MATH, V52, P438, DOI 10.1023/A:1004152916478
   Zhou M., 2012, J MACHINE LEARNING R, P1462
   Zhou M., 2014, P NIPS, P3455
   Zhou M., 2015, NIPS
   Zhou M., 2015, J AM STAT A IN PRESS
   Zhou MY, 2015, JMLR WORKSH CONF PRO, V38, P1135
   Zhou MY, 2015, IEEE T PATTERN ANAL, V37, P307, DOI 10.1109/TPAMI.2013.211
   Zhu J, 2012, J MACH LEARN RES, V13, P2237
NR 70
TC 12
Z9 12
U1 2
U2 4
PU MICROTOME PUBL
PI BROOKLINE
PA 31 GIBBS ST, BROOKLINE, MA 02446 USA
SN 1532-4435
J9 J MACH LEARN RES
JI J. Mach. Learn. Res.
PY 2016
VL 17
BP 1
EP 44
AR 163
PG 44
WC Automation & Control Systems; Computer Science, Artificial Intelligence
SC Automation & Control Systems; Computer Science
GA EH3JU
UT WOS:000391668700001
DA 2020-02-19
ER

PT J
AU Ganin, Y
   Ustinova, E
   Ajakan, H
   Germain, P
   Larochelle, H
   Laviolette, F
   Marchand, M
   Lempitsky, V
AF Ganin, Yaroslav
   Ustinova, Evgeniya
   Ajakan, Hana
   Germain, Pascal
   Larochelle, Hugo
   Laviolette, Francois
   Marchand, Mario
   Lempitsky, Victor
TI Domain-Adversarial Training of Neural Networks
SO JOURNAL OF MACHINE LEARNING RESEARCH
LA English
DT Article
DE domain adaptation; neural network; representation learning; deep
   learning; synthetic data; image classification; sentiment analysis;
   person re-identification
ID ADAPTATION
AB We introduce a new representation learning approach for domain adaptation, in which data at training and test time come from similar but different distributions. Our approach is directly inspired by the theory on domain adaptation suggesting that, for effective domain transfer to be achieved, predictions must be made based on features that cannot discriminate between the training (source) and test (target) domains.
   The approach implements this idea in the context of neural network architectures that are trained on labeled data from the source domain and unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of features that are (i) discriminative for the main learning task on the source domain and (ii) indiscriminate with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation and stochastic gradient descent, and can thus be implemented with little e ff ort using any of the deep learning packages.
   We demonstrate the success of our approach for two distinct classi fi cation problems (document sentiment analysis and image classi fi cation), where state-of-the-art domain adaptation performance on standard benchmarks is achieved. We also validate the approach for descriptor learning task in the context of person re-identi fi cation application.
C1 [Ganin, Yaroslav; Ustinova, Evgeniya; Lempitsky, Victor] Skolkovo Inst Sci & Technol Skoltech, Skolkovo, Moscow Region, Russia.
   [Ajakan, Hana; Germain, Pascal; Laviolette, Francois; Marchand, Mario] Univ Laval, Dept Informat & Genie Logiciel, Quebec City, PQ G1V 0A6, Canada.
   [Larochelle, Hugo] Univ Sherbrooke, Dept Informat, Sherbrooke, PQ J1K 2R1, Canada.
RP Ganin, Y (reprint author), Skolkovo Inst Sci & Technol Skoltech, Skolkovo, Moscow Region, Russia.
EM GANIN@SKOLTECH.RU; EVGENIYA.USTINOVA@SKOLTECH.RU;
   HANA.AJAKAN.1@ULAVAL.CA; PASCAL.GERMAIN@IFT.ULAVAL.CA;
   HUGO.LAROCHELLE@USHERBROOKE.CA; FRANCOIS.LAVIOLETTE@IFT.ULAVAL.CA;
   MARIO.MARCHAND@IFT.ULAVAL.CA; LEMPITSKY@SKOLTECH.RU
FU National Science and Engineering Research Council (NSERC)Natural
   Sciences and Engineering Research Council of Canada [262067, 0122405];
   Russian Ministry of Science and EducationMinistry of Education and
   Science, Russian Federation [RFMEFI57914X0071]; Calcul Quebec; Compute
   Canada; NSERCNatural Sciences and Engineering Research Council of
   Canada; Canada Foundation for Innovation (CFI)Canada Foundation for
   Innovation; NanoQuebec; Fonds de recherche du Quebec - Nature et
   technologies (FRQNT)
FX This work has been supported by National Science and Engineering
   Research Council (NSERC) Discovery grants 262067 and 0122405 as well as
   the Russian Ministry of Science and Education grant RFMEFI57914X0071.
   Computations were performed on the Colosse supercomputer grid at
   Universite Laval, under the auspices of Calcul Quebec and Compute
   Canada. The operations of Colosse are funded by the NSERC, the Canada
   Foundation for Innovation (CFI), NanoQuebec, and the Fonds de recherche
   du Quebec - Nature et technologies (FRQNT). We also thank the Graphics &
   Media Lab, Faculty of Computational Mathematics and Cybernetics,
   Lomonosov Moscow State University for providing the synthetic road signs
   data set.
CR Ajakan Hana, 2014, NIPS 2014 WORKSH TRA
   Arbelaez P., 2011, IEEE T PATT ANAL MAC, V33
   Babenko A, 2014, LECT NOTES COMPUT SC, V8689, P584, DOI 10.1007/978-3-319-10590-1_38
   Baktashmotlagh M, 2013, IEEE I CONF COMP VIS, P769, DOI 10.1109/ICCV.2013.100
   Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4
   Ben-David Shai, 2006, NIPS, P137
   Blitzer J., 2006, P 2006 C EMP METH NA, P120
   Borgwardt KM, 2006, BIOINFORMATICS, V22, pE49, DOI 10.1093/bioinformatics/btl242
   Bruzzone L, 2010, IEEE T PATTERN ANAL, V32, P770, DOI 10.1109/TPAMI.2009.57
   Chen M., 2012, P 29 INT C MACH LEAR, P767, DOI DOI 10.1007/S11222-007-9033-Z
   Chen Q., 2015, CVPR
   Chopra S, 2013, ICML WORKSH CHALL RE
   Ciresan D, 2012, NEURAL NETWORKS, V32, P333, DOI 10.1016/j.neunet.2012.02.023
   Cortes C, 2014, THEOR COMPUT SCI, V519, P103, DOI 10.1016/j.tcs.2013.09.027
   Donahue J., 2014, ICML
   Duchi John, 2010, TECHNICAL REPORT
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Fergus Rob, 2013, CORR
   Fernando B., 2013, ICCV
   Ganin Yaroslav, 2015, ICML, P325
   GLOROT Xavier, 2011, P 28 INT C MACH LEAR, V28, P513, DOI DOI 10.1177/1753193411430810
   Gong BQ, 2012, PROC CVPR IEEE, P2066, DOI 10.1109/CVPR.2012.6247911
   Gong Boqing, 2013, P 30 INT C MACH LEAR, P222
   Gong S, 2014, ADV COMPUT VIS PATT, P1, DOI 10.1007/978-1-4471-6296-4
   Goodfellow IJ, 2014, ADV NEUR IN, V27
   Gopalan R, 2011, IEEE I CONF COMP VIS, P999, DOI 10.1109/ICCV.2011.6126344
   Gray Douglas, 2007, IEEE INT WORKSH PERF
   Hirzer Martin, 2011, SCIA
   Hoffman Judy, 2013, CORR
   Huang Fei, 2012, P 2012 JOINT C EMP M, P1313
   Huang J., 2006, ADV NEURAL INFORM PR, P601
   Jia Y., 2014, CORR
   Kifer D., 2004, P 30 INT C VER LARG, V30, P180, DOI DOI 10.1016/B978-012088469-8/50019-X
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   Lacoste Alexandre, 2012, P 15 INT C ART INT S, V22, P665
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li W, 2013, PROC CVPR IEEE, P3594, DOI 10.1109/CVPR.2013.461
   Li Y., 2014, J NUTR HLTH FOOD ENG, V1, P1
   Liebelt J., 2010, CVPR
   Liu CX, 2013, IEEE I CONF COMP VIS, P441, DOI 10.1109/ICCV.2013.62
   Long M., 2015, CORR
   Ma AJ, 2015, IEEE T IMAGE PROCESS, V24, P1599, DOI 10.1109/TIP.2015.2395715
   Mansour Y., 2009, UAI, P367
   Mansour Yishay, 2009, COLT
   Netzer Y, 2011, NIPS WORKSH DEEP LEA
   Oquab M., 2014, CVPR
   P. Germain, 2013, ICML, P738
   Paisitkriangkrai Sakrapee, 2015, CORR
   Pan SJ, 2011, IEEE T NEURAL NETWOR, V22, P199, DOI 10.1109/TNN.2010.2091281
   Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Stark M., 2010, BMVC, DOI [10.5244/C.24.106, DOI 10.5244/C.24.106]
   Sun B., 2014, BMVC
   Tzeng Eric, 2014, CORR
   van der Maaten L., 2013, CORR
   Vazquez D, 2014, IEEE T PATTERN ANAL, V36, P797, DOI 10.1109/TPAMI.2013.163
   Vincent P, 2008, P 25 INT C MACH LEAR, P1096, DOI DOI 10.1145/1390156.1390294
   Yi D., 2014, CORR
   Zeiler M. D., 2012, CORR, Vabs/1212.5701
   Zhang Ziming, 2014, CORR
   Zhao Rui, 2014, CORR
   Zhong EH, 2010, LECT NOTES ARTIF INT, V6323, P547, DOI 10.1007/978-3-642-15939-8_35
NR 62
TC 341
Z9 347
U1 1
U2 1
PU MICROTOME PUBL
PI BROOKLINE
PA 31 GIBBS ST, BROOKLINE, MA 02446 USA
SN 1532-4435
J9 J MACH LEARN RES
JI J. Mach. Learn. Res.
PY 2016
VL 17
AR 59
PG 35
WC Automation & Control Systems; Computer Science, Artificial Intelligence
SC Automation & Control Systems; Computer Science
GA EH0ZK
UT WOS:000391492800001
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Gulcehre, C
   Bengio, Y
AF Gulcehre, Caglar
   Bengio, Yoshua
TI Knowledge Matters: Importance of Prior Information for Optimization
SO JOURNAL OF MACHINE LEARNING RESEARCH
LA English
DT Article
DE deep learning; neural networks; optimization; evolution of culture;
   curriculum learning; training with hints
AB We explored the effect of introducing prior knowledge into the intermediate level of deep supervised neural networks on two tasks. On a task we designed, all black-box state-of-theart machine learning algorithms which we tested, failed to generalize well. We motivate our work from the hypothesis that, there is a training barrier involved in the nature of such tasks, and that humans learn useful intermediate concepts from other individuals by using a form of supervision or guidance using a curriculum. Our results provide a positive evidence in favor of this hypothesis. In our experiments, we trained a two-tiered MLP architecture on a dataset for which each input image contains three sprites, and the binary target class is 1 if all of three shapes belong to the same category and otherwise the class is 0. In terms of generalization, black-box machine learning algorithms could not perform better than chance on this task. Standard deep supervised neural networks also failed to generalize. However, using a particular structure and guiding the learner by providing intermediate targets in the form of intermediate concepts (the presence of each object) allowed us to solve the task efficiently. We obtained much better than chance, but imperfect results by exploring different architectures and optimization variants. This observation might be an indication of optimization difficulty when the neural network trained without hints on this task. We hypothesize that the learning difficulty is due to the composition of two highly non-linear tasks. Our findings are also consistent with the hypotheses on cultural learning inspired by the observations of training of neural networks sometimes getting stuck, even though good solutions exist, both in terms of training and generalization error.
C1 [Gulcehre, Caglar; Bengio, Yoshua] Univ Montreal, Dept Informat & Rech Operat, Montreal, PQ, Canada.
RP Gulcehre, C (reprint author), Univ Montreal, Dept Informat & Rech Operat, Montreal, PQ, Canada.
EM GULCEHRC@IRO.UMONTREAL.CA; BENGIOY@IRO.UMONTREAL.CA
FU NSERCNatural Sciences and Engineering Research Council of Canada;
   CIFARCanadian Institute for Advanced Research (CIFAR); Compute Canada;
   Canada Research ChairsCanada Research Chairs
FX We would like to thank to the ICLR 2013 reviewers for their insightful
   comments, and NSERC, CIFAR, Compute Canada and Canada Research Chairs
   for funding.
CR Barto AG, 2003, DISCRETE EVENT DYN S, V13, P343
   Ben-Hur A, 2010, METHODS MOL BIOL, V609, P223, DOI 10.1007/978-1-60327-241-4_13
   Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153
   Bengio Y, 2012, TECHNICAL REPORT
   Bengio Y., 2013, NEURAL NETWORKS TRIC
   Bengio Y., 2009, P 26 INT C MACH LEAR
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Bengio Yoshua, 2013, IEEE T PATTERN ANAL
   Bengio Yoshua, 2013, GROWING ADAPTIVE MAC
   Bergstra J., 2010, P PYTH SCI COMP C SC
   Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Breiman L, 1984, CLASSIFICATION REGRE
   Choromanska A., 2015, AISTATS 2015 P 18 IN, P192
   Ciresan DC, 2010, NEURAL COMPUT, V22, P3207, DOI 10.1162/NECO_a_00052
   Dauphin Yann, 2013, ARXIV13013583 U MONT
   Dauphin Yann, 2014, NIPS 2014
   Dawkins Richard, 1976, SELFISH GENE
   Duchi John, 2011, J MACHINE LEARNING R
   Erhan Dumitru, J MACHINE LEARNING R, P625
   Fleuret F, 2011, P NATL ACAD SCI USA, V108, P17621, DOI 10.1073/pnas.1109168108
   Glorot X., 2010, JLMR P TRACK, p[249, 2010], DOI DOI 10.1177/1753193409103364.
   Glorot Xavier, 2011, JWLR W CP
   Goodfellow IJ, 2013, ICML 2013
   Gulcehre C, 2013, ARXIV13111780
   Henrich J, 2003, EVOL ANTHROPOL, V12, P123, DOI 10.1002/evan.10110
   Hinton G. E, 2012, ARXIV12070580
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hsu C-W, 2003, PRACTICAL GUIDE SUPP
   Jarrett K, 2009, IEEE I CONF COMP VIS, P2146, DOI 10.1109/ICCV.2009.5459469
   Khan F, 2011, ADV NEURAL INFORM PR, V24, P1449
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   Krueger KA, 2009, COGNITION, V110, P380, DOI 10.1016/j.cognition.2008.11.014
   Kunapuli Gautam, 2010, P C ART NEUR NETW EN
   Larochelle H, 2009, J MACH LEARN RES, V10, P1
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Mitchell T. M., 1980, NEED BIASES LEARNING
   Mitchell T. M., 1993, ADV NEURAL INFORM PR, P287
   MONTAGUE R, 1970, THEORIA, V36, P373
   Nair V, 2010, ICML, V27, P807, DOI DOI 10.0RG/PAPERS/432.PDF
   O'Sullivan Joseph, 1996, INTEGRATING INITIALI
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Peterson GB, 2004, J EXP ANAL BEHAV, V82, P317, DOI 10.1901/jeab.2004.82-317
   Rifai S, 2012, P 29 INT C MACH LEAR
   Rifai S, 2011, P 28 INT C MACH LEAR
   Salakhutdinov R, 2009, ARTIF INTELL, P448, DOI DOI 10.1109/CVPRW.2009.5206577
   SKINNER BF, 1958, AM PSYCHOL, V13, P94, DOI 10.1037/h0049039
   Solomonoff R. J., 1989, P 6 ISR C ART INT CO, P515
   Tieleman T., 2012, COURSERA NEURAL NETW, V4
   TOWELL GG, 1994, ARTIF INTELL, V70, P119, DOI 10.1016/0004-3702(94)90105-8
   Vincent Pascal, J MACHINE LEARNING R, P3371
   von Ahn L, 2003, LECT NOTES COMPUT SC, V2656, P294
   Weston J, 2008, P 25 INT C MACH LEAR, P1168, DOI DOI 10.1145/1390156.1390303
   Zeiler Matthew D, 2012, ARXIV12125701
NR 54
TC 20
Z9 21
U1 0
U2 0
PU MICROTOME PUBL
PI BROOKLINE
PA 31 GIBBS ST, BROOKLINE, MA 02446 USA
SN 1532-4435
J9 J MACH LEARN RES
JI J. Mach. Learn. Res.
PY 2016
VL 17
AR 8
PG 32
WC Automation & Control Systems; Computer Science, Artificial Intelligence
SC Automation & Control Systems; Computer Science
GA EH0PU
UT WOS:000391467000001
DA 2020-02-19
ER

PT J
AU Henao, R
   Lu, JT
   Lucas, JE
   Ferranti, J
   Carin, L
AF Henao, Ricardo
   Lu, James T.
   Lucas, Joseph E.
   Ferranti, Jeffrey
   Carin, Lawrence
TI Electronic Health Record Analysis via Deep Poisson Factor Models
SO JOURNAL OF MACHINE LEARNING RESEARCH
LA English
DT Article
DE Deep learning; multi-modality learning; Poisson factor model; electronic
   health records; phenotyping
ID CORONARY-HEART-DISEASE; UKPDS OUTCOMES MODEL; RISK ENGINE; PHENOTYPING
   ALGORITHMS; CARDIOVASCULAR-DISEASE; PERFORMANCE; VALIDATION; GENERATION;
   PREDICTION
AB Electronic Health Record (EHR) phenotyping utilizes patient data captured through normal medical practice, to identify features that may represent computational medical phenotypes. These features may be used to identify at-risk patients and improve prediction of patient morbidity and mortality. We present a novel deep multi-modality architecture for EHR analysis (applicable to joint analysis of multiple forms of EHR data), based on Poisson Factor Analysis (PFA) modules. Each modality, composed of observed counts, is represented as a Poisson distribution, parameterized in terms of hidden binary units. Information from different modalities is shared via a deep hierarchy of common hidden units. Activation of these binary units occurs with probability characterized as Bernoulli-Poisson link functions, instead of more traditional logistic link functions. In addition, we demonstrate that PFA modules can be adapted to discriminative modalities. To compute model parameters, we derive efficient Markov Chain Monte Carlo (MCMC) inference that scales efficiently, with significant computational gains when compared to related models based on logistic link functions. To explore the utility of these models, we apply them to a subset of patients from the Duke-Durham patient cohort. We identified a cohort of over 16,000 patients with Type 2 Diabetes Mellitus (T2DM) based on diagnosis codes and laboratory tests out of our patient population of over 240,000. Examining the common hidden units uniting the PFA modules, we identify patient features that represent medical concepts. Experiments indicate that our learned features are better able to predict mortality and morbidity than clinical features identified previously in a large-scale clinical trial.
C1 [Henao, Ricardo; Lucas, Joseph E.; Carin, Lawrence] Duke Univ, Duke Elect & Comp Engn, Durham, NC 27708 USA.
   [Lu, James T.] Duke Univ, Duke Elect & Comp Engn, Duke Sch Med, Durham, NC 27708 USA.
   [Ferranti, Jeffrey] Duke Univ, Duke Sch Med, Durham, NC 27708 USA.
RP Henao, R (reprint author), Duke Univ, Duke Elect & Comp Engn, Durham, NC 27708 USA.
EM R.HENAO@DUKE.EDU; JAMES.LU@DUKE.EDU; JOE@STAT.DUKE.EDU;
   JEFFREY.FERRANTI@DUKE.EDU; LCARIN@DUKE.EDU
OI Henao, Ricardo/0000-0003-4980-845X
FU Information Initiative at Duke; Duke Clinical Research Institute; Duke
   Medicine
FX We would like to acknowledge support for this project from the
   Information Initiative at Duke, Duke Clinical Research Institute and
   Duke Medicine.
CR Amer Diabet Assoc, 2010, DIABETES CARE, V33, pS62, DOI [10.2337/dc11-S062, 10.2337/dc10-S062, 10.2337/dc10-s062]
   Blei D, 2004, ADV NEURAL INFORM PR
   Blei DM, 2007, ANN APPL STAT, V1, P17, DOI 10.1214/07-AOAS114
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Carvalho CM, 2008, J AM STAT ASSOC, V103, P1438, DOI 10.1198/016214508000000869
   Chen T., 2014, INT C MACH LEARN
   Chen YK, 2013, J AM MED INFORM ASSN, V20, pE253, DOI 10.1136/amiajnl-2013-001945
   Clarke PM, 2004, DIABETOLOGIA, V47, P1747, DOI 10.1007/s00125-004-1527-z
   Collett D., 2002, MODELLING BINARY DAT
   Ding N., 2014, ADV NEURAL INFORM PR
   ESCOBAR MD, 1995, J AM STAT ASSOC, V90, P577, DOI 10.2307/2291069
   Fawcett T, 2006, PATTERN RECOGN LETT, V27, P861, DOI 10.1016/j.patrec.2005.10.010
   Friedman J, 2001, SPRINGER SERIES STAT
   Gan Zhe, 2015, INT C MACH LEARN
   Guhaniyogi R., 2014, ARXIV14013632
   Henao R, 2011, J MACH LEARN RES, V12, P863
   Henao Ricardo, 2015, ADV NEURAL INFORM PR
   Henao Ricardo, 2014, ADV NEURAL INFORM PR
   Hinton G. E., 2009, ADV NEURAL INFORM PR
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Ho JC, 2014, J BIOMED INFORM, V52, P199, DOI 10.1016/j.jbi.2014.07.001
   Ho Joyce C., 2014, INT C KNOWL DISC DAT
   Ho Qirong, 2013, ADV NEURAL INFORM PR
   Hoffman M., 2013, J MACHINE LEARNING R
   Hoffman M., 2010, ADV NEURAL INFORM PR
   Hripcsak G, 2013, J AM MED INFORM ASSN, V20, P117, DOI 10.1136/amiajnl-2012-001145
   King P, 1999, BRIT J CLIN PHARMACO, V48, P643, DOI 10.1046/j.1365-2125.1999.00092.x
   Lacoste-Julien S, 2009, ADV NEURAL INFORM PR
   Larochelle H., 2012, ADV NEURAL INFORM PR
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li M., 2014, ADV NEURAL INFORM PR
   Lu SE, 2012, BMC ENDOCR DISORD, V12, DOI 10.1186/1472-6823-12-12
   Maaloe Lars, 2015, ARXIV150104325
   Mareedu Ravi K, 2009, Prev Cardiol, V12, P88, DOI 10.1111/j.1751-7141.2009.00028.x
   Mcauliffe J. D., 2008, ADV NEURAL INFORM PR
   Metcalf PA, 2008, NEW ZEAL MED J, V121, P49
   NEAL RM, 1992, ARTIF INTELL, V56, P71, DOI 10.1016/0004-3702(92)90065-6
   Nelson SJ, 2011, J AM MED INFORM ASSN, V18, P441, DOI 10.1136/amiajnl-2011-000116
   Newton KM, 2013, J AM MED INFORM ASSN, V20, pE147, DOI 10.1136/amiajnl-2012-000896
   Paisley J, 2015, IEEE T PATTERN ANAL, V37, P256, DOI 10.1109/TPAMI.2014.2318728
   PIEGORSCH WW, 1992, AM STAT, V46, P94, DOI 10.2307/2684172
   Ranganath R., 2014, INT C ART INT STAT
   Richesson RL, 2013, J AM MED INFORM ASSN, V20, pE319, DOI 10.1136/amiajnl-2013-001952
   Simmons RK, 2009, DIABETES CARE, V32, P708, DOI 10.2337/dc08-1918
   Sohn Kihyuk, 2014, ADV NEURAL INFORM PR, P2141
   Srivastava N., 2012, ADV NEURAL INFORM PR
   Srivastava N, 2014, J MACH LEARN RES, V15, P2949
   Srivastava Nitish, 2013, UNCERTAINTY ARTIFICI
   Stevens RJ, 2001, CLIN SCI, V101, P671, DOI 10.1042/CS20000335
   Tao LB, 2013, VALUE HEALTH, V16, P1074, DOI 10.1016/j.jval.2013.06.001
   Teh YW, 2006, J AM STAT ASSOC, V101, P1566, DOI 10.1198/016214506000000302
   van Dieren S, 2011, DIABETOLOGIA, V54, P264, DOI 10.1007/s00125-010-1960-0
   Vreeman Daniel J., 2015, J AM MED INFORM ASS
   Welling M., 2011, INT C MACH LEARN
   Williamson Sinead, 2010, INT C MACH LEARN
   Wilson PWF, 1998, CIRCULATION, V97, P1837, DOI 10.1161/01.CIR.97.18.1837
   Yuan Xin, 2015, INT C MACH LEARN
   Zheng Y, 2014, PROC CVPR IEEE, P1370, DOI 10.1109/CVPR.2014.178
   ZHOU M., 2012, INT C ART INT STAT
   Zhou MY, 2015, IEEE T PATTERN ANAL, V37, P307, DOI 10.1109/TPAMI.2013.211
   Zhou Mingyuan, 2015, INT C ART INT STAT
NR 61
TC 3
Z9 3
U1 0
U2 0
PU MICROTOME PUBL
PI BROOKLINE
PA 31 GIBBS ST, BROOKLINE, MA 02446 USA
SN 1532-4435
J9 J MACH LEARN RES
JI J. Mach. Learn. Res.
PY 2016
VL 17
AR 186
PG 32
WC Automation & Control Systems; Computer Science, Artificial Intelligence
SC Automation & Control Systems; Computer Science
GA EH5PO
UT WOS:000391825400001
DA 2020-02-19
ER

PT J
AU Lapuschkin, S
   Binder, A
   Montavon, G
   Muller, KR
   Samek, W
AF Lapuschkin, Sebastian
   Binder, Alexander
   Montavon, Gregoire
   Mueller, Klaus-Robert
   Samek, Wojciech
TI The LRP Toolbox for Artificial Neural Networks
SO JOURNAL OF MACHINE LEARNING RESEARCH
LA English
DT Article
DE layer-wise relevance propagation; explaining classifiers; deep learning;
   artificial neural networks; computer vision
AB The Layer-wise Relevance Propagation (LRP) algorithm explains a classifier's prediction specific to a given data point by attributing relevance scores to important components of the input by using the topology of the learned model itself. With the LRP Toolbox we provide platform-agnostic implementations for explaining the predictions of pre-trained state of the art Caffe networks and stand-alone implementations for fully connected Neural Network models. The implementations for Matlab and python shall serve as a playing field to familiarize oneself with the LRP algorithm and are implemented with readability and transparency in mind. Models and data can be imported and exported using raw text formats, Matlab's. mat files and the. npy format for numpy or plain text.
C1 [Lapuschkin, Sebastian; Samek, Wojciech] Fraunhofer Heinrich Hertz Inst, Video Coding & Analyt, D-10587 Berlin, Germany.
   [Binder, Alexander] Singapore Univ Technol, ISTD, Singapore 487372, Singapore.
   [Montavon, Gregoire; Mueller, Klaus-Robert] Berlin Inst Technol, Machine Learning Grp, D-10623 Berlin, Germany.
   [Mueller, Klaus-Robert] Korea Univ, Dept Brain & Cognit Engn, Seoul 02841, South Korea.
RP Lapuschkin, S (reprint author), Fraunhofer Heinrich Hertz Inst, Video Coding & Analyt, D-10587 Berlin, Germany.
EM SEBASTIAN.LAPUSCHKIN@HHI.FRAUNHOFER.DE; ALEXANDER_BINDER@SUTD.EDU.SG;
   GREGOIRE.MONTAVON@TU-BERLIN.DE; KLAUS-ROBERT.MUELLER@TU-BERLIN.DE;
   WOJCIECH.SAMEK@HHI.FRAUNHOFER.DE
RI Mueller, Klaus-Robert/Y-3547-2019
OI Lapuschkin, Sebastian/0000-0002-0762-7258; Binder,
   Alexander/0000-0001-9605-6209; Mueller, Klaus-Robert/0000-0002-3861-7685
FU Federal Ministry for Education and Research (BMBF)Federal Ministry of
   Education & Research (BMBF) [01IS14013A-E, 01GQ1115]; Deutsche
   Forschungsgesellschaft (DFG)German Research Foundation (DFG) [MU
   987/19-1]; Brain Korea 21 Plus Program by the Korean Government
FX The work was funded in part by the Federal Ministry for Education and
   Research (BMBF) under Grant 01IS14013A-E and Grant 01GQ1115, as well as
   by the Deutsche Forschungsgesellschaft (DFG) under Grant MU 987/19-1 and
   the Brain Korea 21 Plus Program by the Korean Government. Correspondence
   to KRM and WS.
CR Bach S, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0130140
   Baehrens D, 2010, J MACH LEARN RES, V11, P1803
   Chatfield K., 2014, BMVC
   Ciresan D, 2012, PROC CVPR IEEE, P3642, DOI 10.1109/CVPR.2012.6248110
   Ciresan Dan C., 2012, NIPS, P2852
   Collobert R, 2011, J MACH LEARN RES, V12, P2493
   Jia Yangqing, 2014, ARXIV14085093
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   MONTAVON G, 2012, LNCS, V7700
   Montavon G, 2013, NEW J PHYS, V15, DOI 10.1088/1367-2630/15/9/095003
   Montavon Gregoire, 2015, CORR
   Rasmussen Peter M, 2012, BIOSIGNALS
   Samek W., 2015, CORR
   Simonyan K, 2013, CORR
   Socher R., 2013, P C EMP METH NAT LAN, V1631, P1642
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Yu D, 2015, SIGNALS COMMUN TECHN, P1, DOI 10.1007/978-1-4471-5779-3
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
NR 18
TC 22
Z9 22
U1 0
U2 2
PU MICROTOME PUBL
PI BROOKLINE
PA 31 GIBBS ST, BROOKLINE, MA 02446 USA
SN 1532-4435
J9 J MACH LEARN RES
JI J. Mach. Learn. Res.
PY 2016
VL 17
AR 114
PG 5
WC Automation & Control Systems; Computer Science, Artificial Intelligence
SC Automation & Control Systems; Computer Science
GA EH1TS
UT WOS:000391550900001
DA 2020-02-19
ER

PT J
AU Uria, B
   Cote, MA
   Gregor, K
   Murray, I
   Larochelle, H
AF Uria, Benigno
   Cote, Marc-Alexandre
   Gregor, Karol
   Murray, Iain
   Larochelle, Hugo
TI Neural Autoregressive Distribution Estimation
SO JOURNAL OF MACHINE LEARNING RESEARCH
LA English
DT Article
DE deep learning; neural networks; density modeling; unsupervised learning
ID ALGORITHM; MODELS
AB We present Neural Autoregressive Distribution Estimation (NADE) models, which are neural network architectures applied to the problem of unsupervised distribution and density estimation. They leverage the probability product rule and a weight sharing scheme inspired from restricted Boltzmann machines, to yield an estimator that is both tractable and has good generalization performance. We discuss how they achieve competitive performance in modeling both binary and real-valued observations. We also present how deep NADE models can be trained to be agnostic to the ordering of input dimensions used by the autoregressive product rule decomposition. Finally, we also show how to exploit the topological structure of pixels in images using a deep convolutional architecture for NADE.
C1 [Uria, Benigno; Gregor, Karol] Google Deep Mind, London, England.
   [Cote, Marc-Alexandre] Univ Sherbrooke, Dept Comp Sci, Sherbrooke, PQ J1K 2R1, Canada.
   [Murray, Iain] Univ Edinburgh, Sch Informat, Edinburgh EH8 9AB, Midlothian, Scotland.
   [Larochelle, Hugo] Twitter, 141 Portland St,Floor 6, Cambridge, MA 02139 USA.
RP Uria, B (reprint author), Google Deep Mind, London, England.
EM BENIGNO.URIA@GMAIL.COM; MARC-ALEXANDRE.COTE@USHERBROOKE.CA;
   KAROL.GREGOR@GMAIL.COM; I.MURRAY@ED.AC.UK; HLAROCHELLE@TWITTER.COM
CR Al-Rfou R., 2016, ARXIV160502688
   Bache K., 2013, UCI MACHINE LEARNING
   Bengio Y, 2000, ADV NEUR IN, V12, P400
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   BESAG J, 1975, STATISTICIAN, V24, P179, DOI 10.2307/2987782
   Bishop C. M., 1994, 4288 NCRG AST U
   Bornschein Jorg, 2015, P 3 INT C LEARN REPR
   Boulanger-lewandowski N., 2012, P 29 INT C MACH LEAR, P1159
   Burda Yuri, 2016, P 4 INT C LEARN REPR
   Cho K, 2013, NEURAL COMPUT, V25, P805, DOI 10.1162/NECO_a_00397
   Cho K, 2010, IEEE IJCNN
   CHOW CK, 1968, IEEE T INFORM THEORY, V14, P462, DOI 10.1109/TIT.1968.1054142
   Dahl GE, 2013, INT CONF ACOUST SPEE, P8609, DOI 10.1109/ICASSP.2013.6639346
   DAYAN P, 1995, NEURAL COMPUT, V7, P889, DOI 10.1162/neco.1995.7.5.889
   Denton E.L., 2015, ADV NEURAL INFORM PR, P1486
   Desjardins G., 2010, P 13 INT C ART INT S, V9, P145
   Freund Y., 1992, ADV NEURAL INFORM PR, P912
   Frey BJ, 1996, ADV NEUR IN, V8, P661
   GAROFOLO J. S, 1993, DARPA TIMIT ACOUSTIC
   Germain M., 2015, ICML, P881
   Ghahramani Z., 1996, CRGTR961 U TOR
   Goodfellow IJ, 2014, ADV NEURAL INFORM PR, P2672, DOI DOI 10.1017/CB09781139058452
   Graves Alex, 2011, ADV NEURAL INFORM PR, V24, P2348
   Gregor K., 2014, P INT C MACH LEARN, P1242
   Gregor K., 2015, P 32 INT C MACH LEAR, P1462
   Gregor Karol, 2011, TECHNICAL REPORT
   Gretton A., 2007, ADV NEURAL INFORM PR, P513
   Gutmann Michael, 2010, P 13 INT C ART INT S, P297
   Harmeling S, 2011, IEEE T PATTERN ANAL, V33, P1087, DOI 10.1109/TPAMI.2010.145
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   HINTON GE, 1995, SCIENCE, V268, P1158, DOI 10.1126/science.7761831
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hyvarinen A, 2005, J MACH LEARN RES, V6, P695
   Hyvarinen A, 2007, COMPUT STAT DATA AN, V51, P2499, DOI 10.1016/j.csda.2006.09.003
   Hyvarinen A, 2007, IEEE T NEURAL NETWOR, V18, P1529, DOI 10.1109/TNN.2007.895819
   Kingma D. P., 2014, P 2 INT C LEARN REPR
   Kingma D. P., 2015, P 3 INT C LEARN REPR
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   Larochelle H., 2011, ARTIF INTELL, P29
   Larochelle H, 2012, ADV NEURAL INFORM PR, P2708
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Li Y., 2015, P 32 INT C MACH LEAR, P1718
   Marlin B. M., 2010, P 13 INT C ART INT S
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Montavon Gregoire, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P621, DOI 10.1007/978-3-642-35289-8_33
   NEAL RM, 1992, ARTIF INTELL, V56, P71, DOI 10.1016/0004-3702(92)90065-6
   Ngiam J, 2011, P 28 INT C MACH LEAR, P1105
   Ormoneit D, 1995, 9 ANN C NIPS DENV CO, V8, P542
   Raiko Tapani, 2014, ADV NEURAL INFORM PR, V27, P325
   Rezende D.J., 2014, P 31 INT C MACH LEAR
   Salakhutdinov R., 2010, P 27 INT C MACH LEAR, P943
   Salakhutdinov R., 2009, ADV NEURAL INFORM PR, P1598
   Salakhutdinov R., 2008, P 25 INT C MACH LEAR, P872, DOI [10.1145/1390156.1390266, DOI 10.1145/1390156.1390266]
   Salakhutdinov R, 2009, ARTIF INTELL, P448, DOI DOI 10.1109/CVPRW.2009.5206577
   Salakhutdinov Ruslan, 2010, INT C ART INT STAT, P693
   Silva Ricardo, 2011, P 14 INT C ART INT S, V15, P670
   Smolensky P, 1986, PARALLEL DISTRIBUTED, P194
   Smyth P, 1999, MACH LEARN, V36, P59, DOI 10.1023/A:1007511322260
   Sohl-Dickstein J, 2011, P 28 INT C MACH LEAR, V2011, P905
   Springenberg J. T., 2015, P 3 INT C LEARN REPR
   Tang Yichuan, 2012, P 29 INT C MACH LEAR, P505
   Theis Lucas, 2015, ADV NEURAL INFORM PR, P1927
   Tieleman T., 2008, P 25 INT C MACH LEAR, P1064, DOI DOI 10.1145/1390156.1390290
   Tieleman T., 2009, P 26 ANN INT C MACH, P1033, DOI DOI 10.1145/1553374.1553506
   Uria B., 2013, ADV NEURAL INFORM PR, V26, P2175
   Uria Benigno, 2014, P 31 INT C MACH LEAR, P467
   Uria Benigno, 2015, THESIS
   Van Den Oord Aaron, 2016, P 33 INT C IN PRESS
   Verbeek Jakob, 2005, MIXTURE FACTOR ANAL
   Vincent P, 2008, P 25 INT C MACH LEAR, P1096, DOI DOI 10.1145/1390156.1390294
   Welling M., 2005, ADV NEURAL INFORM PR, P1481
   YOUNES L, 1989, PROBAB THEORY REL, V82, P625, DOI 10.1007/BF00341287
   Zheng Y, 2016, IEEE T PATTERN ANAL, V38, P1056, DOI 10.1109/TPAMI.2015.2476802
   Zheng Y, 2015, INT J COMPUT VISION, V113, P67, DOI 10.1007/s11263-014-0765-x
   Zoran D, 2012, ADV NEURAL INFORM PR, V25, P1745
   Zoran D, 2011, IEEE I CONF COMP VIS, P479, DOI 10.1109/ICCV.2011.6126278
NR 76
TC 6
Z9 6
U1 0
U2 0
PU MICROTOME PUBL
PI BROOKLINE
PA 31 GIBBS ST, BROOKLINE, MA 02446 USA
SN 1532-4435
J9 J MACH LEARN RES
JI J. Mach. Learn. Res.
PY 2016
VL 17
AR 205
PG 37
WC Automation & Control Systems; Computer Science, Artificial Intelligence
SC Automation & Control Systems; Computer Science
GA EH5RX
UT WOS:000391831700001
DA 2020-02-19
ER

PT S
AU Bloice, MD
   Holzinger, A
AF Bloice, Marcus D.
   Holzinger, Andreas
BE Holzinger, A
TI A Tutorial on Machine Learning and Data Science Tools with Python
SO MACHINE LEARNING FOR HEALTH INFORMATICS: STATE-OF-THE-ART AND FUTURE
   CHALLENGES
SE Lecture Notes in Computer Science
LA English
DT Article; Book Chapter
DE Machine learning; Deep learning; Neural networks; Tools; Languages;
   Python
AB In this tutorial, we will provide an introduction to the main Python software tools used for applying machine learning techniques to medical data. The focus will be on open-source software that is freely available and is cross platform. To aid the learning experience, a companion GitHub repository is available so that you can follow the examples contained in this paper interactively using Jupyter notebooks. The notebooks will be more exhaustive than what is contained in this chapter, and will focus on medical datasets and healthcare problems. Briefly, this tutorial will first introduce Python as a language, and then describe some of the lower level, general matrix and data structure packages that are popular in the machine learning and data science communities, such as NumPy and Pandas. From there, we will move to dedicated machine learning software, such as SciKit-Learn. Finally we will introduce the Keras deep learning and neural networks library. The emphasis of this paper is readability, with as little jargon used as possible. No previous experience with machine learning is assumed. We will use openly available medical datasets throughout.
C1 [Bloice, Marcus D.; Holzinger, Andreas] Med Univ Graz, Inst Med Informat Stat & Documentat, Holzinger Grp HCI KDD, Graz, Austria.
RP Bloice, MD (reprint author), Med Univ Graz, Inst Med Informat Stat & Documentat, Holzinger Grp HCI KDD, Graz, Austria.
EM marcus.bloice@medunigraz.at; andreas.holzinger@medunigraz.at
RI Holzinger, Andreas/E-9530-2010
OI Holzinger, Andreas/0000-0002-6786-5194
CR Bergstra J., 2010, P 9 PYTH SCI C, P1
   Ciresan DC, 2013, LECT NOTES COMPUT SC, V8150, P411, DOI 10.1007/978-3-642-40763-5_51
   Engblom S, 2016, PARALLEL COMPUT, V56, P1, DOI 10.1016/j.parco.2016.04.001
   Guo Philip, 2014, PYTHON IS NOW MOST P
   Hastie T., 2009, ELEMENTS STAT LEARNI
   Holmes G., 1994, Proceedings of the 1994 Second Australian and New Zealand Conference on Intelligent Information Systems (Cat. No.94TH8019), P357, DOI 10.1109/ANZIIS.1994.396988
   Holzinger Andreas, 2016, Brain Inform, V3, P119, DOI 10.1007/s40708-016-0042-6
   Holzinger A, 2014, BMC BIOINFORMATICS, V15, DOI 10.1186/1471-2105-15-S6-I1
   Jia Yangqing, 2014, ARXIV14085093
   Jordan MI, 2015, SCIENCE, V349, P255, DOI 10.1126/science.aaa8415
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   McKinney W., 2012, PYTHON DATA ANAL
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Read J, 2016, J MACH LEARN RES, V17
   Wolfram S., 1991, MATH SYST DOING MATH
NR 15
TC 1
Z9 1
U1 4
U2 10
PU SPRINGER INTERNATIONAL PUBLISHING AG
PI CHAM
PA GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
SN 0302-9743
EI 1611-3349
BN 978-3-319-50478-0; 978-3-319-50477-3
J9 LECT NOTES COMPUT SC
PY 2016
VL 9605
BP 435
EP 480
DI 10.1007/978-3-319-50478-0_22
D2 10.1007/978-3-319-50478-0
PG 46
WC Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Medical Informatics
SC Computer Science; Medical Informatics
GA BI2EL
UT WOS:000408904100023
OA Green Published
DA 2020-02-19
ER

PT J
AU Wang, J
   Liu, W
   Kumar, S
   Chang, SF
AF Wang, Jun
   Liu, Wei
   Kumar, Sanjiv
   Chang, Shih-Fu
TI Learning to Hash for Indexing Big Data-A Survey
SO PROCEEDINGS OF THE IEEE
LA English
DT Article
DE Approximate nearest neighbor (ANN) search; deep learning; learning to
   hash; semisupervised learning; supervised learning; unsupervised
   learning
ID BINARY-CODES; ITERATIVE QUANTIZATION; PRODUCT QUANTIZATION; PROCRUSTEAN
   APPROACH; SEARCH; SELECTION; RANKING
AB The explosive growth in Big Data has attracted much attention in designing efficient indexing and search methods recently. In many critical applications such as large-scale search and pattern matching, finding the nearest neighbors to a query is a fundamental research problem. However, the straightforward solution using exhaustive comparison is infeasible due to the prohibitive computational complexity and memory requirement. In response, approximate nearest neighbor (ANN) search based on hashing techniques has become popular due to its promising performance in both efficiency and accuracy. Prior randomized hashing methods, e.g., locality-sensitive hashing (LSH), explore data-independent hash functions with random projections or permutations. Although having elegant theoretic guarantees on the search quality in certain metric spaces, performance of randomized hashing has been shown insufficient in many real-world applications. As a remedy, new approaches incorporating data-driven learning methods in development of advanced hash functions have emerged. Such learning-to-hash methods exploit information such as data distributions or class labels when optimizing the hash codes or functions. Importantly, the learned hash codes are able to preserve the proximity of neighboring data in the original feature spaces in the hash code spaces. The goal of this paper is to provide readers with systematic understanding of insights, pros, and cons of the emerging techniques. We provide a comprehensive survey of the learning-to-hash framework and representative techniques of various types, including unsupervised, semisupervised, and supervised. In addition, we also summarize recent hashing approaches utilizing the deep learning models. Finally, we discuss the future direction and trends of research in this area.
C1 [Wang, Jun] E China Normal Univ, Sch Comp Sci & Software Engn, Shanghai 200062, Peoples R China.
   [Liu, Wei] IBM Corp, TJ Watson Res Ctr, Yorktown Hts, NY 10598 USA.
   [Kumar, Sanjiv] Google Res, New York, NY 10011 USA.
   [Chang, Shih-Fu] Columbia Univ, Dept Elect Engn & Comp Sci, New York, NY 10027 USA.
RP Wang, J (reprint author), E China Normal Univ, Sch Comp Sci & Software Engn, Shanghai 200062, Peoples R China.
EM wongjun@gmail.com; wliu@ee.columbia.edu; sanjivk@google.com;
   sfchang@ee.columbia.edu
CR Athitsos V, 2008, IEEE T PATTERN ANAL, V30, P89, DOI 10.1109/TPAMI.2007.1140
   Basri R, 2011, IEEE T PATTERN ANAL, V33, P266, DOI 10.1109/TPAMI.2010.110
   Bawa M., 2005, P 14 INT C WORLD WID, P651, DOI DOI 10.1145/1060745.1060840
   Bellman R., 1957, DYNAMIC PROGRAMMING
   BENTLEY JL, 1975, COMMUN ACM, V18, P509, DOI 10.1145/361002.361007
   Broder A. Z., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P327, DOI 10.1145/276698.276781
   Broder AZ, 1998, COMPRESSION AND COMPLEXITY OF SEQUENCES 1997 - PROCEEDINGS, P21, DOI 10.1109/SEQUEN.1997.666900
   Bronstein MM, 2010, PROC CVPR IEEE, P3594, DOI 10.1109/CVPR.2010.5539928
   Cao L., 2012, P 20 ACM INT C MULT, P299, DOI DOI 10.1145/2393347.2393393
   Cayton L., 2008, P ADV NEUR INF PROC, V20, P233
   Charikar M.S., 2002, P 34 ANN ACM S THEOR, P380, DOI DOI 10.1145/509907.509965
   Chen W., 2015, INT C MACH LEARN, V37, P2285
   Chum O, 2008, BMVC, P812, DOI DOI 10.5244/C.22.50
   Chum O, 2012, PROC CVPR IEEE, P3077, DOI 10.1109/CVPR.2012.6248039
   Chum O, 2009, PROC CVPR IEEE, P17, DOI 10.1109/CVPRW.2009.5206531
   Dai Q, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3430
   Das A. S., 2007, P 16 INT C WORLD WID, P271, DOI [10.1145/1242572.1242610, DOI 10.1145/1242572.1242610]
   Dasgupta A., 2011, P 17 ACM SIGKDD INT, P1073
   Datar M, 2004, P 20 ANN S COMP GEOM, P253, DOI DOI 10.1145/997817.997857
   Datta R, 2008, ACM COMPUT SURV, V40, DOI 10.1145/1348246.1348248
   Davis J. V., 2007, P 24 INT C MACH LEAR, P209, DOI DOI 10.1145/1273496.1273523
   Dong W., 2008, P 17 ACM C INF KNOWL, P669, DOI DOI 10.1145/1458082.1458172
   Fan LX, 2013, IEEE I CONF COMP VIS, P2616, DOI 10.1109/ICCV.2013.325
   Fowlkes C, 2004, IEEE T PATTERN ANAL, V26, P214, DOI 10.1109/TPAMI.2004.1262185
   Freund Y., 1995, COMPUTATIONAL LEARNI, V904, P23, DOI DOI 10.1007/3-540-59119-2_166
   Ge TZ, 2013, PROC CVPR IEEE, P2946, DOI 10.1109/CVPR.2013.379
   Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518
   Gong Y., 2012, P ADV NEUR INF PROC, P1205
   Gong YC, 2013, PROC CVPR IEEE, P484, DOI 10.1109/CVPR.2013.69
   Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193
   Gong YC, 2011, PROC CVPR IEEE, P817, DOI 10.1109/CVPR.2011.5995432
   Gordo A, 2011, PROC CVPR IEEE, P729, DOI 10.1109/CVPR.2011.5995505
   Grauman K., 2013, MACHINE LEARNING COM, P49, DOI DOI 10.1007/978-3-642-28661-2_3
   Grauman K, 2010, COMMUN ACM, V53, P84, DOI 10.1145/1743546.1743570
   Gray R. M., 2006, TOEPLITZ CIRCULANT M
   Gregor K., 2010, P 27 INT C MACH LEAR, P399
   Gu SM, 2013, INT CONF MACH LEARN, P108, DOI 10.1109/ICMLC.2013.6890453
   He J., 2010, P 16 ACM SIGKDD INT, P1129
   He J., 2012, P 29 INT C MACH LEAR, P1127
   He JF, 2012, PROC CVPR IEEE, P3005, DOI 10.1109/CVPR.2012.6248030
   He JF, 2011, PROC CVPR IEEE, P753, DOI 10.1109/CVPR.2011.5995518
   He KM, 2013, PROC CVPR IEEE, P2938, DOI 10.1109/CVPR.2013.378
   Henzinger M., 2006, Proceedings of the Twenty-Ninth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P284, DOI 10.1145/1148170.1148222
   Heo JP, 2012, PROC CVPR IEEE, P2957, DOI 10.1109/CVPR.2012.6248024
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Indyk P., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P604, DOI 10.1145/276698.276876
   Indyk P., 2004, HDB DISCRETE COMPUTA
   Ioffe S, 2010, Proceedings 2010 10th IEEE International Conference on Data Mining (ICDM 2010), P246, DOI 10.1109/ICDM.2010.80
   Jain P., 2008, P ADV NEUR INF PROC, P761
   Jain P., 2008, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2008.4587841
   Jain Prateek, 2010, ADV NEURAL INFORM PR, P928
   Jegou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039
   Jegou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Ji J., 2012, P ADV NEUR INF PROC, V25, P108
   Jiang Y.-G., 2011, P ACM INT C MULT RET, DOI [10.1145/1991996.1992012, DOI 10.1145/1991996.1992012]
   Jiang YG, 2013, IEEE T MULTIMEDIA, V15, P442, DOI 10.1109/TMM.2012.2231061
   Jin ZM, 2013, IEEE I CONF COMP VIS, P257, DOI 10.1109/ICCV.2013.39
   Joly A, 2011, PROC CVPR IEEE, P873, DOI 10.1109/CVPR.2011.5995709
   Kim S, 2012, LECT NOTES COMPUT SC, V7576, P538, DOI 10.1007/978-3-642-33715-4_39
   Knuth DE, 1997, ART COMPUTER PROGRAM
   Kong W., 2012, P ADV NEUR INF PROC, P1655
   Korman S, 2011, IEEE I CONF COMP VIS, P1607, DOI 10.1109/ICCV.2011.6126421
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   Kulis B., 2009, P ADV NEUR INF PROC, P1042
   Kulis B, 2012, IEEE T PATTERN ANAL, V34, P1092, DOI 10.1109/TPAMI.2011.219
   Kulis B, 2009, IEEE I CONF COMP VIS, P2130, DOI 10.1109/ICCV.2009.5459466
   Kulis B, 2009, IEEE T PATTERN ANAL, V31, P2143, DOI 10.1109/TPAMI.2009.151
   Lai HJ, 2015, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2015.7298947
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lee D. C., 2012, P EUR C COMPUT VIS, P648
   Li H, 2014, PROCEEDINGS OF THE 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 2, P495
   Li P., 2010, ADV NEURAL INFORM PR, V23, P1387
   Li P., 2012, P 26 ADV NEUR INF PR, P3122
   Li P., 2010, P 19 INT C WORLD WID, P671
   Li P., 2011, ADV NEURAL INFORM PR, P2672
   Li X, 2013, PROC CVPR IEEE, P2419, DOI 10.1109/CVPR.2013.313
   Lin GS, 2013, IEEE I CONF COMP VIS, P2552, DOI 10.1109/ICCV.2013.317
   Lin RS, 2010, PROC CVPR IEEE, P848, DOI 10.1109/CVPR.2010.5540129
   Lin Y, 2013, PROC CVPR IEEE, P446, DOI 10.1109/CVPR.2013.64
   Liong VE, 2015, PROC CVPR IEEE, P2475, DOI 10.1109/CVPR.2015.7298862
   Liu W., 2010, P 27 INT C MACH LEAR, P679
   Liu W., 2012, P 29 INT C MACH LEAR, P17
   Liu W., 2014, P ADV NEUR INF PROC, P3419
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Liu W, 2011, SER INF MANAGE SCI, V10, P1
   Liu XL, 2013, PROC CVPR IEEE, P1570, DOI 10.1109/CVPR.2013.206
   Lv Q, 2007, P 33 INT C VER LARG, P950, DOI DOI 10.1145/1143844.1143857
   Manku G. S., 2007, P 16 INT C WORLD WID, P141, DOI DOI 10.1145/1242572.1242592
   Masci J., 2014, P INT C LEARN REPR, P1
   Mu Y., 2014, P IEEE INT C COMP VI, P446
   Mu YD, 2012, LECT NOTES COMPUT SC, V7572, P414, DOI 10.1007/978-3-642-33718-5_30
   Mu YD, 2010, PROCEEDINGS OF THE TWENTY-FOURTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-10), P539
   Mu YD, 2010, PROC CVPR IEEE, P3344, DOI 10.1109/CVPR.2010.5540024
   Norouzi M., 2011, INT C MACH LEARN, P353
   Norouzi M, 2012, P NIPS, P1070
   Norouzi M, 2012, PROC CVPR IEEE, P3108, DOI 10.1109/CVPR.2012.6248043
   Omohundro S. M., 1987, Complex Systems, V1, P273
   Ou MD, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P895, DOI 10.1145/2783258.2783283
   Ou MD, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P230
   Ou MD, 2015, PROCEEDINGS OF THE TWENTY-NINTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2894
   Qian BY, 2013, IEEE DATA MINING, P607, DOI 10.1109/ICDM.2013.54
   Raginsky M., 2009, ADV NEURAL INFORM PR, P1509
   RAJARAMAN A, 2012, MINING MASSIVE DATAS
   Rastegari M., 2013, ICML, P1328
   Rastegari M, 2012, LECT NOTES COMPUT SC, V7577, P876, DOI 10.1007/978-3-642-33783-3_63
   Salakhutdinov R., 2007, J MACHINE LEARNING R, P412
   Salakhutdinov R, 2009, INT J APPROX REASON, V50, P969, DOI 10.1016/j.ijar.2008.11.006
   Satuluri V, 2012, PROC VLDB ENDOW, V5, P430, DOI 10.14778/2140436.2140440
   Shakhnarovich G, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P750
   Shakhnarovich G., 2005, THESIS
   Shakhnarovich G., 2006, NEAREST NEIGHBOR MET
   Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598
   Shen FM, 2013, PROC CVPR IEEE, P1562, DOI 10.1109/CVPR.2013.205
   Shi QF, 2010, PROC CVPR IEEE, P2753, DOI 10.1109/CVPR.2010.5540001
   Shi QF, 2009, J MACH LEARN RES, V10, P2615
   Sivic J, 2008, PROC CVPR IEEE, P1, DOI 10.1109/CVPRW.2008.4562950
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   Song J, 2011, P 19 ACM INT C MULT, P423
   Strecha C, 2012, IEEE T PATTERN ANAL, V34, P66, DOI 10.1109/TPAMI.2011.103
   Tong Simon, 2001, J MACHINE LEARNING R, V2, P45, DOI DOI 10.1162/153244302760185243
   Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128
   Trzcinski T, 2012, LECT NOTES COMPUT SC, V7572, P228, DOI 10.1007/978-3-642-33718-5_17
   UHLMANN JK, 1991, INFORM PROCESS LETT, V40, P175, DOI 10.1016/0020-0190(91)90074-R
   Vijayanarasimhan S, 2014, IEEE T PATTERN ANAL, V36, P276, DOI 10.1109/TPAMI.2013.121
   Wang J, 2013, P 21 ACM INT C MULT, P133
   Wang J., 2010, P 27 INT C MACH LEAR, P1127
   Wang J, 2013, IEEE I CONF COMP VIS, P3032, DOI 10.1109/ICCV.2013.377
   Wang J, 2012, IEEE T PATTERN ANAL, V34, P2393, DOI 10.1109/TPAMI.2012.48
   Wang J, 2010, PROC CVPR IEEE, P3424, DOI 10.1109/CVPR.2010.5539994
   Wang Q., 2013, CIKM, P1185
   Wang QF, 2013, SIGIR'13: THE PROCEEDINGS OF THE 36TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH & DEVELOPMENT IN INFORMATION RETRIEVAL, P213
   Wang X, 2013, IEEE I CONF COMP VIS, P2776, DOI 10.1109/ICCV.2013.345
   Weihao Kong, 2012, Proceedings of the 35th Annual International ACM SIGIR Conference on Research & Development in Information Retrieval (SIGIR 2012), P45, DOI 10.1145/2348283.2348293
   Weiss P, 2008, INT CONF ACOUST SPEE, P1173, DOI 10.1109/ICASSP.2008.4517824
   Weiss Y, 2012, LECT NOTES COMPUT SC, V7576, P340, DOI 10.1007/978-3-642-33715-4_25
   Xia RK, 2014, PROCEEDINGS OF THE TWENTY-EIGHTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2156
   XU H, 2011, P IEEE INT C COMP VI, P1631
   Ye GN, 2013, IEEE I CONF COMP VIS, P2272, DOI 10.1109/ICCV.2013.282
   YIANILOS PN, 1993, PROCEEDINGS OF THE FOURTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P311
   Yu Felix, 2014, P 31 INT C MACH LEAR, P946
   Yuan JB, 2012, LECT NOTES COMPUT SC, V7583, P271, DOI 10.1007/978-3-642-33863-2_27
   ZHANG D, 2010, P 33 INT ACM SIGIR C, P18
   Zhang Dan, 2011, P 34 INT ACM SIGIR C, P225, DOI DOI 10.1145/2009916.2009950
   Zhang L, 2013, PROC CVPR IEEE, P1586, DOI 10.1109/CVPR.2013.208
   Zhang X, 2012, PROC CVPR IEEE, P2058, DOI 10.1109/CVPR.2012.6247910
   Zhao F, 2015, PROC CVPR IEEE, P1556, DOI 10.1109/CVPR.2015.7298763
   Zhen Y., 2012, P ADV NEUR INF PROC, P1385
   Zhen Y, 2012, P 18 ACM SIGKDD INT, P940, DOI DOI 10.1145/2339530.2339678
   Zhou K., 2012, KDD, P498
NR 149
TC 138
Z9 144
U1 4
U2 99
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 0018-9219
EI 1558-2256
J9 P IEEE
JI Proc. IEEE
PD JAN
PY 2016
VL 104
IS 1
SI SI
BP 34
EP 57
DI 10.1109/JPROC.2015.2487976
PG 24
WC Engineering, Electrical & Electronic
SC Engineering
GA CZ6ZS
UT WOS:000367250300004
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Ordonez, FJ
   Roggen, D
AF Ordonez, Francisco Javier
   Roggen, Daniel
TI Deep Convolutional and LSTM Recurrent Neural Networks for Multimodal
   Wearable Activity Recognition
SO SENSORS
LA English
DT Article
DE human activity recognition; wearable sensors; deep learning; machine
   learning; sensor fusion; LSTM; neural network
AB Human activity recognition (HAR) tasks have traditionally been solved using engineered features obtained by heuristic processes. Current research suggests that deep convolutional neural networks are suited to automate feature extraction from raw sensor inputs. However, human activities are made of complex sequences of motor movements, and capturing this temporal dynamics is fundamental for successful HAR. Based on the recent success of recurrent neural networks for time series domains, we propose a generic deep framework for activity recognition based on convolutional and LSTM recurrent units, which: (i) is suitable for multimodal wearable sensors; (ii) can perform sensor fusion naturally; (iii) does not require expert knowledge in designing features; and (iv) explicitly models the temporal dynamics of feature activations. We evaluate our framework on two datasets, one of which has been used in a public activity recognition challenge. Our results show that our framework outperforms competing deep non-recurrent networks on the challenge dataset by 4% on average; outperforming some of the previous reported results by up to 9%. Our results show that the framework can be applied to homogeneous sensor modalities, but can also fuse multimodal sensors to improve performance. We characterise key architectural hyperparameters' influence on performance to provide insights about their optimisation.
C1 [Ordonez, Francisco Javier; Roggen, Daniel] Univ Sussex, Sensor Technol Res Ctr, Wearable Technol, Brighton BN1 9RH, E Sussex, England.
RP Ordonez, FJ (reprint author), Univ Sussex, Sensor Technol Res Ctr, Wearable Technol, Brighton BN1 9RH, E Sussex, England.
EM F.Ordonez-Morales@sussex.ac.uk; daniel.roggen@ieee.org
FU Google Faculty Research Award grant "Is deep learning useful for
   wearable activity recognition?"; U.K. EPSRCFirst Grant [EP/N007816/1]
FX This work was partly funded by the Google Faculty Research Award grant
   "Is deep learning useful for wearable activity recognition?" and the
   U.K. EPSRCFirst Grant EP/N007816/1 "Lifelearn: Unbounded activity and
   context awareness". This publication is supported by multiple datasets,
   which are openly available at locations cited in the reference section.
CR Alsheikh MA, 2015, ARXIV151104664
   Avci Akin, 2010, P 23 INT C ARCH COMP, p[1, 10]
   Banos Oresti, 2014, Ambient Assisted Living and Daily Activities. 6th International Work-Conference, IWAAL 2014. Proceedings: LNCS 8868, P91, DOI 10.1007/978-3-319-13105-4_14
   Berchtold M, 2010, WEAR COMP ISWC 2010, P1, DOI DOI 10.1109/ISWC.2010.5665868
   Bulling A, 2014, ACM COMPUT SURV, V46, DOI 10.1145/2499621
   Chavarriaga R, 2013, PATTERN RECOGN LETT, V34, P2033, DOI 10.1016/j.patrec.2012.12.014
   Cheng K.-T., 2011, VLSI DES AUT TEST VL, P1, DOI DOI 10.1109/VDAT.2011.5783575
   Dauphin Yann N, 2015, ARXIV150204390
   Deng L., 2014, INTERSPEECH, P1915
   DIELEMAN S, 2015, LASAGNE 1 RELEASE
   Figo D, 2010, PERS UBIQUIT COMPUT, V14, P645, DOI 10.1007/s00779-010-0293-9
   Gers FA, 2003, J MACH LEARN RES, V3, P115, DOI 10.1162/153244303768966139
   Gordon D, 2014, PERS UBIQUIT COMPUT, V18, P205, DOI 10.1007/s00779-013-0638-2
   Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Japkowicz N., 2002, Intelligent Data Analysis, V6, P429
   Ordonez FJ, 2014, IEEE PERVAS COMPUT, V13, P67, DOI 10.1109/MPRV.2014.52
   Karpathy A., 2015, ARXIV150602078
   Kranz M, 2013, PERVASIVE MOB COMPUT, V9, P203, DOI 10.1016/j.pmcj.2012.06.002
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   LeCun Y., 1998, HDB BRAIN THEORY NEU, P255
   Lee H., 2008, P 22 ANN C ADV NEUR, P1096
   Lee H., 2009, P 26 ANN INT C MACH, P609, DOI DOI 10.1145/1553374.1553453
   Mazilu S., 2014, P ACM C HUM FACT COM
   Ng J, 2015, ARXIV150308909
   Palaz D, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P11
   Patel S, 2012, J NEUROENG REHABIL, V9, DOI 10.1186/1743-0003-9-21
   Pigou L., 2015, ARXIV150601911
   Plotz T., 2011, IJCAI P INT JOINT C, VTwo, P1729, DOI DOI 10.5591/978-1-57735-516-8/LICA111-290
   Preece S. J., 2009, PHYSIOL MEAS, V30, P21
   Rashidi P, 2009, IEEE T SYST MAN CY A, V39, P949, DOI 10.1109/TSMCA.2009.2025137
   Reiss A, 2012, IEEE INT SYM WRBL CO, P108, DOI 10.1109/ISWC.2012.13
   Roggen D, 2010, 2010 Seventh International Conference on Networked Sensing Systems (INSS 2010), P233, DOI 10.1109/INSS.2010.5573462
   Roggen D, 2015, LECT NOTES COMPUT SC, V8965, P151, DOI 10.1007/978-3-319-15582-1_10
   Sainath TN, 2015, INT CONF ACOUST SPEE, P4580, DOI 10.1109/ICASSP.2015.7178838
   Sainath TN, 2015, NEURAL NETWORKS, V64, P39, DOI 10.1016/j.neunet.2014.08.005
   Sermanet P., 2013, ARXIV13126229
   SIEGELMANN HT, 1991, APPL MATH LETT, V4, P77, DOI 10.1016/0893-9659(91)90080-F
   Stiefmeier T, 2008, IEEE PERVAS COMPUT, V7, P42, DOI 10.1109/MPRV.2008.40
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Toshev A, 2014, PROC CVPR IEEE, P1653, DOI 10.1109/CVPR.2014.214
   van den Oord Aaron, 2013, ADV NEURAL INFORM PR, P2643
   Welbourne E., 2014, P 2014 ACM INT JOINT, P873
   Yang JB, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3995
   Zappi P, 2008, LECT NOTES COMPUT SC, V4913, P17
   Zeng M, 2014, 2014 6TH INTERNATIONAL CONFERENCE ON MOBILE COMPUTING, APPLICATIONS AND SERVICES (MOBICASE), P197, DOI 10.4108/icst.mobicase.2014.257786
NR 46
TC 325
Z9 338
U1 34
U2 154
PU MDPI
PI BASEL
PA ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
SN 1424-8220
J9 SENSORS-BASEL
JI Sensors
PD JAN
PY 2016
VL 16
IS 1
AR 115
DI 10.3390/s16010115
PG 25
WC Chemistry, Analytical; Engineering, Electrical & Electronic; Instruments
   & Instrumentation
SC Chemistry; Engineering; Instruments & Instrumentation
GA DE5OC
UT WOS:000370679800049
PM 26797612
OA DOAJ Gold, Green Published, Green Accepted
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Hu, JY
   Zhang, JS
   Zhang, CX
   Wang, J
AF Hu, Junying
   Zhang, Jiangshe
   Zhang, Chunxia
   Wang, Juan
TI A new deep neural network based on a stack of single-hidden-layer
   feedforward neural networks with randomly fixed hidden neurons
SO NEUROCOMPUTING
LA English
DT Article
DE Single-hidden layer feedforward neural network; Manifold regularization;
   Unsupervised learning; Embedding; Stackable structure
ID DIMENSIONALITY
AB Single-hidden layer feedforward neural networks with randomly fixed hidden neurons (RHN-SLFNs) have been shown, both theoretically and experimentally, to be fast and accurate. Besides, it is well known that deep architectures can find higher-level representations, thus can potentially capture relevant higher-level abstractions. But most of current deep learning methods require a long time to solve a non-convex optimization problem. In this paper, we propose a stacked deep neural network, St-URHN-SLFNs, via unsupervised RHN-SLFNs according to stacked generalization philosophy to deal with unsupervised problems. Empirical study on a wide range of data sets demonstrates that the proposed algorithm outperforms the state-of-the-art unsupervised algorithms in terms of accuracy. On the computational effectiveness, the proposed algorithm runs much faster than other deep learning methods, i.e. deep autoencoder (DA) and stacked autoencoder (SAE), and little slower than other methods. (C) 2015 Elsevier B.V. All rights reserved.
C1 [Hu, Junying; Zhang, Jiangshe; Zhang, Chunxia; Wang, Juan] Xi An Jiao Tong Univ, Sch Math & Stat, Xian, Peoples R China.
RP Zhang, JS (reprint author), Xi An Jiao Tong Univ, Sch Math & Stat, Xian, Peoples R China.
EM hujunyingmm@163.com; jszhang@mail.xjtu.edu.cn; cxzhang@mail.xjtu.edu.cn;
   Wangjuan03022204@l63.com
FU National Basic Research Program of China (973 Program)National Basic
   Research Program of China [2013CB329404]; National Natural Science
   Foundation of ChinaNational Natural Science Foundation of China
   [91230101, 61075006, 11131006, 11201367, 11171272]; Research Fund for
   the Doctoral Program of Higher Education of ChinaResearch Fund for the
   Doctoral Program of Higher Education of China (RFDP)Specialized Research
   Fund for the Doctoral Program of Higher Education (SRFDP)
   [20100201120048]
FX This work is supported by the National Basic Research Program of China
   (973 Program) under Grant no. 2013CB329404, the National Natural Science
   Foundation of China under Grant nos. 91230101, 61075006, 11131006,
   11201367, 11171272, the Research Fund for the Doctoral Program of Higher
   Education of China under Grant no. 20100201120048.
CR Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317
   Belkin M., 2003, P 15 ANN C NEUR INF, V15, P929
   Belkin M, 2006, J MACH LEARN RES, V7, P2399
   Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153
   Bengio Yoshua, 2007, ADV NEURAL INFORM PR
   Broomhead D. S., 1988, Complex Systems, V2, P321
   Frank A, 2011, UCI MACHINE LEARNING
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   HORNIK K, 1991, NEURAL NETWORKS, V4, P251, DOI 10.1016/0893-6080(91)90009-T
   Huang G. -B., 2014, IEEE T CYBERN EXTREM, P2168
   Huang GB, 2004, IEEE IJCNN, P985
   Huang GB, 2015, COGN COMPUT, V7, P263, DOI 10.1007/s12559-015-9333-0
   Huang GB, 2015, IEEE COMPUT INTELL M, V10, P18, DOI 10.1109/MCI.2015.2405316
   Kasun LLC, 2013, IEEE INTELL SYST, V28, P31
   Lee KC, 2005, IEEE T PATTERN ANAL, V27, P684, DOI 10.1109/TPAMI.2005.92
   LOWE D, 1989, IEE CONF PUBL, P171
   Lutkepohl H., 1997, HDB MATRICES
   Melacci S, 2011, J MACH LEARN RES, V12, P1149
   Ng AY, 2002, ADV NEUR IN, V14, P849
   PAO YH, 1994, NEUROCOMPUTING, V6, P163, DOI 10.1016/0925-2312(94)90053-1
   Park J, 1991, NEURAL COMPUT, V3, P246, DOI 10.1162/neco.1991.3.2.246
   Samaria F. S., 1994, Proceedings of the Second IEEE Workshop on Applications of Computer Vision (Cat. No.94TH06742), P138, DOI 10.1109/ACV.1994.341300
   SCHMIDT WF, 1992, 11TH IAPR INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION, PROCEEDINGS, VOL II, P1, DOI 10.1109/ICPR.1992.201708
   Sindhwani V., 2005, P 22 INT C MACH LEAR, P824
NR 25
TC 14
Z9 14
U1 0
U2 49
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0925-2312
EI 1872-8286
J9 NEUROCOMPUTING
JI Neurocomputing
PD JAN 1
PY 2016
VL 171
BP 63
EP 72
DI 10.1016/j.neucom.2015.06.017
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA CW3HX
UT WOS:000364883900008
DA 2020-02-19
ER

PT J
AU Zhang, N
   Ding, SF
   Shi, ZZ
AF Zhang, Nan
   Ding, Shifei
   Shi, Zhongzhi
TI Denoising Laplacian multi-layer extreme learning machine
SO NEUROCOMPUTING
LA English
DT Article
DE Extreme learning machine; Semi-supervised learning; Deep learning;
   Denoising; Manifold regularization
ID ELM
AB Most of semi-supervised learning algorithms based on manifold regularization framework are surface learning algorithms, such as semi-supervised ELM (SS-ELM) and Laplacian smooth twin support vector machine (Lap-STSVM). Multi-layer extreme learning machine (ML-ELM) stacks extreme learning machine based auto encoder (ELM-AE) to create a multi-layer neural network. ML-ELM not only approximates the complicated function but also achieves fast training time. The outputs of ELM-AE are the same as inputs, which cannot guarantee the effectiveness of the learning feature representations. We put forward extreme learning machine based denoising auto encoder (ELM-DAE) which introduces local denoising criterion into ELM-AE and is used as the basic component for Denoising ML-ELM. Resembling ML-ELM, Denoising ML-ELM stacks ELM-DAE to create a deep network. And then we introduce manifold regularization into the model of Denoising ML-ELM and propose denoising Laplacian ML-ELM (Denoising Lap-ML-ELM). Denoising Lap-ML-ELM is more efficient than SS-ELM in classification and does not need to spend too much time. Experimental results show that Denoising ML-ELM and Denoising Lap-ML-ELM are effective learning algorithms. (C) 2015 Elsevier B.V. All rights reserved.
C1 [Zhang, Nan; Ding, Shifei] China Univ Min & Technol, Sch Comp Sci & Technol, Xuzhou 221116, Peoples R China.
   [Zhang, Nan; Ding, Shifei] China Univ Min & Technol, Jiangsu Key Lab Mine Mech & Elect Equipment, Xuzhou 221116, Peoples R China.
   [Ding, Shifei; Shi, Zhongzhi] Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
RP Ding, SF (reprint author), China Univ Min & Technol, Sch Comp Sci & Technol, Xuzhou 221116, Peoples R China.
EM dingsf@cumt.edu.cn
OI Zhang, Nan/0000-0001-9620-5665
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61379101]; National Key Basic Research Program of
   ChinaNational Basic Research Program of China [2013CB329502]
FX This work is supported by the National Natural Science Foundation of
   China (No. 61379101), and the National Key Basic Research Program of
   China (No. 2013CB329502).
CR Bai Z, 2014, IEEE T CYBERNETICS, V44, P1858, DOI 10.1109/TCYB.2014.2298235
   Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153
   Cambria E, 2013, IEEE INTELL SYST, V28, P30, DOI 10.1109/MIS.2013.140
   Chen H, 2014, NEURAL NETWORKS, V53, P119, DOI 10.1016/j.neunet.2014.01.015
   Chen WJ, 2014, INT J MACH LEARN CYB, V5, P459, DOI 10.1007/s13042-013-0183-3
   Deng Wan-Yu, 2010, Chinese Journal of Computers, V33, P279, DOI 10.3724/SP.J.1016.2010.00279
   Ding SF, 2014, NEURAL COMPUT APPL, V25, P549, DOI 10.1007/s00521-013-1522-8
   Gu Y., NEUROCOMPUTING
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Huang G.-B., 2004, P INT JOINT C NEUR N, P25
   Huang G, 2014, IEEE T CYBERNETICS, V44, P2405, DOI 10.1109/TCYB.2014.2307349
   Huang GB, 2006, NEUROCOMPUTING, V70, P489, DOI 10.1016/j.neucom.2005.12.126
   Huang GB, 2014, COGN COMPUT, V6, P376, DOI 10.1007/s12559-014-9255-2
   Kasun LLC, 2013, IEEE INTELL SYST, V28, P31
   Larochelle H, 2009, J MACH LEARN RES, V10, P1
   Lee H, 2011, COMMUN ACM, V54, P95, DOI 10.1145/2001269.2001295
   Lin SB, 2015, IEEE T NEUR NET LEAR, V26, P21, DOI 10.1109/TNNLS.2014.2336665
   Liu JF, 2011, NEUROCOMPUTING, V74, P2566, DOI 10.1016/j.neucom.2010.12.043
   Masci J, 2011, LECT NOTES COMPUT SC, V6791, P52, DOI 10.1007/978-3-642-21735-7_7
   Miche Y, 2011, NEUROCOMPUTING, V74, P2413, DOI 10.1016/j.neucom.2010.12.042
   Miche Y, 2010, IEEE T NEURAL NETWOR, V21, P158, DOI 10.1109/TNN.2009.2036259
   Vincent P, 2008, P 25 INT C MACH LEAR, P1096, DOI DOI 10.1145/1390156.1390294
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Zong WW, 2013, NEUROCOMPUTING, V101, P229, DOI 10.1016/j.neucom.2012.08.010
NR 25
TC 19
Z9 22
U1 4
U2 45
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0925-2312
EI 1872-8286
J9 NEUROCOMPUTING
JI Neurocomputing
PD JAN 1
PY 2016
VL 171
BP 1066
EP 1074
DI 10.1016/j.neucom.2015.07.058
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA CW3HX
UT WOS:000364883900105
DA 2020-02-19
ER

PT J
AU Chandra, B
   Sharma, RK
AF Chandra, B.
   Sharma, Rajesh K.
TI Fast learning in Deep Neural Networks
SO NEUROCOMPUTING
LA English
DT Article
DE Deep learning; Deep Neural Network; Denoising autoencoder
AB The paper aims at speeding up Deep Neural Networks (DNN) since this is one of the major bottlenecks in deep learning. This has been achieved by parameterizing the weight matrix using low rank factorization and periodic functions. By parameterization, the weight matrix is split into two matrices of smaller size of rank K with periodic functions. A shrinkage parameter has been introduced which helps in reducing the number of parameters and thus helps in increasing the speed to a great extent. Performance of the proposed parameterization is compared with standard DNN, DNN based on weight factorization alone and on periodic-bounded weights. This has been demonstrated on benchmark datasets MNIST and MNIST variants. (C) 2015 Elsevier B.V. All rights reserved.
C1 [Chandra, B.] Indian Inst Technol, Dept Math, Comp Sci Grp, Delhi, India.
   [Sharma, Rajesh K.] Indian Inst Technol, Dept Math, Delhi, India.
RP Chandra, B (reprint author), Indian Inst Technol, Dept Math, Comp Sci Grp, Delhi, India.
EM bchandra104@yahoo.co.in
FU Council of Scientific and Industrial research (CSIR) IndiaCouncil of
   Scientific & Industrial Research (CSIR) - India [09/086(1170)/2013 EMRI]
FX The Council of Scientific and Industrial research (CSIR) India
   (09/086(1170)/2013 EMRI), is thankfully acknowledged for providing
   research fellowship to Rajesh Kumar Sharma. The authors would like to
   thank the anonymous reviewers for their valuable comments and
   suggestions which helped to improve the paper.
CR Collobert R, 2011, J MACH LEARN RES, V12, P2493
   Dahl GE, 2012, IEEE T AUDIO SPEECH, V20, P30, DOI 10.1109/TASL.2011.2134090
   Dan Ciresan, 2012, P 2012 IEEE C COMP V
   Denil M., 2013, ADV NEURAL INFORM PR, P2148
   Denton E. L., 2014, ADV NEURAL INFORM PR, P1269
   Dong Yu, 2012, P 2012 IEEE INT C AC
   Glorot Xavier, 2010, P INT C ART INT STAT
   Goodfellow I. J., 2013, P 30 INT C MACH LEAR
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Jian Xue, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P6359, DOI 10.1109/ICASSP.2014.6854828
   Jian Xue, 2013, INTERSPEECH
   Jimmy Ba, 2014, ADV NEURAL INFORM PR
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   LeCun Y, 1998, LECT NOTES COMPUT SC, V1524, P9
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee H, 2009, P 26 ANN INT C MACH
   Matthew D. Z., 2012, ARXIV12125701
   Navdeep Jaitly, 2012, INTERSPEECH
   RIFAI S, 2011, HIGHER ORDER CONTRAC, V6912, P645
   Ronan C., 2008, P 25 INT C MACH LEAR
   Sainath TN, 2013, INT CONF ACOUST SPEE, P6655, DOI 10.1109/ICASSP.2013.6638949
   Salah Rifai, 2011, P 28 INT C MACH LEAR
   Schraudolph NN, 1998, LECT NOTES COMPUT SC, V1524, P207
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Xavier Glorot, 2011, P 14 INT C ART INT S, V15
NR 26
TC 18
Z9 22
U1 1
U2 62
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0925-2312
EI 1872-8286
J9 NEUROCOMPUTING
JI Neurocomputing
PD JAN 1
PY 2016
VL 171
BP 1205
EP 1215
DI 10.1016/j.neucom.2015.07.093
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA CW3HX
UT WOS:000364883900119
DA 2020-02-19
ER

PT J
AU Huerta, I
   Fernandez, C
   Segura, C
   Hernando, J
   Prati, A
AF Huerta, Ivan
   Fernandez, Carles
   Segura, Carlos
   Hernando, Javier
   Prati, Andrea
TI A deep analysis on age estimation
SO PATTERN RECOGNITION LETTERS
LA English
DT Article; Proceedings Paper
CT 1st International Workshop on Soft Biometrics held in conjunction with
   the European Computer Vision Conference (ECCV)
CY SEP 07, 2014
CL Zurich, SWITZERLAND
DE Age estimation; Deep learning; DNN; CCA; HOG; LBP
AB The automatic estimation of age from face images is increasingly gaining attention, as it facilitates applications including advanced video surveillance, demographic statistics collection, customer profiling, or search optimization in large databases. Nevertheless, it becomes challenging to estimate age from uncontrollable environments, with insufficient and incomplete training data, dealing with strong person-specificity and high within-range variance. These difficulties have been recently addressed with complex and strongly hand-crafted descriptors, difficult to replicate and compare. This paper presents two novel approaches: first, a simple yet effective fusion of descriptors based on texture and local appearance; and second, a deep learning scheme for accurate age estimation. These methods have been evaluated under a diversity of settings, and the extensive experiments carried out on two large databases (MORPH and FRGC) demonstrate state-of-the-art results over previous work. (C) 2015 Elsevier B.V. All rights reserved.
C1 [Huerta, Ivan; Prati, Andrea] Univ IUAV, DPDCE, I-30135 Venice, Italy.
   [Fernandez, Carles; Segura, Carlos] Herta Secur, Barcelona 08037, Spain.
   [Hernando, Javier] Univ Politecn Cataluna, ES-08034 Barcelona, Spain.
RP Huerta, I (reprint author), Univ IUAV, DPDCE, Santa Croce 1957, I-30135 Venice, Italy.
EM huertacasado@iuav.it
RI Hernando, Javier/G-1863-2014; Prati, Andrea/B-7440-2014
OI Prati, Andrea/0000-0002-1211-529X; Huerta, Ivan/0000-0002-4189-4034
FU Spanish Ministry of Science and Innovation (MICINN) through the
   Torres-Quevedo funding program [PTQ-11-04401, PTQ-11-04400]
FX This work has been partially supported by the Spanish Ministry of
   Science and Innovation (MICINN) through the Torres-Quevedo funding
   program (PTQ-11-04401 and PTQ-11-04400). We thank the reviewers for
   their useful comments and suggestions.
CR Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1016/j.cviu.2007.09.014
   Chang K. Y., 2011, COMP VIS PATT REC CV, P585
   Choi SE, 2011, PATTERN RECOGN, V44, P1262, DOI 10.1016/j.patcog.2010.12.005
   Dalal N, 2005, PROC CVPR IEEE, P886
   Fernandez C., 2014, FFER CONJUN IN PRESS
   Gallagher AC, 2009, PROC CVPR IEEE, P256, DOI 10.1109/CVPRW.2009.5206828
   Geng X, 2007, IEEE T PATTERN ANAL, V29, P2234, DOI 10.1109/TPAMI.2007.70733
   Geng X, 2013, IEEE T PATTERN ANAL, V35, P2401, DOI 10.1109/TPAMI.2013.51
   Gunay Asuman, 2008, P 23 INT S COMP INF, P1, DOI DOI 10.1109/ISCIS.2008.4717926
   Guo G., 2013, 10 INT C AUT FAC GES
   Guo G., 2014, IMAGE VIS COMPUT
   Guo GD, 2011, PROC CVPR IEEE, P657, DOI 10.1109/CVPR.2011.5995404
   Guo GD, 2009, PROC CVPR IEEE, P112, DOI 10.1109/CVPRW.2009.5206681
   Han H., 2013, INT C BIOM ICB
   Huang R., 2014, VISUAL COMPUT, P1
   Huerta I., 2014, SOFT BIOMET IN PRESS
   Jia Yangqing, 2014, ARXIV14085093
   Lanitis A, 2002, IEEE T PATTERN ANAL, V24, P442, DOI 10.1109/34.993553
   LANITIS A, 2004, TSMC B, V34, P621
   Liang J., 2012, 21 INT C PATT REC IC, P2496
   Minear M, 2004, BEHAV RES METH INS C, V36, P630, DOI 10.3758/BF03206543
   Montilla A, 2009, 2009 16TH IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOLS 1-6, P2465, DOI 10.1109/ICIP.2009.5414103
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Oro D., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P530, DOI 10.1109/ICCVW.2011.6130288
   Phillips PJ, 2005, PROC CVPR IEEE, P947
   Ricanek K, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P341
   Riesenhuber M, 1999, NAT NEUROSCI, V2, P1019
   Sun Y, 2013, PROC CVPR IEEE, P3476, DOI 10.1109/CVPR.2013.446
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Tan XY, 2010, IEEE T IMAGE PROCESS, V19, P1635, DOI 10.1109/TIP.2010.2042645
   Weng R., 2013, AFGR
   Yang M, 2011, PROC CVPR IEEE, P505, DOI 10.1109/CVPR.2011.5995481
   Yang ZG, 2007, LECT NOTES COMPUT SC, V4642, P464
NR 33
TC 31
Z9 32
U1 1
U2 15
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0167-8655
EI 1872-7344
J9 PATTERN RECOGN LETT
JI Pattern Recognit. Lett.
PD DEC 15
PY 2015
VL 68
BP 239
EP 249
DI 10.1016/j.patrec.2015.06.006
PN 2
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA CW7LZ
UT WOS:000365181900004
OA Green Published
DA 2020-02-19
ER

PT J
AU Dhillon, PS
   Foster, DP
   Ungar, LH
AF Dhillon, Paramveer S.
   Foster, Dean P.
   Ungar, Lyle H.
TI Eigenwords: Spectral Word Embeddings
SO JOURNAL OF MACHINE LEARNING RESEARCH
LA English
DT Article
DE spectral learning; CCA; word embeddings; NLP
ID CANONICAL CORRELATION
AB Spectral learning algorithms have recently become popular in data-rich domains, driven in part by recent advances in large scale randomized SVD, and in spectral estimation of Hidden Markov Models. Extensions of these methods lead to statistical estimation algorithms which are not only fast, scalable, and useful on real data sets, but are also provably correct. Following this line of research, we propose four fast and scalable spectral algorithms for learning word embeddings low dimensional real vectors (called Eigenwords) that capture the "meaning" of words from their context. All the proposed algorithms harness the multi-view nature of text data i.e. the left and right context of each word, are fast to train and have strong theoretical properties. Some of the variants also have lower sample complexity and hence higher statistical power for rare words. We provide theory which establishes relationships between these algorithms and optimality criteria for the estimates they provide. We also perform thorough qualitative and quantitative evaluation of Eigenwords showing that simple linear approaches give performance comparable to or superior than the state-of-the-art non-linear deep learning based methods.
C1 [Dhillon, Paramveer S.] MIT, Sloan Sch Management, Cambridge, MA 02142 USA.
   [Foster, Dean P.] Univ Penn, Wharton Sch, Dept Stat, Philadelphia, PA 19104 USA.
   [Ungar, Lyle H.] Univ Penn, Dept Comp & Informat Sci, Philadelphia, PA 19104 USA.
RP Dhillon, PS (reprint author), MIT, Sloan Sch Management, Cambridge, MA 02142 USA.
EM DHILLON@MIT.EDU; FOSTER@WHARTON.UPENN.EDU; UNGAR@CIS.UPENN.EDU
CR Afonso S., 2002, P INT C LING RES EV, P1698
   Ando RK, 2005, J MACH LEARN RES, V6, P1817
   ANSCOMBE FJ, 1948, BIOMETRIKA, V35, P246, DOI 10.1093/biomet/35.3-4.246
   Bach F. R., 2005, 688 TR U CAL
   Bansal Mohit, 2014, P ACL
   Bird S., 2009, NATURAL LANGUAGE PRO
   Bird Steven, 2004, P ACL 2004 INT POST
   Blum A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P92, DOI 10.1145/279943.279962
   Brown P. F., 1992, Computational Linguistics, V18, P467
   Cohen S. B., 2012, P 50 ANN M ASS COMP, P223
   Collobert R., 2008, P 25 ICML, V25, P160, DOI [10.1145/1390156.1390177, DOI 10.1145/1390156.1390177]
   Collobert R, 2011, J MACH LEARN RES, V12, P2493
   DAUME H, 2004, [No title captured]
   Dhillon P., 2011, P NIPS, V24
   Dhillon P., 2012, P EMNLP CONLL
   Dhillon P S., 2012, P ICML
   Dumais S.T., 1988, P SIGCHI C HUM FACT, P281, DOI [10.1145/57167.57214, DOI 10.1145/57167.57214]
   Fellbaum Christiane, 1998, WORDNET
   Finkelstein Lev, 2001, P 10 INT C WORLD WID, P406, DOI DOI 10.1145/371920.372094
   Foster D.P., 2008, TECHNICAL REPORT
   GLAHN HR, 1968, J ATMOS SCI, V25, P23, DOI 10.1175/1520-0469(1968)025<0023:CCAIRT>2.0.CO;2
   Halko N., 2011, SIAM REV
   Hardoon D., 2008, EURO MIN C CONT OPT
   Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814
   HOTELLING H, 1935, [No title captured]
   Hsu D., 2009, P COLT
   Huang E. H., 2012, ANN M ASS COMP LING, P873
   Huang F., 2013, COMPUTATIONAL LINGUI
   Huang Fei, 2009, P IJCNLP AFNLP SING, p495 
   Jurafsky D., 2000, SPEECH LANGUAGE PROC
   Kakade SM, 2007, LECT NOTES COMPUT SC, V4539, P82, DOI 10.1007/978-3-540-72927-3_8
   Koehn P, 2005, MT SUMMIT, P79, DOI DOI 10.3115/1626355.1626380
   Koo Terry, 2008, P ACL
   Kromann Matthias T., 2003, P 2 WORKSH TREEB LIN, P217
   Lamar M., 2010, ACL, P215
   Landauer T., 2008, DISCOURSE PROCESSES
   Lefever E., 2013, P SEMEVAL 2013 ATL U
   Marcus M.P., 1993, COMPUT LINGUIST, V19, P313, DOI DOI 10.1080/07494460903404410
   Mikolov T, 2013, ADV NEURAL INFORM PR, V27, P3111, DOI DOI 10.1162/JMLR.2003.3.4-5.951
   Mikolov T., 2013, EFFICIENT ESTIMATION
   Mnih A., 2007, P 24 INT C MACH LEAR, P641, DOI DOI 10.1145/1273496.1273577
   Netrapalli P., 2013, ADV NEURAL INFORM PR, P2796
   Parikh Ankur P., 2014, P ACL
   Pennebaker James W, 2001, MAHWAY L ERLBAUM ASS, V71, P2001
   Pereira Fernando, 1993, P 31 ANN M ASS COMP, P183, DOI DOI 10.3115/981574.981598
   Ratinov L., 2009, P 13 C COMP NAT LANG, P147, DOI DOI 10.3115/1596374.1596399
   Rose T., 2002, LREC, P827
   Rudelson M., 2010, NONASYMPTOTIC THEORY
   Rudnick A., 2013, P SEMEVAL 2013
   Seligman M., 2011, FLOURISH VISIONARY N
   Siddiqi S., 2010, P AISTATS
   Simov K., 2002, P LREC
   Smith Noah A, 2005, P 43 ANN M ASS COMP, P354
   Socher R., 2012, P 2012 JOINT C EMP M, P1201
   Socher R., 2013, P C EMP METH NAT LAN, V2013, P1631, DOI DOI 10.1371/JOURNAL.PONE.0073791
   Stewart G., 1990, SVD SIGNAL PROCESSIN, P99
   Stewart G. W., 1990, MATRIX PERTURBATION
   Suzuki J., 2008, P ACL
   Tackstrom Oscar, 2012, P 2012 C N AM CHAPT, P477
   Teufel S, 2010, STRUCTURE SCI ARTICL
   Turian J, 2010, ACL 2010: 48TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, P384
   Turney PD, 2010, J ARTIF INTELL RES, V37, P141, DOI 10.1613/jair.2934
   Zhang T, 2003, P 7 C NAT LANG LEARN, V4, P204
NR 63
TC 17
Z9 18
U1 0
U2 3
PU MICROTOME PUBL
PI BROOKLINE
PA 31 GIBBS ST, BROOKLINE, MA 02446 USA
SN 1532-4435
J9 J MACH LEARN RES
JI J. Mach. Learn. Res.
PD DEC
PY 2015
VL 16
BP 3035
EP 3078
PG 44
WC Automation & Control Systems; Computer Science, Artificial Intelligence
SC Automation & Control Systems; Computer Science
GA DD4JJ
UT WOS:000369888000024
DA 2020-02-19
ER

PT J
AU Cui, ZY
   Ge, SS
   Cao, ZJ
   Yang, JY
   Ren, HL
AF Cui, Zongyong
   Ge, Shuzhi Sam
   Cao, Zongjie
   Yang, Jianyu
   Ren, Hongliang
TI Analysis of Different Sparsity Methods in Constrained RBM for Sparse
   Representation in Cognitive Robotic Perception
SO JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS
LA English
DT Article; Proceedings Paper
CT 10th National Meeting of Artificial and Computational Intelligence
   (ENIAC)
CY 2013
CL Fortaleza, BRAZIL
DE Restricted Boltzmann machine; Sparse constraint
ID NONNEGATIVE MATRIX FACTORIZATION; DEEP BELIEF NETWORKS; MODEL
AB Cognitive robotic systems nowadays are intensively involving learning algorithms to achieve highly adaptive and intelligent behaviors, including actuation, sensing, perception and adaptive control. Deep learning has emerged as an effective approach in image-based robotic perception and actions. Towards cognitive robotic perception based on deep learning, this paper focuses the Constrained Restricted Boltzmann Machine (RBM) on visual images for sparse feature representation. Inspired by sparse coding, the sparse constraints are performed on the hidden layer of RBM to obtain sparse and effective feature representation from perceived visual images. The RBM with Sparse Constraint (RBMSC) is proposed with a generalized optimization problem, where the constraints are applied on the probability density of hidden units directly to obtain more sparse representation. This paper presents three novel RBM variants, namely L-1-RBM, L-2-RBM, and L-1/2-RBM constrained by L-1-norm, L-2-norm, and L-1/2-norm on RBM, respectively. A Deep Belief Network with two hidden layers is built for comparison between each RBM variants. The experiments on MNIST database (Mixed National Institute of Standards and Technology database) show that the L-1/2-RBM can obtain more sparse representation than RBM, L-1-RBM, L-2-RBM, and Sparse-RBM (SRBM) in terms of sparseness metric. For further verification, the proposed methods are still tested on MNIST Variations dataset. The recognition results from perceived images in MNIST and MNIST Variations demonstrate that our proposed constrained RBM variants are feasible for object cognitive and perception, and the proposed L-1/2-RBM and L-1-RBM outperforms RBM and SRBM in terms of object recognition.
C1 [Cui, Zongyong; Cao, Zongjie; Yang, Jianyu] Univ Elect Sci & Technol China, Sch Elect Engn, Chengdu 611731, Peoples R China.
   [Cui, Zongyong; Ren, Hongliang] Natl Univ Singapore, Dept Biomed Engn, Singapore 117575, Singapore.
   [Cui, Zongyong; Ge, Shuzhi Sam] Natl Univ Singapore, Social Robot Lab, Interact Digital Media Inst, Singapore 119613, Singapore.
RP Cui, ZY (reprint author), Univ Elect Sci & Technol China, Sch Elect Engn, Chengdu 611731, Peoples R China.
EM cuizongyong@163.com
RI Ren, Hongliang/N-9194-2017
OI Ren, Hongliang/0000-0002-6488-1551
FU China Scholarship CouncilChina Scholarship Council [201306070061]
FX This research was supported by China Scholarship Council (No.
   201306070061).
CR Andrew Ng, 2011, CS294A LECT NOTES, P72
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Chan T. H., 2014, ARXIV14043606
   Ciresan D., 2010, ARXIV10030358
   Cui Z., 2014, P 2014 IEEE IN PRESS
   Eslami SMA, 2014, INT J COMPUT VISION, V107, P155, DOI 10.1007/s11263-013-0669-1
   Ge SS, 2012, IEEE T CONTR SYST T, V20, P787, DOI 10.1109/TCST.2011.2145378
   Goertzel B, 2014, ATLANTIS THINK MACH, V6, P143, DOI 10.2991/978-94-6239-030-0_8
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hoyer PO, 2004, J MACH LEARN RES, V5, P1457
   Hoyer PO, 2002, NEURAL NETWORKS FOR SIGNAL PROCESSING XII, PROCEEDINGS, P557, DOI 10.1109/NNSP.2002.1030067
   Huang GB, 2012, PROC CVPR IEEE, P2518, DOI 10.1109/CVPR.2012.6247968
   Ian L., 2014, ARXIV13013592V6
   Kim C.-J, 1999, STATE SPACE MODELS R, V1
   Langkvist M, 2014, PATTERN RECOGN LETT, V42, P11, DOI 10.1016/j.patrec.2014.01.008
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565
   Lee H., 2007, ADV NEURAL INFORM PR, P873
   Lee H, 2011, COMMUN ACM, V54, P95, DOI 10.1145/2001269.2001295
   Liu C, 2014, STUD BIG DATA, V1, P153, DOI 10.1007/978-3-642-40837-3_5
   Mohamed AR, 2012, IEEE T AUDIO SPEECH, V20, P14, DOI 10.1109/TASL.2011.2109382
   Ngiam J., 2011, ADV NEURAL INFORM PR, P1125
   Noda K, 2014, ROBOT AUTON SYST, V62, P721, DOI 10.1016/j.robot.2014.03.003
   Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0
   Ortega A.C., 2014, SOFT COMPUTING BUSIN, V537, P225
   Pascual-Montano A, 2006, IEEE T PATTERN ANAL, V28, P403, DOI 10.1109/TPAMI.2006.60
   Salakhutdinov Ruslan, 2009, THESIS
   Sarikaya R, 2014, IEEE-ACM T AUDIO SPE, V22, P778, DOI 10.1109/TASLP.2014.2303296
   Shuzhi Sam Ge, 2011, Proceedings of the 2011 4th International Conference on Interaction Sciences: Information Technology, Human and Digital Content (ICIS 2011), P90
   Sohn K., 2013, P 30 INT C MACH LEAR, V28, P217
   Tang YZ, 2012, APPL OPTICS, V51, P6641, DOI 10.1364/AO.51.006641
   Xu ZB, 2012, IEEE T NEUR NET LEAR, V23, P1013, DOI 10.1109/TNNLS.2012.2197412
   Zhou SS, 2014, NEUROCOMPUTING, V131, P312, DOI 10.1016/j.neucom.2013.10.011
NR 34
TC 5
Z9 5
U1 1
U2 23
PU SPRINGER
PI DORDRECHT
PA VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN 0921-0296
EI 1573-0409
J9 J INTELL ROBOT SYST
JI J. Intell. Robot. Syst.
PD DEC
PY 2015
VL 80
SU 1
SI SI
BP S121
EP S132
DI 10.1007/s10846-015-0213-3
PG 12
WC Computer Science, Artificial Intelligence; Robotics
SC Computer Science; Robotics
GA DB0HK
UT WOS:000368189000008
DA 2020-02-19
ER

PT J
AU Chen, CLP
   Zhang, CY
   Chen, L
   Gan, M
AF Chen, C. L. Philip
   Zhang, Chun-Yang
   Chen, Long
   Gan, Min
TI Fuzzy Restricted Boltzmann Machine for the Enhancement of Deep Learning
SO IEEE TRANSACTIONS ON FUZZY SYSTEMS
LA English
DT Article
DE Deep learning; fuzzy deep networks; fuzzy restricted Boltzmann machine;
   image classification; image inpainting; restricted Boltzmann machine
   (RBM)
ID BAYESIAN-ESTIMATION; COEFFICIENT
AB In recent years, deep learning caves out a research wave in machine learning. With outstanding performance, more and more applications of deep learning in pattern recognition, image recognition, speech recognition, and video processing have been developed. Restricted Boltzmann machine (RBM) plays an important role in current deep learning techniques, as most of existing deep networks are based on or related to it. For regular RBM, the relationships between visible units and hidden units are restricted to be constants. This restriction will certainly downgrade the representation capability of the RBM. To avoid this flaw and enhance deep learning capability, the fuzzy restricted Boltzmann machine (FRBM) and its learning algorithm are proposed in this paper, in which the parameters governing the model are replaced by fuzzy numbers. This way, the original RBM becomes a special case in the FRBM, when there is no fuzziness in the FRBM model. In the process of learning FRBM, the fuzzy free energy function is defuzzified before the probability is defined. The experimental results based on bar-and-stripe benchmark inpainting and MNIST handwritten digits classification problems show that the representation capability of FRBM model is significantly better than the traditional RBM. Additionally, the FRBM also reveals better robustness property compared with RBM when the training data are contaminated by noises.
C1 [Chen, C. L. Philip; Zhang, Chun-Yang; Chen, Long] Univ Macau, Fac Sci & Technol, Dept Comp & Informat Sci, Macau 999078, Peoples R China.
   [Gan, Min] Hefei Univ Technol, Dept Comp & Informat Sci, Hefei 230009, Peoples R China.
RP Zhang, CY (reprint author), Fuzhou Univ, Coll Math & Comp Sci, Fuzhou 350116, Peoples R China.
EM philip.chen@ieee.org; cyzhangfst@gmail.com; longchen@umac.mo;
   aganmin@aliyun.com
RI Chen, C. L. Philip/O-2657-2016
OI Chen, C. L. Philip/0000-0001-5451-7230; Chen, Long/0000-0003-0184-5446;
   Gan, Min/0000-0002-2756-0054
FU Macau Science and Technology Development Fund [008/2010/A1]; UM
   Multiyear Research Grants; National Natural Science Foundation of
   ChinaNational Natural Science Foundation of China [61203106]
FX This work was supported by the Macau Science and Technology Development
   Fund under Grant 008/2010/A1, UM Multiyear Research Grants, and the
   National Natural Science Foundation of China under Grant 61203106.
CR Aoyagi M, 2013, J ALGEBR STUD, V4, P31
   Aoyagi M, 2012, NEURAL COMPUT, V24, P1569, DOI 10.1162/NECO_a_00271
   Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Bishop CM, 2006, PATTERN RECOGNITION
   Buckley J. J., 2009, FUZZY PROBABILITIES
   Castillo O, 2008, STUD FUZZ SOFT COMP, V223, P1
   Chen CLP, 2014, INFORM SCIENCES, V275, P314, DOI 10.1016/j.ins.2014.01.015
   Cho K. H., 2013, INT JOINT C NEUR NET, P1, DOI [10.1109/IJCNN.2013.6706831, DOI 10.1109/IJCNN.2013.6706831]
   Ciresan D, 2012, PROC CVPR IEEE, P3642, DOI 10.1109/CVPR.2012.6248110
   Dutta S, 2015, SOFT COMPUT, V19, P99, DOI 10.1007/s00500-014-1356-z
   Fischer A, 2014, PATTERN RECOGN, V47, P25, DOI 10.1016/j.patcog.2013.05.025
   Hao PY, 2008, IEEE T FUZZY SYST, V16, P428, DOI 10.1109/TFUZZ.2007.896359
   Hinton G, 2005, AISTATS, V10, P33
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton G. E., 2010, 2010003 UTML TR DEP
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hinton GE, 2007, TRENDS COGN SCI, V11, P428, DOI 10.1016/j.tics.2007.09.004
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178
   Karnik NN, 2001, INFORM SCIENCES, V132, P195, DOI 10.1016/S0020-0255(01)00069-X
   Klir G. J., 1995, FUZZY SETS FUZZY LOG
   Lodwick WA, 2010, STUD FUZZ SOFT COMP, V254, P33
   Pedrycz W., 2008, HDB GRANULAR COMPUTI
   Salakhutdinov R., 2010, P NIPS, V22, P1607
   Salakhutdinov R., 2007, P INT C MACH LEARN, V24, P791, DOI DOI 10.1145/1273496.1273596
   Salakhutdinov R, 2013, IEEE T PATTERN ANAL, V35, P1958, DOI 10.1109/TPAMI.2012.269
   Salakhutdinov R, 2012, NEURAL COMPUT, V24, P1967, DOI 10.1162/NECO_a_00311
   Smolensky P, 1986, PARALLEL DISTRIBUTED, P194
   Sutskever I., 2007, J MACH LEARN RES, P548
   Taylor G.W., 2007, ADV NEURAL INFORM PR, V19, P1345
   Taylor GW, 2011, J MACH LEARN RES, V12, P1025
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Wang HF, 2000, FUZZY SET SYST, V112, P355, DOI 10.1016/S0165-0114(97)00375-8
   Wang N, 2014, ARXIV14015900
   Zhang CY, 2014, IEEE SYS MAN CYBERN, P4037, DOI 10.1109/SMC.2014.6974564
NR 37
TC 72
Z9 75
U1 3
U2 74
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1063-6706
EI 1941-0034
J9 IEEE T FUZZY SYST
JI IEEE Trans. Fuzzy Syst.
PD DEC
PY 2015
VL 23
IS 6
BP 2163
EP 2173
DI 10.1109/TFUZZ.2015.2406889
PG 11
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
SC Computer Science; Engineering
GA CX8XV
UT WOS:000365989300021
DA 2020-02-19
ER

PT J
AU Luus, FPS
   Salmon, BP
   van den Bergh, F
   Maharaj, BTJ
AF Luus, F. P. S.
   Salmon, B. P.
   van den Bergh, F.
   Maharaj, B. T. J.
TI Multiview Deep Learning for Land-Use Classification
SO IEEE GEOSCIENCE AND REMOTE SENSING LETTERS
LA English
DT Article
DE Feature extraction; neural network applications; neural network
   architecture; remote sensing; urban areas
AB A multiscale input strategy for multiview deep learning is proposed for supervised multispectral land-use classification, and it is validated on a well-known data set. The hypothesis that simultaneous multiscale views can improve composition-based inference of classes containing size-varying objects compared to single-scale multiview is investigated. The end-to-end learning system learns a hierarchical feature representation with the aid of convolutional layers to shift the burden of feature determination from hand-engineering to a deep convolutional neural network (DCNN). This allows the classifier to obtain problem-specific features that are optimal for minimizing the multinomial logistic regression objective, as opposed to user-defined features which trade optimality for generality. A heuristic approach to the optimization of the DCNN hyperparameters is used, based on empirical performance evidence. It is shown that a single DCNN can be trained simultaneously with multiscale views to improve prediction accuracy over multiple single-scale views. Competitive performance is achieved for the UC Merced data set, where the 93.48% accuracy of multiview deep learning outperforms the 85.37% accuracy of SIFT-based methods and the 90.26% accuracy of unsupervised feature learning.
C1 [Luus, F. P. S.; Maharaj, B. T. J.] Univ Pretoria, Dept Elect Elect & Comp Engn, ZA-0002 Pretoria, South Africa.
   [Salmon, B. P.] Univ Tasmania, Sch Engn, Hobart, Tas 7001, Australia.
   [Salmon, B. P.] Univ Tasmania, ICT, Hobart, Tas 7001, Australia.
   [van den Bergh, F.] CSIR, Remote Sensing Res Unit, Meraka Inst, ZA-0001 Pretoria, South Africa.
RP Luus, FPS (reprint author), Univ Pretoria, Dept Elect Elect & Comp Engn, ZA-0002 Pretoria, South Africa.
EM luus@ieee.org
OI Maharaj, Bodhaswar TJ/0000-0002-3703-3637
FU National Research Foundation of South AfricaNational Research Foundation
   - South Africa
FX This work was supported by the National Research Foundation of South
   Africa.
CR Bastien F., 2012, P DEEP LEARN UNS FEA, P1
   Chen SZ, 2015, IEEE T GEOSCI REMOTE, V53, P1947, DOI 10.1109/TGRS.2014.2351395
   Chen XY, 2014, IEEE GEOSCI REMOTE S, V11, P1797, DOI 10.1109/LGRS.2014.2309695
   Chen YS, 2015, IEEE J-STARS, V8, P2381, DOI 10.1109/JSTARS.2015.2388577
   Chen YS, 2014, IEEE J-STARS, V7, P2094, DOI 10.1109/JSTARS.2014.2329330
   Glorot X., 2010, JLMR P TRACK, p[249, 2010], DOI DOI 10.1177/1753193409103364.
   Hu F, 2014, INT GEOSCI REMOTE SE, P1273, DOI 10.1109/IGARSS.2014.6946665
   Huang W, 2015, IEEE GEOSCI REMOTE S, V12, P1037, DOI 10.1109/LGRS.2014.2376034
   Jovanovic V, 2014, 2014 22ND TELECOMMUNICATIONS FORUM TELFOR (TELFOR), P889, DOI 10.1109/TELFOR.2014.7034547
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Negrel R., 2014, P 12 INT WORKSH CONT, P1, DOI DOI 10.1109/CBMI.2014.6849835
   Qiqi Zhu, 2014, 2014 IEEE Geoscience and Remote Sensing Symposium. (IGARSS). Proceedings, P2854, DOI 10.1109/IGARSS.2014.6947071
   Sutskever I., 2013, P 30 INT C MACH LEAR, P1139
   Yang Y., 2010, P 18 SIGSPATIAL INT, P270, DOI DOI 10.1145/1869790.1869829
   Zhang F, 2015, IEEE T GEOSCI REMOTE, V53, P2175, DOI 10.1109/TGRS.2014.2357078
NR 15
TC 101
Z9 108
U1 9
U2 164
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1545-598X
EI 1558-0571
J9 IEEE GEOSCI REMOTE S
JI IEEE Geosci. Remote Sens. Lett.
PD DEC
PY 2015
VL 12
IS 12
BP 2448
EP 2452
DI 10.1109/LGRS.2015.2483680
PG 5
WC Geochemistry & Geophysics; Engineering, Electrical & Electronic; Remote
   Sensing; Imaging Science & Photographic Technology
SC Geochemistry & Geophysics; Engineering; Remote Sensing; Imaging Science
   & Photographic Technology
GA CW4VA
UT WOS:000364993500019
DA 2020-02-19
ER

PT J
AU Wang, ST
   Jiang, YZ
   Chung, FL
   Qian, PJ
AF Wang, Shitong
   Jiang, Yizhang
   Chung, Fu-Lai
   Qian, Pengjiang
TI Feedforward kernel neural networks, generalized least learning machine,
   and its deep learning with application to image classification
SO APPLIED SOFT COMPUTING
LA English
DT Article
DE Feedforward kernel neural networks; Least learning machine; Kernel
   principal component analysis (KPCA); Hidden-layer-tuning-free learning;
   Deep architecture and learning
ID REGRESSION; REPRESENTATION; APPROXIMATION; SHAPE
AB In this paper, the architecture of feedforward kernel neural networks (FKNN) is proposed, which can include a considerably large family of existing feedforward neural networks and hence can meet most practical requirements. Different from the common understanding of learning, it is revealed that when the number of the hidden nodes of every hidden layer and the type of the adopted kernel based activation functions are pre-fixed, a special kernel principal component analysis (KPCA) is always implicitly executed, which can result in the fact that all the hidden layers of such networks need not be tuned and their parameters can be randomly assigned and even may be independent of the training data. Therefore, the least learning machine (LLM) is extended into its generalized version in the sense of adopting much more error functions rather than mean squared error (MSE) function only. As an additional merit, it is also revealed that rigorous Mercer kernel condition is not required in FKNN networks. When the proposed architecture of FKNN networks is constructed in a layer-by-layer way, i.e., the number of the hidden nodes of every hidden layer may be determined only in terms of the extracted principal components after the explicit execution of a KPCA, we can develop FKNN's deep architecture such that its deep learning framework (DLF) has strong theoretical guarantee. Our experimental results about image classification manifest that the proposed FKNN's deep architecture and its DLF based learning indeed enhance the classification performance. (C) 2015 Elsevier B.V. All rights reserved.
C1 [Wang, Shitong; Jiang, Yizhang; Qian, Pengjiang] Jiangnan Univ, Sch Digital Media, Wuxi, Jiangsu, Peoples R China.
   [Wang, Shitong; Jiang, Yizhang; Chung, Fu-Lai] Hong Kong Polytech Univ, Dept Comp, Hong Kong, Hong Kong, Peoples R China.
RP Wang, ST (reprint author), Jiangnan Univ, Sch Digital Media, Wuxi, Jiangsu, Peoples R China.
EM wxwangst@aliyun.com
RI Qian, Pengjiang/AAC-1399-2020; Jiang, Yizhang/V-2171-2019
OI Jiang, Yizhang/0000-0002-4558-9803; CHUNG, Fu Lai
   Korris/0000-0001-5294-8168
FU Hong Kong Polytechnic UniversityHong Kong Polytechnic University
   [G-UA3W]; National Natural Science Foundation of ChinaNational Natural
   Science Foundation of China [61272210, 2015NSFC]; Natural Science
   Foundation of Jiangsu ProvinceJiangsu Planned Projects for Postdoctoral
   Research FundsNatural Science Foundation of Jiangsu Province [BK2011003,
   BK2011417]; JiangSu 333 Expert Engineering [BRA2011142]; Fundamental
   Research Funds for the Central UniversitiesFundamental Research Funds
   for the Central Universities [JUSRP111A38]; Postgraduate Student's
   Creative Research Fund of Jiangsu Province
FX This work was supported in part by the Hong Kong Polytechnic University
   under Grant G-UA3W, and by the National Natural Science Foundation of
   China under Grants 61272210, 2015NSFC, and by the Natural Science
   Foundation of Jiangsu Province under Grant BK2011003, BK2011417, JiangSu
   333 Expert Engineering under Grant BRA2011142, the Fundamental Research
   Funds for the Central Universities under Grant JUSRP111A38 and 2013
   Postgraduate Student's Creative Research Fund of Jiangsu Province.
CR ACHLIOPTAS D, 2001, ADV NEURAL INFORM PR, P335
   Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Cho Y., 2010, ADV NEURAL INFORM PR, P342
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411
   Deng ZH, 2008, PATTERN RECOGN, V41, P1363, DOI 10.1016/j.patcog.2007.09.013
   Du K.-L., 2006, NEURAL NETWORKS SOFT
   Duda RO, 2000, PATTERN CLASSIFICATI
   Erhan D, 2010, J MACH LEARN RES, V11, P625
   Feng GR, 2009, IEEE T NEURAL NETWOR, V20, P1352, DOI 10.1109/TNN.2009.2024147
   FRIEDMAN JH, 1991, ANN STAT, V19, P1, DOI 10.1214/aos/1176347963
   Grossman D., 2004, P 21 INT C MACH LEAR, P46, DOI DOI 10.1145/1015330.1015339
   Hajebi K., 2011, P 22 INT JOINT C ART
   Hieu Trung Huynh, 2009, Proceedings of the 2009 Fifth International Joint Conference on INC, IMS and IDC, P303, DOI 10.1109/NCM.2009.206
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   HORNIK K, 1991, NEURAL NETWORKS, V4, P251, DOI 10.1016/0893-6080(91)90009-T
   Hu WJ, 2012, NEURAL NETWORKS, V27, P60, DOI 10.1016/j.neunet.2011.10.005
   Huang GB, 2006, NEUROCOMPUTING, V70, P489, DOI 10.1016/j.neucom.2005.12.126
   Huang GB, 2006, IEEE T NEURAL NETWOR, V17, P879, DOI 10.1109/TNN.2006.875977
   Huang GB, 2012, IEEE T SYST MAN CY B, V42, P513, DOI 10.1109/TSMCB.2011.2168604
   Huang GB, 2010, NEUROCOMPUTING, V74, P155, DOI 10.1016/j.neucom.2010.02.019
   Jun W, 2011, INT J MACH LEARN CYB, V2, P261, DOI 10.1007/s13042-011-0024-1
   LESHNO M, 1993, NEURAL NETWORKS, V6, P861, DOI 10.1016/S0893-6080(05)80131-5
   Li GX, 2012, IEEE T KNOWL DATA EN, V24, P2040, DOI 10.1109/TKDE.2011.160
   Liao W., 2010, P 2010 IEEE 17 INT C, P26
   Liu Y, 2014, IND ENG CHEM RES, V53, P5248, DOI 10.1021/ie401347k
   Liu Y, 2012, IND ENG CHEM RES, V51, P4313, DOI 10.1021/ie201650u
   Miche Y, 2010, IEEE T NEURAL NETWOR, V21, P158, DOI 10.1109/TNN.2009.2036259
   Mitaim S, 2001, IEEE T FUZZY SYST, V9, P637, DOI 10.1109/91.940974
   Mitchell B, 2012, 2012 11TH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA 2012), VOL 1, P162, DOI 10.1109/ICMLA.2012.34
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Qian PJ, 2012, IEEE T SYST MAN CY B, V42, P672, DOI 10.1109/TSMCB.2011.2172604
   Rong HJ, 2009, IEEE T SYST MAN CY B, V39, P1067, DOI 10.1109/TSMCB.2008.2010506
   Scholkopf B., 2002, LEARNING KERNELS
   Taylor JS, 2004, KERNEL METHODS PATTE
   Tian HX, 2010, IEEE T AUTOM SCI ENG, V7, P73, DOI 10.1109/TASE.2008.2005640
   Tsang IWH, 2006, IEEE T NEURAL NETWOR, V17, P1126, DOI 10.1109/TNN.2006.878123
   Vapnik VN, 1995, NATURE STAT LEARNING
   Wang F, 2010, IEEE T NEURAL NETWOR, V21, P319, DOI 10.1109/TNN.2009.2036998
   Wang ST, 2007, APPL SOFT COMPUT, V7, P577, DOI 10.1016/j.asoc.2006.04.008
   Wang ST, 2007, ARTIF INTELL MED, V39, P65, DOI 10.1016/j.artmed.2006.08.001
   Wang ST, 2015, NEUROCOMPUTING, V149, P295, DOI 10.1016/j.neucom.2014.01.065
   Wang ST, 2014, APPL SOFT COMPUT, V21, P677, DOI 10.1016/j.asoc.2014.04.001
   Wang ST, 2014, IEEE T CYBERNETICS, V44, P1, DOI 10.1109/TSMCB.2012.2236828
   Wang X, 2010, THESIS
   Wang XMM, 2010, PATTERN RECOGN, V43, P2753, DOI 10.1016/j.patcog.2010.02.013
   Yang J, 2004, IEEE T PATTERN ANAL, V26, P131, DOI 10.1109/TPAMI.2004.1261097
   Zhang CS, 2010, NEUROCOMPUTING, V73, P959, DOI 10.1016/j.neucom.2009.08.014
   Zhu QY, 2005, PATTERN RECOGN, V38, P1759, DOI 10.1016/j.patcog.2005.03.028
NR 49
TC 16
Z9 19
U1 1
U2 47
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 1568-4946
EI 1872-9681
J9 APPL SOFT COMPUT
JI Appl. Soft. Comput.
PD DEC
PY 2015
VL 37
BP 125
EP 141
DI 10.1016/j.asoc.2015.07.040
PG 17
WC Computer Science, Artificial Intelligence; Computer Science,
   Interdisciplinary Applications
SC Computer Science
GA CW5WR
UT WOS:000365067800011
DA 2020-02-19
ER

PT J
AU Manning, CD
AF Manning, Christopher D.
TI Computational Linguistics and Deep Learning
SO COMPUTATIONAL LINGUISTICS
LA English
DT Article
C1 [Manning, Christopher D.] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.
   [Manning, Christopher D.] Stanford Univ, Dept Linguist, Stanford, CA 94305 USA.
RP Manning, CD (reprint author), Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.
EM manning@cs.stanford.edu
CR Chomsky Noam, 1970, READINGS ENGLISH TRA, P184
   Houston A, 1985, THESIS
   Kim Y, 2014, P ACL 2014 WORKSH LA, P61
   Kulkarni V, 2015, PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW 2015), P625, DOI 10.1145/2736277.2741627
   Manning C. D., 1999, FDN STAT NATURAL LAN
   Mikolov T, 2013, ADV NEURAL INFORM PR, V27, P3111, DOI DOI 10.1162/JMLR.2003.3.4-5.951
   Luong MT, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P11
   Pennington J, 2014, P 2014 C EMP METH NA, V2014, P1532, DOI DOI 10.3115/V1/D14-1162
   Prince A., 2004, OPTIMALITY THEORY CO
   Ross John Robert, 1972, 8 REG M CHIC LING SO, V8, P316
   Rumelhart D.E., 1986, PARALLEL DISTRIBUTED, V1
   Sagi E, 2011, CURRENT METHODS HIST, P161
   Smolensky P, 2006, HARMONIC MIND NEURAL, V1
   Sutskever I., 2014, ADV NEURAL INFORM PR, P3104, DOI DOI 10.1007/S10107-014-0839-0
   Tabor Whitney, 1994, THESIS
NR 15
TC 31
Z9 34
U1 5
U2 50
PU MIT PRESS
PI CAMBRIDGE
PA ONE ROGERS ST, CAMBRIDGE, MA 02142-1209 USA
SN 0891-2017
EI 1530-9312
J9 COMPUT LINGUIST
JI Comput. Linguist.
PD DEC
PY 2015
VL 41
IS 4
BP 701
EP 707
DI 10.1162/COLI_a_00239
PG 7
WC Computer Science, Artificial Intelligence; Computer Science,
   Interdisciplinary Applications; Linguistics; Language & Linguistics
SC Computer Science; Linguistics
GA DA5AD
UT WOS:000367813400006
DA 2020-02-19
ER

PT J
AU Park, SW
   Park, J
   Bong, K
   Shin, D
   Lee, J
   Choi, S
   Yoo, HJ
AF Park, Seong-Wook
   Park, Junyoung
   Bong, Kyeongryeol
   Shin, Dongjoo
   Lee, Jinmook
   Choi, Sungpill
   Yoo, Hoi-Jun
TI An Energy-Efficient and Scalable Deep Learning/Inference Processor With
   Tetra-Parallel MIMD Architecture for Big Data Applications
SO IEEE TRANSACTIONS ON BIOMEDICAL CIRCUITS AND SYSTEMS
LA English
DT Article; Proceedings Paper
CT 62nd IEEE International Solid-State Circuits Conference (ISSCC)
CY FEB 22-26, 2015
CL San Francisco, CA
SP Inst Elect & Elect Engineers
DE Convolutional deep belief networks; deep inference; deep learning; fog
   computing; semi-supervised learning
ID VISION
AB Deep Learning algorithm is widely used for various pattern recognition applications such as text recognition, object recognition and action recognition because of its best-in-class recognition accuracy compared to hand-crafted algorithm and shallow learning based algorithms. Long learning time caused by its complex structure, however, limits its usage only in high-cost servers or many-core GPU platforms so far. On the other hand, the demand on customized pattern recognition within personal devices will grow gradually as more deep learning applications will be developed. This paper presents a SoC implementation to enable deep learning applications to run with low cost platforms such as mobile or portable devices. Different from conventional works which have adopted massively-parallel architecture, this work adopts task-flexible architecture and exploits multiple parallelism to cover complex functions of convolutional deep belief network which is one of popular deep learning/inference algorithms. In this paper, we implement the most energy-efficient deep learning and inference processor for wearable system. The implemented 2.5 mm x4.0 mm deep learning/inference processor is fabricated using 65 nm 8-metal CMOS technology for a battery-powered platform with real-time deep inference and deep learning operation. It consumes 185 mW average power, and 213.1 mW peak power at 200 MHz operating frequency and 1.2 V supply voltage. It achieves 411.3 GOPS peak performance and 1.93 TOPS/W energy efficiency, which is 2.07x higher than the state-of-the-art.
C1 [Park, Seong-Wook; Park, Junyoung; Bong, Kyeongryeol; Shin, Dongjoo; Lee, Jinmook; Choi, Sungpill; Yoo, Hoi-Jun] Korea Adv Inst Sci & Technol, Taejon 305701, South Korea.
RP Park, SW (reprint author), Korea Adv Inst Sci & Technol, Taejon 305701, South Korea.
EM seongwook.park@kaist.ac.kr
RI YOO, HOI-JUN/C-1558-2011
CR Armbrust M, 2010, COMMUN ACM, V53, P50, DOI 10.1145/1721654.1721672
   Bonomi F., 2012, P 1 ED MCC WORKSH MO, V2012, P13, DOI DOI 10.1145/2342509.2342513
   Chen TS, 2014, ACM SIGPLAN NOTICES, V49, P269, DOI 10.1145/2541940.2541967
   Coates A., 2013, P 30 INT C MACH LEAR, p1337~1345
   Hadsell R, 2009, J FIELD ROBOT, V26, P120, DOI 10.1002/rob.20276
   Hinton G. E, 2012, ARXIV12070580
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Kim J., 2014, J ELECT MAT, V2014, P1, DOI DOI 10.1167/14.7.1.[
   Kim SK, 2010, ANN IEEE SYM FIELD P, P201, DOI 10.1109/FCCM.2010.38
   Kim SK, 2009, I C FIELD PROG LOGIC, P367, DOI 10.1109/FPL.2009.5272262
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   LeCun Yann, 1990, ADV NEURAL INFORM PR, P396, DOI DOI 10.1111/DSU.12130
   Lee H., 2009, P 26 ANN INT C MACH, P609, DOI DOI 10.1145/1553374.1553453
   Liao JG, 1998, J COMPUT GRAPH STAT, V7, P253, DOI 10.2307/1390703
   Lu J, 2015, IEEE J SOLID-ST CIRC, V50, P270, DOI 10.1109/JSSC.2014.2356197
   Mike H., 2012, ANAL INTELS IVY BRID
   NVIDIA, N VIDIA GEFORCE GTX
   Park SK, 2015, IEEE INT MEM WORKSH, P1
   Peter M., 2011, 800145 SP
   Pham PH, 2012, MIDWEST SYMP CIRCUIT, P1044, DOI 10.1109/MWSCAS.2012.6292202
   Texas Instruments, TMS320C6678 TEX INST
   Tong S., 2001, P 9 ACM INT C MULT, P107, DOI DOI 10.1145/500141.500159
   Wang T, 2012, INT C PATT RECOG, P3304
   Zhang C., 2015, P 2015 ACM SIGDA INT, P161, DOI [10.1145/2684746.2689060, DOI 10.1145/2684746.2689060]
NR 25
TC 9
Z9 9
U1 1
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1932-4545
EI 1940-9990
J9 IEEE T BIOMED CIRC S
JI IEEE Trans. Biomed. Circuits Syst.
PD DEC
PY 2015
VL 9
IS 6
BP 838
EP 848
DI 10.1109/TBCAS.2015.2504563
PG 11
WC Engineering, Biomedical; Engineering, Electrical & Electronic
SC Engineering
GA DE3NM
UT WOS:000370536800010
PM 26780817
DA 2020-02-19
ER

PT J
AU Chan, TH
   Jia, K
   Gao, SH
   Lu, JW
   Zeng, ZN
   Ma, Y
AF Chan, Tsung-Han
   Jia, Kui
   Gao, Shenghua
   Lu, Jiwen
   Zeng, Zinan
   Ma, Yi
TI PCANet: A Simple Deep Learning Baseline for Image Classification?
SO IEEE TRANSACTIONS ON IMAGE PROCESSING
LA English
DT Article
DE Convolution neural network; deep learning; PCA network; random network;
   LDA network; face recognition; handwritten digit recognition; object
   classification
ID FACE-RECOGNITION; PATTERNS; MAGNITUDES; ALGORITHM; MODELS
AB In this paper, we propose a very simple deep learning network for image classification that is based on very basic data processing components: 1) cascaded principal component analysis (PCA); 2) binary hashing; and 3) blockwise histograms. In the proposed architecture, the PCA is employed to learn multistage filter banks. This is followed by simple binary hashing and block histograms for indexing and pooling. This architecture is thus called the PCA network (PCANet) and can be extremely easily and efficiently designed and learned. For comparison and to provide a better understanding, we also introduce and study two simple variations of PCANet: 1) RandNet and 2) LDANet. They share the same topology as PCANet, but their cascaded filters are either randomly selected or learned from linear discriminant analysis. We have extensively tested these basic networks on many benchmark visual data sets for different tasks, including Labeled Faces in the Wild (LFW) for face verification; the MultiPIE, Extended Yale B, AR, Facial Recognition Technology (FERET) data sets for face recognition; and MNIST for hand-written digit recognition. Surprisingly, for all tasks, such a seemingly naive PCANet model is on par with the state-of-the-art features either prefixed, highly hand-crafted, or carefully learned [by deep neural networks (DNNs)]. Even more surprisingly, the model sets new records for many classification tasks on the Extended Yale B, AR, and FERET data sets and on MNIST variations. Additional experiments on other public data sets also demonstrate the potential of PCANet to serve as a simple but highly competitive baseline for texture classification and object recognition.
C1 [Chan, Tsung-Han] MediaTek Inc, Hsinchu 30078, Taiwan.
   [Jia, Kui] Univ Macau, Fac Sci & Technol, Dept Comp & Informat Sci, Macau, Peoples R China.
   [Gao, Shenghua; Ma, Yi] ShanghaiTech Univ, Sch Informat Sci & Technol, Shanghai 200031, Peoples R China.
   Tsinghua Univ, Dept Automat, Beijing 100084, Peoples R China.
   [Zeng, Zinan] Adv Digital Sci Ctr, Singapore 138632, Singapore.
   [Ma, Yi] Univ Illinois, Dept Elect & Comp Engn, Urbana, IL 61801 USA.
RP Chan, TH (reprint author), MediaTek Inc, Hsinchu 30078, Taiwan.
EM thchan@ieee.org; kuijia@umac.mo; gaoshh@shanghaitech.edu.cn;
   elujiwen@gmail.com; zeng.zinan@gmail.com; mayi@shanghaitech.edu.cn
CR Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   Barkan O, 2013, IEEE I CONF COMP VIS, P1960, DOI 10.1109/ICCV.2013.246
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bo L., 2010, ADV NEURAL INFORM PR, P244
   Broadhurst R. E., 2006, P WORKSH TEXT AN SYN, P1
   Bruna J, 2013, IEEE T PATTERN ANAL, V35, P1872, DOI 10.1109/TPAMI.2012.230
   Burges CJC, 2003, IEEE T SPEECH AUDI P, V11, P165, DOI 10.1109/TSA.2003.811538
   Chai ZH, 2014, IEEE T INF FOREN SEC, V9, P14, DOI 10.1109/TIFS.2013.2290064
   Chen D, 2013, PROC CVPR IEEE, P3025, DOI 10.1109/CVPR.2013.389
   Ciresan D, 2012, PROC CVPR IEEE, P3642, DOI 10.1109/CVPR.2012.6248110
   Coates A., 2001, P 14 INT C ART INT S, P215
   Crosier M, 2010, INT J COMPUT VISION, V88, P447, DOI 10.1007/s11263-009-0315-0
   Cui Z, 2013, PROC CVPR IEEE, P3554, DOI 10.1109/CVPR.2013.456
   Dalal N, 2005, PROC CVPR IEEE, P886
   Deng WH, 2012, IEEE T PATTERN ANAL, V34, P1864, DOI 10.1109/TPAMI.2012.30
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fan H, 2014, LEARNING DEEP FACE R
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Fei-Fei L, 2005, PROC CVPR IEEE, P524
   Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464
   Goodfellow I., 2013, P 30 ICML, P1
   Grauman K, 2005, IEEE I CONF COMP VIS, P1458
   Gross Ralph, 2008, P 8 IEEE INT C AUT F, P1, DOI DOI 10.1109/AFGR.2008.4813399
   Hayman E, 2004, LECT NOTES COMPUT SC, V2034, P253
   He K., 2015, SPATIAL PYRAMID POOL
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI 10.1007/978-3-319-10578-9_23
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Huang Gary B., 2007, 0749 U MASS
   Hussain SU, 2012, BRIT MACH VIS C, P11
   Jarrett K, 2009, IEEE I CONF COMP VIS, P2146, DOI 10.1109/ICCV.2009.5459469
   Jia Y., 2013, CAFFE OPEN SOURCE CO
   Kavukcuoglu K., 2010, ADV NEURAL INFORM PR, V23, P1090
   Keysers D, 2007, IEEE T PATTERN ANAL, V29, P1422, DOI 10.1109/TPAMI.2007.1153
   Krizhevsky A., 2014, CUDA CONVNET
   Krizhevsky A, 2009, LEARNING MULTIPLE LA
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Larochelle H., 2007, P 24 INT C MACH LEAR, P473, DOI DOI 10.1145/1273496.1273556
   Lazebnik S., 2006, P IEEE INT C COMP VI, V2, P2169, DOI DOI 10.1109/CVPR.2006.68
   Le Q.V., 2010, ADV NEURAL INFORM PR, V23, P1279
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee H., 2009, P 26 ANN INT C MACH, P609, DOI DOI 10.1145/1553374.1553453
   Lin M., 2014, NETWORK IN NETWORK
   Liu CJ, 2002, IEEE T IMAGE PROCESS, V11, P467, DOI 10.1109/TIP.2002.999679
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu JW, 2013, IEEE T PATTERN ANAL, V35, P39, DOI 10.1109/TPAMI.2012.70
   Martinez AM, 1998, 24 CVC
   Vu NS, 2012, IEEE T IMAGE PROCESS, V21, P1352, DOI 10.1109/TIP.2011.2166974
   Phillips PJ, 1998, IMAGE VISION COMPUT, V16, P295, DOI 10.1016/S0262-8856(97)00070-X
   Rifai S., 2011, P 28 ICML, P1
   Russakovsky O., 2014, IMAGENET LARGE SCALE
   Sifre L, 2013, PROC CVPR IEEE, P1233, DOI 10.1109/CVPR.2013.163
   Snoek J., 2012, ADV NEURAL INFORM PR, P2951
   Sohn K., 2012, P 29 INT C MACH LEAR, P1311
   Sohn K., 2013, P 30 INT C MACH LEAR, V28, P217
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Tan XY, 2010, IEEE T IMAGE PROCESS, V19, P1635, DOI 10.1109/TIP.2010.2042645
   Varma M, 2009, IEEE T PATTERN ANAL, V31, P2032, DOI 10.1109/TPAMI.2008.182
   Vu NS, 2013, IEEE T INF FOREN SEC, V8, P295, DOI 10.1109/TIFS.2012.2224866
   Wolf L, 2011, IEEE T PATTERN ANAL, V33, P1978, DOI 10.1109/TPAMI.2010.230
   Xie SF, 2010, IEEE T IMAGE PROCESS, V19, P1349, DOI 10.1109/TIP.2010.2041397
   Yu H, 2001, PATTERN RECOGN, V34, P2067, DOI 10.1016/S0031-3203(00)00162-X
   Yu K., 2010, P 27 INT C MACH LEAR, P1215
   Yu K, 2011, PROC CVPR IEEE, P1713, DOI 10.1109/CVPR.2011.5995732
   Zeiler M. D., 2013, P ICLR
NR 65
TC 399
Z9 458
U1 58
U2 514
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1057-7149
EI 1941-0042
J9 IEEE T IMAGE PROCESS
JI IEEE Trans. Image Process.
PD DEC
PY 2015
VL 24
IS 12
BP 5017
EP 5032
DI 10.1109/TIP.2015.2475625
PG 16
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
SC Computer Science; Engineering
GA CS3XF
UT WOS:000362008200015
PM 26340772
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Hong, CQ
   Yu, J
   Wan, J
   Tao, DC
   Wang, M
AF Hong, Chaoqun
   Yu, Jun
   Wan, Jian
   Tao, Dacheng
   Wang, Meng
TI Multimodal Deep Autoencoder for Human Pose Recovery
SO IEEE TRANSACTIONS ON IMAGE PROCESSING
LA English
DT Article
DE Human pose recovery; deep learning; multi-modal learning; hypergraph;
   back propagation
ID 3D HUMAN POSE; RECOGNITION; TRACKING; POINTS
AB Video-based human pose recovery is usually conducted by retrieving relevant poses using image features. In the retrieving process, the mapping between 2D images and 3D poses is assumed to be linear in most of the traditional methods. However, their relationships are inherently non-linear, which limits recovery performance of these methods. In this paper, we propose a novel pose recovery method using non-linear mapping with multi-layered deep neural network. It is based on feature extraction with multimodal fusion and back-propagation deep learning. In multimodal fusion, we construct hypergraph Laplacian with low-rank representation. In this way, we obtain a unified feature description by standard eigen-decomposition of the hypergraph Laplacian matrix. In back-propagation deep learning, we learn a non-linear mapping from 2D images to 3D poses with parameter fine-tuning. The experimental results on three data sets show that the recovery error has been reduced by 20%-25%, which demonstrates the effectiveness of the proposed method.
C1 [Hong, Chaoqun] Xiamen Univ Technol, Coll Comp & Informat Engn, Xiamen 361024, Peoples R China.
   [Yu, Jun; Wan, Jian] Hangzhou Dianzi Univ, Sch Comp Sci, Hangzhou 310018, Zhejiang, Peoples R China.
   [Tao, Dacheng] Univ Technol Sydney, Ctr Quantum Computat & Intelligent Syst, Ultimo, NSW 2007, Australia.
   [Tao, Dacheng] Univ Technol Sydney, Fac Engn & Informat Technol, Ultimo, NSW 2007, Australia.
   [Wang, Meng] Hefei Univ Technol, Sch Comp Sci & Informat Engn, Hefei 230009, Peoples R China.
RP Hong, CQ (reprint author), Xiamen Univ Technol, Coll Comp & Informat Engn, Xiamen 361024, Peoples R China.
EM cqhong@xmut.edu.cn; yujun@hdu.edu.cn; wanjian@hdu.edu.cn;
   dacheng.tao@uts.edu.au; eric.mengwang@gmail.com
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61472110, 61202145, 61272393, 61322201, 61432019];
   National 973 Program of ChinaNational Basic Research Program of China
   [2014CB347600]; Natural Science Foundation of Fujian Province,
   ChinaNatural Science Foundation of Fujian Province [2014J01256];
   Zhejiang Provincial Natural Science Foundation of ChinaNatural Science
   Foundation of Zhejiang Province [LR15F020002]; Hong Kong Scholar
   Programme [XJ2013038]; Australian Research Council ProjectAustralian
   Research Council [DP-120103730, FT-130101457, LP-140100569]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61472110, Grant 61202145, Grant
   61272393, Grant 61322201, and Grant 61432019, in part by the National
   973 Program of China under Grant 2014CB347600, in part by the Natural
   Science Foundation of Fujian Province, China, under Grant 2014J01256, in
   part by the Zhejiang Provincial Natural Science Foundation of China
   under Grant LR15F020002, in part by the Hong Kong Scholar Programme
   under Grant XJ2013038, and in part by the Australian Research Council
   Project under Grant DP-120103730, Grant FT-130101457, and Grant
   LP-140100569. The associate editor coordinating the review of this
   manuscript and approving it for publication was Mr. Pierre-Marc Jodoin.
CR Agarwal A, 2006, IEEE T PATTERN ANAL, V28, P44, DOI 10.1109/TPAMI.2006.21
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   Bengio Y, 2006, ADV NEURAL INFORM PR, P153
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Bo LF, 2010, INT J COMPUT VISION, V87, P28, DOI 10.1007/s11263-008-0204-y
   Brand M., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1237, DOI 10.1109/ICCV.1999.790422
   Cheng B, 2011, IEEE I CONF COMP VIS, P2439, DOI 10.1109/ICCV.2011.6126528
   Dalal N, 2005, PROC CVPR IEEE, P886
   Everts I, 2014, IEEE T IMAGE PROCESS, V23, P1569, DOI 10.1109/TIP.2014.2302677
   Howe NR, 2000, ADV NEUR IN, V12, P820
   Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248
   Lee MW, 2009, IEEE T PATTERN ANAL, V31, P27, DOI 10.1109/TPAMI.2008.35
   Li SJ, 2015, LECT NOTES COMPUT SC, V9004, P332, DOI 10.1007/978-3-319-16808-1_23
   Liu G., 2010, P INT C MACH LEARN, V27, P663
   Liu JM, 2014, IEEE T IMAGE PROCESS, V23, P4022, DOI 10.1109/TIP.2014.2343458
   Mairal J., 2009, P 26 ANN INT C MACH, P689, DOI DOI 10.1145/1553374.1553463
   Mori G, 2005, IEEE T PATTERN ANAL, V27, P1832, DOI 10.1109/TPAMI.2005.220
   Mori G, 2002, LECT NOTES COMPUT SC, V2352, P666
   Ngiam J, 2011, IEEE INT C MACH LEAR, V28, P689, DOI DOI 10.1145/2647868
   Ouyang WL, 2014, PROC CVPR IEEE, P2337, DOI 10.1109/CVPR.2014.299
   Palm R. B., 2012, THESIS TU DENMARK LY
   Rosales R, 2000, PROC CVPR IEEE, P721, DOI 10.1109/CVPR.2000.854946
   Sapp B, 2013, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR.2013.471
   Shakhnarovich G, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P750
   Shen J, 2014, IEEE T IMAGE PROCESS, V23, P4786, DOI 10.1109/TIP.2014.2358082
   SIDENBLADH H, 2000, EUR C COMP VIS, V2, P702
   Sigal L, 2010, INT J COMPUT VISION, V87, P1, DOI 10.1007/s11263-009-0293-2
   Srivastava N., 2012, ADV NEURAL INFORM PR, P2222, DOI DOI 10.1109/CVPR.2013.49
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Tao DP, 2016, IEEE T NEUR NET LEAR, V27, P1122, DOI 10.1109/TNNLS.2015.2461554
   Tao DP, 2016, IEEE T CYBERNETICS, V46, P756, DOI 10.1109/TCYB.2015.2414920
   Tian Y., 2010, P AS C COMP VIS, V3, P679
   Toshev A, 2014, PROC CVPR IEEE, P1653, DOI 10.1109/CVPR.2014.214
   Vincent P, 2008, P 25 INT C MACH LEAR, P1096, DOI DOI 10.1145/1390156.1390294
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Wang CY, 2014, PROC CVPR IEEE, P2369, DOI 10.1109/CVPR.2014.303
   Xia TA, 2010, IEEE T SYST MAN CY B, V40, P1438, DOI 10.1109/TSMCB.2009.2039566
   Yang KF, 2014, IEEE T IMAGE PROCESS, V23, P5020, DOI 10.1109/TIP.2014.2361210
   Yang M, 2006, INT C PATT RECOG, P958
   Yang Y, 2011, PROC CVPR IEEE, P1385, DOI 10.1109/CVPR.2011.5995741
   Zhang WC, 2014, IEEE T IMAGE PROCESS, V23, P5374, DOI 10.1109/TIP.2014.2364113
   ZHOU D, 2007, P ADV NEUR INF PROC, V19, P1601
   Zhou HY, 2014, IEEE T IMAGE PROCESS, V23, P3468, DOI 10.1109/TIP.2014.2329765
NR 43
TC 228
Z9 228
U1 14
U2 128
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1057-7149
EI 1941-0042
J9 IEEE T IMAGE PROCESS
JI IEEE Trans. Image Process.
PD DEC
PY 2015
VL 24
IS 12
BP 5659
EP 5670
DI 10.1109/TIP.2015.2487860
PG 12
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
SC Computer Science; Engineering
GA CV1VU
UT WOS:000364047000001
PM 26452284
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Yu, YT
   Guan, HY
   Ji, Z
AF Yu, Yongtao
   Guan, Haiyan
   Ji, Zheng
TI Automated Detection of Urban Road Manhole Covers Using Mobile Laser
   Scanning Data
SO IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS
LA English
DT Article
DE Deep learning; manhole cover; mobile laser scanning (MLS); random
   forest; road distress; road safety
ID DRIVER-ASSISTANCE; EXTRACTION
AB This paper proposes a novel framework for automated detection of urban road manhole covers using mobile laser scanning (MLS) data. First, to narrow searching regions and reduce the computational complexity, road surface points are segmented from a raw point cloud via a curb-based road surface segmentation approach and rasterized into a georeferenced intensity image through inverse distance weighted interpolation. Then, a supervised deep learning model is developed to construct a multilayer feature generation model for depicting high-order features of local image patches. Next, a random forest model is trained to learnmappings from high-order patch features to the probabilities of the existence of urban road manhole covers centered at specific locations. Finally, urban road manhole covers are detected from georeferenced intensity images based on the multilayer feature generation model and random forest model. Quantitative evaluations show that the proposed algorithm achieves an average completeness, correctness, quality, and F-1-measure of 0.955, 0.959, 0.917, and 0.957, respectively, in detecting urban road manhole covers from georeferenced intensity images. Comparative studies demonstrate the advantageous performance of the proposed algorithm over other existing methods for rapid and automated detection of urban road manhole covers using MLS data.
C1 [Yu, Yongtao] Xiamen Univ, Fujian Key Lab Sensing & Comp Smart Cities, Xiamen 361005, Peoples R China.
   [Guan, Haiyan] Nanjing Univ Informat Sci & Technol, Coll Geog & Remote Sensing, Nanjing 210044, Jiangsu, Peoples R China.
   [Ji, Zheng] Wuhan Univ, Sch Remote Sensing Informat & Engn, Wuhan 430079, Peoples R China.
RP Yu, YT (reprint author), Xiamen Univ, Fujian Key Lab Sensing & Comp Smart Cities, Xiamen 361005, Peoples R China.
EM allennessy.yu@gmail.com; guanhy.nj@nuist.edu.cn; jz07@whu.edu.cn
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [41471379]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 41471379.
CR Brogan M, 2013, IET COMPUT VIS, V7, P209, DOI 10.1049/iet-cvi.2012.0085
   Broggi A, 2013, IEEE T INTELL TRANSP, V14, P1403, DOI 10.1109/TITS.2013.2262331
   Carneiro G, 2013, IEEE T PATTERN ANAL, V35, P2592, DOI 10.1109/TPAMI.2013.96
   Chen B, 2013, IEEE T PATTERN ANAL, V35, P1887, DOI 10.1109/TPAMI.2013.19
   Cheng H, 2007, IEEE T INTELL TRANSP, V8, P157, DOI 10.1109/TITS.2006.890073
   Cheng Y., 2006, OPT TECH, V32, P504
   Choi J, 2012, IEEE T INTELL TRANSP, V13, P974, DOI 10.1109/TITS.2011.2179802
   Drewniok C, 1997, INT J COMPUT VISION, V24, P187, DOI 10.1023/A:1007919223573
   Du SZ, 2014, IEEE SIGNAL PROC LET, V21, P51, DOI 10.1109/LSP.2013.2290547
   Gall J, 2011, IEEE T PATTERN ANAL, V33, P2188, DOI 10.1109/TPAMI.2011.70
   Guan HY, 2014, REMOTE SENS LETT, V5, P1042, DOI 10.1080/2150704X.2014.994716
   Guan HY, 2014, ISPRS J PHOTOGRAMM, V87, P93, DOI 10.1016/j.isprsjprs.2013.11.005
   Havens S, 2013, IEEE T GEOSCI REMOTE, V51, P3328, DOI 10.1109/TGRS.2012.2220549
   Ji SP, 2012, INT ARCH PHOTOGRAMM, V39-B3, P281
   Mahapatra D, 2014, IEEE T IMAGE PROCESS, V23, P1504, DOI 10.1109/TIP.2014.2305073
   Murray S, 2011, IET INTELL TRANSP SY, V5, P221, DOI 10.1049/iet-its.2010.0105
   Ni BB, 2013, IEEE T IMAGE PROCESS, V22, P739, DOI 10.1109/TIP.2012.2222895
   Niigaki H, 2012, INT C PATT RECOG, P2009
   Rutzinger M, 2009, IEEE J-STARS, V2, P11, DOI 10.1109/JSTARS.2009.2012488
   Salakhutdinov R, 2009, ARTIF INTELL, P448, DOI DOI 10.1109/CVPRW.2009.5206577
   Salakhutdinov R, 2013, IEEE T PATTERN ANAL, V35, P1958, DOI 10.1109/TPAMI.2012.269
   Salakhutdinov R, 2012, NEURAL COMPUT, V24, P1967, DOI 10.1162/NECO_a_00311
   Tanaka N., 2000, P IAPR WORKSH MACH V, P387
   Tieleman T., 2008, P 25 INT C MACH LEAR, P1064, DOI DOI 10.1145/1390156.1390290
   Timofte R., 2011, P IEEE INT C COMP VI, P188, DOI DOI 10.1109/ICCVW.2011.6130242
   Williams K, 2013, REMOTE SENS-BASEL, V5, P4652, DOI 10.3390/rs5094652
   Yu YT, 2014, IEEE GEOSCI REMOTE S, V11, P1549, DOI 10.1109/LGRS.2014.2301195
   Zheng NN, 2004, IEEE INTELL SYST, V19, P8
NR 28
TC 12
Z9 12
U1 3
U2 40
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1524-9050
EI 1558-0016
J9 IEEE T INTELL TRANSP
JI IEEE Trans. Intell. Transp. Syst.
PD DEC
PY 2015
VL 16
IS 6
DI 10.1109/TITS.2015.2413812
PG 12
WC Engineering, Civil; Engineering, Electrical & Electronic; Transportation
   Science & Technology
SC Engineering; Transportation
GA CX8MS
UT WOS:000365958500025
DA 2020-02-19
ER

PT J
AU He, SF
   Lau, RWH
   Liu, WX
   Huang, Z
   Yang, QX
AF He, Shengfeng
   Lau, Rynson W. H.
   Liu, Wenxi
   Huang, Zhe
   Yang, Qingxiong
TI SuperCNN: A Superpixelwise Convolutional Neural Network for Salient
   Object Detection
SO INTERNATIONAL JOURNAL OF COMPUTER VISION
LA English
DT Article
DE Convolutional neural networks; Deep learning; Feature learning; Saliency
   detection
ID VISUAL-ATTENTION; MAP
AB Existing computational models for salient object detection primarily rely on hand-crafted features, which are only able to capture low-level contrast information. In this paper, we learn the hierarchical contrast features by formulating salient object detection as a binary labeling problem using deep learning techniques. A novel superpixelwise convolutional neural network approach, called SuperCNN, is proposed to learn the internal representations of saliency in an efficient manner. In contrast to the classical convolutional networks, SuperCNN has four main properties. First, the proposed method is able to learn the hierarchical contrast features, as it is fed by two meaningful superpixel sequences, which is much more effective for detecting salient regions than feeding raw image pixels. Second, as SuperCNN recovers the contextual information among superpixels, it enables large context to be involved in the analysis efficiently. Third, benefiting from the superpixelwise mechanism, the required number of predictions for a densely labeled map is hugely reduced. Fourth, saliency can be detected independent of region size by utilizing a multiscale network structure. Experiments show that SuperCNN can robustly detect salient objects and outperforms the state-of-the-art methods on three benchmark datasets.
C1 [He, Shengfeng; Lau, Rynson W. H.; Liu, Wenxi; Huang, Zhe; Yang, Qingxiong] City Univ Hong Kong, Kowloon Tong, Hong Kong, Peoples R China.
RP He, SF (reprint author), City Univ Hong Kong, Kowloon Tong, Hong Kong, Peoples R China.
EM shengfeng_he@yahoo.com; rynson.lau@cityu.edu.hk; magicpiratex@gmail.com;
   igamenovoer@gmail.com; qiyang@cityu.edu.hk
RI He, Shengfeng/E-5682-2016; Yang, Qingxiong/K-1729-2015
OI He, Shengfeng/0000-0002-3802-4644; LAU, Rynson W H/0000-0002-8957-8129;
   Yang, Qingxiong/0000-0002-4378-2335
FU RGC of Hong Kong (RGC) [CityU 115112, CityU 21201914]
FX We would like to thank the anonymous reviewers for their insightful
   comments and constructive suggestions. The work described in this paper
   was partially supported by a GRF Grant and an ECS grant from the RGC of
   Hong Kong (RGC Ref.: CityU 115112 and CityU 21201914).
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Arbelaez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Avidan S, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239461
   Bell RM, 2007, SIGKDD EXPLOR NEWSL, V9, P75, DOI DOI 10.1145/1345448.1345465
   Borji A., 2012, ECCV
   Borji A, 2012, PROC CVPR IEEE, P438, DOI 10.1109/CVPR.2012.6247706
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Cheng MM, 2013, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2013.193
   Ciresan D, 2012, PROC CVPR IEEE, P3642, DOI 10.1109/CVPR.2012.6248110
   Ciresan D, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P1918, DOI 10.1109/IJCNN.2011.6033458
   Collobert R., 2011, BIGLEARN NIPS WORKSH
   Einhauser W, 2003, EUR J NEUROSCI, V17, P1089, DOI 10.1046/j.1460-9568.2003.02508.x
   Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231
   Girshick R., 2014, CVPR
   Goferman S., 2010, CVPR
   Harel J, 2007, ADV NEURAL INF PROCE, P545, DOI DOI 10.7551/mitpress/7503.003.0073
   He SF, 2014, LECT NOTES COMPUT SC, V8691, P110, DOI 10.1007/978-3-319-10578-9_8
   Hinton G. E., 2012, ABS12070580 CORR
   Intriligator J, 2001, COGNITIVE PSYCHOL, V43, P171, DOI 10.1006/cogp.2001.0755
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Itti L, 2001, NAT REV NEUROSCI, V2, P194, DOI 10.1038/35058500
   Jiang H, 2011, BMVC
   Jiang HZ, 2013, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2013.271
   Jiang P, 2013, ICCV
   KOCH C, 1985, HUM NEUROBIOL, V4, P219
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee TS, 2003, J OPT SOC AM A, V20, P1434, DOI 10.1364/JOSAA.20.001434
   Li S., 2014, INT J COMPUT VISION, P1
   Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70
   Lu Y, 2012, PROC CVPR IEEE, P1067, DOI 10.1109/CVPR.2012.6247785
   Ma Y.-F., 2003, P 11 ACM INT C MULT, P374, DOI DOI 10.1145/957013.957094
   Macaluso E, 2002, CEREB CORTEX, V12, P357, DOI 10.1093/cercor/12.4.357
   Marchesotti L, 2009, IEEE I CONF COMP VIS, P2232, DOI 10.1109/ICCV.2009.5459467
   Margolin R., 2013, CVPR
   Min Chen, 2011, IEEE INFOCOM 2011 - IEEE Conference on Computer Communications Workshops, P409, DOI 10.1109/INFCOMW.2011.5928847
   Ming-Chng, 2013, ICCV
   Moore AP, 2008, P IEEE C COMP VIS PA, DOI DOI 10.1109/CVPR.2008.4587471
   Osadchy M, 2007, J MACH LEARN RES, V8, P1197
   Parkhurst D, 2002, VISION RES, V42, P107, DOI 10.1016/S0042-6989(01)00250-4
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Pinheiro P., 2014, P 31 INT C MACH LEAR, V32, P82, DOI DOI 10.1016/J.VETPAR.2009.02.011
   Shen C, 2012, DEEP LEARN UNS FEAT
   Sun Y, 2013, PROC CVPR IEEE, P3476, DOI 10.1109/CVPR.2013.446
   Szegedy Christian, 2014, ABS14094842 CORR
   Tatler BW, 2007, J VISION, V7, DOI 10.1167/7.14.4
   Toet A, 2011, IEEE T PATTERN ANAL, V33, P2131, DOI 10.1109/TPAMI.2011.53
   Winnemoeller H, 2012, COMPUT GRAPH-UK, V36, P740, DOI 10.1016/j.cag.2012.03.004
   Yan Q., 2013, CVPR
   Yang Chih-Yuan, 2013, CVPR
NR 51
TC 81
Z9 91
U1 2
U2 69
PU SPRINGER
PI DORDRECHT
PA VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN 0920-5691
EI 1573-1405
J9 INT J COMPUT VISION
JI Int. J. Comput. Vis.
PD DEC
PY 2015
VL 115
IS 3
BP 330
EP 344
DI 10.1007/s11263-015-0822-0
PG 15
WC Computer Science, Artificial Intelligence
SC Computer Science
GA CW6EH
UT WOS:000365089800006
DA 2020-02-19
ER

PT J
AU Yang, L
   Tian, SW
   Yu, L
   Ye, FY
   Qian, J
   Qian, YR
AF Yang, Liu
   Tian, Shengwei
   Yu, Long
   Ye, Feiyue
   Qian, Jin
   Qian, Yurong
TI DEEP LEARNING FOR EXTRACTING WATER BODY FROM LANDSAT IMAGERY
SO INTERNATIONAL JOURNAL OF INNOVATIVE COMPUTING INFORMATION AND CONTROL
LA English
DT Article
DE Unsupervised feature learning; Deep Learning; Water body extraction;
   Feature Expansion Algorithm; Stacked sparse autoencoders
AB There are regional limitations in traditional methods of water body extraction. For different terrain, all the methods rely heavily on carefully hand-engineered feature selection and large amounts of prior knowledge. Due to the difficulty and high cost in acquiring, the labeled data of remote sensing is relatively small. Thus, there exist some challenges in the classification of huge amount of high dimension remote sensing data. Deep Learning has a good capacity of hierarchical feature learning from unlabeled data. Stacked sparse autoencoder (SSAE), one deep learning method, is widely investigated for image recognition. In this paper, a new water body extraction model based on SSAE is established. At first, current useful features (NDWI, NDVI, NDBI and so forth) are collected to construct unique feature matrix for each pixel. Next, a Feature Expansion Algorithm (FEA) is designed by taking account of the influence of neighboring pixels to expand feature matrixes. Setting the expansion features as inputs, SSAE is trained to extract water body. The experimental results showed that the proposed model outperformed Support Vector Machine (SVM) and traditional neural network (NN). Meanwhile, the proposed FEA explored more distinct features of water body so that the accuracy of water body extraction was improved to a great extent.
C1 [Yang, Liu; Tian, Shengwei; Qian, Yurong] Xinjiang Univ, Sch Software, Urumqi, Peoples R China.
   [Yang, Liu; Tian, Shengwei; Yu, Long; Ye, Feiyue; Qian, Jin] Jiangsu Univ Technol, Key Lab Cloud Comp & Intelligent Informat Proc Ch, 1801 Zhongwu Rd, Changzhou 213001, Peoples R China.
   [Yang, Liu; Tian, Shengwei; Yu, Long; Ye, Feiyue; Qian, Jin] Jiangsu Univ Technol, Coll Comp Engn, Changzhou 213001, Peoples R China.
   [Yu, Long] Xinjiang Univ, Network Ctr, Urumqi 830046, Peoples R China.
RP Ye, FY (reprint author), Jiangsu Univ Technol, Key Lab Cloud Comp & Intelligent Informat Proc Ch, 1801 Zhongwu Rd, Changzhou 213001, Peoples R China.
EM yangliu_xju@163.com; tianshengwei@163.com; yul_xju@163.com;
   yfy@jstu.edu.cn; qjqjlqyf@163.com
FU Open Project of Key Laboratory of Cloud Computing and Intelligent
   Information Processing of Changzhou City [CM20123004-KF02]; National
   Natural Science Foundation of ChinaNational Natural Science Foundation
   of China [61363083]; Research Innovation Project of Graduate Student in
   Xinjiang [XJGRI2014033]
FX The study is under the auspices of Open Project of Key Laboratory of
   Cloud Computing and Intelligent Information Processing of Changzhou City
   (CM20123004-KF02). Meanwhile, this research is supported by the National
   Natural Science Foundation of China (No. 61363083) and Research
   Innovation Project of Graduate Student in Xinjiang (No. XJGRI2014033).
CR Andrew N., 2011, CS294 A LECT NOTES, V72
   Arel I, 2010, IEEE COMPUT INTELL M, V5, P13, DOI 10.1109/MCI.2010.938364
   BARTON IJ, 1989, REMOTE SENS ENVIRON, V30, P89, DOI 10.1016/0034-4257(89)90050-3
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Braud D.H., 1998, OSRAPD TECHNICAL REP, V97-002
   [陈华芳 Chen Huafang], 2004, [遥感技术与应用, Remote sensing Technology and Application], V19, P479
   Deng J., 2013, HUM ASS C AFF COMP I
   Dixon B, 2008, INT J REMOTE SENS, V29, P1185, DOI 10.1080/01431160701294661
   Du Y. Y., 1998, J REMOTE SENSING, V2, P264
   Gimpel Kevin, 2010, HUM LANG TECHN 2010, P733
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hu Xiaolin, 2013, COMMUN CCF, V9, P64
   Jian X., 2014, SCI TECHNOL ENG, V14, P267
   Jiang H, 2014, REMOTE SENS-BASEL, V6, P5067, DOI 10.3390/rs6065067
   Jupp D.L.B., 1985, LANDSAT BASED INTERP
   Liu Y, 2013, COMPUT GEOSCI-UK, V59, P98, DOI 10.1016/j.cageo.2013.03.024
   Lu JJ, 1992, REMOTE SENS ENVIRON, P17
   [吕启 Lu Qi], 2014, [计算机研究与发展, Journal of Computer Research and Development], V51, P1911
   McFeeters S K, 1996, GEOGRAPHICAL RES, V17, P1425
   McFeeters SK, 1996, INT J REMOTE SENS, V17, P1425, DOI 10.1080/01431169608948714
   Mnih V, 2010, LECT NOTES COMPUT SC, V6316, P210, DOI 10.1007/978-3-642-15567-3_16
   MOLLERJENSEN L, 1990, PHOTOGRAMM ENG REM S, V56, P899
   Qi ZX, 2012, REMOTE SENS ENVIRON, V118, P21, DOI 10.1016/j.rse.2011.11.001
   Senaras C., 2014, GEOSC REM SENS S IGR
   [沈金祥 Shen Jinxiang], 2012, [国土资源遥感, Remote Sensing for Land & Resources], P84
   Shin HC, 2013, IEEE T PATTERN ANAL, V35, P1930, DOI 10.1109/TPAMI.2012.277
   Sun WX, 2003, IEEE T GEOSCI REMOTE, V41, P883, DOI 10.1109/TGRS.2003.810707
   Tuerk A., 2008, IEEE INT C SPEECH SI
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Wang J., 2004, MINE SURVEYING, V4, P30
   Wang Wei-wu, 2013, Journal of System Simulation, V25, P2196
   Wang Y. N., 2013, 21 INT C GEOINF KAIF
   Xu T., 2010, GEOSPATIAL INFORM, V3, P64
   Yu D, 2011, IEEE SIGNAL PROC MAG, V28, P145, DOI 10.1109/MSP.2010.939038
   [余凯 Yu Kai], 2013, [计算机研究与发展, Journal of Computer Research and Development], V50, P1799
   Zha Y, 2003, INT J REMOTE SENS, V24, P583, DOI 10.1080/01431160304987
   Zhang Y, 2007, INT J REMOTE SENS, V28, P2077, DOI 10.1080/01431160500406870
   Zhou YA, 2014, IEEE J-STARS, V7, P4301, DOI 10.1109/JSTARS.2014.2360436
NR 38
TC 6
Z9 6
U1 1
U2 4
PU ICIC INTERNATIONAL
PI KUMAMOTO
PA TOKAI UNIV, 9-1-1, TOROKU, KUMAMOTO, 862-8652, JAPAN
SN 1349-4198
EI 1349-418X
J9 INT J INNOV COMPUT I
JI Int. J. Innov. Comp. Inf. Control
PD DEC
PY 2015
VL 11
IS 6
BP 1913
EP 1929
PG 17
WC Computer Science, Artificial Intelligence
SC Computer Science
GA VA2XO
UT WOS:000409770800006
DA 2020-02-19
ER

PT J
AU Ciompi, F
   de Hoop, B
   van Riel, SJ
   Chung, K
   Scholten, ET
   Oudkerk, M
   de Jong, PA
   Prokop, M
   van Ginneken, B
AF Ciompi, Francesco
   de Hoop, Bartjan
   van Riel, Sarah J.
   Chung, Kaman
   Scholten, Ernst Th.
   Oudkerk, Matthijs
   de Jong, Pim A.
   Prokop, Mathias
   van Ginneken, Bram
TI Automatic classification of pulmonary peri-fissural nodules in computed
   tomography using an ensemble of 2D views and a convolutional neural
   network out-of-the-box
SO MEDICAL IMAGE ANALYSIS
LA English
DT Article
DE Chest CT; Peri-fissural nodules; Lung cancer screening; Convolutional
   neural networks; OverFeat; Deep learning
ID LUNG-CANCER; PERIFISSURAL NODULES; CT
AB In this paper, we tackle the problem of automatic classification of pulmonary peri-fissural nodules (PFNs). The classification problem is formulated as a machine learning approach, where detected nodule candidates are classified as PFNs or non-PFNs. Supervised learning is used, where a classifier is trained to label the detected nodule. The classification of the nodule in 3D is formulated as an ensemble of classifiers trained to recognize PFNs based on 2D views of the nodule. In order to describe nodule morphology in 2D views, we use the output of a pre-trained convolutional neural network known as OverFeat. We compare our approach with a recently presented descriptor of pulmonary nodule morphology, namely Bag of Frequencies, and illustrate the advantages offered by the two strategies, achieving performance of AUC = 0.868, which is close to the one of human experts. (C) 2015 Elsevier B.V. All rights reserved.
C1 [Ciompi, Francesco; van Riel, Sarah J.; Chung, Kaman; Scholten, Ernst Th.; van Ginneken, Bram] Radboud Univ Nijmegen, Med Ctr, Dept Radiol & Nucl Med, Diagnost Image Anal Grp, NL-6525 ED Nijmegen, Netherlands.
   [de Hoop, Bartjan; de Jong, Pim A.] Univ Med Ctr, Utrecht, Netherlands.
   [Oudkerk, Matthijs] Univ Groningen, Univ Med Ctr Groningen, Groningen, Netherlands.
   [Prokop, Mathias] Radboud Univ Nijmegen, Med Ctr, Dept Radiol, NL-6525 ED Nijmegen, Netherlands.
   [van Ginneken, Bram] Fraunhofer Mevis, Bremen, Germany.
RP Ciompi, F (reprint author), Radboud Univ Nijmegen, Med Ctr, Dept Radiol & Nucl Med, Diagnost Image Anal Grp, NL-6525 ED Nijmegen, Netherlands.
EM francesco.ciompi@radboudumc.nl
RI Ciompi, Francesco/P-5598-2015; Prokop, W.M./H-8081-2014; van Ginneken,
   Bram/A-3728-2012
OI Ciompi, Francesco/0000-0001-8327-9606; van Ginneken,
   Bram/0000-0003-2028-8972
FU Netherlands Organization for Scientific ResearchNetherlands Organization
   for Scientific Research (NWO) [639.023.207]
FX This project was funded by a research grant from the Netherlands
   Organization for Scientific Research, Project number 639.023.207.
CR Aberle DR, 2011, NEW ENGL J MED, V365, P395, DOI 10.1056/NEJMoa1102873
   Ahn MI, 2010, RADIOLOGY, V254, P949, DOI 10.1148/radiol.09090031
   American Cancer Society, 2014, CANC FACTS FIG 2014
   American College of Radiology, 2014, LUNG CT SCREEN REP D
   Armato SG, 2011, MED PHYS, V38, P915, DOI 10.1118/1.3528204
   Bar Y., 2015, SPIE MED IMAGING
   Bar Y, 2015, I S BIOMED IMAGING, P294, DOI 10.1109/ISBI.2015.7163871
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Ciompi F., 2014, P ANN M RAD SOC N AM
   Ciompi F, 2015, IEEE T MED IMAGING, V34, P962, DOI 10.1109/TMI.2014.2371821
   Dana KJ, 1999, ACM T GRAPHIC, V18, P1, DOI 10.1145/300776.300778
   de Hoop B, 2012, RADIOLOGY, V265, P611, DOI 10.1148/radiol.12112351
   Gatsonis CA, 2011, RADIOLOGY, V258, P243, DOI 10.1148/radiol.10091808
   Jacobs C, 2014, MED IMAGE ANAL, V18, P374, DOI 10.1016/j.media.2013.12.001
   Klik MAJ, 2006, I S BIOMED IMAGING, P494
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kuhnigk JM, 2006, IEEE T MED IMAGING, V25, P417, DOI 10.1109/TMI.2006.871547
   McWilliams A, 2013, NEW ENGL J MED, V369, P910, DOI 10.1056/NEJMoa1214726
   Messay T, 2010, MED IMAGE ANAL, V14, P390, DOI 10.1016/j.media.2010.02.004
   Niemeijer M, 2011, IEEE T MED IMAGING, V30, P215, DOI 10.1109/TMI.2010.2072789
   Pastorino U, 2012, EUR J CANCER PREV, V21, P308, DOI 10.1097/CEJ.0b013e328351e1b6
   Pedersen JH, 2009, J THORAC ONCOL, V4, P608, DOI 10.1097/JTO.0b013e3181a0d98f
   Russakovsky O., 2014, ARXIV14090575
   Rutter CM, 2000, ACAD RADIOL, V7, P413, DOI 10.1016/S1076-6332(00)80381-5
   Sermanet P., 2014, P INT C LEARN REPR I
   Sharif Razavian Ali, 2014, P IEEE C COMP VIS PA
   Teramoto A, 2013, INT J COMPUT ASS RAD, V8, P193, DOI 10.1007/s11548-012-0767-5
   van den Bergh KAM, 2008, CANCER, V113, P396, DOI 10.1002/cncr.23590
   van Ginneken B, 2015, I S BIOMED IMAGING, P286, DOI 10.1109/ISBI.2015.7163869
   Vapnik VN, 1995, NATURE STAT LEARNING
NR 30
TC 104
Z9 116
U1 6
U2 62
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 1361-8415
EI 1361-8423
J9 MED IMAGE ANAL
JI Med. Image Anal.
PD DEC
PY 2015
VL 26
IS 1
BP 195
EP 202
DI 10.1016/j.media.2015.08.001
PG 8
WC Computer Science, Artificial Intelligence; Computer Science,
   Interdisciplinary Applications; Engineering, Biomedical; Radiology,
   Nuclear Medicine & Medical Imaging
SC Computer Science; Engineering; Radiology, Nuclear Medicine & Medical
   Imaging
GA DA0MI
UT WOS:000367490800016
PM 26458112
DA 2020-02-19
ER

PT J
AU Barros, P
   Jirak, D
   Weber, C
   Wermter, S
AF Barros, Pablo
   Jirak, Doreen
   Weber, Cornelius
   Wermter, Stefan
TI Multimodal emotional state recognition using sequence-dependent deep
   hierarchical features
SO NEURAL NETWORKS
LA English
DT Article
DE Emotion recognition; Deep learning; Convolutional Neural Networks;
   Hierarchical features; Human Robot Interaction
ID BODY GESTURE; FACE; MODEL
AB Emotional state recognition has become an important topic for human-robot interaction in the past years. By determining emotion expressions, robots can identify important variables of human behavior and use these to communicate in a more human-like fashion and thereby extend the interaction possibilities. Human emotions are multimodal and spontaneous, which makes them hard to be recognized by robots. Each modality has its own restrictions and constraints which, together with the non-structured behavior of spontaneous expressions, create several difficulties for the approaches present in the literature, which are based on several explicit feature extraction techniques and manual modality fusion. Our model uses a hierarchical feature representation to deal with spontaneous emotions, and learns how to integrate multiple modalities for non-verbal emotion recognition, making it suitable to be used in an HRI scenario. Our experiments show that a significant improvement of recognition accuracy is achieved when we use hierarchical features and multimodal information, and our model improves the accuracy of state-of-the-art approaches from 82.5% reported in the literature to 91.3% for a benchmark dataset on spontaneous emotion expressions. (C) 2015 The Authors. Published by Elsevier Ltd.
C1 [Barros, Pablo; Jirak, Doreen; Weber, Cornelius; Wermter, Stefan] Univ Hamburg, Dept Informat, Knowledge Technol, D-22527 Hamburg, Germany.
RP Barros, P (reprint author), Univ Hamburg, Dept Informat, Knowledge Technol, Vogt Koelln Str 30, D-22527 Hamburg, Germany.
EM barros@informatik.uni-hamburg.de; jirak@informatik.uni-hamburg.de;
   weber@informatik.uni-hamburg.de; wermter@informatik.uni-hamburg.de
OI Barros, Pablo/0000-0002-6517-682X
FU CAPES Brazilian Federal AgencyCAPES [5951-13-5]
FX The authors thank Dr. Hatice Gunes for providing the FABO Database. This
   work was partially supported by CAPES Brazilian Federal Agency for the
   Support and Evaluation of Graduate Education (p.n.5951-13-5).
CR Adolphs R, 2002, CURR OPIN NEUROBIOL, V12, P169, DOI 10.1016/S0959-4388(02)00301-X
   Aggarwal JK, 2011, ACM COMPUT SURV, V43, DOI 10.1145/1922649.1922653
   Bar M, 2007, TRENDS COGN SCI, V11, P280, DOI 10.1016/j.tics.2007.05.005
   Barros P, 2014, IEEE-RAS INT C HUMAN, P646, DOI 10.1109/HUMANOIDS.2014.7041431
   Breazeal C., 2004, WHO NEEDS EMOTIONS
   Cabanac M, 2002, BEHAV PROCESS, V60, P69, DOI 10.1016/S0376-6357(02)00078-5
   Chen SZ, 2013, IMAGE VISION COMPUT, V31, P175, DOI 10.1016/j.imavis.2012.06.014
   EKMAN P, 1971, J PERS SOC PSYCHOL, V17, P124, DOI 10.1037/h0030377
   Ekman P., 1978, FACIAL ACTION CODING
   Erhan D, 2010, J MACH LEARN RES, V11, P625
   Fasel B, 2002, FOURTH IEEE INTERNATIONAL CONFERENCE ON MULTIMODAL INTERFACES, PROCEEDINGS, P529, DOI 10.1109/ICMI.2002.1167051
   FUKUSHIMA K, 1980, BIOL CYBERN, V36, P193, DOI 10.1007/BF00344251
   Gadanho SC, 2004, J MACH LEARN RES, V4, P385, DOI 10.1162/153244304773633870
   Glorot X., 2011, P 14 INT C ART INT S, V15, P315
   Gu Y., 2013, PLOS ONE, V8
   Gunes H, 2006, INT C PATT RECOG, P1148
   Gunes H, 2009, IEEE T SYST MAN CY B, V39, P64, DOI 10.1109/TSMCB.2008.927269
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   HUBEL DH, 1959, J PHYSIOL-LONDON, V148, P574, DOI 10.1113/jphysiol.1959.sp006308
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kret ME, 2013, FRONT HUM NEUROSCI, V7, DOI 10.3389/fnhum.2013.00810
   Le Q. V., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3361, DOI 10.1109/CVPR.2011.5995496
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lerner JS, 2000, COGNITION EMOTION, V14, P473, DOI 10.1080/026999300402763
   MORISHIMA S, 1993, IEEE VIRTUAL REALITY ANNUAL INTERNATIONAL SYMPOSIUM, P486, DOI 10.1109/VRAIS.1993.380740
   Ramirez-Amaro K, 2013, IEEE-RAS INT C HUMAN, P456, DOI 10.1109/HUMANOIDS.2013.7030014
   Ranzato MA, 2007, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2007.383157
   Simard PY, 2003, PROC INT CONF DOC, P958
   Spexard TP, 2007, IEEE T ROBOT, V23, P852, DOI 10.1109/TRO.2007.904903
   Tokuno S., 2011, DEF SCI RES C EXP DS, P1
   Velusamy S, 2011, INT CONF ACOUST SPEE, P2028
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Wagner J, 2011, IEEE T AFFECT COMPUT, V2, P206, DOI 10.1109/T-AFFC.2011.12
   WALLIS G, 1993, IEEE IJCNN, P1087
   Wang L, 2015, IEEE T IMAGE PROCESS, V24, P1424, DOI 10.1109/TIP.2015.2403231
   Wang YQ, 2014, IMAGE PROCESS ON LIN, V4, P128, DOI 10.5201/ipol.2014.104
   Wiskott L, 2002, NEURAL COMPUT, V14, P715, DOI 10.1162/089976602317318938
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhao LW, 1998, PACIFIC GRAPHICS '98, PROCEEDINGS, P161, DOI 10.1109/PCCGA.1998.732100
NR 40
TC 25
Z9 25
U1 0
U2 35
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0893-6080
EI 1879-2782
J9 NEURAL NETWORKS
JI Neural Netw.
PD DEC
PY 2015
VL 72
BP 140
EP 151
DI 10.1016/j.neunet.2015.09.009
PG 12
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA CY9BF
UT WOS:000366701700011
PM 26548943
OA Other Gold
DA 2020-02-19
ER

PT J
AU Li, C
   Sanchez, RV
   Zurita, G
   Cerrada, V
   Cabrera, D
   Vasquez, RE
AF Li, Chuan
   Sanchez, Rene-Vinicio
   Zurita, Grover
   Cerrada, Vlariela
   Cabrera, Diego
   Vasquez, Rafael E.
TI Multimodal deep support vector classification with homologous features
   and its application to gearbox fault diagnosis
SO NEUROCOMPUTING
LA English
DT Article
DE Deep learning; Support vector classification; Multimodal homologous
   feature; Gearbox; Fault diagnosis
ID ROLLING ELEMENT BEARING; VIBRATION; DEMODULATION; NETWORKS; ENERGY;
   MODEL
AB Gearboxes are crucial transmission components in mechanical systems. Fault diagnosis is an important tool to maintain gearboxes in healthy conditions. It is challenging to recognize fault existences and, if any, failure patterns in such transmission elements due to their complicated configurations. This paper addresses a multimodal deep support vector classification (MDSVC) approach, which employs separation-fusion based deep learning in order to perform fault diagnosis tasks for gearboxes. Considering that different modalities can be made to describe same object, multimodal homologous features of the gearbox vibration measurements are first separated in time, frequency and wavelet modalities, respectively. A Gaussian-Bernoulli deep Boltzmann machine (GDBM) without final output is subsequently suggested to learn pattern representations for features in each modality. A support vector classifier is finally applied to fuse GDBMs in different modalities towards the construction of the MDSVC model. With the present model, "deep" representations from "wide" modalities improve fault diagnosis capabilities. Fault diagnosis experiments were carried out to evaluate the proposed method on both spur and helical gearboxes. The proposed model achieves the best fault classification rate in experiments when compared to representative deep and shallow learning methods. Results indicate that the proposed separation-fusion based deep learning strategy is effective for the gearbox fault diagnosis. (C) 2015 Elsevier B.V. All rights reserved.
C1 [Li, Chuan] Chongqing Technol & Business Univ, Res Ctr Syst Hlth Maintenance, Chongqing 400067, Peoples R China.
   [Li, Chuan; Sanchez, Rene-Vinicio; Zurita, Grover; Cerrada, Vlariela; Cabrera, Diego] Univ Politecn Salesiana, Dept Mech Engn, Cuenca, Ecuador.
   [Vasquez, Rafael E.] Univ Pontificia Bolivariana, Dept Mech Engn, Medellin, Colombia.
RP Li, C (reprint author), Chongqing Technol & Business Univ, Res Ctr Syst Hlth Maintenance, Chongqing 400067, Peoples R China.
EM chuanli@21cn.com
RI Li, Chuan/K-1904-2012; L., Rene Vinicio Sanchez/O-5259-2019; Cabrera,
   Diego/I-8837-2019
OI Cabrera, Diego/0000-0003-1023-871X; Sanchez, Rene
   Vinicio/0000-0003-0395-9228; Vasquez, Rafael E./0000-0003-4871-8823; Li,
   Chuan/0000-0003-0004-1497; Zurita, Grover/0000-0003-1338-0092
FU Secretariat for Higher Education, Science, Technology and Innovation of
   the Republic of Ecuador; National Natural Science Foundation of
   ChinaNational Natural Science Foundation of China [51375517]; Chongqing
   Innovation Team in University [KJTD201313]; Youth Science and Technology
   Talents Project of Chongqing [cstc2014kj-rc-qnrc00003]
FX This work is supported in part by the Prometeo Project of the
   Secretariat for Higher Education, Science, Technology and Innovation of
   the Republic of Ecuador, the National Natural Science Foundation of
   China (51375517), the Project of Chongqing Innovation Team in University
   (KJTD201313), and the Youth Science and Technology Talents Project of
   Chongqing (cstc2014kj-rc-qnrc00003). The valuable comments and
   suggestions from the editors and the two reviewers are very much
   appreciated.
CR Amarnath M, 2014, MEASUREMENT, V58, P154, DOI 10.1016/j.measurement.2014.08.015
   Bafroui HH, 2014, NEUROCOMPUTING, V133, P437, DOI 10.1016/j.neucom.2013.12.018
   Bozchalooi IS, 2007, J SOUND VIB, V308, P246, DOI 10.1016/j.jsv.2007.07.038
   Chen FF, 2013, MEASUREMENT, V46, P220, DOI 10.1016/j.measurement.2012.06.009
   Cho K. H., 2013, INT JOINT C NEUR NET, P1, DOI [10.1109/IJCNN.2013.6706831, DOI 10.1109/IJCNN.2013.6706831]
   Gharavian MH, 2013, NEUROCOMPUTING, V121, P150, DOI 10.1016/j.neucom.2013.04.033
   Guo L, 2009, J VIB CONTROL, V15, P1349, DOI 10.1177/1077546308095224
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hjelm RD, 2014, NEUROIMAGE, V96, P245, DOI 10.1016/j.neuroimage.2014.03.048
   Katayama K, 2003, PHYSICA A, V322, P531, DOI 10.1016/S0378-4371(02)01803-4
   Lei YG, 2014, MEASUREMENT, V48, P292, DOI 10.1016/j.measurement.2013.11.012
   Lei YG, 2012, MEAS SCI TECHNOL, V23, DOI 10.1088/0957-0233/23/5/055605
   Leng B, 2015, NEUROCOMPUTING, V151, P593, DOI 10.1016/j.neucom.2014.06.084
   Li C., 2011, SMART MATER STRUCT, V20
   Li C, 2015, MECH SYST SIGNAL PR, V64-65, P132, DOI 10.1016/j.ymssp.2015.04.004
   Li C, 2012, J SOUND VIB, V331, P5864, DOI 10.1016/j.jsv.2012.07.045
   Li C, 2012, MECH SYST SIGNAL PR, V26, P205, DOI 10.1016/j.ymssp.2011.07.001
   Noda K, 2014, ROBOT AUTON SYST, V62, P721, DOI 10.1016/j.robot.2014.03.003
   Pandya DH, 2013, EXPERT SYST APPL, V40, P4137, DOI 10.1016/j.eswa.2013.01.033
   Raad A, 2008, MECH SYST SIGNAL PR, V22, P574, DOI 10.1016/j.ymssp.2007.09.011
   Rafiee J, 2010, EXPERT SYST APPL, V37, P4568, DOI 10.1016/j.eswa.2009.12.051
   Salakhutdinov R, 2012, NEURAL COMPUT, V24, P1967, DOI 10.1162/NECO_a_00311
   Salakhutdinov Ruslan, 2009, THESIS
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Shen CQ, 2013, MEASUREMENT, V46, P1551, DOI 10.1016/j.measurement.2012.12.011
   Srivastava N, 2014, J MACH LEARN RES, V15, P2949
   Su ZQ, 2015, NEUROCOMPUTING, V157, P208, DOI 10.1016/j.neucom.2015.01.016
   Tamilselvan P, 2013, RELIAB ENG SYST SAFE, V115, P124, DOI 10.1016/j.ress.2013.02.022
   Tang Y., 2013, WORKSH REPR LEARN IC
   Tayarani-Bathaie SS, 2014, NEUROCOMPUTING, V125, P153, DOI 10.1016/j.neucom.2012.06.050
   Tian Y, 2015, NEUROCOMPUTING, V151, P296, DOI 10.1016/j.neucom.2014.09.036
   Tran VT, 2014, EXPERT SYST APPL, V41, P4113, DOI 10.1016/j.eswa.2013.12.026
   Wang Y, 2015, MECH SYST SIGNAL PR, V54-55, P259, DOI 10.1016/j.ymssp.2014.09.002
   Wu JD, 2005, NDT&E INT, V38, P605, DOI 10.1016/j.ndteint.2005.02.007
   Yan RQ, 2014, SIGNAL PROCESS, V96, P1, DOI 10.1016/j.sigpro.2013.04.015
   Yin S, 2015, IEEE-ASME T MECH, V20, P2613, DOI 10.1109/TMECH.2014.2358674
   Yin S, 2015, IEEE T IND ELECTRON, V62, P1651, DOI 10.1109/TIE.2014.2345331
   Yin S, 2015, IEEE T IND ELECTRON, V62, P657, DOI 10.1109/TIE.2014.2308133
   Yin S, 2014, NEUROCOMPUTING, V145, P263, DOI 10.1016/j.neucom.2014.05.035
   Yin S, 2014, INT J SYST SCI, V45, P1375, DOI 10.1080/00207721.2014.886136
   Yin S, 2014, IEEE T IND ELECTRON, V61, P6418, DOI 10.1109/TIE.2014.2301773
   Zhang WL, 2015, NEUROIMAGE, V108, P214, DOI 10.1016/j.neuroimage.2014.12.061
   Zhou SS, 2014, NEUROCOMPUTING, V131, P312, DOI 10.1016/j.neucom.2013.10.011
NR 43
TC 117
Z9 127
U1 16
U2 222
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0925-2312
EI 1872-8286
J9 NEUROCOMPUTING
JI Neurocomputing
PD NOV 30
PY 2015
VL 168
BP 119
EP 127
DI 10.1016/j.neucom.2015.06.008
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA CO4XU
UT WOS:000359165000012
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Zhong, SH
   Liu, Y
   Li, B
   Long, J
AF Zhong, Sheng-hua
   Liu, Yan
   Li, Bin
   Long, Jing
TI Query-oriented unsupervised multi-document summarization via deep
   learning model
SO EXPERT SYSTEMS WITH APPLICATIONS
LA English
DT Article
DE Deep learning; Query-oriented summarization; Multi-document; Neocortex
   simulation
ID INFERENCE; ALGORITHM
AB Capturing the compositional process from words to documents is a key challenge in natural language processing and information retrieval: Extractive style query-oriented multi-document summarization generates a summary by extracting a proper set of sentences from multiple documents based on pre-given query. This paper proposes a novel document summarization framework based on deep learning model, which has been shown outstanding extraction ability in many real-world applications. The framework consists of three parts: concepts extraction, summary generation, and reconstruction validation. A new query-oriented extraction technique is proposed to extract information distributed in multiple documents. Then, the whole deep architecture is fine-tuned by minimizing the information loss in reconstruction validation. According to the concepts extracted from deep architecture layer by layer, dynamic programming is used to seek most informative set of sentences for the summary. Experiment on three benchmark datasets (DUC 2005, 2006, and 2007) assess and confirm the effectiveness of the proposed framework and algorithms. Experiment results show that the proposed method outperforms state-of-the-art extractive summarization approaches. Moreover, we also provide the statistical analysis of query words based on Amazon's Mechanical Turk (MTurk) crowdsourcing platform. There exists underlying relationships from topic words to the content which can contribute to summarization task. (C) 2015 Elsevier Ltd. All rights reserved.
C1 [Zhong, Sheng-hua] Shen Zhen Univ, Coll Comp Sci & Software Engn, Shenzhen 518060, Guangdong, Peoples R China.
   [Zhong, Sheng-hua; Liu, Yan] Hong Kong Polytech Univ, Dept Comp, Kowloon 999077, Hong Kong, Peoples R China.
   [Li, Bin] City Univ Hong Kong, Dept Linguist & Translat, Kowloon 999077, Hong Kong, Peoples R China.
   [Long, Jing] Nanjing Univ, Sch Business, Nanjing 210093, Jiangsu, Peoples R China.
RP Liu, Y (reprint author), Hong Kong Polytech Univ, Dept Comp, Kowloon 999077, Hong Kong, Peoples R China.
EM csshzhong@szu.edu.cn; csyliu@comp.polyu.edu; binli2@cityu.edu.hk;
   longjing@nju.edu.cn
OI LI, Bin/0000-0002-3956-8408; LIU, Yan/0000-0003-4242-4840
FU National Natural Science Foundation of China (NSFC)National Natural
   Science Foundation of China [61373122]
FX This research was supported by National Natural Science Foundation of
   China (NSFC) 61373122.
CR Ballan L, 2009, IEEE INT CON MULTI, P474, DOI 10.1109/ICME.2009.5202537
   Barton RA, 1996, P ROY SOC B-BIOL SCI, V263, P173, DOI 10.1098/rspb.1996.0028
   Bashir M, 2013, SIGIR'13: THE PROCEEDINGS OF THE 36TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH & DEVELOPMENT IN INFORMATION RETRIEVAL, P909
   BAXENDALE PB, 1958, IBM J RES DEV, V2, P354, DOI 10.1147/rd.24.0354
   Berger A, 2000, 38TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P294
   Brabham D. C., 2008, CONVERGENCE-US, V14, P75, DOI [DOI 10.1177/1354856507084420, 10.1177/1354856507084420]
   Cao L., 2009, P 17 ACM INT C MULT, P125
   Cao Z., 2015, P 29 AAAI C ART INT, P1
   Cao Z., 2007, P 24 INT C MACH LEAR, P129, DOI DOI 10.1145/1273496.1273513
   Chen EK, 2008, INT CONF ACOUST SPEE, P829, DOI 10.1109/ICASSP.2008.4517738
   Dahl G., 2010, P 24 ANN C NEUR INF, P1
   Dang H., 2005, P DUC 2005
   Denil M., 2014, EXTRACTION SALIENT S, P1
   EDMUNDSON HP, 1969, J ACM, V16, P264, DOI 10.1145/321510.321519
   Felleman DJ, 1991, CEREB CORTEX, V1, P1, DOI 10.1093/cercor/1.1.1
   Filatova  E., 2004, P 20 INT C COMP LING, P397
   Goldstein J., 2000, P 2000 NAACL ANLP WO, V4, P40
   Hinton G., 2012, NEURAL NETWORKS TRIC, V9, P599, DOI DOI 10.1007/978-3-642-35289-8_32
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton G. E., 2007, AISTATS
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hinton GE, 2007, TRENDS COGN SCI, V11, P428, DOI 10.1016/j.tics.2007.09.004
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Jin Feng, 2010, P 23 INT C COMP LING, P525
   Joachims T., 2002, P 8 ACM SIGKDD INT C, P133, DOI DOI 10.1145/775047.775067
   Khanpour H., 2009, THESIS U MALAYA
   Larochelle H., 2007, P 24 INT C MACH LEAR, P473, DOI DOI 10.1145/1273496.1273556
   Lee TS, 2003, J OPT SOC AM A, V20, P1434, DOI 10.1364/JOSAA.20.001434
   LEUBA G, 1994, ANAT EMBRYOL, V190, P351
   Lin C. Y., 2004, TEXT SUMMARIZATION B, P74
   Lin Hui, 2010, HUMAN LANGUAGE TECHN, P912
   Liu Y., 2009, P 17 ACM INT C MULT, P55
   Liu Y., 2012, P 26 AAAI C ART INT, P1699
   Loni B., 2014, P 5 ACM MULT SYST C, P41, DOI DOI 10.1145/2557642.2563675
   LUHN HP, 1958, IBM J RES DEV, V2, P159, DOI 10.1147/rd.22.0159
   McDonald R, 2007, LECT NOTES COMPUT SC, V4425, P557
   Ouyang Y, 2011, INFORM PROCESS MANAG, V47, P227, DOI 10.1016/j.ipm.2010.03.005
   PORTER MF, 1980, PROGRAM-AUTOM LIBR, V14, P130, DOI 10.1108/eb046814
   Radev DR, 2004, INFORM PROCESS MANAG, V40, P919, DOI 10.1016/j.ipm.2003.10.006
   Shen D., 2007, P 17 INT C ART INT, P2863
   Smolensky P, 1986, PARALLEL DISTRIBUTED, P194
   Song W, 2011, EXPERT SYST APPL, V38, P9112, DOI 10.1016/j.eswa.2010.12.102
   Tang J., 2009, P 9 SIAM INT C DAT M, P1148
   Vapnik VN, 1995, NATURE STAT LEARNING
   Wan X., 2009, P 18 ACM C INF KNOWL, P1609
   Wan XJ, 2009, 21ST INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-09), PROCEEDINGS, P1586
   Wei FR, 2010, KNOWL INF SYST, V22, P245, DOI 10.1007/s10115-009-0194-2
   Wong K., 2008, P 22 INT C COMP LING, P985, DOI DOI 10.3115/1599081.1599205
NR 48
TC 17
Z9 17
U1 1
U2 93
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0957-4174
EI 1873-6793
J9 EXPERT SYST APPL
JI Expert Syst. Appl.
PD NOV 30
PY 2015
VL 42
IS 21
BP 8146
EP 8155
DI 10.1016/j.eswa.2015.05.034
PG 10
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic; Operations Research & Management Science
SC Computer Science; Engineering; Operations Research & Management Science
GA CQ7HB
UT WOS:000360772500073
DA 2020-02-19
ER

PT J
AU Zhou, YC
   Hu, QH
   Liu, J
   Jia, Y
AF Zhou, Yucan
   Hu, Qinghua
   Liu, Jie
   Jia, Yuan
TI Combining heterogeneous deep neural networks with conditional random
   fields for Chinese dialogue act recognition
SO NEUROCOMPUTING
LA English
DT Article
DE Dialogue act recognition; Heterogeneous features; Deep learning;
   Conditional random fields
ID SPEECH
AB Dialogue act (DA) recognition is a fundamental step for computers to understand natural-language dialogues because it can reflect the intention of a speaker. However, it is difficult to adapt traditional machine learning models to the dialogue act recognition task due to the heterogeneous features, statistical dependence between the DA tags, and complex relationship between features and the DA tags. In this paper, we propose a new model which combines heterogeneous deep neural networks with conditional random fields (HDNN-CRF) to solve this problem. The proposed model has two main advantages. First, the heterogeneous deep neural networks (HDNN) model, which is extended from the deep neural networks (DNN), retains the powerful ability of representation learning and adds a new skill of dealing with heterogeneous features effectively. Second, the conditional random fields (CRF) can capture the statistical dependence between the DA tags which carries important information to determine the DA tag of the current utterance. To verify the effectiveness of the proposed model, we conduct several experiments on a Chinese corpus, called CASIA-CASSIL corpus. Ten kinds of features are extracted from the utterances. In the experiment, we give some quantitative analysis of these kinds of features. What's more, when comparing classification accuracies of the proposed model and some other models, the proposed model has achieved the best performance. (C) 2015 Elsevier B.V. All rights reserved.
C1 [Zhou, Yucan; Hu, Qinghua] Tianjin Univ, Sch Comp Sci & Technol, Tianjin 300072, Peoples R China.
   [Liu, Jie] Nankai Univ, Coll Comp & Control Engn, Tianjin 300071, Peoples R China.
   [Jia, Yuan] Chinese Acad Social Sci, Inst Linguist, Beijing, Peoples R China.
RP Hu, QH (reprint author), Tianjin Univ, Sch Comp Sci & Technol, Tianjin 300072, Peoples R China.
EM zhouyucan@tju.edu.cn; huqinghua@tju.edu.cn
OI Hu, Qinghua/0000-0001-7765-8095
FU National Program on Key Basic Research ProjectNational Basic Research
   Program of China [2013CB329304]; National Natural Science Foundation of
   ChinaNational Natural Science Foundation of China [61222210]; Key
   Program of National Natural Science Foundation of ChinaNational Natural
   Science Foundation of China [61432011]
FX This work is supported by National Program on Key Basic Research Project
   (2013CB329304), National Natural Science Foundation of China (No.
   61222210), and Key Program of National Natural Science Foundation of
   China (No. 61432011).
CR Arel I., IEEE COMPUT INTELL M, P5
   Austin J.L., 1955, DO THINGS WORDS, P495
   Bengio Y., 2013, HDB NEURAL INFORM PR, P1, DOI DOI 10.1007/978-3-642-36657-4_1
   Deng L., APSIPA T SIGNAL INFO, V3, pe2
   Dhillon R., TR04002 ICSI
   Dielmann A, 2008, IEEE T AUDIO SPEECH, V16, P1303, DOI 10.1109/TASL.2008.922463
   Fernandez R., 2002, INT C SPEECH PROS 20
   Grau Sergio, 2004, 9 C SPEECH COMP
   He X., 2004, P IEEE INT C COMP VI, V2, P11
   Irie Y., 2004, INTERSPEECH
   Jurafsky Dan, 1997, SWITCHBOARD SWBD DAM, P97
   Kral P., 2005, INTERSPEECH, P825
   Krogh A, 2001, J MOL BIOL, V305, P567, DOI 10.1006/jmbi.2000.4315
   Lafferty J.D., 2001, P INT C MACH LEARN, V18, P282
   Lan KC, 2008, J AM SOC INF SCI TEC, V59, P859, DOI 10.1002/asi.20777
   Lee J.-w., 1997, P ACL WORKSH SPOK LA, P10
   Leech G., 2003, OXFORD HDB COMPUTATI
   Levin L., 2003, P 4 SIGDIAL WORKSH D
   Liu Y., 2004, P ICSLP, P577
   Louwerse M. M., 2006, FLAIRS C, P758
   Mast M., CONNECTIONIST STAT S
   McCallum A, 2003, P 7 C NAT LANG LEARN, V4, P188, DOI DOI 10.3115/1119176.1119206
   Ngiam J, 2011, IEEE INT C MACH LEAR, V28, P689, DOI DOI 10.1145/2647868
   PINTO D, 2003, P 26 ANN INT ACM SIG, P235
   Popescu-Belis A., 62 ISSCO
   Quarteroni S, 2011, INT CONF ACOUST SPEE, P5596
   Quattoni A., 2004, NIPS, V17, P1097
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   REITHINGER N, 1995, P 33 ANN M ASS COMP, P116, DOI DOI 10.3115/981658.981674
   Ries K, 1999, INT CONF ACOUST SPEE, P497, DOI 10.1109/ICASSP.1999.758171
   Rumelhart D.E., 1985, TECHNICAL REPORT
   Sarawagi Sunita, 2004, ADV NEURAL INFORM PR, V17, P1
   Searle John R., 1969, SPEECH ACTS ESSAY PH
   Sha F., 2003, P 2003 C N AM CHAPT, P134, DOI [10.3115/1073445.1073473, DOI 10.3115/1073445.1073473]
   Shriberg E, 2000, SPEECH COMMUN, V32, P127, DOI 10.1016/S0167-6393(00)00028-5
   Songwook Lee, 2002, International Journal of Computer Processing of Oriental Languages, V15, P231
   Sridhar VKR, 2009, COMPUT SPEECH LANG, V23, P407, DOI 10.1016/j.csl.2008.12.001
   Srivastava N., 2012, ADV NEURAL INFORM PR, P2222, DOI DOI 10.1109/CVPR.2013.49
   Stolcke A, 2000, COMPUT LINGUIST, V26, P339, DOI 10.1162/089120100561737
   Tavafi Maryam, 2013, P SIGDIAL 2013 C, P117
   Taylor P, 1998, LANG SPEECH, V41, P493, DOI 10.1177/002383099804100411
   VARGA AP, 1990, INT CONF ACOUST SPEE, P845, DOI 10.1109/ICASSP.1990.115970
   Wu P, 2013, P 21 ACM INT C MULT, P153, DOI 10.1145/2502081.2502112
   Yamato J., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P379, DOI 10.1109/CVPR.1992.223161
   Yoshimura T, 1996, ICSLP 96 - FOURTH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE PROCESSING, PROCEEDINGS, VOLS 1-4, P837, DOI 10.1109/ICSLP.1996.607731
   Yu D, 2011, IEEE SIGNAL PROC MAG, V28, P145, DOI 10.1109/MSP.2010.939038
   Zhang RX, 2013, IEEE T AUDIO SPEECH, V21, P649, DOI 10.1109/TASL.2012.2229984
   Zhou Y., 2013, OCEANS SAN DIEGO 201, P1
NR 48
TC 15
Z9 16
U1 1
U2 42
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0925-2312
EI 1872-8286
J9 NEUROCOMPUTING
JI Neurocomputing
PD NOV 30
PY 2015
VL 168
BP 408
EP 417
DI 10.1016/j.neucom.2015.05.086
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA CO4XU
UT WOS:000359165000040
DA 2020-02-19
ER

PT J
AU Zhang, YQ
   Li, X
   Zhang, ZF
   Wu, F
   Zhao, LM
AF Zhang, Yaqing
   Li, Xi
   Zhang, Zhongfei
   Wu, Fei
   Zhao, Liming
TI Deep learning driven blockwise moving object detection with binary scene
   modeling
SO NEUROCOMPUTING
LA English
DT Article
DE Moving object detection; Deep learning; Feature binarization
AB As an important and challenging topic in computer vision, moving object detection is typically posed as a problem of scene analysis, which usually requires both robust feature description and effective statistical modeling. In general, the conventional detection methods make use of pre-defined hand-crafted features and sophisticated background models for scene analysis. Therefore, they usually have a low generalization capability of adapting to different scenes with diverse spatio-temporal motion information. In the face of high-definition video data, sophisticated statistical modeling often suffers from an expensive computation or memory cost because of its low efficiency in evaluation and parallelization. In order to address this issue, we propose a deep learning based block-wise scene analysis method equipped with a binary spatio-temporal scene model. Based on the stacked denoising autoencoder, the deep learning module of the proposed method aims to learn an effective deep image representation encoding the intrinsic scene information, which leads to the robustness of feature description. Furthermore, the proposed binary scene model captures the spatio-temporal scene distribution information in the Hamming space, which ensures the high efficiency of moving object detection. Experimental results on several datasets demonstrate the effectiveness and efficiency of the proposed method. (C) 2015 Elsevier B.V. All rights reserved.
C1 [Zhang, Yaqing] Zhejiang Univ, Dept Informat Sci & Elect Engn, Hangzhou, Zhejiang, Peoples R China.
RP Li, X (reprint author), Zhejiang Univ, Dept Informat Sci & Elect Engn, Hangzhou, Zhejiang, Peoples R China.
EM xilizju@zju.edu.cn
RI Zhao, Liming/I-8253-2016; Li, Xi/L-1234-2013
OI Zhao, Liming/0000-0002-1467-1230; 
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61472353]; China Knowledge Centre for Engineering
   Sciences and Technology; Fundamental Research Funds for the Central
   UniversitiesFundamental Research Funds for the Central Universities;
   National Basic Research Program of ChinaNational Basic Research Program
   of China [2012CB316400]; NVIDIA CUDA Research Center Program; Zhejiang
   Provincial Engineering Center on Media Data Cloud Processing and
   Analysis; U.S. National Science FoundationNational Science Foundation
   (NSF) [CCF-1017828]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61472353, the China Knowledge Centre for
   Engineering Sciences and Technology, the Fundamental Research Funds for
   the Central Universities, the National Basic Research Program of China
   under Grant 2012CB316400, the NVIDIA CUDA Research Center Program and by
   the Zhejiang Provincial Engineering Center on Media Data Cloud
   Processing and Analysis. The work of Z. Zhang was also supported by the
   U.S. National Science Foundation under Grant CCF-1017828.
CR Chen SY, 2012, IEEE T IND INFORM, V8, P118, DOI 10.1109/TII.2011.2173202
   Cong Z., EURASIP J IMAGE VIDE, V2011
   Elgammal A, 2002, P IEEE, V90, P1151, DOI 10.1109/JPROC.2002.801448
   Goodfellow I., 2009, ADV NEURAL INFORM PR, V22, P646
   Goyette N., 2012, IEEE COMP SOC C COMP, P1, DOI DOI 10.1109/CVPRW.2012.6238919
   Guyon C., 2012, PRINCIPAL COMPONENT, V1, P223, DOI DOI 10.5772/38267
   Harville M, 2001, IEEE WORKSHOP ON DETECTION AND RECOGNITION OF EVENTS IN VIDEO, PROCEEDINGS, P3, DOI 10.1109/EVENT.2001.938860
   Heikkila M, 2006, IEEE T PATTERN ANAL, V28, P657, DOI 10.1109/TPAMI.2006.68
   Hinton G., 2010, MOMENTUM, V9, P926
   Hofmann M., 2012, P IEEE COMP SOC C CO, P38, DOI DOI 10.1109/CVPRW.2012.6238925
   Huang JZ, 2009, IEEE I CONF COMP VIS, P64, DOI 10.1109/ICCV.2009.5459202
   Javed O, 2002, IEEE WORKSHOP ON MOTION AND VIDEO COMPUTING (MOTION 2002), PROCEEDINGS, P22, DOI 10.1109/MOTION.2002.1182209
   Moshe Y, 2012, PROC CVPR IEEE, P3210, DOI 10.1109/CVPR.2012.6248056
   Oliver NM, 2000, IEEE T PATTERN ANAL, V22, P831, DOI 10.1109/34.868684
   Stauffer C., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P246, DOI 10.1109/CVPR.1999.784637
   Tang P, 2007, PROCEEDINGS OF THE FOURTH INTERNATIONAL CONFERENCE ON IMAGE AND GRAPHICS, P530, DOI 10.1109/ICIG.2007.61
   Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Wang N., 2013, ADV NEURAL INFORM PR, P809
   Xu P, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P107, DOI 10.1145/2647868.2654914
   Yilmaz A, 2006, ACM COMPUT SURV, V38, DOI 10.1145/1177352.1177355
   Zen G., 2013, P 4 ACM IEEE INT WOR, P17, DOI DOI 10.1145/2510650.2510653
   Zhou XW, 2013, IEEE T PATTERN ANAL, V35, P597, DOI 10.1109/TPAMI.2012.132
NR 23
TC 27
Z9 29
U1 3
U2 89
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0925-2312
EI 1872-8286
J9 NEUROCOMPUTING
JI Neurocomputing
PD NOV 30
PY 2015
VL 168
BP 454
EP 463
DI 10.1016/j.neucom.2015.05.082
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA CO4XU
UT WOS:000359165000044
DA 2020-02-19
ER

PT J
AU Mocanu, DC
   Ammar, HB
   Lowet, D
   Driessens, K
   Liotta, A
   Weiss, G
   Tuyls, K
AF Mocanu, Decebal Constantin
   Ammar, Haitham Bou
   Lowet, Dietwig
   Driessens, Kurt
   Liotta, Antonio
   Weiss, Gerhard
   Tuyls, Karl
TI Factored four way conditional restricted Boltzmann machines for activity
   recognition
SO PATTERN RECOGNITION LETTERS
LA English
DT Article; Proceedings Paper
CT 1st International Workshop on Multimodal Pattern Recognition of Social
   Signals in Human Computer Interaction (MPRSS)
CY NOV 11, 2012
CL Tsukuba, JAPAN
SP Int Assoc Pattern Recognit (IAPR), TC3
DE Activity recognition; Deep learning; Restricted Boltzmann machines
AB This paper introduces a new learning algorithm for human activity recognition capable of simultaneous regression and classification. Building upon conditional restricted Boltzmann machines (CRBMs), Factored four way conditional restricted Boltzmann machines (FFW-CRBMs) incorporate a new label layer and four-way interactions among the neurons from the different layers. The additional layer gives the classification nodes a similar strong multiplicative effect compared to the other layers, and avoids that the classification neurons are overwhelmed by the (much larger set of) other neurons. This makes FFW-CRBMs capable of performing activity recognition, prediction and self auto evaluation of classification within one unified framework. As a second contribution, sequential Markov chain contrastive divergence (SMcCD) is introduced. SMcCD modifies Contrastive Divergence to compensate for the extra complexity of FFW-CRBMs during training. Two sets of experiments one on benchmark datasets and one a robotic platform for smart companions show the effectiveness of FFW-CRBMs. (C) 2015 Elsevier BAT. All rights reserved.
C1 [Mocanu, Decebal Constantin; Liotta, Antonio] Eindhoven Univ Technol, Dept Elect Engn, NL-5612 AZ Eindhoven, Netherlands.
   [Ammar, Haitham Bou] Univ Penn, Grasp Lab, Philadelphia, PA 19104 USA.
   [Lowet, Dietwig] Philips Res, Human Interact & Experiences, NL-5656 AE Eindhoven, Netherlands.
   [Driessens, Kurt; Weiss, Gerhard] Maastricht Univ, Dept Knowledge Engn, NL-6211 LH Maastricht, Netherlands.
   [Tuyls, Karl] Univ Liverpool, Dept Comp Sci, Liverpool L69 3BX, Merseyside, England.
RP Mocanu, DC (reprint author), Eindhoven Univ Technol, Dept Elect Engn, Dolech 2, NL-5612 AZ Eindhoven, Netherlands.
EM d.c.mocanu@tue.nl
RI Tuyls, Karl P/Q-7328-2018; Liotta, Antonio/G-9532-2014
OI Tuyls, Karl P/0000-0001-7929-1944; Liotta, Antonio/0000-0002-2773-4421;
   Mocanu, Decebal Constantin/0000-0002-5636-7683
FU FP7 European project Florence (Multi Purpose Mobile Robot for Ambient
   Assisted Living) [ICT-2009-248730]
FX This research has been partly funded by the FP7 European project
   Florence under grant ICT-2009-248730 (Multi Purpose Mobile Robot for
   Ambient Assisted Living).
CR Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chen LM, 2012, IEEE T SYST MAN CY C, V42, P790, DOI 10.1109/TSMCC.2012.2198883
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411
   Gehler P. V., 2006, P 23 INT C MACH LEAR, P2006
   Hinton G., 2010, TECHNICAL REPORT
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Jones N, 2014, NATURE, V505, P146, DOI 10.1038/505146a
   Larochelle H., 2008, CLASSIFICATION USING, P536
   Laserson J., 2011, XRDS, V18, P29, DOI DOI 10.1145/2000775.2000787
   Lowet D., 2012, WORKSH AMB INT INFRA
   Lowet D., 2012, ROMAN 2012
   Manzanares A., 2012, WORKSH IROS 2012 ASS
   Memisevic R, 2010, NEURAL COMPUT, V22, P1473, DOI 10.1162/neco.2010.01-09-953
   Ofli F., 2013, P IEEE WORKSH APPL C
   Salakhutdinov R., 2007, P INT C MACH LEARN, V24, P791, DOI DOI 10.1145/1273496.1273596
   Smolensky P, 1986, PARALLEL DISTRIBUTED, P194
   Sutskever I., 2007, J MACH LEARN RES, P548
   Taylor GW, 2011, J MACH LEARN RES, V12, P1025
   Taylor GW, 2009, P 26 ANN INT C MACH, P1025, DOI DOI 10.1145/1553374.1553505
   Yu S, 2011, J NEUROSCI, V31, P17514, DOI 10.1523/JNEUROSCI.3127-11.2011
   Zhang S, 2008, 2008 10TH IEEE INTERNATIONAL CONFERENCE ON E-HEALTH NETWORKING, APPLICATIONS AND SERVICES, P171, DOI 10.1109/HEALTH.2008.4600131
NR 22
TC 15
Z9 16
U1 0
U2 19
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0167-8655
EI 1872-7344
J9 PATTERN RECOGN LETT
JI Pattern Recognit. Lett.
PD NOV 15
PY 2015
VL 66
BP 100
EP 108
DI 10.1016/j.patrec.2015.01.013
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA CS7NU
UT WOS:000362271100012
DA 2020-02-19
ER

PT J
AU Mocanu, DC
   Pokhrel, J
   Garella, JP
   Seppanen, J
   Liotou, E
   Narwaria, M
AF Mocanu, Decebal Constantin
   Pokhrel, Jeevan
   Garella, Juan Pablo
   Seppanen, Janne
   Liotou, Eirini
   Narwaria, Manish
TI No-reference video quality measurement: added value of machine learning
SO JOURNAL OF ELECTRONIC IMAGING
LA English
DT Article
DE no-reference video quality assessment; deep learning; subjective
   studies; objective studies; quality of experience
ID IMAGE
AB Video quality measurement is an important component in the end-to-end video delivery chain. Video quality is, however, subjective, and thus, there will always be interobserver differences in the subjective opinion about the visual quality of the same video. Despite this, most existing works on objective quality measurement typically focus only on predicting a single score and evaluate their prediction accuracies based on how close it is to the mean opinion scores (or similar average based ratings). Clearly, such an approach ignores the underlying diversities in the subjective scoring process and, as a result, does not allow further analysis on how reliable the objective prediction is in terms of subjective variability. Consequently, the aim of this paper is to analyze this issue and present a machine-learning based solution to address it. We demonstrate the utility of our ideas by considering the practical scenario of video broadcast transmissions with focus on digital terrestrial television (DTT) and proposing a no-reference objective video quality estimator for such application. We conducted meaningful verification studies on different video content (including video clips recorded from real DTT broadcast transmissions) in order to verify the performance of the proposed solution. (C) 2015 SPIE and IS&T
C1 [Mocanu, Decebal Constantin] Eindhoven Univ Technol, Dept Elect Engn, FLX 9-104,POB 513, NL-5600 MB Eindhoven, Netherlands.
   [Pokhrel, Jeevan] Montimage, F-75013 Paris, France.
   [Garella, Juan Pablo] Univ Republica, Fac Ingn, Montevideo 11300, Uruguay.
   [Seppanen, Janne] VTT Tech Res Ctr Finland Ltd, Network Performance Team, Oulu 90590, Finland.
   [Liotou, Eirini] Univ Athens, Dept Informat & Telecommun, Athens 15784, Greece.
   [Narwaria, Manish] Dhirubhai Ambani Inst Informat & Commun Technol, Gandhinagar 382007, Gujarat, India.
RP Mocanu, DC (reprint author), Eindhoven Univ Technol, Dept Elect Engn, FLX 9-104,POB 513, NL-5600 MB Eindhoven, Netherlands.
EM d.c.mocanu@tue.nl
RI Liotou, Eirini/Q-3394-2019
OI Mocanu, Decebal Constantin/0000-0002-5636-7683; Narwaria,
   Manish/0000-0001-7789-5322
FU Qualinet [COST IC 1003]; European Commission under the FP7-PEOPLE
   MITN-CROSSFIRE project [317126]; Tekes, the Finnish Funding Agency for
   Technology and InnovationFinnish Funding Agency for Technology &
   Innovation (TEKES); Comision Academica de Posgrado, Universidad de la
   Republica, Uruguay
FX The work was supported in part by funding from Qualinet (COST IC 1003),
   which is gratefully acknowledged. Eirini Liotou's work was supported by
   the European Commission under the auspices of the FP7-PEOPLE
   MITN-CROSSFIRE project (Grant 317126). Janne Seppanen's work was
   supported by Tekes, the Finnish Funding Agency for Technology and
   Innovation, under Quality of Experience Estimators in Networks and Next
   Generation Over-the-Top Multimedia Services) projects. Juan Pablo
   Garella's work was partially funded by Comision Academica de Posgrado,
   Universidad de la Republica, Uruguay.
CR Aguiar E., 2012, IEEE 20 INT WORKSH Q, P1
   Ammar Haitham Bou, 2013, Machine Learning and Knowledge Discovery in Databases. European Conference (ECML PKDD 2013). Proceedings: LNCS 8189, P449, DOI 10.1007/978-3-642-40991-2_29
   Bakircioglu H, 2000, EUR J OPER RES, V126, P319, DOI 10.1016/S0377-2217(99)00481-6
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Chikkerur S, 2011, IEEE T BROADCAST, V57, P165, DOI 10.1109/TBC.2011.2104671
   Cramer C., 1998, IEEE Potentials, V17, P29, DOI 10.1109/45.652854
   Gastaldo P, 2002, IEEE T NEURAL NETWOR, V13, P939, DOI 10.1109/TNN.2002.1021894
   Gehler Peter V, 2006, P 23 INT C MACH LEAR, P337, DOI DOI 10.1145/1143844.1143887
   Gelenbe E, 1990, NEURAL COMPUT, V2, P239, DOI 10.1162/neco.1990.2.2.239
   Ghadiyaram D, 2014, 2014 IEEE GLOBAL CONFERENCE ON SIGNAL AND INFORMATION PROCESSING (GLOBALSIP), P946, DOI 10.1109/GlobalSIP.2014.7032260
   Hastie T, 2001, ELEMENTS STAT LEARNI
   Hinton G., 2012, NEURAL NETWORKS TRIC, V9, P599, DOI DOI 10.1007/978-3-642-35289-8_32
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hossfeld T, 2011, INT WORK QUAL MULTIM, P131, DOI 10.1109/QoMEX.2011.6065690
   ITU-R, 2012, METH SUBJ ASS QUAL T
   Jones N, 2014, NATURE, V505, P146, DOI 10.1038/505146a
   Joskowicz J., 2014, P LAT AM NETW C
   Joskowicz J, 2014, INT J DIGIT MULTIMED, DOI 10.1155/2014/242531
   Karapanos E, 2009, CHI2009: PROCEEDINGS OF THE 27TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P639
   Konuk B, 2013, IEEE IMAGE PROC, P54, DOI 10.1109/ICIP.2013.6738012
   Larochelle H., 2008, P 25 INT C MACH LEAR, P536, DOI [10.1145/1390156.1390224, DOI 10.1145/1390156.1390224]
   Laserson J., 2011, XRDS, V18, P29, DOI DOI 10.1145/2000775.2000787
   Le Callet P, 2006, IEEE T NEURAL NETWOR, V17, P1316, DOI 10.1109/TNN.2006.879766
   Liu Changlin, 2011, QUALITY EXPERIENCE E
   Mocanu DC, 2015, PROCEEDINGS OF THE 2015 IFIP/IEEE INTERNATIONAL SYMPOSIUM ON INTEGRATED NETWORK MANAGEMENT (IM), P1278, DOI 10.1109/INM.2015.7140481
   Mocanu DC, 2015, PATTERN RECOGN LETT, V66, P100, DOI 10.1016/j.patrec.2015.01.013
   Mocanu DC, 2014, IEEE IMAGE PROC, P758, DOI 10.1109/ICIP.2014.7025152
   Mocanu E, 2014, IEEE SYS MAN CYBERN, P1, DOI 10.1109/SMC.2014.6973875
   Mohamed S, 2002, IEEE T CIRC SYST VID, V12, P1071, DOI 10.1109/TCSVT.2002.806808
   Pechard S., 2008, P INT WORKSH IM MED, P6
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Qualinet, 2012, QUAL WHIT PAP DEF QU
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Salakhutdinov R., 2007, P INT C MACH LEARN, V24, P791, DOI DOI 10.1145/1273496.1273596
   Shahid M, 2014, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2014-40
   Singh K D, 2010, 2010 18th International Packet Video Workshop (PV 2010), P150, DOI 10.1109/PV.2010.5706832
   Singh KD, 2012, CONSUM COMM NETWORK, P127, DOI 10.1109/CCNC.2012.6181070
   Smolensky P, 1986, PARALLEL DISTRIBUTED, P194
   Sogaard J, 2015, IEEE T CIRC SYST VID, V25, P1637, DOI 10.1109/TCSVT.2015.2397207
   Sotelo R., 2015, BROADB MULT SYST BRO, P1
   Staelens N, 2013, IEEE T CIRC SYST VID, V23, P1322, DOI 10.1109/TCSVT.2013.2243052
   Tang HX, 2014, PROC CVPR IEEE, P2877, DOI 10.1109/CVPR.2014.368
   Varela M, 2013, INT WORK QUAL MULTIM, P70, DOI 10.1109/QoMEX.2013.6603213
   VQEG, 2003, SG09 VQEG ITUT
   Xu JT, 2014, IEEE IMAGE PROC, P491, DOI 10.1109/ICIP.2014.7025098
   Zhu KF, 2015, IEEE T CIRC SYST VID, V25, P533, DOI 10.1109/TCSVT.2014.2363737
NR 46
TC 12
Z9 12
U1 0
U2 8
PU IS&T & SPIE
PI BELLINGHAM
PA 1000 20TH ST, BELLINGHAM, WA 98225 USA
SN 1017-9909
EI 1560-229X
J9 J ELECTRON IMAGING
JI J. Electron. Imaging
PD NOV
PY 2015
VL 24
IS 6
AR 061208
DI 10.1117/1.JEI.24.6.061208
PG 15
WC Engineering, Electrical & Electronic; Optics; Imaging Science &
   Photographic Technology
SC Engineering; Optics; Imaging Science & Photographic Technology
GA DL8AH
UT WOS:000375861200008
DA 2020-02-19
ER

PT J
AU Tsai, CH
   Chih, YT
   Wong, WH
   Lee, CY
AF Tsai, Chang-Hung
   Chih, Yu-Ting
   Wong, Wing Hung
   Lee, Chen-Yi
TI A Hardware-Efficient Sigmoid Function With Adjustable Precision for a
   Neural Network System
SO IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS II-EXPRESS BRIEFS
LA English
DT Article
DE Adjustable precision; deep learning; hardware efficient; neural network;
   sigmoid function
ID ARCHITECTURE; PROCESSOR
AB A hardware-efficient sigmoid function calculator with adjustable precision for neural network and deep-learning applications is proposed in this brief. By adopting the bit-plane format of the input and output values, the computational latency of the processing time can be dynamically reduced according to the user configuration. To reduce the hardware cost, the coefficients used to calculate the sigmoid value can be shared for multiple calculators without any structural hazard. In addition, the restricted constraint is applied in the coefficients' training stage to further simplify the computation in the calculation stage with a negligible quality loss. A test module is designed for the proposal and operated at 300 MHz to achieve 75 million sigmoid calculations per second. Implemented in 90-nm CMOS technology, the core of the calculator costs 1663 gates, and a 1-kb globally shared memory is used to store the coefficients.
C1 [Tsai, Chang-Hung; Chih, Yu-Ting; Lee, Chen-Yi] Natl Chiao Tung Univ, Dept Elect Engn, Hsinchu 300, Taiwan.
   [Tsai, Chang-Hung; Chih, Yu-Ting; Lee, Chen-Yi] Natl Chiao Tung Univ, Inst Elect, Hsinchu 300, Taiwan.
   [Wong, Wing Hung] Stanford Univ, Dept Stat, Stanford, CA 94305 USA.
RP Tsai, CH (reprint author), Natl Chiao Tung Univ, Dept Elect Engn, Hsinchu 300, Taiwan.
EM allen@si2lab.org; yutingchih@si2lab.org; whwong@stanford.edu;
   cylee@si2lab.org
FU Ministry of Science and Technology of TaiwanMinistry of Science and
   Technology, Taiwan [MOST 103-2911-I-009-056]; National Science
   FoundationNational Science Foundation (NSF) [DMS-1330132, DMS-1407557]
FX This work was supported in part by the Ministry of Science and
   Technology of Taiwan under Grant MOST 103-2911-I-009-056 and in part by
   the National Science Foundation under Grants DMS-1330132 and
   DMS-1407557. This brief was recommended by Associate Editor Z. Zhang.
CR Al-Nsour M., 1998, P IEEE MIDW S CIRC S, P571
   Bishop CM, 2006, PATTERN RECOGNITION
   Chen TW, 2011, IEEE J EM SEL TOP C, V1, P357, DOI 10.1109/JETCAS.2011.2165231
   Chen TW, 2010, IEEE T VLSI SYST, V18, P957, DOI 10.1109/TVLSI.2009.2017543
   Craciun S, 2013, IEEE INT CONF ASAP, P370
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Kim SK, 2009, I C FIELD PROG LOGIC, P367, DOI 10.1109/FPL.2009.5272262
   Kim Y., 2014, P IEEE AS SOL STAT C, P213
   Le Ly D, 2010, IEEE T NEURAL NETWOR, V21, P1780, DOI 10.1109/TNN.2010.2073481
   Leboeuf K, 2008, THIRD 2008 INTERNATIONAL CONFERENCE ON CONVERGENCE AND HYBRID INFORMATION TECHNOLOGY, VOL 1, PROCEEDINGS, P1070, DOI 10.1109/ICCIT.2008.131
   LECUN Y., MNIST HANDWRITTEN DI
   Lee S, 2011, IEEE T NEURAL NETWOR, V22, P64, DOI 10.1109/TNN.2010.2085443
   Muller KR, 2001, IEEE T NEURAL NETWOR, V12, P181, DOI 10.1109/72.914517
   Ouyang J., 2014, P HOTCHIPS AUG
   Painkras E, 2013, IEEE J SOLID-ST CIRC, V48, P1943, DOI 10.1109/JSSC.2013.2259038
   Park SW, 2015, INFORMATION, COMMUNICATION AND ENVIRONMENT: MARINE NAVIGATION AND SAFETY OF SEA TRANSPORTATION, P93
   PIAZZA F, 1993, IEEE IJCNN, P1401
   Tsai CH, 2014, IEEE INT SYMP CIRC S, P157, DOI 10.1109/ISCAS.2014.6865089
NR 18
TC 13
Z9 14
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1549-7747
EI 1558-3791
J9 IEEE T CIRCUITS-II
JI IEEE Trans. Circuits Syst. II-Express Briefs
PD NOV
PY 2015
VL 62
IS 11
BP 1073
EP 1077
DI 10.1109/TCSII.2015.2456531
PG 5
WC Engineering, Electrical & Electronic
SC Engineering
GA CX8XN
UT WOS:000365988500013
DA 2020-02-19
ER

PT J
AU Alexander, WH
   Brown, JW
AF Alexander, William H.
   Brown, Joshua W.
TI Hierarchical Error Representation: A Computational Model of Anterior
   Cingulate and Dorsolateral Prefrontal Cortex
SO NEURAL COMPUTATION
LA English
DT Article
ID OBSESSIVE-COMPULSIVE DISORDER; SPATIAL WORKING-MEMORY;
   SHORT-TERM-MEMORY; COGNITIVE CONTROL; FRONTAL-CORTEX;
   FUNCTIONAL-ORGANIZATION; INTEGRATIVE THEORY; NEURAL MECHANISMS; DOPAMINE
   NEURONS; DECISION-MAKING
AB Anterior cingulate and dorsolateral prefrontal cortex (ACC and dlPFC, respectively) are core components of the cognitive control network. Activation of these regions is routinely observed in tasks that involve monitoring the external environment and maintaining information in order to generate appropriate responses. Despite the ubiquity of studies reporting coactivation of these two regions, a consensus on how they interact to support cognitive control has yet to emerge. In this letter, we present a new hypothesis and computational model of ACC and dlPFC. The error representation hypothesis states that multidimensional error signals generated by ACC in response to surprising outcomes are used to train representations of expected error in dlPFC, which are then associated with relevant task stimuli. Error representations maintained in dlPFC are in turn used to modulate predictive activity in ACC in order to generate better estimates of the likely outcomes of actions. We formalize the error representation hypothesis in a new computational model based on our previous model of ACC. The hierarchical error representation (HER) model of ACC/dlPFC suggests a mechanism by which hierarchically organized layers within ACC and dlPFC interact in order to solve sophisticated cognitive tasks. In a series of simulations, we demonstrate the ability of the HER model to autonomously learn to perform structured tasks in a manner comparable to human performance, and we show that the HER model outperforms current deep learning networks by an order of magnitude.
C1 [Alexander, William H.; Brown, Joshua W.] Indiana Univ, Dept Psychol & Brain Sci, Bloomington, IN 47405 USA.
   [Alexander, William H.] Univ Ghent, Dept Expt Psychol, B-9000 Ghent, Belgium.
RP Alexander, WH (reprint author), Indiana Univ, Dept Psychol & Brain Sci, Bloomington, IN 47405 USA.
EM william.alexander@ugent.be; jwmbrown@indiana.edu
FU Intelligence Advanced Research Projects Activity (IARPA) via Department
   of the Interior (DOI) [D10PC20023]; FWO-Flanders Odysseus II Award
   [G.OC44.13N]
FX This work was supported in part by the Intelligence Advanced Research
   Projects Activity (IARPA) via Department of the Interior (DOI) contract
   number D10PC20023. The U.S. government is authorized to reproduce and
   distribute reprints for governmental purposes notwithstanding any
   copyright annotation thereon. The views and conclusions contained here
   are those of the authors and should not be interpreted as necessarily
   representing the official policies or endorsements, expressed or
   implied, of IARPA, DOI, or the U.S. government. W.H.A. was supported in
   part by FWO-Flanders Odysseus II Award G.OC44.13N. We thank Michael
   Frank, Anne Collins, Tom Verguts, Massimo Silvetti, Derek Nee, and
   Maynard James Keenan for discussion and critical feedback.
CR Abi-Dargham A, 2002, J NEUROSCI, V22, P3708
   Alexander WH, 2014, FRONT COMPUT NEUROSC, V8, DOI 10.3389/fncom.2014.00069
   Alexander WH, 2011, NAT NEUROSCI, V14, P1338, DOI 10.1038/nn.2921
   Amador N, 2000, J NEUROPHYSIOL, V84, P2166
   Amiez C, 2014, CEREB CORTEX, V24, P563, DOI 10.1093/cercor/bhs329
   ANDERSEN RA, 1985, SCIENCE, V230, P456, DOI 10.1126/science.4048942
   Asaad WF, 2000, J NEUROPHYSIOL, V84, P451
   Aston-Jones G, 2005, ANNU REV NEUROSCI, V28, P403, DOI 10.1146/annurev.neuro.28.061604.135709
   Awh E, 2001, TRENDS COGN SCI, V5, P119, DOI 10.1016/S1364-6613(00)01593-X
   Baddeley A, 1974, PSYCHOL LEARN MOTIV, V8, P47, DOI [10.1016/S0079-7421(08)60452-1, DOI 10.1016/S0079-7421(08)60452-1]
   Badre D, 2008, TRENDS COGN SCI, V12, P193, DOI 10.1016/j.tics.2008.02.004
   Badre D, 2007, J COGNITIVE NEUROSCI, V19, P2082, DOI 10.1162/jocn.2007.19.12.2082
   Badre D, 2012, CEREB CORTEX, V22, P527, DOI 10.1093/cercor/bhr117
   Badre D, 2010, NEURON, V66, P315, DOI 10.1016/j.neuron.2010.03.025
   Badre D, 2009, NAT REV NEUROSCI, V10, P659, DOI 10.1038/nrn2667
   BARBAS H, 1989, J COMP NEUROL, V286, P353, DOI 10.1002/cne.902860306
   Barbas H, 1997, CEREB CORTEX, V7, P635, DOI 10.1093/cercor/7.7.635
   Barto A., 2003, DISCRETE EVENT DYN S, V13, P341, DOI DOI 10.1023/A:1025696116075
   BARTO AG, 1995, ARTIF INTELL, V72, P81, DOI 10.1016/0004-3702(94)00011-O
   BARTO AG, 1983, IEEE T SYST MAN CYB, V13, P834, DOI 10.1109/TSMC.1983.6313077
   Behrens TEJ, 2007, NAT NEUROSCI, V10, P1214, DOI 10.1038/nn1954
   Botvinick MM, 2008, TRENDS COGN SCI, V12, P201, DOI 10.1016/j.tics.2008.02.009
   Botvinick MM, 2009, COGNITION, V113, P262, DOI 10.1016/j.cognition.2008.08.011
   Botvinick MM, 2001, PSYCHOL REV, V108, P624, DOI 10.1037//0033-295X.108.3.624
   Braver TS, 2001, NEUROIMAGE, V14, P48, DOI 10.1006/nimg.2001.0791
   Brown JW, 2005, SCIENCE, V307, P1118, DOI 10.1126/science.1105783
   Brunel N, 2001, J COMPUT NEUROSCI, V11, P63, DOI 10.1023/A:1011204814320
   Bush G, 2000, TRENDS COGN SCI, V4, P215, DOI 10.1016/S1364-6613(00)01483-2
   Carter CS, 1998, SCIENCE, V280, P747, DOI 10.1126/science.280.5364.747
   CLARK SA, 1988, NATURE, V332, P444, DOI 10.1038/332444a0
   COHEN JD, 1993, SCHIZOPHRENIA BULL, V19, P85, DOI 10.1093/schbul/19.1.85
   Collins A, 2012, PLOS BIOL, V10, DOI 10.1371/journal.pbio.1001293
   Collins AGE, 2013, PSYCHOL REV, V120, P190, DOI 10.1037/a0030852
   Cooper R, 2000, COGN NEUROPSYCHOL, V17, P297, DOI 10.1080/026432900380427
   COWAN N, 1988, PSYCHOL BULL, V104, P163, DOI 10.1037/0033-2909.104.2.163
   D'Esposito M, 1999, BRAIN COGNITION, V41, P66, DOI 10.1006/brcg.1999.1096
   Denys D, 2004, J CLIN PSYCHIAT, V65, P11
   Dixon ML, 2014, BRAIN RES, V1572, P26, DOI 10.1016/j.brainres.2014.05.012
   Donoso M, 2014, SCIENCE, V344, P1481, DOI 10.1126/science.1252254
   Friston KJ, 2010, NAT REV NEUROSCI, V11, P607, DOI 10.1038/nrn2787-c2
   Friston KJ, 2009, TRENDS COGN SCI, V13, P293, DOI 10.1016/j.tics.2009.04.005
   Garavan H, 1998, MEM COGNITION, V26, P263, DOI 10.3758/BF03201138
   GARIANO RF, 1988, BRAIN RES, V462, P194, DOI 10.1016/0006-8993(88)90606-3
   GEHRING WJ, 1990, PSYCHOPHYSIOLOGY, V27, P34
   Genovesio A, 2005, NEURON, V47, P307, DOI 10.1016/j.neuron.2005.06.006
   Glascher J, 2010, NEURON, V66, P585, DOI 10.1016/j.neuron.2010.04.016
   GOODMAN WK, 1990, J CLIN PSYCHIAT, V51, P36
   GROSSBERG S, 1980, PSYCHOL REV, V87, P1, DOI 10.1037/0033-295X.87.1.1
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Holroyd CB, 2004, NAT NEUROSCI, V7, P497, DOI 10.1038/nn1238
   Holroyd CB, 2002, PSYCHOL REV, V109, P679, DOI [10.1037//0033-295X.109.4.679, 10.1037/0033-295X.109.4.679]
   Holroyd CB, 2012, TRENDS COGN SCI, V16, P122, DOI 10.1016/j.tics.2011.12.008
   Hussar CR, 2013, J NEUROSCI, V33, P972, DOI 10.1523/JNEUROSCI.4075-12.2013
   Izhikevich EM, 2007, CEREB CORTEX, V17, P2443, DOI 10.1093/cercor/bhl152
   Jahn A, 2014, NEUROIMAGE, V95, P80, DOI 10.1016/j.neuroimage.2014.03.050
   Jong N. K, 2008, P 25 INT C MACH LEAR, P432
   Jun JK, 2010, J NEUROSCI, V30, P916, DOI 10.1523/JNEUROSCI.2062-09.2010
   KEHOE EJ, 1986, J EXP PSYCHOL ANIM B, V12, P186, DOI 10.1037/0097-7403.12.2.186
   Kennerley SW, 2006, NAT NEUROSCI, V9, P940, DOI 10.1038/nn1724
   KLOPF AH, 1972, BRAIN FUNCTION ADAPT
   Koechlin E, 2003, SCIENCE, V302, P1181, DOI 10.1126/science.1088545
   Kolling N, 2012, SCIENCE, V336, P95, DOI 10.1126/science.1216930
   Kouneiher F, 2009, NAT NEUROSCI, V12, P939, DOI 10.1038/nn.2321
   Krueger K. A., 2011, SEQUENTIAL LEARNING
   Krueger KA, 2009, COGNITION, V110, P380, DOI 10.1016/j.cognition.2008.11.014
   MacDonald AW, 2000, SCIENCE, V288, P1835, DOI 10.1126/science.288.5472.1835
   MACKINTOSH NJ, 1975, PSYCHOL REV, V82, P276, DOI 10.1037/h0076778
   Medalla M, 2009, NEURON, V61, P609, DOI 10.1016/j.neuron.2009.01.006
   Miller EK, 2001, ANNU REV NEUROSCI, V24, P167, DOI 10.1146/annurev.neuro.24.1.167
   Miller EK, 1996, J NEUROSCI, V16, P5154
   Montague PR, 1996, J NEUROSCI, V16, P1936, DOI 10.1523/jneurosci.16-05-01936.1996
   Muly EC, 1998, J NEUROSCI, V18, P10553
   MURASE S, 1993, NEUROSCI LETT, V157, P53, DOI 10.1016/0304-3940(93)90641-W
   Murphy BL, 1996, P NATL ACAD SCI USA, V93, P1325, DOI 10.1073/pnas.93.3.1325
   Nee DE, 2014, CEREB CORTEX, V24, P2377, DOI 10.1093/cercor/bht091
   Nee DE, 2013, CEREB CORTEX, V23, P2146, DOI 10.1093/cercor/bhs194
   Nee DE, 2012, NEUROIMAGE, V63, P1285, DOI 10.1016/j.neuroimage.2012.08.034
   Nee DE, 2011, NEUROIMAGE, V54, P528, DOI 10.1016/j.neuroimage.2010.08.027
   NIKI H, 1979, BRAIN RES, V171, P213, DOI 10.1016/0006-8993(79)90328-7
   O'Doherty JP, 2011, ANN NY ACAD SCI, V1239, P118, DOI 10.1111/j.1749-6632.2011.06290.x
   O'Reilly RC, 2006, NEURAL COMPUT, V18, P283, DOI 10.1162/089976606775093909
   PEARCE JM, 1980, PSYCHOL REV, V87, P532, DOI 10.1037/0033-295X.87.6.532
   Rainer G, 1999, J NEUROSCI, V19, P5493
   Rao RPN, 1999, NAT NEUROSCI, V2, P79, DOI 10.1038/4580
   RESCORLA R A, 1971, Learning and Motivation, V2, P113, DOI 10.1016/0023-9690(71)90002-6
   Reynolds JR, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0030284
   Riggall AC, 2012, J NEUROSCI, V32, P12990, DOI 10.1523/JNEUROSCI.1892-12.2012
   Rougier NP, 2005, P NATL ACAD SCI USA, V102, P7338, DOI 10.1073/pnas.0502455102
   Rudebeck PH, 2008, J NEUROSCI, V28, P13775, DOI 10.1523/JNEUROSCI.3541-08.2008
   Schultz W, 1997, SCIENCE, V275, P1593, DOI 10.1126/science.275.5306.1593
   Seger CA, 2010, ANNU REV NEUROSCI, V33, P203, DOI 10.1146/annurev.neuro.051508.135546
   Seo H, 2007, CEREB CORTEX, V17, pI110, DOI 10.1093/cercor/bhm064
   ServanSchreiber D, 1996, ARCH GEN PSYCHIAT, V53, P1105
   Shima K, 1998, SCIENCE, V282, P1335, DOI 10.1126/science.282.5392.1335
   Sutton R. S., 1990, P 7 INT C MACH LEARN, P216
   SUTTON RS, 1990, LEARNING COMPUTATION, P497
   TABER MT, 1993, NEUROPSYCHOPHARMACOL, V9, P271, DOI 10.1038/npp.1993.63
   Taren AA, 2011, J NEUROSCI, V31, P5026, DOI 10.1523/JNEUROSCI.5762-10.2011
   Wallis JD, 2003, J NEUROPHYSIOL, V90, P1790, DOI 10.1152/jn.00086.2003
   Walton ME, 2004, NAT NEUROSCI, V7, P1259, DOI 10.1038/nn1339
   WIDROW B, 1960, ADAPTIVE SWITCHING C, P96
   WILLIAMS SM, 1993, CEREB CORTEX, V3, P199, DOI 10.1093/cercor/3.3.199
   Yamada M, 2010, NEUROSCI RES, V67, P162, DOI 10.1016/j.neures.2010.02.011
   Yeung N, 2004, PSYCHOL REV, V111, P931, DOI [10.1037/0033-295X.111.4.931, 10.1037/0033-295x.111.4.931]
   Yu AJ, 2005, NEURON, V46, P681, DOI 10.1016/j.neuron.2005.04.026
   Yu CS, 2011, NEUROIMAGE, V54, P2571, DOI 10.1016/j.neuroimage.2010.11.018
   Zarr N., 2015, HIERARCHICAL E UNPUB
NR 107
TC 32
Z9 32
U1 0
U2 16
PU MIT PRESS
PI CAMBRIDGE
PA ONE ROGERS ST, CAMBRIDGE, MA 02142-1209 USA
SN 0899-7667
EI 1530-888X
J9 NEURAL COMPUT
JI Neural Comput.
PD NOV
PY 2015
VL 27
IS 11
BP 2354
EP 2410
DI 10.1162/NECO_a_00779
PG 57
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA CV6RG
UT WOS:000364397200003
PM 26378874
DA 2020-02-19
ER

PT J
AU Yu, YT
   Guan, HY
   Ji, Z
AF Yu, Yongtao
   Guan, Haiyan
   Ji, Zheng
TI Rotation-Invariant Object Detection in High-Resolution Satellite Imagery
   Using Superpixel-Based Deep Hough Forests
SO IEEE GEOSCIENCE AND REMOTE SENSING LETTERS
LA English
DT Article
DE Airplane detection; deep learning; Hough forest; object detection;
   rotation invariance; ship detection
ID VEHICLE DETECTION; TARGET DETECTION; MODEL
AB This letter presents a rotation-invariant method for detecting geospatial objects from high-resolution satellite images. First, a superpixel segmentation strategy is proposed to generate meaningful and nonredundant patches. Second, a multilayer deep feature generation model is developed to generate high-level feature representations of patches using deep learning techniques. Third, a set of multiscale Hough forests with embedded patch orientations is constructed to cast rotation-invariant votes for estimating object centroids. Quantitative evaluations on the images collected from Google Earth service show that an average completeness, correctness, quality, and F-1 - measure values of 0.958, 0.969, 0.929, and 0.963, respectively, are obtained. Comparative studies with three existing methods demonstrate the superior performance of the proposed method in accurately and correctly detecting objects that are arbitrarily oriented and of varying sizes.
C1 [Yu, Yongtao] Huaiyin Inst Technol, Fac Comp & Software Engn, Huaian 223003, Peoples R China.
   [Guan, Haiyan] Nanjing Univ Informat Sci & Technol, Coll Geog & Remote Sensing, Nanjing 210044, Jiangsu, Peoples R China.
   [Ji, Zheng] Wuhan Univ, Sch Remote Sensing Informat & Engn, Wuhan 430079, Peoples R China.
RP Guan, HY (reprint author), Nanjing Univ Informat Sci & Technol, Coll Geog & Remote Sensing, Nanjing 210044, Jiangsu, Peoples R China.
EM allennessy.yu@gmail.com; guanhy.nj@nuist.edu.cn; jz07@whu.edu.cn
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [41501501]; Natural Science Foundation of Jiangsu
   ProvinceJiangsu Planned Projects for Postdoctoral Research FundsNatural
   Science Foundation of Jiangsu Province [BK20151524]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 41501501 and the Natural Science
   Foundation of Jiangsu Province under Grant BK20151524. (Corresponding
   author: Haiyan Guan.)
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Bai X, 2014, IEEE T GEOSCI REMOTE, V52, P6508, DOI 10.1109/TGRS.2013.2296782
   Bhagavathy S, 2006, IEEE T GEOSCI REMOTE, V44, P3706, DOI 10.1109/TGRS.2006.881741
   Chen XY, 2014, IEEE GEOSCI REMOTE S, V11, P1797, DOI 10.1109/LGRS.2014.2309695
   Cheng G, 2014, ISPRS J PHOTOGRAMM, V98, P119, DOI 10.1016/j.isprsjprs.2014.10.002
   Cheng G, 2013, ISPRS J PHOTOGRAMM, V85, P32, DOI 10.1016/j.isprsjprs.2013.08.001
   Durieux L, 2008, ISPRS J PHOTOGRAMM, V63, P399, DOI 10.1016/j.isprsjprs.2008.01.005
   Eikvil L, 2009, ISPRS J PHOTOGRAMM, V64, P65, DOI 10.1016/j.isprsjprs.2008.09.005
   Gall J, 2011, IEEE T PATTERN ANAL, V33, P2188, DOI 10.1109/TPAMI.2011.70
   Hu XY, 2013, IEEE GEOSCI REMOTE S, V10, P466, DOI 10.1109/LGRS.2012.2210188
   Lei Z, 2012, IEEE T GEOSCI REMOTE, V50, P1206, DOI 10.1109/TGRS.2011.2166966
   Li Y, 2012, IEEE GEOSCI REMOTE S, V9, P886, DOI 10.1109/LGRS.2012.2183337
   Li ZC, 2011, IEEE T IMAGE PROCESS, V20, P2017, DOI 10.1109/TIP.2010.2099128
   Qin RJ, 2014, ISPRS J PHOTOGRAMM, V96, P179, DOI 10.1016/j.isprsjprs.2014.07.007
   Salakhutdinov R, 2013, IEEE T PATTERN ANAL, V35, P1958, DOI 10.1109/TPAMI.2012.269
   Salakhutdinov R, 2012, NEURAL COMPUT, V24, P1967, DOI 10.1162/NECO_a_00311
   Scott GJ, 2011, IEEE T GEOSCI REMOTE, V49, P1603, DOI 10.1109/TGRS.2010.2088404
   Sun H, 2012, IEEE GEOSCI REMOTE S, V9, P109, DOI 10.1109/LGRS.2011.2161569
   Xu J, 2014, IEEE GEOSCI REMOTE S, V11, P2070, DOI 10.1109/LGRS.2014.2319082
   Zhang WC, 2014, IEEE GEOSCI REMOTE S, V11, P74, DOI 10.1109/LGRS.2013.2246538
NR 20
TC 16
Z9 17
U1 0
U2 45
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1545-598X
EI 1558-0571
J9 IEEE GEOSCI REMOTE S
JI IEEE Geosci. Remote Sens. Lett.
PD NOV
PY 2015
VL 12
IS 11
BP 2183
EP 2187
DI 10.1109/LGRS.2015.2432135
PG 5
WC Geochemistry & Geophysics; Engineering, Electrical & Electronic; Remote
   Sensing; Imaging Science & Photographic Technology
SC Geochemistry & Geophysics; Engineering; Remote Sensing; Imaging Science
   & Photographic Technology
GA CV4DG
UT WOS:000364215500001
DA 2020-02-19
ER

PT J
AU Hahn, WE
   Lewkowitz, S
   Lacombe, DC
   Barenholtz, E
AF Hahn, William Edward
   Lewkowitz, Stephanie
   Lacombe, Daniel C., Jr.
   Barenholtz, Elan
TI Deep learning human actions from video via sparse filtering and locally
   competitive algorithms
SO MULTIMEDIA TOOLS AND APPLICATIONS
LA English
DT Article
DE Neuroscience; Computer vision; Sparse coding; Sparse filtering; Locally
   competitive algorithms
AB Physiological and psychophysical evidence suggest that early visual cortex compresses the visual input on the basis of spatial and orientation-tuned filters. Recent computational advances have suggested that these neural response characteristics may reflect a 'sparse coding' architecture-in which a small number of neurons need to be active for any given image-yielding critical structure latent in natural scenes. Here we present a novel neural network architecture combining a sparse filter model and locally competitive algorithms (LCAs), and demonstrate the network's ability to classify human actions from video. Sparse filtering is an unsupervised feature learning algorithm designed to optimize the sparsity of the feature distribution directly without having the need to model the data distribution. LCAs are defined by a system of differential equations where the initial conditions define an optimization problem and the dynamics converge to a sparse decomposition of the input vector. We applied this architecture to train a classifier on categories of motion in human action videos. Inputs to the network were small 3D patches taken from frame differences in the videos. Dictionaries were derived for each action class and then activation levels for each dictionary were assessed during reconstruction of a novel test patch. Overall, classification accuracy was at a parts per thousand 97 %. We discuss how this sparse filtering approach provides a natural framework for multi-sensory and multimodal data processing including RGB video, RGBD video, hyper-spectral video, and stereo audio/video streams.
C1 [Hahn, William Edward; Barenholtz, Elan] Florida Atlantic Univ, Ctr Complex Syst & Brain Sci, Boca Raton, FL 33431 USA.
   [Lacombe, Daniel C., Jr.; Barenholtz, Elan] Florida Atlantic Univ, Dept Psychol, Boca Raton, FL 33431 USA.
   [Lewkowitz, Stephanie] Florida Atlantic Univ, Dept Phys, Boca Raton, FL 33431 USA.
RP Hahn, WE (reprint author), Florida Atlantic Univ, Ctr Complex Syst & Brain Sci, Boca Raton, FL 33431 USA.
EM williamedwardhahn@gmail.com; elanbarenholtz@gmail.com
CR Atick JJ, 1990, NEURAL COMPUT, V2, P308, DOI 10.1162/neco.1990.2.3.308
   ATKINSON J, 1988, PERCEPTION, V17, P587, DOI 10.1068/p170587
   Blakemore C., 1970, DEV BRAIN DEPENDS VI
   Blakemore C., 1969, EXISTENCE NEURONES H
   Boureau Y., 2010, P 27 INT C MACH LEAR, P111, DOI DOI 10.1016/J.NEUNET.2012.02.023
   Boyd S., 2004, CONVEX OPTIMIZATION
   CANDY JC, 1971, AT&T TECH J, V50, P1889, DOI 10.1002/j.1538-7305.1971.tb02587.x
   Castrodad A, 2012, INT J COMPUT VISION, V100, P1, DOI 10.1007/s11263-012-0534-7
   JONES JP, 1987, J NEUROPHYSIOL, V58, P1233
   LEGGE GE, 1980, J OPT SOC AM, V70, P1458, DOI 10.1364/JOSA.70.001458
   Ngiam J., 2011, ADV NEURAL INFORM PR, P1125
   Olshausen BA, 2004, CURR OPIN NEUROBIOL, V14, P481, DOI 10.1016/j.conb.2004.07.007
   Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7
   Rozell Christopher, 2007, Proceedings 2007 IEEE International Conference on Image Processing, ICIP 2007, P169
   Rozell CJ, 2008, NEURAL COMPUT, V20, P2526, DOI 10.1162/neco.2008.03-07-486
   SACHS MB, 1971, J OPT SOC AM, V61, P1176, DOI 10.1364/JOSA.61.001176
   Schmidt M., 2009, TR200919 U BRIT COL
   Schuldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462
NR 18
TC 3
Z9 3
U1 0
U2 23
PU SPRINGER
PI DORDRECHT
PA VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN 1380-7501
EI 1573-7721
J9 MULTIMED TOOLS APPL
JI Multimed. Tools Appl.
PD NOV
PY 2015
VL 74
IS 22
BP 10097
EP 10110
DI 10.1007/s11042-015-2808-x
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic
SC Computer Science; Engineering
GA CV1LQ
UT WOS:000364019400015
DA 2020-02-19
ER

PT J
AU Wu, HB
   Gu, XD
AF Wu, Haibing
   Gu, Xiaodong
TI Towards dropout training for convolutional neural networks
SO NEURAL NETWORKS
LA English
DT Article
DE Deep learning; Convolutional neural networks; Max-pooling dropout
AB Recently, dropout has seen increasing use in deep learning. For deep convolutional neural networks, dropout is known to work well in fully-connected layers. However, its effect in convolutional and pooling layers is still not clear. This paper demonstrates that max-pooling dropout is equivalent to randomly picking activation based on a multinomial distribution at training time. In light of this insight, we advocate employing our proposed probabilistic weighted pooling, instead of commonly used max-pooling, to act as model averaging at test time. Empirical evidence validates the superiority of probabilistic weighted pooling. We also empirically show that the effect of convolutional dropout is not trivial, despite the dramatically reduced possibility of over-fitting due to the convolutional architecture. Elaborately designing dropout training simultaneously in max-pooling and fully-connected layers, we achieve state-of-the-art performance on MNIST, and very competitive results on CIFAR-10 and CIFAR-100, relative to other approaches without data augmentation. Finally, we compare max-pooling dropout and stochastic pooling, both of which introduce stochasticity based on multinomial distributions at pooling stage. (C) 2015 Elsevier Ltd. All rights reserved.
C1 [Wu, Haibing; Gu, Xiaodong] Fudan Univ, Dept Elect Engn, Shanghai 200433, Peoples R China.
RP Gu, XD (reprint author), Fudan Univ, Dept Elect Engn, Shanghai 200433, Peoples R China.
EM haibingwu13@fudan.edu.cn; xdgu@fudan.edu.cn
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61371148]
FX This work was supported in part by National Natural Science Foundation
   of China under grant 61371148.
CR Baldi P, 2014, ARTIF INTELL, V210, P78, DOI 10.1016/j.artint.2014.02.004
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.1007/BF00058655
   Goodfellow I. J., 2013, P 30 INT C MACH LEAR
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton G. E, 2012, ARXIV12070580
   Krizhevsky A., 2009, THESIS U TORONTO
   Krizhevsky A., 2012, ADV NEURAL INFORM PR
   Lecun Yann, 1998, P IEEE
   Ledoux M., 1991, PROBABILITY BANACH S
   Lin M., 2014, P 3 INT C LEARN REPR
   Mackay D. J. C., 1995, BAYESIAN METHODS BAC
   Springenberg J. T., 2014, P 3 INT C LEARN REPR
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Vinod N., 2010, P 27 INT C MACH LEAR
   Wager S, 2013, ADV NEURAL INFORM PR
   Wan L, 2013, P 30 INT C MACH LEAR
   Warde F. D., 2014, P 3 INT C LEARN REPR
   Zeiler M. D., 2013, P 2 INT C LEARN REPR
NR 19
TC 53
Z9 58
U1 2
U2 50
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0893-6080
EI 1879-2782
J9 NEURAL NETWORKS
JI Neural Netw.
PD NOV
PY 2015
VL 71
BP 1
EP 10
DI 10.1016/j.neunet.2015.07.007
PG 10
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA CV3LO
UT WOS:000364160900001
PM 26277608
DA 2020-02-19
ER

PT J
AU Tang, DY
   Qin, B
   Liu, T
AF Tang, Duyu
   Qin, Bing
   Liu, Ting
TI Deep learning for sentiment analysis: successful approaches and future
   challenges
SO WILEY INTERDISCIPLINARY REVIEWS-DATA MINING AND KNOWLEDGE DISCOVERY
LA English
DT Article
ID SPACES; MODELS
AB Sentiment analysis (also known as opinion mining) is an active research area in natural language processing. It aims at identifying, extracting and organizing sentiments from user generated texts in social networks, blogs or product reviews. A lot of studies in literature exploit machine learning approaches to solve sentiment analysis tasks from different perspectives in the past 15 years. Since the performance of a machine learner heavily depends on the choices of data representation, many studies devote to building powerful feature extractor with domain expert and careful engineering. Recently, deep learning approaches emerge as powerful computational models that discover intricate semantic representations of texts automatically from data without feature engineering. These approaches have improved the state-of-the-art in many sentiment analysis tasks including sentiment classification of sentences/documents, sentiment extraction and sentiment lexicon learning. In this paper, we provide an overview of the successful deep learning approaches for sentiment analysis tasks, lay out the remaining challenges and provide some suggestions to address these challenges. WIREs Data Mining Knowl Discov 2015, 5:292-303. doi: 10.1002/widm.1171 For further resources related to this article, please visit the .
C1 [Tang, Duyu; Qin, Bing; Liu, Ting] Harbin Inst Technol, Dept Comp Sci & Technol, Harbin 150006, Peoples R China.
RP Qin, B (reprint author), Harbin Inst Technol, Dept Comp Sci & Technol, Harbin 150006, Peoples R China.
EM qinb@ir.hit.edu.cn
FU National High Technology Development 863 Program of ChinaNational High
   Technology Research and Development Program of China [2015AA015407];
   National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61133012, 61273321]; Baidu Fellowship; IBM
FX We gratefully acknowledge the helpful discussions with Yaming Sun. We
   thank the editor and anonymous reviewers for their helpful comments and
   feedbacks. This work was supported by the National High Technology
   Development 863 Program of China (No. 2015AA015407) and National Natural
   Science Foundation of China (Nos 61133012 and 61273321). Duyu Tang is
   supported by Baidu Fellowship and IBM Ph.D. Fellowship.
CR Baccianella S., 2010, P 7 INT C LANG RES O, V10, P2200, DOI DOI citeulike-article-id:9238846
   Bai B, 2010, INFORM RETRIEVAL, V13, P291, DOI 10.1007/s10791-009-9117-9
   Baker L. D., 1998, Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P96, DOI 10.1145/290941.290970
   Bengio Y, 2003, J MACH LEARN RES, V3, P1137, DOI 10.1162/153244303322533223
   Bengio Y., 2015, DEEP LEARNING UNPUB
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bespalov D., 2011, P 20 ACM INT C INF K, P375, DOI DOI 10.1145/2063576.2063635
   Bhatia P, 2015, ARXIV150901599
   Bo Pang, 2008, Foundations and Trends in Information Retrieval, V2, P1, DOI 10.1561/1500000001
   Breck E, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2683
   Brown P. F., 1992, Computational Linguistics, V18, P467
   Chen Lu, 2012, ICWSM
   Choi Y., 2005, P C HUM LANG TECHN E, P355, DOI DOI 10.3115/1220575.1220620
   Collobert R., 2008, P 25 ICML, V25, P160, DOI [10.1145/1390156.1390177, DOI 10.1145/1390156.1390177]
   DEERWESTER S, 1990, J AM SOC INFORM SCI, V41, P391, DOI 10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9
   Deng L, 2015, MPQA 3 0 ENTITY EVEN
   Ding X., 2008, P 2008 INT C WEB SEA, P231, DOI DOI 10.1145/1341531.1341561
   Dong L, 2014, PROCEEDINGS OF THE 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 2, P49
   Dong L, 2015, COMPUT LINGUIST, V41, P293, DOI 10.1162/COLI_a_00221
   Esuli A., 2007, P 45 ANN M ASS COMP, P442
   Esuli A, 2005, P 14 ACM INT C INF K, P617, DOI DOI 10.1145/1099554.1099713
   Faruqui M., 2014, ARXIV14114166
   Feldman R, 2013, COMMUN ACM, V56, P82, DOI 10.1145/2436256.2436274
   Feng S., 2011, P C EMP METH NAT LAN, P1092
   Feng Song, 2013, P 51 ANN M ASS COMP, P1774
   Frege H., 1949, READINGS PHILOS ANAL, P85
   GLOROT Xavier, 2011, P 28 INT C MACH LEAR, V28, P513, DOI DOI 10.1177/1753193411430810
   Gutmann MU, 2012, J MACH LEARN RES, V13, P307
   Harris Z. S., 1954, WORD
   Hatzivassiloglou V, 1997, 35TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 8TH CONFERENCE OF THE EUROPEAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P174
   Hermann K. M., 2013, ACL, P894
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hovy E., 2004, P 20 INT C COMP LING, P1367, DOI DOI 10.3115/1220355.1220555
   Hu M., 2004, P 10 ACM SIGKDD INT, V04, P168, DOI DOI 10.1145/1014052.1014073
   Huang E. H., 2012, ANN M ASS COMP LING, P873
   Irsoy O, 2014, P 2014 C EMP METH NA, P720, DOI [10.1109/BIBM.2015.7359761, DOI 10.1109/BIBM.2015.7359761]
   Irsoy O., 2014, ADV NEURAL INFORM PR, V27, P2096
   Jiang L, 2011, PROC OF THE 49TH ANN, V1, P151
   Johnson Rie, 2014, ARXIV14121058
   Kalchbrenner N., 2014, ARXIV14042188
   Kim Y, 2014, ARXIV14085882
   Labutov Igor, 2013, ACL, P489
   Le Quoc V, 2014, ARXIV14054053
   Lebret R, 2013, 196986 EPFL ID RES I
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Levy O, 2014, PROCEEDINGS OF THE 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 2, P302
   Li J., 2015, ARXIV150601066
   Li J, 2015, ARXIV150601070
   Li J., 2014, ARXIV14123714
   Li J., 2015, ARXIV150300185
   Li Jiwei, 2015, ARXIV150601057
   Li Jiwei, 2014, P 2014 C EMP METH NA, P2061
   Liu B, 2012, SYNTHESIS LECT HUMAN, P1, DOI [DOI 10.2200/S00416ED1V01Y201204HLT016, 10.2200/S00416ED1V01Y201204HLT016]
   Liu P, 2015, C EMP METH NAT LANG
   Lund K, 1996, BEHAV RES METH INSTR, V28, P203, DOI 10.3758/BF03204766
   Ma J, 2014, PROCEEDINGS OF THE 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P144
   Maas A.L., 2011, P 49 ANN M ASS COMP, P142
   Manning C. D., 1999, FDN STAT NATURAL LAN
   Martin JH, 2000, SPEECH LANGUAGE PROC
   Mikolov T, 2013, ADV NEURAL INFORM PR, V27, P3111, DOI DOI 10.1162/JMLR.2003.3.4-5.951
   Mikolov T, 2013, ARXIV13013981
   Mitchell J, 2010, COGNITIVE SCI, V34, P1388, DOI 10.1111/j.1551-6709.2010.01106.x
   Mnih A., 2007, P 24 INT C MACH LEAR, P641, DOI DOI 10.1145/1273496.1273577
   Mnih A., 2013, ADV NEURAL INFORM PR, P2265
   Mohammad S. M., 2013, 2 JOINT C LEX COMP S, P321
   Morin F, 2005, P AISTATS, V5, P246
   Nakagawa T, 2010, HUMAN LANGUAGE TECHN, P786
   Paltoglou G, 2010, ACL 2010: 48TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, P1386
   Pang B, 2002, PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING, P79
   Paulus Romain, 2014, ADV NEURAL INFORM PR, P2888
   Pennington J, 2014, P 2014 C EMP METH NA, V2014, P1532, DOI DOI 10.3115/V1/D14-1162
   Qiu GA, 2011, COMPUT LINGUIST, V37, P9, DOI 10.1162/coli_a_00034
   Qiu S., 2014, P COLING 2014 25 INT, P141
   Ravichandran D., 2009, P 12 C EUR CHAPT ASS, P675, DOI DOI 10.3115/1609067.1609142
   Reckman H, 2013, 2 JOINT C LEX COMP S, P513
   Severyn A., 2015, P C N AM CHAPT ASS C
   Severyn A, 2015, SIGIR 2015: PROCEEDINGS OF THE 38TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P959, DOI 10.1145/2766462.2767830
   Smith N. A., 2011, SYNTHESIS LECT HUMAN, V4, P1
   Smith Noah A, 2005, P 43 ANN M ASS COMP, P354
   Socher R., 2012, P 2012 JOINT C EMP M, P1201
   Socher R., 2013, P C EMP METH NAT LAN, V2013, P1631, DOI DOI 10.1371/JOURNAL.PONE.0073791
   Socher R., 2011, P C EMP METH NAT LAN, P151
   Su Fangzhong, 2009, P HUM LANG TECHN 200, P1
   Taboada M, 2011, COMPUT LINGUIST, V37, P267, DOI 10.1162/COLI_a_00049
   Tai K. S., 2015, ARXIV150300075
   Tang D, 2015, P ACL
   Tang D., 2014, SEMEVAL, V2014, P208
   Tang D., 2014, P 25 INT C COMP LING, P172
   Tang D, 2014, P EMNLP DOH QAT
   Tang D., 2015, P 2015 C EMP METH NA, P422, DOI DOI 10.18653/V1/D15-1167
   Tang D, 2015, P IJCAI BUEN AIR ARG
   Tang DY, 2015, IEEE-ACM T AUDIO SPE, V23, P1750, DOI 10.1109/TASLP.2015.2449071
   Tang DY, 2014, PROCEEDINGS OF THE 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P1555
   Thelwall M, 2012, J AM SOC INF SCI TEC, V63, P163, DOI 10.1002/asi.21662
   Turney PD, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P417
   Velikovich L, 2010, HUMAN LANGUAGE TECHN, P777, DOI DOI 10.1056/NEJM199902113400601
   Vo DT, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P1347
   Wang S., 2012, P 50 ANN M ASS COMP, V2, P90
   Wiebe J, 2006, COLING/ACL 2006, VOLS 1 AND 2, PROCEEDINGS OF THE CONFERENCE, P1065
   Wilson T., 2005, P C HUM LANG TECHN E, P347, DOI DOI 10.3115/1220575.1220619
   Wu L, 2015, INT C INTEL HUM MACH, DOI 10.1109/IHMSC.2015.257
   Xu L., 2013, ACL, P1764
   Yang B, 2013, P 51 ANN M ASS COMP, P1640
   Yang Bishan, 2012, P 2012 JOINT C EMP M, P1335
   Yogatama D, 2014, ARXIV14062035
   Zhang Meishan, 2015, C EMP METH NAT LANG
   Zhao J., 2012, P 18 ACM SIGKDD INT, P1528, DOI DOI 10.1145/2339530.2339772
   Zhu X., 2015, ARXIV150304881
NR 108
TC 23
Z9 27
U1 6
U2 111
PU WILEY PERIODICALS, INC
PI SAN FRANCISCO
PA ONE MONTGOMERY ST, SUITE 1200, SAN FRANCISCO, CA 94104 USA
SN 1942-4787
EI 1942-4795
J9 WIRES DATA MIN KNOWL
JI Wiley Interdiscip. Rev.-Data Mining Knowl. Discov.
PD NOV-DEC
PY 2015
VL 5
IS 6
BP 292
EP 303
DI 10.1002/widm.1171
PG 12
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods
SC Computer Science
GA CU6XX
UT WOS:000363679700003
DA 2020-02-19
ER

PT J
AU Zou, Q
   Ni, LH
   Zhang, T
   Wang, Q
AF Zou, Qin
   Ni, Lihao
   Zhang, Tong
   Wang, Qian
TI Deep Learning Based Feature Selection for Remote Sensing Scene
   Classification
SO IEEE GEOSCIENCE AND REMOTE SENSING LETTERS
LA English
DT Article
DE Deep belief network (DBN); feature learning; iterative deep learning;
   scene recognition; scene understanding
ID IMAGE RETRIEVAL
AB With the popular use of high-resolution satellite images, more and more research efforts have been placed on remote sensing scene classification/recognition. In scene classification, effective feature selection can significantly boost the final performance. In this letter, a novel deep-learning-based feature-selection method is proposed, which formulates the featureselection problem as a feature reconstruction problem. Note that the popular deep-learning technique, i.e., the deep belief network (DBN), achieves feature abstraction by minimizing the reconstruction error over the whole feature set, and features with smaller reconstruction errors would hold more feature intrinsics for image representation. Therefore, the proposed method selects features that are more reconstructible as the discriminative features. Specifically, an iterative algorithm is developed to adapt the DBN to produce the inquired reconstruction weights. In the experiments, 2800 remote sensing scene images of seven categories are collected for performance evaluation. Experimental results demonstrate the effectiveness of the proposed method.
C1 [Zou, Qin; Ni, Lihao; Wang, Qian] Wuhan Univ, Sch Comp Sci, Wuhan 430072, Hubei, Peoples R China.
   [Zhang, Tong] Wuhan Univ, State Key Lab Informat Engn Surveying Mapping & R, Wuhan 430079, Hubei, Peoples R China.
RP Zou, Q (reprint author), Wuhan Univ, Sch Comp Sci, Wuhan 430072, Hubei, Peoples R China.
EM qzou@whu.edu.cn; lhni@whu.edu.cn; zhangt@whu.edu.cn; qianwang@whu.edu.cn
OI Wang, Qian/0000-0002-8967-8525
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61301277, 41371431]; National Basic Research
   Program of ChinaNational Basic Research Program of China [2012CB719906,
   2012CB725303]; 3551 Optics Valley Talents Scheme of Wuhan East Lake
   High-Tech Zone
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61301277 and 41371431, by the National
   Basic Research Program of China under Grants 2012CB719906 and
   2012CB725303, and by the 3551 Optics Valley Talents Scheme of Wuhan East
   Lake High-Tech Zone. (Corresponding author: Tong Zhang.)
CR Aptoula E, 2014, IEEE T GEOSCI REMOTE, V52, P3023, DOI 10.1109/TGRS.2013.2268736
   Bengio Y, 2006, ADV NEURAL INFORM PR, P153
   Camps-Valls G, 2010, IEEE GEOSCI REMOTE S, V7, P587, DOI 10.1109/LGRS.2010.2041896
   Deng L, 2013, FOUND TRENDS SIGNAL, V7, pI, DOI 10.1561/2000000039
   Ferecatu M, 2007, IEEE T GEOSCI REMOTE, V45, P818, DOI 10.1109/TGRS.2007.892007
   Gomez-Chova L, 2008, IEEE GEOSCI REMOTE S, V5, P336, DOI 10.1109/LGRS.2008.916070
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Ibrahim R, 2014, IEEE ENG MED BIO, P3957, DOI 10.1109/EMBC.2014.6944490
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Pal M, 2010, IEEE T GEOSCI REMOTE, V48, P2297, DOI 10.1109/TGRS.2009.2039484
   Piedra-Fernandez JA, 2010, IEEE T GEOSCI REMOTE, V48, P4193, DOI 10.1109/TGRS.2010.2050067
   Ranzato M., 2006, ADV NEURAL INFORM PR, V19, P1137
   Rumelhart D.E., 1986, PARALLEL DISTRIBUTED, V1
   Wei W, 2012, J APPL REMOTE SENS, V6, DOI 10.1117/1.JRS.6.061504
   Yang H, 2011, IEEE J-STARS, V4, P660, DOI 10.1109/JSTARS.2011.2120598
   Zou Q, 2014, PATTERN RECOGN LETT, V49, P146, DOI 10.1016/j.patrec.2014.07.002
NR 17
TC 157
Z9 171
U1 21
U2 180
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1545-598X
EI 1558-0571
J9 IEEE GEOSCI REMOTE S
JI IEEE Geosci. Remote Sens. Lett.
PD NOV
PY 2015
VL 12
IS 11
BP 2321
EP 2325
DI 10.1109/LGRS.2015.2475299
PG 5
WC Geochemistry & Geophysics; Engineering, Electrical & Electronic; Remote
   Sensing; Imaging Science & Photographic Technology
SC Geochemistry & Geophysics; Engineering; Remote Sensing; Imaging Science
   & Photographic Technology
GA CV4DG
UT WOS:000364215500029
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Chen, JS
   Kang, XG
   Liu, Y
   Wang, ZJ
AF Chen, Jiansheng
   Kang, Xiangui
   Liu, Ye
   Wang, Z. Jane
TI Median Filtering Forensics Based on Convolutional Neural Networks
SO IEEE SIGNAL PROCESSING LETTERS
LA English
DT Article
DE Convolutional neural networks; deep learning; hierarchical
   representations; median filtering forensics
ID TRACES
AB Median filtering detection has recently drawn much attention in image editing and image anti-forensic techniques. Current image median filtering forensics algorithms mainly extract features manually. To deal with the challenge of detecting median filtering from small-size and compressed image blocks, by taking into account of the properties of median filtering, we propose a median filtering detection method based on convolutional neural networks (CNNs), which can automatically learn and obtain features directly from the image. To our best knowledge, this is the first work of applying CNNs in median filtering image forensics. Unlike conventional CNN models, the first layer of our CNN framework is a filter layer that accepts an image as the input and outputs its median filtering residual (MFR). Then, via alternating convolutional layers and pooling layers to learn hierarchical representations, we obtain multiple features for further classification. We test the proposed method on several experiments. The results show that the proposed method achieves significant performance improvements, especially in the cut-and-paste forgery detection.
C1 [Chen, Jiansheng; Kang, Xiangui; Liu, Ye] Sun Yat Sen Univ, Sch Informat Sci & Technol, Guangzhou 510006, Guangdong, Peoples R China.
   [Wang, Z. Jane] Univ British Columbia, Elect & Comp Engn Dept, Vancouver, BC V6T 1Z4, Canada.
RP Chen, JS (reprint author), Sun Yat Sen Univ, Sch Informat Sci & Technol, Guangzhou 510006, Guangdong, Peoples R China.
EM isskxg@mail.sysu.edu.cn; zjanew@ece.ubc.ca
FU National Science Foundation of ChinaNational Natural Science Foundation
   of China [61379155, U1135001, 61332012]; 973 ProgramNational Basic
   Research Program of China [2011CB302204]; NSF of Guangdong
   provinceNational Natural Science Foundation of Guangdong Province
   [s2013020012788]
FX This work was supported by the National Science Foundation of China
   under Grants 61379155, U1135001, and 61332012, the 973 Program under
   Grant 2011CB302204, and the NSF of Guangdong province under Grant
   s2013020012788. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Yao Zhao.
CR Bas P., 2011, LNCS, V6958, P59, DOI DOI 10.1007/978-3-642-24178-9_5
   Boureau YL, 2010, PROC CVPR IEEE, P2559, DOI 10.1109/CVPR.2010.5539963
   Browne M, 2003, LECT NOTES ARTIF INT, V2903, P641
   Cao G, 2010, IEEE INT CON MULTI, P89, DOI 10.1109/ICME.2010.5583869
   Chen C., 2012, P INF HID BERK CA US
   Collobert R., 2008, P 25 ICML, V25, P160, DOI [10.1145/1390156.1390177, DOI 10.1145/1390156.1390177]
   Gole T., 2010, P ACM S APPL COMP MA, P22
   Kang XG, 2013, IEEE T INF FOREN SEC, V8, P1456, DOI 10.1109/TIFS.2013.2273394
   Kirchner M., 2010, P SPIE ELECT IMAG ME, V7541, P1
   Kirchner M, 2008, IEEE T INF FOREN SEC, V3, P582, DOI 10.1109/TIFS.2008.2008214
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Larochelle H, 2009, J MACH LEARN RES, V10, P1
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Liu Y, 1996, IEEE C EVOL COMPUTAT, P670, DOI 10.1109/ICEC.1996.542681
   Luo WQ, 2010, IEEE T INF FOREN SEC, V5, P480, DOI 10.1109/TIFS.2010.2051426
   Popescu AC, 2005, IEEE T SIGNAL PROCES, V53, P758, DOI 10.1109/TSP.2004.839932
   Salakhutdinov R, 2009, ARTIF INTELL, P448, DOI DOI 10.1109/CVPRW.2009.5206577
   Schaefer G, 2004, PROC SPIE, V5307, P472, DOI 10.1117/12.525375
   Scherer D., 2010, P INT C ART NEUR NET
   Stamm MC, 2011, IEEE T INF FOREN SEC, V6, P1050, DOI 10.1109/TIFS.2011.2119314
   Stamm MC, 2010, INT CONF ACOUST SPEE, P1698, DOI 10.1109/ICASSP.2010.5495488
   *USDA, NAT RES CONS SERV PH
   Yuan HD, 2011, IEEE T INF FOREN SEC, V6, P1335, DOI 10.1109/TIFS.2011.2161761
NR 23
TC 104
Z9 109
U1 8
U2 73
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1070-9908
EI 1558-2361
J9 IEEE SIGNAL PROC LET
JI IEEE Signal Process. Lett.
PD NOV
PY 2015
VL 22
IS 11
BP 1849
EP 1853
DI 10.1109/LSP.2015.2438008
PG 5
WC Engineering, Electrical & Electronic
SC Engineering
GA CK1VC
UT WOS:000355994700005
DA 2020-02-19
ER

PT J
AU Hutter, F
   Lucke, J
   Schmidt-Thieme, L
AF Hutter, Frank
   Luecke, Joerg
   Schmidt-Thieme, Lars
TI Beyond Manual Tuning of Hyperparameters
SO KUNSTLICHE INTELLIGENZ
LA English
DT Article
DE Hyperparameter optimization; Automatic machine learning; Autonomous
   learning; Deep learning
ID NEURAL-NETWORKS; OPTIMIZATION; SEARCH
AB The success of hand-crafted machine learning systems in many applications raises the question of making machine learning algorithms more autonomous, i.e., to reduce the requirement of expert input to a minimum. We discuss two strategies towards this goal: (1) automated optimization of hyperparameters (including mechanisms for feature selection, preprocessing, model selection, etc) and (2) the development of algorithms with reduced sets of hyperparameters. Since many research directions (e.g., deep learning), show a tendency towards increasingly complex algorithms with more and more hyperparamters, the demand for both of these strategies continuously increases. We review recent hyperparameter optimization methods and discuss data-driven approaches to avoid the introduction of hyperparameters using unsupervised learning. We end in discussing how these complementary strategies can work hand-in-hand, representing a very promising approach towards autonomous machine learning.
C1 [Hutter, Frank] Univ Freiburg, Freiburg, Germany.
   [Luecke, Joerg] Carl von Ossietzky Univ Oldenburg, Dept Med Phys & Acoust Cluster Excellence Hearing, Oldenburg, Germany.
   [Schmidt-Thieme, Lars] Univ Hildesheim, Hildesheim, Germany.
RP Hutter, F (reprint author), Univ Freiburg, Freiburg, Germany.
EM fh@cs.uni-freiburg.de; joerg.luecke@uni-oldenburg.de;
   schmidt-thieme@ismll.uni-hildesheim.de
FU German Research Foundation's Priority Programme "Autonomous Learning''
   [DFG SPP 1527]
FX The authors gratefully acknowledge funding through the German Research
   Foundation's Priority Programme "Autonomous Learning'' (DFG SPP 1527).
CR Adams Ryan Prescott, 2009, ARXIV10010160
   Aha DW, 1992, GENERALIZING CASE ST, P1
   Bardenet R., 2013, P ICML 13
   Bengio Y, 2000, NEURAL COMPUT, V12, P1889, DOI 10.1162/089976600300015187
   Bergstra J, 2013, ARXIV13063476
   Bergstra J., 2013, P ICML 13
   Bergstra J., 2011, P NIPS 11
   Bergstra J, 2012, J MACH LEARN RES, V13, P281
   Berkes P, 2008, P NIPS 08, V21
   Blockeel H, 2006, LECT NOTES COMPUT SC, V3933, P72
   Brazdil P., 1994, Machine Learning: ECML-94. European Conference on Machine Learning. Proceedings, P83
   Brochu E., 2010, ARXIV10122599
   Castiello C, 2005, LECT NOTES ARTIF INT, V3558, P457
   Ciresan D, 2012, PROC CVPR IEEE, P3642, DOI 10.1109/CVPR.2012.6248110
   Dayan P, 1997, FOUND COMPUT MATH, P43
   Domhan Tobias, 2014, ICML 2014 AUTOML WOR
   Eggensperger K., 2013, NIPS WORKSH BAYES OP
   Engels R, 1998, ECAI 1998: 13TH EUROPEAN CONFERENCE ON ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P430
   Fawcett C, 2013, P 10 MIC, P123
   Feurer M., 2015, P AAAI 15
   Giraud-Carrier C, 2000, P 17 INT C MACH LEAR, P743
   Gomes TAF, 2012, NEUROCOMPUTING, V75, P3, DOI 10.1016/j.neucom.2011.07.005
   Goodfellow I, 2012, P ICML 12
   Griffiths TL, 2008, CAMB HANDB PSYCHOL, P59
   Grosse S., 2012, GI LECT NOTES INFORM, P27
   Guerra SB, 2008, LECT NOTES COMPUT SC, V5163, P523
   Guo XC, 2008, NEUROCOMPUTING, V71, P3211, DOI 10.1016/j.neucom.2008.04.027
   Henery R.J., 1994, MACHINE LEARNING NEU
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Ho TK, 2002, IEEE T PATTERN ANAL, V24, P289, DOI 10.1109/34.990132
   Hutter E, 2014, P 31 INT C MACH LEAR, P754
   Hutter F, 2011, P LION 5
   Hutter F, 2013, P LION 7
   Hutter F, 2009, J ARTIF INTELL RES, V36, P267, DOI 10.1613/jair.2861
   Jones DR, 1998, J GLOBAL OPTIM, V13, P455, DOI 10.1023/A:1008306431147
   KING RD, 1995, APPL ARTIF INTELL, V9, P289, DOI 10.1080/08839519508945477
   Kingma D., 2014, ADV NEURAL INFORM PR, V27, P3581
   Kulick J, 2013, P IJCAI 13
   LeCun Y., 2001, INTELLIGENT SIGNAL P, P306
   LeCun Y, 2010, IEEE INT SYMP CIRC S, P253, DOI 10.1109/ISCAS.2010.5537907
   LeCun Yann, 1995, HDB BRAIN THEORY NEU, V3361, P310
   Lee TS, 2003, J OPT SOC AM A, V20, P1434, DOI 10.1364/JOSAA.20.001434
   Lemke C., 2013, ARTIF INTELL REV, V44, P1
   Lucke J, 2008, J MACH LEARN RES, V9, P1227
   Mann G., 2014, JMLR P, P1077
   Maron O, 1994, ADV NEURAL INFORM PR, V6, P59
   Martius G, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0063400
   Mohamed S, 2012, P ICML 12
   Murray I., 2010, ADV NEURAL INF PROCE, V23, P1723
   Pasemann F, 2013, LECT NOTES COMPUT SC, V7902, P481, DOI 10.1007/978-3-642-38679-4_48
   Pavel YPPAF, 2002, ECML PKDD 02 WORKSH, P111
   Pinto F, 2014, ECAI 2014 WORKSH MET, P32
   Reif M., 2012, P ICPRAM 12, P273
   Reif M, 2012, MACH LEARN, V87, P357, DOI 10.1007/s10994-012-5286-7
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Schonlau M., 1998, NEW DEV APPL EXPT DE, P11, DOI DOI 10.1214/LNMS/1215456182
   Sheikh AS, 2014, J MACH LEARN RES, V15, P2653
   SIDENBLADH H, 2000, EUR C COMP VIS, V2, P702
   Smith Michael R., 2014, ARXIV14057292
   Smith MR, 2014, ARXIV14071890
   Smith-Miles KA, 2008, ACM COMPUT SURV, V41, DOI 10.1145/1456650.1456656
   Snoek J., 2012, P NIPS 12
   Srinivas N., 2010, P ICML 10
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Swersky K, 2013, P ICML 13
   Swersky K., 2013, NIPS WORKSH BAYES OP
   Swersky Kevin, 2014, ARXIV14063896
   Thornton C, 2013, P KDD 13
   Vanschoren J, 2012, MACH LEARN, V87, P127, DOI 10.1007/s10994-011-5277-0
   Vilalta R, 2002, ARTIF INTELL REV, V18, P77, DOI 10.1023/A:1019956318069
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Wager S., 2013, ADV NEURAL INFORM PR, V26, P351
   Weng P, 2013, P ECML PKDD WORKSH R
NR 73
TC 16
Z9 16
U1 1
U2 4
PU SPRINGER HEIDELBERG
PI HEIDELBERG
PA TIERGARTENSTRASSE 17, D-69121 HEIDELBERG, GERMANY
SN 0933-1875
EI 1610-1987
J9 KUNSTL INTELL
JI Kunstl. Intell.
PD NOV
PY 2015
VL 29
IS 4
BP 329
EP 337
DI 10.1007/s13218-015-0381-0
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA VA6DN
UT WOS:000410153700003
DA 2020-02-19
ER

PT J
AU Zhou, P
   Gu, XJ
   Zhang, J
   Fei, MR
AF Zhou, Peng
   Gu, Xiaojing
   Zhang, Jie
   Fei, Minrui
TI A priori trust inference with context-aware stereotypical deep learning
SO KNOWLEDGE-BASED SYSTEMS
LA English
DT Article
DE Multi-agent systems; Stereotypical trust model; Deep learning
ID REPUTATION
AB In multi-agent systems, stereotypical trust models are widely used to bootstrap a priori trust in case historical trust evidences are unavailable. These models can work well if and only if malicious agents share some common features (i.e., stereotypes) in their profiles and these features can be detected. However, this condition may not hold for all the adversarial scenarios. Smart attackers can show different trustworthiness to different agents and services (i.e., launching context-correlated attacks). In this paper, we propose CAST, a novel Context-Aware Stereotypical Trust deep learning framework. CAST coins a comprehensive set of seven context-aware stereotypes, each of which can capture a unique type of context-correlated attacks, as well as a deep learning architecture to keep the trust stereotyping robust (i.e., resist training errors). The basic idea is to construct a multi-layer perceptive structure to learn the latent correlations between context-aware stereotypes and the trustworthiness, and thus can estimate the new trust by taking into account the context information. We have evaluated CAST using a rich set of experiments over a simulated multi-agent system. The experimental results have successfully confirmed that, our CAST can achieve approximately tens of times higher trust inference accuracy in average than the competing algorithms in the presence of context-correlated attacks, and more importantly can maintain a much better trust inference robustness against stereotyping errors. (C) 2015 Elsevier B.V. All rights reserved.
C1 [Zhou, Peng; Fei, Minrui] Shanghai Univ, Sch Mechatron Engn & Automat, Shanghai 200041, Peoples R China.
   [Gu, Xiaojing] E China Univ Sci & Technol, Sch Informat Sci & Engn, Shanghai 200237, Peoples R China.
   [Zhang, Jie] Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
   [Fei, Minrui] Shanghai Univ, Shanghai Key Lab Power Stn Automat Technol, Shanghai 200041, Peoples R China.
RP Gu, XJ (reprint author), E China Univ Sci & Technol, Sch Informat Sci & Engn, Shanghai 200237, Peoples R China.
EM pzhou@shu.edu.cn; xjing.gu@ecust.edu.cn; ZhangJ@ntu.edu.sg;
   mrfei@staff.shu.edu.cn
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61502293, 61205017]; Key Project of Science and
   Technology Commission of Shanghai MunicipalityScience & Technology
   Commission of Shanghai Municipality (STCSM) [14JC1402200];
   ASTAR/I2R-SERC, Public Sector Research Funding (PSF) SingaporeAgency for
   Science Technology & Research (ASTAR) [M4070212.020]; Fundamental
   Research Funds for the Central Universities of ChinaFundamental Research
   Funds for the Central Universities
FX The work was partially supported by National Natural Science Foundation
   of China under Grant Nos. 61502293 and 61205017, the Key Project of
   Science and Technology Commission of Shanghai Municipality under Grant
   No. 14JC1402200, the ASTAR/I2R-SERC, Public Sector Research Funding
   (PSF) Singapore (M4070212.020) and the Fundamental Research Funds for
   the Central Universities of China.
CR ARMSTRONG JS, 1992, INT J FORECASTING, V8, P69, DOI 10.1016/0169-2070(92)90008-W
   Ben Saied Y, 2013, COMPUT SECUR, V39, P351, DOI 10.1016/j.cose.2013.09.001
   Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Bin Yu, 2002, Proceedings of the First International Joint Conference on Autonomous Agents and Multiagent Systems, P294
   BURNETT C., 2010, P 9 INT C AUT AG MUL, P241, DOI DOI 10.1016/J.DSS.2005.05.019
   Burnett C., 2011, P 14 INT WORKSH TRUS, P25
   Burnett C, 2013, ACM T INTEL SYST TEC, V4, DOI 10.1145/2438653.2438661
   Collobert R., 2008, P 25 ICML, V25, P160, DOI [10.1145/1390156.1390177, DOI 10.1145/1390156.1390177]
   Dahlhaus R, 1996, STOCH PROC APPL, V62, P139, DOI 10.1016/0304-4149(95)00090-9
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Gilks W., 2005, MARKOV CHAIN MONTE C
   GILKS WR, 1992, J R STAT SOC C-APPL, V41, P337
   Gu XJ, 2015, ENTROPY-SWITZ, V17, P3806, DOI 10.3390/e17063806
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hui Fang, 2012, 2012 IEEE 11th International Conference on Trust, Security and Privacy in Computing and Communications (TrustCom), P698, DOI 10.1109/TrustCom.2012.29
   Huynh TD, 2006, AUTON AGENT MULTI-AG, V13, P119, DOI 10.1007/s10458-005-6825-4
   Jiang RY, 2013, INT CONF QUALITY REL, P813, DOI 10.1109/QR2MSE.2013.6625694
   Josang A, 2007, DECIS SUPPORT SYST, V43, P618, DOI 10.1016/j.dss.2005.05.019
   Kamvar S.D., 2003, P 12 INT C WORLD WID, P640, DOI DOI 10.1145/775152.775242
   Liu XJ, 2009, PROCEEDINGS OF THE 4TH INTERNATIONAL CONFERENCE ON PRODUCT INNOVATION MANAGEMENT, VOLS I AND II, P7, DOI 10.1145/1645953.1645958
   Liu Yan, 2012, P 27 NAT C ART INT A
   Macrae CN, 2001, BRIT J PSYCHOL, V92, P239, DOI 10.1348/000712601162059
   Pinyol I, 2013, ARTIF INTELL REV, V40, P1, DOI 10.1007/s10462-011-9277-z
   Rich E., 1979, COGNITIVE SCI, V3, P329, DOI DOI 10.1207/S15516709COG0304_3
   Rosen K. H., 2011, DISCRETE MATH ITS AP
   Sabater J, 2005, ARTIF INTELL REV, V24, P33, DOI 10.1007/s10462-004-0041-5
   Sensoy Murat, 2014, COMPUT INTELL
   SHORE JE, 1980, IEEE T INFORM THEORY, V26, P26, DOI 10.1109/TIT.1980.1056144
   Teacy W.T.L., 2005, P 4 INT AUT AG MULT
   Teacy WTL, 2006, AUTON AGENT MULTI-AG, V12, P183, DOI 10.1007/s10458-006-5952-x
   Wallis G, 1999, TRENDS COGN SCI, V3, P22, DOI 10.1016/S1364-6613(98)01261-3
   Wang YH, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1551
   Xiong L, 2004, IEEE T KNOWL DATA EN, V16, P843, DOI 10.1109/TKDE.2004.1318566
   Zhou P, 2015, IEEE T INF FOREN SEC, V10, P613, DOI 10.1109/TIFS.2015.2389145
   Zhou P, 2013, COMPUT SECUR, V39, P431, DOI 10.1016/j.cose.2013.09.007
   Zhou P, 2013, COMPUT NETW, V57, P3522, DOI 10.1016/j.comnet.2013.08.005
NR 39
TC 12
Z9 13
U1 0
U2 24
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0950-7051
EI 1872-7409
J9 KNOWL-BASED SYST
JI Knowledge-Based Syst.
PD NOV
PY 2015
VL 88
BP 97
EP 106
DI 10.1016/j.knosys.2015.08.003
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA CT2DG
UT WOS:000362611600009
DA 2020-02-19
ER

PT J
AU Zhu, SH
   Shi, Z
   Sun, CJ
   Shen, SH
AF Zhu, Songhao
   Shi, Zhe
   Sun, Chengjian
   Shen, Shuhan
TI Deep neural network based image annotation
SO PATTERN RECOGNITION LETTERS
LA English
DT Article
DE Deep learning; Multi-label; Multi-modal; Image annotation
AB Multilabel image annotation is one of the most important open problems in computer vision field. Unlike existing works that usually use conventional visual features to annotate images, features based on deep learning have shown potential to achieve outstanding performance. In this work, we propose a multimodal deep learning framework, which aims to optimally integrate multiple deep neural networks pretrained with convolutional neural networks. In particular, the proposed framework explores a unified two stage learning scheme that consists of (i) learning to fine-tune the parameters of deep neural network with respect to each individual modality, and (ii) learning to find the optimal combination of diverse modalities simultaneously in a coherent process. Experiments conducted on a variety of public datasets evaluate the performance of the proposed framework for multilabel image annotation, in which the encouraging results validate the effectiveness of the proposed algorithms. (C) 2015 Elsevier B.V. All rights reserved.
C1 [Zhu, Songhao; Shi, Zhe; Sun, Chengjian] Nanjing Univ Posts & Telecommun, Sch Automat, Nanjing 210046, Jiangsu, Peoples R China.
   [Shen, Shuhan] Chinese Acad Sci, Inst Automat, Beijing 110093, Peoples R China.
RP Zhu, SH (reprint author), Nanjing Univ Posts & Telecommun, Sch Automat, Nanjing 210046, Jiangsu, Peoples R China.
EM zhush@njupt.edu.cn
FU Postdoctoral Foundation of ChinaChina Postdoctoral Science Foundation
   [2014M550297]; Postdoctoral Foundation of Jiangsu Province [1302087B];
   Education Reform Research and Practice Program of Jiangsu Province
   [JGZZ13_041]; Graduate Research and Innovation Program of Jiangsu
   [KYLX15_0854, SJZZ15_0105]
FX This work is supported by Postdoctoral Foundation of China under No.
   2014M550297, Postdoctoral Foundation of Jiangsu Province under No.
   1302087B, Education Reform Research and Practice Program of Jiangsu
   Province under No. JGZZ13_041, and Graduate Research and Innovation
   Program of Jiangsu under No. KYLX15_0854 and SJZZ15_0105.
CR Cesa-Bianchi N., 2006, PREDICTION LEARNING
   Charalampous K, 2014, IMAGE VISION COMPUT, V32, P916, DOI 10.1016/j.imavis.2014.08.003
   Chua T., 2009, P ACM INT C IM VID R, P1, DOI DOI 10.1145/1646396
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Gong Y., 2013, COMPUT RES REPOSIT, P1
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Huang Y, 2013, IEEE IMAGE PROC, P2897, DOI 10.1109/ICIP.2013.6738596
   Kiros R., 2012, ADV NEURAL INFORM PR, P917
   Lazebnik S., 2006, P IEEE INT C COMP VI, V2, P2169, DOI DOI 10.1109/CVPR.2006.68
   Lee H., 2009, P 26 ANN INT C MACH, P609, DOI DOI 10.1145/1553374.1553453
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mohamed AR, 2012, IEEE T AUDIO SPEECH, V20, P14, DOI 10.1109/TASL.2011.2109382
   Ranzato M., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2857, DOI 10.1109/CVPR.2011.5995710
   Rumelhart D. E., 1988, NEUROCOMPUTING FDN R
   Srivastava N., 2012, P INT C MACH LEARN W, P1
   van de Sande KEA, 2010, IEEE T PATTERN ANAL, V32, P1582, DOI 10.1109/TPAMI.2009.154
   Wang JJ, 2010, PROC CVPR IEEE, P3360, DOI 10.1109/CVPR.2010.5540018
   Zeiler MD, 2010, PROC CVPR IEEE, P2528, DOI 10.1109/CVPR.2010.5539957
   Zhang ML, 2007, PATTERN RECOGN, V40, P2038, DOI 10.1016/j.patcog.2006.12.019
   Zhong S., 2011, P 19 ACM INT C MULT, P343
NR 20
TC 10
Z9 10
U1 0
U2 32
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0167-8655
EI 1872-7344
J9 PATTERN RECOGN LETT
JI Pattern Recognit. Lett.
PD NOV 1
PY 2015
VL 65
BP 103
EP 108
DI 10.1016/j.patrec.2015.07.037
PG 6
WC Computer Science, Artificial Intelligence
SC Computer Science
GA CS6KH
UT WOS:000362187000015
DA 2020-02-19
ER

PT J
AU An, H
   Shim, HM
   Na, SI
   Lee, S
AF An, Hongsub
   Shim, Hyeon-min
   Na, Sang-il
   Lee, Sangmin
TI Split and merge algorithm for deep learning and its application for
   additional classes
SO PATTERN RECOGNITION LETTERS
LA English
DT Article
DE Deep neural networks; Genetic algorithm; Deep belief networks; Feature
   extractor
AB In this paper, we propose a novel split training and merge algorithm for deep learning. The proposed algorithm improves recognition accuracy and suggests a new approach for retraining. The algorithm is motivated by the genetic algorithm (GA) and is composed of two procedures. The first procedure initializes two individual networks using deep belief networks (DBNs), and the second procedure merges the two networks using the GA. Biases and weights of the network that is trained using DBNs are represented as a matrix between each layer, and each row of this matrix is used as a chromosome in the merge procedure. To evaluate the performance, we conduct two set of experiments. The first set is to recognize accuracy of the proposed algorithm, and the second set is for a new retraining approach. The results show that the proposed algorithm has a lower average error rate (6.34 +/- 4.57%) than the DBNs, and it can add classes at a lower average error rate (9.06 +/- 6.17% and 10.17 +/- 4.51%) without pre-training using the restrict Boltzmann machines (RBMs) for existing classes data. (C) 2015 Elsevier B.V. All rights reserved.
C1 [An, Hongsub; Lee, Sangmin] Inha Univ, Dept Elect Engn, High Tech 716, Inchon 402751, South Korea.
   [Shim, Hyeon-min; Lee, Sangmin] Inha Univ, Inst Informat & Elect Res, Inchon 402751, South Korea.
   [Na, Sang-il] SK Planet Co Ltd, Songnam 463400, Gyeonggi Do, South Korea.
RP Lee, S (reprint author), Inha Univ, Dept Elect Engn, High Tech 716, Inha Ro 100, Inchon 402751, South Korea.
EM lunatic0831@gmail.com; elecage@gmail.com; sang.il.na79@gmail.com;
   sanglee@inha.ac.kr
OI An, Hongsub/0000-0002-6960-5652; Shim, Hyeon-min/0000-0003-4337-3264
FU MSIP of Korea, under the C-ITRC [IITP-2015-H8601-15-1003]; NRF of Korea
   - Ministry of Education [2010-0020163]
FX This work was supported by the MSIP of Korea, under the C-ITRC
   (IITP-2015-H8601-15-1003) supervised by the IITP and Basic Science
   Research Program through the NRF of Korea funded by the Ministry of
   Education (2010-0020163).
CR Awad M, 2007, IEEE GEOSCI REMOTE S, V4, P571, DOI 10.1109/LGRS.2007.903064
   David O. E., 2014, P 2014 C COMP GEN EV, V14, P1451, DOI [10.1145/2598394.2602287, DOI 10.1145/2598394.2602287]
   De Stefano C, 2014, PATTERN RECOGN LETT, V35, P130, DOI 10.1016/j.patrec.2013.01.026
   Freund Y., 1999, P ADV NEUR INF PROC, V4, P912
   Goldber D. E., 1988, Machine Learning, V3, P95, DOI 10.1023/A:1022602019183
   Hinton G., 2010, MOMENTUM, V9, P926
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Holland JH, 1975, ADAPTATION NATURAL A
   HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554
   LeCun Y., 1998, MNIST DATABASE HANDW
   Montana D. J., 1989, IJCAI-89 Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, P762
   Palm R.B., 2012, PREDICTION CANDIDATE
   PAPE L, 2011, P 2011 INT JOINT C N, P1191, DOI DOI 10.1109/IJCNN.2011.6033359
   Rumelhart D.E., 1988, COGN MODEL
   Smolensky Paul, 1986, INFORM PROCESSING DY
   Song JH, 2014, IEICE T FUND ELECTR, VE97A, P661, DOI 10.1587/transfun.E97.A.661
   Vapnik VN, 1999, IEEE T NEURAL NETWOR, V10, P988, DOI 10.1109/72.788640
   Zhang CX, 2014, PATTERN RECOGN LETT, V36, P161, DOI 10.1016/j.patrec.2013.10.009
NR 20
TC 1
Z9 1
U1 1
U2 33
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0167-8655
EI 1872-7344
J9 PATTERN RECOGN LETT
JI Pattern Recognit. Lett.
PD NOV 1
PY 2015
VL 65
BP 137
EP 144
DI 10.1016/j.patrec.2015.07.024
PG 8
WC Computer Science, Artificial Intelligence
SC Computer Science
GA CS6KH
UT WOS:000362187000020
DA 2020-02-19
ER

PT J
AU Rebai, I
   BenAyed, Y
AF Rebai, Ilyes
   BenAyed, Yassine
TI Text-to-speech synthesis system with Arabic diacritic recognition system
SO COMPUTER SPEECH AND LANGUAGE
LA English
DT Article
DE Text-to-speech synthesis; Statistical parametric; Deep neural networks;
   Natural language processing; Diacritization system
ID SELECTION
AB Text-to-speech synthesis system has been widely studied for many languages. However, speech synthesis for Arabic language has not sufficient progresses and it is still in its first stage. Statistical parametric synthesis based on hidden Markov models was the most commonly applied approach for Arabic language. Recently, synthesized speech quality based on deep neural networks was found as intelligible as human voice. This paper describes a Text-To-Speech (TTS) synthesis system for modern standard Arabic language based on statistical parametric approach and Mel-cepstral coefficients. Deep neural networks achieved state-of-the-art performance in a wide range of tasks, including speech synthesis. Our ITS system includes a diacritization system which is very important for Arabic TTS application. Our diacritization system is also based on deep neural networks. In addition to the use deep techniques, different methods were also proposed to model the acoustic parameters in order to address the problem of acoustic models accuracy. They are based on linguistic and acoustic characteristics (e.g. letter position based diacritization system, unit types based synthesis system, diacritic marks based synthesis system) and based on deep learning techniques (stacked generalization techniques). Experimental results show that our diacritization system can generate a diacritized text with high accuracy. As regards the speech synthesis system, the experimental results and subjective evaluation show that our proposed method for synthesis system can generate intelligible and natural speech. (C) 2015 Elsevier Ltd. All rights reserved.
C1 [Rebai, Ilyes; BenAyed, Yassine] Univ Comp Sci & Multimedia, MIRACL Multimedia InfoRmat Syst & Adv Comp Lab, Sfax, Tunisia.
RP Rebai, I (reprint author), Univ Comp Sci & Multimedia, MIRACL Multimedia InfoRmat Syst & Adv Comp Lab, Tunis St Km 10,Technopole, Sfax, Tunisia.
EM rebai_ilyes@hotmail.fr
OI rebai, ilyes/0000-0003-4504-3146
CR Abdel-Hamid O, 2006, INTERSPEECH 2006 AND 9TH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE PROCESSING, VOLS 1-5, P1332
   Al-Said Ghadeer, 2009, Journal of Computer Sciences, V5, P207, DOI 10.3844/jcs.2009.207.213
   Attia M., 2005, THESIS FACULTY ENG C
   Badrashiny M., 2009, THESIS U CAIRO CAIRO
   Bahaadini S., 2011, IEEE INT WORKSH MACH, P1
   Barra-Chicote R, 2010, SPEECH COMMUN, V52, P394, DOI 10.1016/j.specom.2009.12.007
   Black AW, 2007, INT CONF ACOUST SPEE, P1229
   Chouireb F, 2008, SIGNAL IMAGE VIDEO P, V2, P73, DOI 10.1007/s11760-007-0038-z
   Ciresan D. C., 2010, CORR
   Clark RAJ, 2007, SPEECH COMMUN, V49, P317, DOI 10.1016/j.specom.2007.01.014
   Deng L., 2014, MSRTR201421
   Ekpenyong M, 2014, SPEECH COMMUN, V56, P243, DOI 10.1016/j.specom.2013.02.003
   Elshafei M, 2002, INFORM SCIENCES, V140, P255, DOI 10.1016/S0020-0255(01)00175-X
   Elshafei M., 2006, P INT C MACH LEARN M, P128
   Fares TS, 2008, AIP CONF PROC, V1019, P93, DOI 10.1063/1.2953060
   Hamad Mazin, 2011, Proceedings of the 2011 IEEE Student Conference on Research and Development (SCOReD 2011), P409, DOI 10.1109/SCOReD.2011.6148774
   Hunt AJ, 1996, INT CONF ACOUST SPEE, P373, DOI 10.1109/ICASSP.1996.541110
   Imai S., 1983, ELECTR COMMUN 1, V66, P10, DOI DOI 10.1002/ecja.4400660203
   Khorsheed M.S., 2012, J SOFTWARE ENG APPL, P124, DOI 10.4236/jsea.2012.512b024
   Martin K, 2013, P INT 2013, P2589
   Nose T, 2007, IEICE T INF SYST, VE90D, P1406, DOI 10.1093/ietisy/e90-d.9.1406
   Phan Son Thanh, 2013, IJACST, V2
   Qian Y, 2006, LECT NOTES COMPUT SC, V4274, P223
   Raghavendra E. V., 2010, NAT C COMM, P1
   Tokuda K, 2013, P IEEE, V101, P1234, DOI 10.1109/JPROC.2013.2251852
   Yamagishi J, 2005, IEICE T INF SYST, VE88D, P502, DOI 10.1093/ietisy/e88-d.3.502
   Zeki M., 2010, IEEE INT C COMP COMM, P1
   Zen HG, 2013, INT CONF ACOUST SPEE, P7962, DOI 10.1109/ICASSP.2013.6639215
   Zen H, 2009, SPEECH COMMUN, V51, P1039, DOI 10.1016/j.specom.2009.04.004
NR 29
TC 4
Z9 4
U1 0
U2 26
PU ACADEMIC PRESS LTD- ELSEVIER SCIENCE LTD
PI LONDON
PA 24-28 OVAL RD, LONDON NW1 7DX, ENGLAND
SN 0885-2308
EI 1095-8363
J9 COMPUT SPEECH LANG
JI Comput. Speech Lang.
PD NOV
PY 2015
VL 34
IS 1
SI SI
BP 43
EP 60
DI 10.1016/j.csl.2015.04.002
PG 18
WC Computer Science, Artificial Intelligence
SC Computer Science
GA CL8JX
UT WOS:000357222100003
DA 2020-02-19
ER

PT J
AU Gu, Y
   Chen, YQ
   Liu, JF
   Jiang, XL
AF Gu, Yang
   Chen, Yiqiang
   Liu, Junfa
   Jiang, Xinlong
TI Semi-supervised deep extreme learning machine for Wi-Fi based
   localization
SO NEUROCOMPUTING
LA English
DT Article
DE Wi-Fi indoor localization; Semi-supervised learning; Deep learning;
   Extreme Learning Machine (ELM)
AB Along with the proliferation of mobile devices and wireless signal coverage, indoor localization based on Wi-Fi gets great popularity. Fingerprint based method is the mainstream approach for Wi-Fi indoor localization, for it can achieve high localization performance as long as labeled data are sufficient. However, the number of labeled data is always limited due to the high cost of data acquisition. Nowadays, crowd sourcing becomes an effective approach to gather large number of data; meanwhile, most of them are unlabeled. Therefore, it is worth studying the use of unlabeled data to improve localization performance. To achieve this goal, a novel algorithm Semi-supervised Deep Extreme Learning Machine (SDELM) is proposed, which takes the advantages of semi-supervised learning, Deep Leaning (DL), and Extreme Learning Machine (ELM), so that the localization performance can be improved both in the feature extraction procedure and in the classifier. The experimental results in real indoor environments show that the proposed SDELM not only outperforms other compared methods but also reduces the calibration effort with the help of unlabeled data. (C) 2015 Elsevier B.V. All rights reserved.
C1 [Gu, Yang; Chen, Yiqiang; Liu, Junfa; Jiang, Xinlong] Chinese Acad Sci, Inst Comp Technol, Beijing, Peoples R China.
   [Gu, Yang; Chen, Yiqiang; Liu, Junfa; Jiang, Xinlong] Beijing Key Lab Mobile Comp & Pervas Device, Beijing, Peoples R China.
   [Gu, Yang; Jiang, Xinlong] Univ Chinese Acad Sci, Beijing, Peoples R China.
RP Chen, YQ (reprint author), Chinese Acad Sci, Inst Comp Technol, Beijing, Peoples R China.
FU Natural Science Foundation of ChinaNational Natural Science Foundation
   of China [61173066, 61070110]; Beijing Natural Science FoundationBeijing
   Natural Science Foundation [4144085]
FX This work is supported by Natural Science Foundation of China
   (No.61173066, No.61070110), and Beijing Natural Science Foundation
   (No.4144085).
CR Bahl P., 2000, Proceedings IEEE INFOCOM 2000. Conference on Computer Communications. Nineteenth Annual Joint Conference of the IEEE Computer and Communications Societies (Cat. No.00CH37064), P775, DOI 10.1109/INFCOM.2000.832252
   Bahl P., 2000, MSRTR200012
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chapelle O., 2006, SEMISUPERVISED LEARN, P1
   Chatzimilioudis G, 2012, IEEE INTERNET COMPUT, V16, P36, DOI 10.1109/MIC.2012.70
   Chen YQ, 2006, IEEE T KNOWL DATA EN, V18, P877, DOI 10.1109/TKDE.2006.112
   de Moraes L.F.M., 2006, P 4 ACM INT WORKSH M, P92, DOI DOI 10.1145/1164783.1164799.
   Del Mundo L. B., 2011, 2011 International Conference on ICT Convergence, P20, DOI 10.1109/ICTC.2011.6082543
   Fang SH, 2012, IEEE T MOBILE COMPUT, V11, P100, DOI 10.1109/TMC.2011.30
   Fet N, 2013, UBICOMP'13: PROCEEDINGS OF THE 2013 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING, P499, DOI 10.1145/2493432.2493459
   Haeberlen A., 2004, P 10 ANN INT C MOB C, P70, DOI DOI 10.1145/1023720.1023728
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Huang GB, 2004, I C CONT AUTOMAT ROB, P1029
   Huang GB, 2004, IEEE IJCNN, P985
   Huang GB, 2006, NEUROCOMPUTING, V70, P489, DOI 10.1016/j.neucom.2005.12.126
   Huang Guangbin, 2005, INT J INF TECHNOL, V11, P16
   Kaemarungsi K, 2004, PROCEEDINGS OF MOBIQUITOUS 2004, P14
   Kaemarungsi K., 2006, 1 INT S WIR PERV COM
   Kasun LLC, 2013, IEEE INTELL SYST, V28, P31
   Kjaergaard MB, 2008, INT CONF PERVAS COMP, P110, DOI 10.1109/PERCOM.2008.75
   Laoudias C., 2013, INT C IND POS IND NA, P25
   Ledlie J., 2011, P INT C IND POS IND, P1, DOI 10.1109/ipin.2011.6071942 2-s2.0-
   Lee H., 2007, ADV NEURAL INF PROCE, P801
   Lin Quan, 2011, Acta Automatica Sinica, V37, P316, DOI 10.3724/SP.J.1004.2011.00316
   Liu JF, 2011, NEUROCOMPUTING, V74, P2566, DOI 10.1016/j.neucom.2010.12.043
   Morgan N, 2012, IEEE T AUDIO SPEECH, V20, P7, DOI 10.1109/TASL.2011.2116010
   Pan JJ, 2012, IEEE T PATTERN ANAL, V34, P587, DOI 10.1109/TPAMI.2011.165
   Pulkkinen T, 2011, LECT NOTES COMPUT SC, V6791, P355, DOI 10.1007/978-3-642-21735-7_44
   Quoc V., 2012, P 29 INT MACH LEARN
   Yang CH, 2005, INT C COMP SUPP COOP, P118
   Yang Q, 2008, IEEE INTELL SYST, V23, P8, DOI 10.1109/MIS.2008.4
   Youssef MA, 2003, PROCEEDINGS OF THE FIRST IEEE INTERNATIONAL CONFERENCE ON PERVASIVE COMPUTING AND COMMUNICATIONS (PERCOM 2003), P143, DOI 10.1109/PERCOM.2003.1192736
   Yu K, 2011, PROC CVPR IEEE, P1713, DOI 10.1109/CVPR.2011.5995732
   Zeinalipour-Yazti D, 2013, IEEE T KNOWL DATA EN, V25, P1240, DOI 10.1109/TKDE.2012.55
   Zhang Yong, 2010, Computer Engineering, V36, P277
   ZHENG VW, 2008, [No title captured], P1421
   Zhu Jian, 2007, Journal of Northeastern University (Natural Science), V28, P1094
NR 39
TC 23
Z9 25
U1 2
U2 82
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0925-2312
EI 1872-8286
J9 NEUROCOMPUTING
JI Neurocomputing
PD OCT 20
PY 2015
VL 166
BP 282
EP 293
DI 10.1016/j.neucom.2015.04.011
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA CM5TI
UT WOS:000357751200028
DA 2020-02-19
ER

PT J
AU Zhang, L
   Shi, ZW
   Wu, J
AF Zhang, Lu
   Shi, Zhenwei
   Wu, Jun
TI A Hierarchical Oil Tank Detector With Deep Surrounding Features for
   High-Resolution Optical Satellite Imagery
SO IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE
   SENSING
LA English
DT Article
DE Convolutional neural network (CNN); deep learning; ellipse and line
   segment detector (ELSD); oil tank detection; surrounding information
ID CLASSIFICATION; EXTRACTION; TRANSFORMS
AB Automatic oil tank detection plays a very important role for remote sensing image processing. To accomplish the task, a hierarchical oil tank detector with deep surrounding features is proposed in this paper. The surrounding features extracted by the deep learning model aim at making the oil tanks more easily to recognize, since the appearance of oil tanks is a circle and this information is not enough to separate targets from the complex background. The proposed method is divided into three modules: 1) candidate selection; 2) feature extraction; and 3) classification. First, a modified ellipse and line segment detector (ELSD) based on gradient orientation is used to select candidates in the image. Afterward, the feature combing local and surrounding information together is extracted to represent the target. Histogram of oriented gradients (HOG) which can reliably capture the shape information is extracted to characterize the local patch. For the surrounding area, the convolutional neural network (CNN) trained in ImageNet Large Scale Visual Recognition Challenge 2012 (ILSVRC2012) contest is applied as a blackbox feature extractor to extract rich surrounding feature. Then, the linear support vector machine (SVM) is utilized as the classifier to give the final output. Experimental results indicate that the proposed method is robust under different complex backgrounds and has high detection rate with low false alarm.
C1 [Zhang, Lu; Shi, Zhenwei] Beihang Univ, Sch Astronaut, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
   [Zhang, Lu; Shi, Zhenwei] Beihang Univ, Beijing Key Lab Digital Media, Beijing 100191, Peoples R China.
   [Zhang, Lu; Shi, Zhenwei] Beihang Univ, Sch Astronaut, Image Proc Ctr, Beijing 100191, Peoples R China.
   [Wu, Jun] Space Star Technol Co Ltd, Dept Remote Sensing & Mapping, Beijing 100086, Peoples R China.
   [Wu, Jun] State Key Lab Space Ground Integrated Informat Te, Beijing 100086, Peoples R China.
RP Shi, ZW (reprint author), Beihang Univ, Sch Astronaut, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
EM lu_zhang0928@163.com; shizhenwei@buaa.edu.cn; wujunok@foxmail.com
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61273245, 91120301]; Beijing Natural Science
   FoundationBeijing Natural Science Foundation [4152031]; State Key
   Laboratory of Virtual Reality Technology and Systems, Beihang University
   [VR-2014-ZZ-02]; Fundamental Research Funds for the Central
   UniversitiesFundamental Research Funds for the Central Universities
   [YWF-14-YHXY-028, YWF-15-YHXY-003]; Open Research Fund of the State Key
   Laboratory of Space-Ground Integrated Information Technology
   [2014_CXJJ-YG_08]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61273245 and Grant 91120301, in part by
   the Beijing Natural Science Foundation under Grant 4152031, in part by
   the funding project of State Key Laboratory of Virtual Reality
   Technology and Systems, Beihang University under Grant VR-2014-ZZ-02, in
   part by the Fundamental Research Funds for the Central Universities
   under Grant YWF-14-YHXY-028 and Grant YWF-15-YHXY-003, and the in part
   by the Open Research Fund of the State Key Laboratory of Space-Ground
   Integrated Information Technology under Grant 2014_CXJJ-YG_08.
   (Corresponding author: Zhenwei Shi.)
CR An ZY, 2014, OPTIK, V125, P2768, DOI 10.1016/j.ijleo.2013.12.003
   Cai XY, 2014, 2014 IEEE WORKSHOP ON ELECTRONICS, COMPUTER AND APPLICATIONS, P408, DOI 10.1109/IWECA.2014.6845643
   [陈爱军 CHEN Aijun], 2006, [光电工程, Opto-Electronic Engineering], V33, P96
   Chen XY, 2013, 2013 SECOND IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION (ACPR 2013), P54, DOI 10.1109/ACPR.2013.5
   Corbane C, 2008, SENSORS-BASEL, V8, P2959, DOI 10.3390/s8052959
   Dalal N, 2005, PROC CVPR IEEE, P886
   DAUGMAN JG, 1988, IEEE T ACOUST SPEECH, V36, P1169, DOI 10.1109/29.1644
   Desolneux A., 2007, GESTALT THEORY IMAGE, V34
   Donahue J., 2014, P INT C MACH LEARN, P647
   DUDA RO, 1972, COMMUN ACM, V15, P11, DOI 10.1145/361237.361242
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Han X., 2012, OPTICAL ENG, V51, P1
   [韩现伟 Han Xianwei], 2011, [电子与信息学报, Journal of Electronics & Information Technology], V33, P66, DOI 10.3724/SP.J.1146.2010.00112
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hollitt C, 2013, MACH VISION APPL, V24, P683, DOI 10.1007/s00138-012-0420-x
   Huang X, 2007, IEEE GEOSCI REMOTE S, V4, P260, DOI 10.1109/LGRS.2006.890540
   Huang X, 2014, ISPRS J PHOTOGRAMM, V90, P36, DOI 10.1016/j.isprsjprs.2014.01.008
   Huang X, 2013, IEEE T GEOSCI REMOTE, V51, P257, DOI 10.1109/TGRS.2012.2202912
   Jin XY, 2004, INT GEOSCI REMOTE SE, P1095
   KIRYATI N, 1991, PATTERN RECOGN, V24, P303, DOI 10.1016/0031-3203(91)90073-E
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kushwaha NK, 2013, DEFENCE SCI J, V63, P298, DOI 10.14429/dsj.63.2737
   Leitloff J, 2010, IEEE T GEOSCI REMOTE, V48, P2795, DOI 10.1109/TGRS.2010.2043109
   [李斌 LI Bin], 2008, [光电工程, Opto-Electronic Engineering], V35, P30
   Li ZC, 2011, IEEE T IMAGE PROCESS, V20, P2017, DOI 10.1109/TIP.2010.2099128
   Moranduzzo T, 2014, IEEE T GEOSCI REMOTE, V52, P6356, DOI 10.1109/TGRS.2013.2296351
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Ok AO, 2015, INT ARCH PHOTOGRAMM, V2-3, P149, DOI 10.5194/isprsannals-II-3-W4-149-2015
   Ok AO, 2015, IEEE GEOSCI REMOTE S, V12, P1347, DOI 10.1109/LGRS.2015.2401600
   Ok AO, 2014, IEEE T GEOSCI REMOTE, V52, P3125, DOI 10.1109/TGRS.2013.2270372
   Olson CF, 1999, COMPUT VIS IMAGE UND, V73, P329, DOI 10.1006/cviu.1998.0728
   Patraucean V, 2012, LECT NOTES COMPUT SC, V7573, P572, DOI 10.1007/978-3-642-33709-3_41
   Powers D.M.W., 2007, SIE07001
   Shi ZW, 2014, IEEE T GEOSCI REMOTE, V52, P4511, DOI 10.1109/TGRS.2013.2282355
   Tang Y., 2013, P ICML 2013 WORKSH R
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vapnik V, 1997, ADV NEUR IN, V9, P281
   von Gioi RG, 2010, IEEE T PATTERN ANAL, V32, P722, DOI 10.1109/TPAMI.2008.300
   Wei Wu, 2011, Proceedings of the 2011 IEEE International Conference on Spatial Data Mining and Geographical Knowledge Services (ICSDM 2011), P567, DOI 10.1109/ICSDM.2011.5969110
   Yao Y., 2014, P SPIE COS PHOTON AS
   Zhu CX, 2012, INT GEOSCI REMOTE SE, P6016, DOI 10.1109/IGARSS.2012.6352236
NR 43
TC 43
Z9 43
U1 8
U2 44
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1939-1404
EI 2151-1535
J9 IEEE J-STARS
JI IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens.
PD OCT
PY 2015
VL 8
IS 10
SI SI
BP 4895
EP 4909
DI 10.1109/JSTARS.2015.2467377
PG 15
WC Engineering, Electrical & Electronic; Geography, Physical; Remote
   Sensing; Imaging Science & Photographic Technology
SC Engineering; Physical Geography; Remote Sensing; Imaging Science &
   Photographic Technology
GA DC0JV
UT WOS:000368904000028
DA 2020-02-19
ER

PT J
AU Zhang, CY
   Chen, CLP
   Gan, M
   Chen, L
AF Zhang, Chun-Yang
   Chen, C. L. Philip
   Gan, Min
   Chen, Long
TI Predictive Deep Boltzmann Machine for Multiperiod Wind Speed Forecasting
SO IEEE TRANSACTIONS ON SUSTAINABLE ENERGY
LA English
DT Article
DE Deep Boltzmann machine (DBM); deep learning; time series; wind speed
   prediction
ID POWER-GENERATION; MODEL
AB It is important to forecast the wind speed for managing operations in wind power plants. However, wind speed prediction is extremely complex and difficult due to the volatility and deviation of the wind. As existing forecasting methods directly model the raw wind speed data, it is difficult for them to provide higher inference accuracy. Differently, this paper presents a sophisticated deep-learning technique for short-term and long-term wind speed forecast, i.e., the predictive deep Boltzmann machine (PDBM) and corresponding learning algorithm. The proposed deep model forecasts wind speed by analyzing the higher level features abstracted from lower level features of the wind speed data. These automatically learnt features are very informative and appropriate for the prediction. The proposed PDBM is a deep stochastic model that can represent the wind speed very well, and is inspired by two aspects. 1) The stochastic model is suitable to capture the probabilistic characteristics of wind speed. 2) Recent developments in neural networks with deep architectures show that deep generative models have competitive capability to approximate nonlinear and nonsmooth functions. The evaluation of the proposed PDBM model is depicted by both hour-ahead and day-ahead prediction experiments based on real wind speed datasets. The prediction accuracy of the PDBM model outperforms existing methods by more than 10%.
C1 [Zhang, Chun-Yang; Chen, C. L. Philip; Chen, Long] Univ Macau, Fac Sci & Technol, Dept Comp & Informat Sci, Macau 999078, Peoples R China.
   [Gan, Min] Hefei Univ Technol, Sch Elect Engn & Automat, Hefei 230009, Peoples R China.
RP Zhang, CY (reprint author), Univ Macau, Fac Sci & Technol, Dept Comp & Informat Sci, Macau 999078, Peoples R China.
EM cyzhangfst@gmail.com
RI Chen, C. L. Philip/O-2657-2016
OI Chen, C. L. Philip/0000-0001-5451-7230; Chen, Long/0000-0003-0184-5446;
   Gan, Min/0000-0002-2756-0054
FU Macau Science and Technology development fund [008/2010/A1]; UM
   Multiyear Research Grants; National Natural Science Foundation of
   ChinaNational Natural Science Foundation of China [61203106]
FX This work was supported in part by Macau Science and Technology
   development fund under Grant 008/2010/A1, in part by UM Multiyear
   Research Grants, and in part by the National Natural Science Foundation
   of China under Grant 61203106. Paper no. TSTE-00518-2014.
CR Ahlstrom, 2005, IEEE Power & Energy Magazine, V3, P57, DOI 10.1109/MPAE.2005.1524621
   Al-Hmouz A, 2012, IEEE T LEARN TECHNOL, V5, P226, DOI 10.1109/TLT.2011.36
   Azad HB, 2014, IEEE T SUSTAIN ENERG, V5, P546, DOI 10.1109/TSTE.2014.2300150
   Bengio Y., 2007, NIPS
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Bhaskar K, 2012, IEEE T SUSTAIN ENERG, V3, P306, DOI 10.1109/TSTE.2011.2182215
   Bishop CM, 2006, PATTERN RECOGNITION
   Bludszuweit H, 2008, IEEE T POWER SYST, V23, P983, DOI 10.1109/TPWRS.2008.922526
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chen CLP, 2015, IEEE T FUZZY SYST, V23, P2163, DOI 10.1109/TFUZZ.2015.2406889
   Chen CLP, 1999, IEEE T SYST MAN CY B, V29, P62, DOI 10.1109/3477.740166
   Chen NY, 2014, IEEE T POWER SYST, V29, P656, DOI 10.1109/TPWRS.2013.2282366
   D'Amico G, 2014, PHYSICA A, V406, P59, DOI 10.1016/j.physa.2014.03.034
   Damousis IG, 2004, IEEE T ENERGY CONVER, V19, P352, DOI 10.1109/TEC.2003.821865
   El-Fouly THM, 2008, IEEE T ENERGY CONVER, V23, P191, DOI 10.1109/TEC.2007.905069
   Ernst B, 2007, IEEE POWER ENERGY M, V5, P78, DOI 10.1109/MPE.2007.906306
   Gan M, 2010, INFORM SCIENCES, V180, P4370, DOI 10.1016/j.ins.2010.07.012
   Haykin S., 2008, NEURAL NETWORKS LEAR
   Hinton G., 2012, LNCS, V7700, P599, DOI DOI 10.1007/978-3-642-35289-8_32
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hu QH, 2014, IEEE T SUSTAIN ENERG, V5, P866, DOI 10.1109/TSTE.2013.2295402
   Jung J, 2014, RENEW SUST ENERG REV, V31, P762, DOI 10.1016/j.rser.2013.12.054
   Karki R, 2012, IEEE T SUSTAIN ENERG, V3, P498, DOI 10.1109/TSTE.2012.2190999
   Kusiak A, 2009, IEEE T ENERGY CONVER, V24, P125, DOI 10.1109/TEC.2008.2006552
   Lin Q, 2014, IEEE T SUSTAIN ENERG, V5, P804, DOI 10.1109/TSTE.2014.2304971
   Louka P, 2008, J WIND ENG IND AEROD, V96, P2348, DOI 10.1016/j.jweia.2008.03.013
   Osorio GJ, 2015, RENEW ENERG, V75, P301, DOI 10.1016/j.renene.2014.09.058
   Ren Y, 2015, IEEE T SUSTAIN ENERG, V6, P236, DOI 10.1109/TSTE.2014.2365580
   Salakhutdinov R, 2012, NEURAL COMPUT, V24, P1967, DOI 10.1162/NECO_a_00311
   Sapankevych NL, 2009, IEEE COMPUT INTELL M, V4, P24, DOI 10.1109/MCI.2009.932254
   Shamshad A, 2005, ENERGY, V30, P693, DOI 10.1016/j.energy.2004.05.026
   Shukur OB, 2015, RENEW ENERG, V76, P637, DOI 10.1016/j.renene.2014.11.084
   Sideratos G, 2007, IEEE T POWER SYST, V22, P258, DOI 10.1109/TPWRS.2006.889078
   Ul Haque A, 2014, IEEE T POWER SYST, V29, P1663, DOI 10.1109/TPWRS.2014.2299801
   Wan C, 2014, IEEE T POWER SYST, V29, P1033, DOI 10.1109/TPWRS.2013.2287871
   Wu YK, 2007, 2007 IEEE LAUSANNE POWERTECH, VOLS 1-5, P504, DOI 10.1109/PCT.2007.4538368
   Zhang G, 2012, IEEE T IND INFORM, V8, P819, DOI 10.1109/TII.2012.2205392
NR 40
TC 52
Z9 56
U1 3
U2 85
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1949-3029
J9 IEEE T SUSTAIN ENERG
JI IEEE Trans. Sustain. Energy
PD OCT
PY 2015
VL 6
IS 4
BP 1416
EP 1425
DI 10.1109/TSTE.2015.2434387
PG 10
WC Green & Sustainable Science & Technology; Energy & Fuels; Engineering,
   Electrical & Electronic
SC Science & Technology - Other Topics; Energy & Fuels; Engineering
GA CR9MS
UT WOS:000361680800025
DA 2020-02-19
ER

PT J
AU Hu, CP
   Bai, X
   Qi, L
   Chen, P
   Xue, GJ
   Mei, L
AF Hu, Chuanping
   Bai, Xiang
   Qi, Li
   Chen, Pan
   Xue, Gengjian
   Mei, Lin
TI Vehicle Color Recognition With Spatial Pyramid Deep Learning
SO IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS
LA English
DT Article
DE Color recognition; deep learning; convolutional neural network (CNN);
   spatial pyramid (SP); intelligent transportation
ID SHAPE; ROBUST
AB Color, as a notable and stable attribute of vehicles, can serve as a useful and reliable cue in a variety of applications in intelligent transportation systems. Therefore, vehicle color recognition in natural scenes has become an important research topic in this area. In this paper, we propose a deep-learning-based algorithm for automatic vehicle color recognition. Different from conventional methods, which usually adopt manually designed features, the proposed algorithm is able to adaptively learn representation that is more effective for the task of vehicle color recognition, which leads to higher recognition accuracy and avoids preprocessing. Moreover, we combine the widely used spatial pyramid strategy with the original convolutional neural network architecture, which further boosts the recognition accuracy. To the best of our knowledge, this is the first work that employs deep learning in the context of vehicle color recognition. The experiments demonstrate that the proposed approach achieves superior performance over conventional methods.
C1 [Hu, Chuanping; Qi, Li; Xue, Gengjian; Mei, Lin] Minist Publ Secur, Res Inst 3, Shanghai 200031, Peoples R China.
   [Bai, Xiang; Chen, Pan] Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan 430074, Peoples R China.
RP Qi, L (reprint author), Minist Publ Secur, Res Inst 3, Shanghai 200031, Peoples R China.
EM quick.qi@foxmail.com
RI Qi, LI/AAF-3389-2019
OI Chen, Pan/0000-0002-1296-9770
FU National 863 ProjectNational High Technology Research and Development
   Program of China [2013AA014601]; National Science and Technology Major
   Project [2013ZX01033002-003]; National Natural Science Foundation of
   ChinaNational Natural Science Foundation of China [61222308]
FX This work was supported in part by the National 863 Project under Grant
   2013AA014601, by the National Science and Technology Major Project under
   Grant 2013ZX01033002-003, and by the National Natural Science Foundation
   of China under Grant 61222308.
CR Agrawal P, 2014, LECT NOTES COMPUT SC, V8695, P329, DOI 10.1007/978-3-319-10584-0_22
   Anagnostopoulos CNE, 2006, IEEE T INTELL TRANSP, V7, P377, DOI 10.1109/TITS.2006.880641
   Bai X, 2014, IEEE T IMAGE PROCESS, V23, P3935, DOI 10.1109/TIP.2014.2336542
   BECKER GS, 1974, J LEGAL STUD, V3, P1, DOI 10.1086/467507
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   Boser B. E., 1992, Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, P144, DOI 10.1145/130385.130401
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76
   Chen P, 2014, IEEE T INTELL TRANSP, V15, P2340, DOI 10.1109/TITS.2014.2308897
   Ciresan D, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P1918, DOI 10.1109/IJCNN.2011.6033458
   Dalal N, 2005, PROC CVPR IEEE, P886
   Datta R, 2008, ACM COMPUT SURV, V40, DOI 10.1145/1348246.1348248
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dule E, 2010, ARTIF INT SER WSEAS, P250
   ELAAD E, 1992, J APPL PSYCHOL, V77, P757, DOI 10.1037/0021-9010.77.5.757
   Eslami SMA, 2014, INT J COMPUT VISION, V107, P155, DOI 10.1007/s11263-013-0669-1
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Gevers T, 2007, IMAGE PROCESS SER, P203
   Girshick R., 2013, ARXIV13112524
   Gonzalez A, 2014, IEEE T INTELL TRANSP, V15, P228, DOI 10.1109/TITS.2013.2277662
   Hinton G. E., 1994, ADV NEURAL INFORMATI, P3
   Huang J, 1997, PROC CVPR IEEE, P762, DOI 10.1109/CVPR.1997.609412
   Jarrett K, 2009, IEEE I CONF COMP VIS, P2146, DOI 10.1109/ICCV.2009.5459469
   Jia Y., 2013, CAFFE OPEN SOURCE CO
   Kender J. R., 1976, SATURATION HEU NORMA
   Kim JB, 2003, PATTERN RECOGN LETT, V24, P113, DOI 10.1016/S0167-8655(02)00194-0
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   Lazebnik S., 2006, P IEEE INT C COMP VI, V2, P2169, DOI DOI 10.1109/CVPR.2006.68
   Le Roux N, 2008, NEURAL COMPUT, V20, P1631, DOI 10.1162/neco.2008.04-07-510
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Ma JY, 2014, IEEE T IMAGE PROCESS, V23, P1706, DOI 10.1109/TIP.2014.2307478
   Ma JY, 2013, PATTERN RECOGN, V46, P3519, DOI 10.1016/j.patcog.2013.05.017
   Maji S, 2008, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2008.4587630
   Nair V, 2010, ICML, V27, P807, DOI DOI 10.0RG/PAPERS/432.PDF
   Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222
   Qiu GP, 2002, LECT NOTES COMPUT SC, V2383, P100
   Seo YW, 2015, IEEE T INTELL TRANSP, V16, P708, DOI 10.1109/TITS.2014.2335535
   Sermanet P., 2013, ARXIV13126229
   Sermanet P, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P2809, DOI 10.1109/IJCNN.2011.6033589
   Tian B, 2014, IEEE T INTELL TRANSP, V15, P597, DOI 10.1109/TITS.2013.2283302
   van de Sande KEA, 2010, IEEE T PATTERN ANAL, V32, P1582, DOI 10.1109/TPAMI.2009.154
   van de Sande KEA, 2011, IEEE T MULTIMEDIA, V13, P60, DOI 10.1109/TMM.2010.2091400
   van de Weijer J, 2006, IEEE T PATTERN ANAL, V28, P150, DOI 10.1109/TPAMI.2006.3
   Wang XG, 2014, PATTERN RECOGN, V47, P2116, DOI 10.1016/j.patcog.2013.12.008
   Wang Y, 2011, INT CONF ACOUST SPEE, P1, DOI 10.1109/PLASMA.2011.5993071
   Wen Y, 2011, IEEE T INTELL TRANSP, V12, P830, DOI 10.1109/TITS.2011.2114346
   Yang M., 2011, P INT C HEAT EXCH FO, P1, DOI DOI 10.1109/GEOLNFORMATICS.2011.5980909
   Yao C, 2014, IEEE T IMAGE PROCESS, V23, P4737, DOI 10.1109/TIP.2014.2353813
   Zhang J., 2010, IEEE T INTELL TRANSP, V11, P1624
NR 51
TC 25
Z9 30
U1 4
U2 45
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1524-9050
EI 1558-0016
J9 IEEE T INTELL TRANSP
JI IEEE Trans. Intell. Transp. Syst.
PD OCT
PY 2015
VL 16
IS 5
BP 2925
EP 2934
DI 10.1109/TITS.2015.2430892
PG 10
WC Engineering, Civil; Engineering, Electrical & Electronic; Transportation
   Science & Technology
SC Engineering; Transportation
GA CS4ZI
UT WOS:000362084600051
DA 2020-02-19
ER

PT J
AU Yuan, Y
   Mou, LC
   Lu, XQ
AF Yuan, Yuan
   Mou, Lichao
   Lu, Xiaoqiang
TI Scene Recognition by Manifold Regularized Deep Learning Architecture
SO IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
LA English
DT Article
DE Deep architecture; machine learning; manifold kernel; manifold
   regularization; scene recognition
ID CLASSIFICATION; FEATURES; SPARSE; DIMENSION
AB Scene recognition is an important problem in the field of computer vision, because it helps to narrow the gap between the computer and the human beings on scene understanding. Semantic modeling is a popular technique used to fill the semantic gap in scene recognition. However, most of the semantic modeling approaches learn shallow, one-layer representations for scene recognition, while ignoring the structural information related between images, often resulting in poor performance. Modeled after our own human visual system, as it is intended to inherit humanlike judgment, a manifold regularized deep architecture is proposed for scene recognition. The proposed deep architecture exploits the structural information of the data, making for a mapping between visible layer and hidden layer. By the proposed approach, a deep architecture could be designed to learn the high-level features for scene recognition in an unsupervised fashion. Experiments on standard data sets show that our method outperforms the state-of-the-art used for scene recognition.
C1 [Yuan, Yuan; Mou, Lichao; Lu, Xiaoqiang] Chinese Acad Sci, Xian Inst Opt & Precis Mech, Ctr OpT IMagery Anal & Learning OPTIMAL, State Key Lab Transient Opt & Photon, Xian 710119, Shaanxi, Peoples R China.
RP Yuan, Y (reprint author), Chinese Acad Sci, Xian Inst Opt & Precis Mech, Ctr OpT IMagery Anal & Learning OPTIMAL, State Key Lab Transient Opt & Photon, Xian 710119, Shaanxi, Peoples R China.
EM yuany@opt.ac.cn; moulichao@opt.ac.cn; luxq666666@opt.ac.cn
FU National Basic Research Program (973 Program) of ChinaNational Basic
   Research Program of China [2012CB719905]; National Natural Science
   Foundation of ChinaNational Natural Science Foundation of China
   [61172143, 61100079]
FX This work was supported in part by the National Basic Research Program
   (973 Program) of China under Grant 2012CB719905 and in part by the
   National Natural Science Foundation of China under Grant 61172143 and
   Grant 61100079.
CR Boiman O, 2008, PROC CVPR IEEE, P1992, DOI 10.1109/CVPR.2008.4587598
   Bosch A, 2008, IEEE T PATTERN ANAL, V30, P712, DOI 10.1109/TPAMI.2007.70716
   Cai D., 2009, SPECTRAL REGRESSION
   Chang E, 2003, IEEE T CIRC SYST VID, V13, P26, DOI 10.1109/TCSVT.2002.808079
   Chang H, 2004, PROC CVPR IEEE, P275
   Cheng J, 2012, IEEE T MED IMAGING, V31, P2355, DOI 10.1109/TMI.2012.2218118
   Dixit M, 2011, PROC CVPR IEEE, P937, DOI 10.1109/CVPR.2011.5995674
   Donahue J., 2014, P INT C MACH LEARN, P647
   Eckhorn R, 1999, IEEE T NEURAL NETWOR, V10, P464, DOI 10.1109/72.761705
   Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231
   Fei-Fei L, 2005, PROC CVPR IEEE, P524
   Goh H, 2014, IEEE T NEUR NET LEAR, V25, P2212, DOI 10.1109/TNNLS.2014.2307532
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Huang GB, 2012, PROC CVPR IEEE, P2518, DOI 10.1109/CVPR.2012.6247968
   Joachims T., 1998, Machine Learning: ECML-98. 10th European Conference on Machine Learning. Proceedings, P137
   Kim Y, 2013, INT CONF ACOUST SPEE, P3687, DOI 10.1109/ICASSP.2013.6638346
   Kong D., 2012, P ICML
   Kwitt R, 2012, LECT NOTES COMPUT SC, V7575, P359, DOI 10.1007/978-3-642-33765-9_26
   Lazebnik S., 2006, P IEEE INT C COMP VI, V2, P2169, DOI DOI 10.1109/CVPR.2006.68
   Le Q. V., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3361, DOI 10.1109/CVPR.2011.5995496
   Lee H., 2009, P 26 ANN INT C MACH, P609, DOI DOI 10.1145/1553374.1553453
   Li L., 2010, ADV NEURAL INFORM PR
   Li LJ, 2007, IEEE I CONF COMP VIS, P345
   Liu C, 2009, PROC CVPR IEEE, P1972, DOI 10.1109/CVPRW.2009.5206536
   Liu L., 2013, ACM INT C MULT, P997
   Mittelman R, 2013, PROC CVPR IEEE, P476, DOI 10.1109/CVPR.2013.68
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Parikh D, 2011, PROC CVPR IEEE, P1681, DOI 10.1109/CVPR.2011.5995451 
   Pinheiro P., 2014, P 31 INT C MACH LEAR, V32, P82, DOI DOI 10.1016/J.VETPAR.2009.02.011
   Quelhas P, 2007, IEEE T PATTERN ANAL, V29, P1575, DOI 10.1109/TPAMI.2007.1155
   Shi JB, 1997, PROC CVPR IEEE, P731, DOI 10.1109/CVPR.1997.609407
   Sindhwani V., 2005, P 22 INT C MACH LEAR, P824
   Socher R., 2011, P 28 INT C MACH LEAR, P129, DOI DOI 10.1007/978-3-540-87479-9
   Sohn K, 2011, IEEE I CONF COMP VIS, P2643, DOI 10.1109/ICCV.2011.6126554
   Tao DC, 2007, IEEE T PATTERN ANAL, V29, P1700, DOI 10.1109/TPAMI.2007.1096
   Tao DC, 2009, IEEE T PATTERN ANAL, V31, P260, DOI 10.1109/TPAMI.2008.70
   Taylor GW, 2010, LECT NOTES COMPUT SC, V6316, P140, DOI 10.1007/978-3-642-15567-3_11
   Torresani L, 2010, LECT NOTES COMPUT SC, V6311, P776, DOI 10.1007/978-3-642-15549-9_56
   Vailaya A, 2001, IEEE T IMAGE PROCESS, V10, P117, DOI 10.1109/83.892448
   Vogel J, 2007, INT J COMPUT VISION, V72, P133, DOI 10.1007/s11263-006-8614-1
   Wang DL, 2005, IEEE T NEURAL NETWOR, V16, P1401, DOI 10.1109/TNN.2005.852235
   Wong WK, 2011, IEEE T NEURAL NETWOR, V22, P1668, DOI 10.1109/TNN.2011.2162429
   Wu JX, 2009, IEEE I CONF COMP VIS, P630, DOI 10.1109/ICCV.2009.5459178
   XIAO JX, 2010, PROC CVPR IEEE, P3485, DOI DOI 10.1109/CVPR.2010.5539970
   Xu ZB, 2010, SCI CHINA INFORM SCI, V53, P1159, DOI 10.1007/s11432-010-0090-0
   Yao J, 2012, PROC CVPR IEEE, P702, DOI 10.1109/CVPR.2012.6247739
   Yiming Yang, 1999, Information Retrieval, V1, P69, DOI 10.1023/A:1009982220290
   Yu J, 2013, PATTERN RECOGN, V46, P483, DOI 10.1016/j.patcog.2012.08.006
   Yu XD, 2011, IEEE I CONF COMP VIS, P810, DOI 10.1109/ICCV.2011.6126320
   Zhou G., 2012, JMLR WORKSH C P, P1453
   Zhou TY, 2013, IEEE T IMAGE PROCESS, V22, P244, DOI 10.1109/TIP.2012.2202678
   Zhou TY, 2011, DATA MIN KNOWL DISC, V22, P340, DOI 10.1007/s10618-010-0182-x
NR 53
TC 94
Z9 96
U1 4
U2 60
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 2162-237X
EI 2162-2388
J9 IEEE T NEUR NET LEAR
JI IEEE Trans. Neural Netw. Learn. Syst.
PD OCT
PY 2015
VL 26
IS 10
BP 2222
EP 2233
DI 10.1109/TNNLS.2014.2359471
PG 12
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Computer Science, Theory & Methods; Engineering,
   Electrical & Electronic
SC Computer Science; Engineering
GA CS8SM
UT WOS:000362358800001
PM 25622326
DA 2020-02-19
ER

PT J
AU Gao, SH
   Zhang, YT
   Jia, K
   Lu, JW
   Zhang, YY
AF Gao, Shenghua
   Zhang, Yuting
   Jia, Kui
   Lu, Jiwen
   Zhang, Yingying
TI Single Sample Face Recognition via Learning Deep Supervised Autoencoders
SO IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY
LA English
DT Article
DE Single training sample per person; face recognition; supervised
   auto-encoder; deep architecture
ID ONE TRAINING IMAGE; SPARSE REPRESENTATION; VERIFICATION; FLDA
AB This paper targets learning robust image representation for single training sample per person face recognition. Motivated by the success of deep learning in image representation, we propose a supervised autoencoder, which is a new type of building block for deep architectures. There are two features distinct our supervised autoencoder from standard autoencoder. First, we enforce the faces with variants to be mapped with the canonical face of the person, for example, frontal face with neutral expression and normal illumination; Second, we enforce features corresponding to the same person to be similar. As a result, our supervised autoencoder extracts the features which are robust to variances in illumination, expression, occlusion, and pose, and facilitates the face recognition. We stack such supervised autoencoders to get the deep architecture and use it for extracting features in image representation. Experimental results on the AR, Extended Yale B, CMU-PIE, and Multi-PIE data sets demonstrate that by coupling with the commonly used sparse representation-based classification, our stacked supervised autoencoders-based face representation significantly outperforms the commonly used image representations in single sample per person face recognition, and it achieves higher recognition accuracy compared with other deep learning models, including the deep Lambertian network, in spite of much less training data and without any domain information. Moreover, supervised autoencoder can also be used for face verification, which further demonstrates its effectiveness for face representation.
C1 [Gao, Shenghua; Zhang, Yuting] ShanghaiTech Univ, Shanghai 200444, Peoples R China.
   [Jia, Kui] Univ Macau, Macau 999078, Peoples R China.
   [Lu, Jiwen] Adv Digital Sci Ctr, Singapore 138632, Singapore.
   [Zhang, Yingying] Zhejiang Univ, Hangzhou 310027, Zhejiang, Peoples R China.
RP Gao, SH (reprint author), ShanghaiTech Univ, Shanghai 200444, Peoples R China.
EM gaoshh@shanghaitech.edu.cn; zyt@zju.edu.cn; kuijia@gmail.com;
   jiwen.lu@adsc.com.sg; zhangyy2@shanghaitech.edu.cn
FU Shanghai Pujiang ProgramShanghai Pujiang Program [15PJ1405700]
FX This work was supported by the Shanghai Pujiang Program under Grant
   15PJ1405700. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Patrizio Campisi.
CR Arashloo SR, 2014, IEEE T INF FOREN SEC, V9, P2100, DOI 10.1109/TIFS.2014.2359587
   Arasu S., 2013, P IEEE 6 INT C BIOM, P1, DOI DOI 10.1109/BTAS.2013.6712721
   Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228
   Bengio Yoshua, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P437, DOI 10.1007/978-3-642-35289-8_26
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Chen SC, 2004, PATTERN RECOGN LETT, V25, P1173, DOI 10.1016/j.patrec.2004.03.012
   Chen SC, 2004, PATTERN RECOGN, V37, P1553, DOI 10.1016/j.patcog.2003.12.010
   CHOPRA S, 2005, PROC CVPR IEEE, P539, DOI DOI 10.1109/CVPR.2005.202
   Erhan D, 2010, J MACH LEARN RES, V11, P625
   Fan H, 2014, LEARNING DEEP FACE R
   Gao QX, 2008, APPL MATH COMPUT, V205, P726, DOI 10.1016/j.amc.2008.05.019
   Gao SH, 2013, IEEE T IMAGE PROCESS, V22, P423, DOI 10.1109/TIP.2012.2215620
   Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464
   Glorot X., 2011, P 14 INT C ART INT S, V15, P315
   Glorot X., 2010, JLMR P TRACK, p[249, 2010], DOI DOI 10.1177/1753193409103364.
   Gross R, 2010, IMAGE VISION COMPUT, V28, P807, DOI 10.1016/j.imavis.2009.08.002
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hu JL, 2014, PROC CVPR IEEE, P1875, DOI 10.1109/CVPR.2014.242
   Japkowicz N, 2000, NEURAL COMPUT, V12, P531, DOI 10.1162/089976600300015691
   Kim TK, 2005, IEEE T PATTERN ANAL, V27, P318, DOI 10.1109/TPAMI.2005.58
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Le Q. V., 2011, P ADV NEUR INF PROC
   Le Q. V., 2012, P 29 INT C MACH LEAR, P81, DOI DOI 10.1109/MSP.2011.940881
   Le Q.V., 2011, P 28 INT C MACH LEAR, P265
   Lennie P, 2003, CURR BIOL, V13, P493, DOI 10.1016/S0960-9822(03)00135-0
   Li FF, 2006, IEEE T PATTERN ANAL, V28, P594, DOI 10.1109/TPAMI.2006.79
   Li HX, 2015, LECT NOTES COMPUT SC, V9005, P17, DOI 10.1007/978-3-319-16811-1_2
   Li HX, 2013, PROC CVPR IEEE, P3499, DOI 10.1109/CVPR.2013.449
   Martinez AM, 2002, IEEE T PATTERN ANAL, V24, P748, DOI 10.1109/TPAMI.2002.1008382
   Martinez AM, 1998, 24 CVC
   Nair V, 2010, ICML, V27, P807, DOI DOI 10.0RG/PAPERS/432.PDF
   Ng A., 2011, LECT NOTES CS294A
   Pinto Nicolas, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2591, DOI 10.1109/CVPRW.2009.5206605
   Rifai S, 2011, LECT NOTES ARTIF INT, V6912, P645, DOI 10.1007/978-3-642-23783-6_41
   Salah R, 2011, P 28 INT C MACH LEAR, P833, DOI 10.1016/j.wasman.2010.10.006
   Salakhutdinov R., 2007, J MACHINE LEARNING R, P412
   Shan S., 2002, IEEE INT S CIRC SYST, V2, P81
   Sim T, 2002, FIFTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P53, DOI 10.1109/AFGR.2002.1004130
   Simonyan K, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.8
   Su Y, 2010, PROC CVPR IEEE, P2699, DOI 10.1109/CVPR.2010.5539990
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Tan XY, 2006, PATTERN RECOGN, V39, P1725, DOI 10.1016/j.patcog.2006.03.013
   Tang Y., 2013, P 30 INT C MACH LEAR
   Tang Y., 2012, P 29 INT C MACH LEAR, P1623
   Turk M. A., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P586, DOI 10.1109/CVPR.1991.139758
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Wolf L, 2009, IEEE I CONF COMP VIS, P897, DOI 10.1109/ICCV.2009.5459323
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Wu JX, 2002, PATTERN RECOGN LETT, V23, P1711, DOI 10.1016/S0167-8655(02)00134-4
   Xie J., 2012, P ADV NEUR INF PROC, P350
   Yang J, 2004, IEEE T PATTERN ANAL, V26, P131, DOI 10.1109/TPAMI.2004.1261097
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang L, 2011, IEEE I CONF COMP VIS, P471, DOI 10.1109/ICCV.2011.6126277
   Zhu Z., 2014, RECOVER CANONICAL VI
   Zhu ZY, 2013, IEEE I CONF COMP VIS, P113, DOI 10.1109/ICCV.2013.21
   Zou W., 2012, P ADV NEUR INF PROC, P3203
NR 57
TC 77
Z9 80
U1 6
U2 78
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1556-6013
EI 1556-6021
J9 IEEE T INF FOREN SEC
JI IEEE Trans. Inf. Forensic Secur.
PD OCT
PY 2015
VL 10
IS 10
BP 2108
EP 2118
DI 10.1109/TIFS.2015.2446438
PG 11
WC Computer Science, Theory & Methods; Engineering, Electrical & Electronic
SC Computer Science; Engineering
GA CQ8WQ
UT WOS:000360891300007
DA 2020-02-19
ER

PT J
AU Byeon, W
   Liwicki, M
   Breuel, TM
AF Byeon, Wonmin
   Liwicki, Marcus
   Breuel, Thomas M.
TI Scene analysis by mid-level attribute learning using 2D LSTM networks
   and an application to web-image tagging
SO PATTERN RECOGNITION LETTERS
LA English
DT Article
DE Recurrent neural network; LSTM; Mid-level attribute learning; Scene
   analysis; Web-image tagging
AB This paper describes an approach to scene analysis based on supervised training of 2D Long Short-Term Memory recurrent neural networks (LSTM networks). Unlike previous methods, our approach requires no manual construction of feature hierarchies or incorporation of other prior knowledge. Rather, like deep learning approaches using convolutional networks, our recognition networks are trained directly on raw pixel values. However, in contrast to convolutional neural networks, our approach uses 2D LSTM networks at all levels. Our networks yield per pixel mid-level classifications of input images; since training data for such applications is not available in large numbers, we describe an approach to generating artificial training data, and then evaluate the trained networks on real-world images. Our approach performed significantly better than others methods including Convolutional Neural Networks (ConvNet), yet using two orders of magnitude fewer parameters. We further show the experiment on a recently published dataset, outdoor scene attribute dataset for fair comparisons of scene attribute learning which had significant performance improvement (ca. 21%). Finally, our approach is successfully applied on a real-world application, automatic web-image tagging. (C) 2015 Elsevier B.V. All rights reserved.
C1 [Byeon, Wonmin; Liwicki, Marcus; Breuel, Thomas M.] Univ Kaiserslautern, D-67663 Kaiserslautern, Germany.
   [Byeon, Wonmin] German Res Ctr Artificial Intelligence DKFI, D-67663 Kaiserslautern, Germany.
RP Byeon, W (reprint author), Univ Kaiserslautern, Gottlieb Daimler Str, D-67663 Kaiserslautern, Germany.
EM wonmin.byeon@dfki.de; liwicki@cs.uni-kl.de; tmb@cs.uni-kl.de
CR Arvis V., 2004, Image Analysis & Stereology, V23, P63
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Bianconi F, 2011, J ELECTRON IMAGING, V20, DOI 10.1117/1.3651210
   Blake A, 2011, MARKOV RANDOM FIELDS FOR VISION AND IMAGE PROCESSING, P1
   Bosch A, 2007, IEEE I CONF COMP VIS, P1863
   Byeon W, 2014, IEEE IMAGE PROC, P4373, DOI 10.1109/ICIP.2014.7025887
   Byeon W, 2014, INT C PATT RECOG, P1144, DOI 10.1109/ICPR.2014.206
   Csurka G., 2004, WORKSH STAT LEARN CO, V1, P1, DOI DOI 10.1234/12345678
   Drimbarean A, 2001, PATTERN RECOGN LETT, V22, P1161, DOI 10.1016/S0167-8655(01)00058-7
   Farhadi A, 2009, PROC CVPR IEEE, P1778, DOI 10.1109/CVPRW.2009.5206772
   Ferrari V., 2007, P 21 ANN C NEUR INF
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Gould S, 2009, IEEE I CONF COMP VIS, P1, DOI 10.1109/ICCV.2009.5459211
   Graves A., 2012, SUPERVISED SEQUENCE, P385
   Graves A., 2008, NIPS, P545
   Graves A, 2007, LECT NOTES COMPUT SC, V4668, P549
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Jia Y., 2013, CAFFE OPEN SOURCE CO
   Karpathy A., 2014, LARGE SCALE VIDEO CL
   Kavukcuoglu K., 2010, ADV NEURAL INFORM PR, V23, P1090
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, V1, P4
   Krizhevsky A., 2009, LEARNING MULTIPLE LA, V1, P7
   Ladicky L, 2010, LECT NOTES COMPUT SC, V6314, P424, DOI 10.1007/978-3-642-15561-1_31
   Lazebnik S., 2006, P IEEE INT C COMP VI, V2, P2169, DOI DOI 10.1109/CVPR.2006.68
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Liu C, 2009, PROC CVPR IEEE, P1972, DOI 10.1109/CVPRW.2009.5206536
   Liu L, 2012, IEEE T PATTERN ANAL, V34, P574, DOI 10.1109/TPAMI.2011.145
   Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188
   Mittelman R, 2013, PROC CVPR IEEE, P476, DOI 10.1109/CVPR.2013.68
   Patterson G, 2012, PROC CVPR IEEE, P2751, DOI 10.1109/CVPR.2012.6247998
   PERRONNIN F, 2007, IEEE C COMP VIS PATT, V1, P1
   Roth P. M., 2008, ICGTR0108 GRAZ U TEC
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sanchez J, 2011, PROC CVPR IEEE, P1665, DOI 10.1109/CVPR.2011.5995504
   Shuo Wang, 2013, Computer Vision - ACCV 2012. 11th Asian Conference on Computer Vision. Revised Selected Papers, P796, DOI 10.1007/978-3-642-37444-9_62
   Sun Y, 2014, ADV NEURAL INFORM PR, V60, P1988
   Varma M, 2002, LECT NOTES COMPUT SC, V2352, P255
   Vedaldi A, 2010, P 18 ACM INT C MULT, P1469, DOI DOI 10.1145/1873951.1874249
   Vedaldi A, 2009, IEEE I CONF COMP VIS, P606, DOI 10.1109/ICCV.2009.5459183
   Wang S, 2013, PROC CVPR IEEE, P3111, DOI 10.1109/CVPR.2013.400
   XIAO JX, 2010, PROC CVPR IEEE, P3485, DOI DOI 10.1109/CVPR.2010.5539970
NR 41
TC 7
Z9 7
U1 0
U2 20
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0167-8655
EI 1872-7344
J9 PATTERN RECOGN LETT
JI Pattern Recognit. Lett.
PD OCT 1
PY 2015
VL 63
BP 23
EP 29
DI 10.1016/j.patrec.2015.06.003
PG 7
WC Computer Science, Artificial Intelligence
SC Computer Science
GA CP4YQ
UT WOS:000359888900004
OA Green Published
DA 2020-02-19
ER

PT J
AU Kuen, J
   Lim, KM
   Lee, CP
AF Kuen, Jason
   Lim, Kian Ming
   Lee, Chin Poo
TI Self-taught learning of a deep invariant representation for visual
   tracking via temporal slowness principle
SO PATTERN RECOGNITION
LA English
DT Article
DE Visual tracking; Temporal slowness; Deep learning; Self-taught learning;
   Invariant representation
ID OBJECT TRACKING
AB Visual representation is crucial for visual tracking method's performances. Conventionally, visual representations adopted in visual tracking rely on hand-crafted computer vision descriptors. These descriptors were developed generically without considering tracking-specific information. In this paper, we propose to learn complex-valued invariant representations from tracked sequential image patches, via strong temporal slowness constraint and stacked convolutional autoencoders. The deep slow local representations are learned offline on unlabeled data and transferred to the observational model of our proposed tracker. The proposed observational model retains old training samples to alleviate drift, and collect negative samples which are coherent with target's motion pattern for better discriminative tracking. With the learned representation and online training samples, a logistic regression classifier is adopted to distinguish target from background, and retrained online to adapt to appearance changes. Subsequently, the observational model is integrated into a particle filter framework to perform visual tracking. Experimental results on various challenging benchmark sequences demonstrate that the proposed tracker performs favorably against several state-of-the-art trackers. (C) 2015 Elsevier Ltd. All rights reserved.
C1 [Kuen, Jason; Lim, Kian Ming; Lee, Chin Poo] Multimedia Univ, Fac Informat Sci & Technol, Cyberjaya, Malaysia.
RP Lim, KM (reprint author), Multimedia Univ, Fac Informat Sci & Technol, Cyberjaya, Malaysia.
EM jason7fd@gmail.com; kmlim@mmu.edu.my; cplee@mmu.edu.my
RI Kuen, Jason/AAA-1809-2020
OI Lim, Kian Ming/0000-0003-1929-7978
CR [Anonymous], 2007, P SOC PHOTO-OPT INS
   Arulampalam MS, 2002, IEEE T SIGNAL PROCES, V50, P174, DOI 10.1109/78.978374
   Babenko B, 2009, PROC CVPR IEEE, P983, DOI 10.1109/CVPRW.2009.5206737
   Bengio Y., 2009, ADV NEURAL INFORM PR, P99
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Coates A., 2011, INT C ART INT STAT, V15, P215, DOI 10.1177/1753193410390845
   Collins RT, 2005, IEEE T PATTERN ANAL, V27, P1631, DOI 10.1109/TPAMI.2005.205
   Erhan D., 2009, 1341 U MONTR
   Fan JL, 2010, IEEE T NEURAL NETWOR, V21, P1610, DOI 10.1109/TNN.2010.2066286
   Glorot X., 2011, P 14 INT C ART INT S, V15, P315
   GORDON NJ, 1993, IEE PROC-F, V140, P107, DOI 10.1049/ip-f-2.1993.0015
   Grabner Helmut, 2006, P BMVC, V1, P6, DOI DOI 10.5244/C.20.6
   Hare S, 2011, IEEE I CONF COMP VIS, P263, DOI 10.1109/ICCV.2011.6126251
   JAIN R, 1979, IEEE T PATTERN ANAL, V1, P206, DOI 10.1109/TPAMI.1979.4766907
   Jepson AD, 2003, IEEE T PATTERN ANAL, V25, P1296, DOI 10.1109/TPAMI.2003.1233903
   Jia X, 2012, PROC CVPR IEEE, P1822, DOI 10.1109/CVPR.2012.6247880
   Jin J, 2013, INF SCI SYST CISS 20, P1
   Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239
   Kim M, 2012, PATTERN RECOGN, V45, P1050, DOI 10.1016/j.patcog.2011.08.026
   Klein Dominik A., 2011, IEEE International Conference on Robotics and Automation, P4411
   Klein DA, 2010, IEEE INT C INT ROBOT, P772, DOI 10.1109/IROS.2010.5650583
   Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7
   Larochelle H, 2009, J MACH LEARN RES, V10, P1
   Lazebnik S., 2006, P IEEE INT C COMP VI, V2, P2169, DOI DOI 10.1109/CVPR.2006.68
   Le Q. V., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3361, DOI 10.1109/CVPR.2011.5995496
   Le Q. V., 2011, ADV NEURAL INFORM PR, P1017
   Le Q.V., 2011, P 28 INT C MACH LEAR, P265
   Li X, 2013, ACM T INTEL SYST TEC, V4, DOI 10.1145/2508037.2508039
   Liu F., ARXIV13101690
   Liwicki S, 2012, IEEE T NEUR NET LEAR, V23, P1624, DOI 10.1109/TNNLS.2012.2208654
   Mobahi H., 2009, P 26 ANN INT C MACH, P737
   Netzer Y, 2011, NIPS WORKSH DEEP LEA
   Raina R., 2007, LEARNING, P759, DOI DOI 10.1145/1273496.1273592
   Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7
   Salah R, 2011, P 28 INT C MACH LEAR, P833, DOI 10.1016/j.wasman.2010.10.006
   Tang F, 2007, IEEE I CONF COMP VIS, P992
   Vedaldi A, 2010, P 18 ACM INT C MULT, P1469, DOI DOI 10.1145/1873951.1874249
   Vincent P, 2008, P 25 INT C MACH LEAR, P1096, DOI DOI 10.1145/1390156.1390294
   Wang D, 2013, IEEE T IMAGE PROCESS, V22, P314, DOI 10.1109/TIP.2012.2202677
   Wang HZ, 2007, IEEE T PATTERN ANAL, V29, P1661, DOI 10.1109/TPAMl.2007.1112
   Wang N., 2013, ADV NEURAL INFORM PR, P809
   Wang Q, 2012, IEEE T IMAGE PROCESS, V21, P4454, DOI 10.1109/TIP.2012.2205700
   Wang Q, 2012, IEEE T IMAGE PROCESS, V21, P3296, DOI 10.1109/TIP.2012.2190085
   Wang S, 2011, IEEE I CONF COMP VIS, P1323, DOI 10.1109/ICCV.2011.6126385
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Yu K, 2011, PROC CVPR IEEE, P1713, DOI 10.1109/CVPR.2011.5995732
   Zhang KH, 2013, IEEE T IMAGE PROCESS, V22, P4664, DOI 10.1109/TIP.2013.2277800
   Zhang KH, 2012, LECT NOTES COMPUT SC, V7574, P864, DOI 10.1007/978-3-642-33712-3_62
   Zhang KH, 2013, PATTERN RECOGN, V46, P397, DOI 10.1016/j.patcog.2012.07.013
   Zou W.Y., 2012, ADV NEURAL INFORM PR, P3212
   Zou W.Y., 2011, NIPS WORKSH DEEP LEA
NR 51
TC 17
Z9 21
U1 1
U2 45
PU ELSEVIER SCI LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND
SN 0031-3203
EI 1873-5142
J9 PATTERN RECOGN
JI Pattern Recognit.
PD OCT
PY 2015
VL 48
IS 10
BP 2964
EP 2982
DI 10.1016/j.patcog.2015.02.012
PG 19
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
SC Computer Science; Engineering
GA CL8TD
UT WOS:000357246100002
DA 2020-02-19
ER

PT J
AU Ding, SY
   Lin, L
   Wang, GR
   Chao, HY
AF Ding, Shengyong
   Lin, Liang
   Wang, Guangrun
   Chao, Hongyang
TI Deep feature learning with relative distance comparison for person
   re-identification
SO PATTERN RECOGNITION
LA English
DT Article
DE Person re-identification; Deep learning; Distance comparison
AB Identifying the same individual across different scenes is an important yet difficult task in intelligent video surveillance. Its main difficulty lies in how to preserve similarity of the same person against large appearance and structure variation while discriminating different individuals. In this paper, we present a scalable distance driven feature learning framework based on the deep neural network for person re-identification, and demonstrate its effectiveness to handle the existing challenges. Specifically, given the training images with the class labels (person IDs), we first produce a large number of triplet units, each of which contains three images, i.e. one person with a matched reference and a mismatched reference. Treating the units as the input, we build the convolutional neural network to generate the layered representations, and follow with the L2 distance metric. By means of parameter optimization, our framework tends to maximize the relative distance between the matched pair and the mismatched pair for each triplet unit. Moreover, a nontrivial issue arising with the framework is that the triplet organization cubically enlarges the number of training triplets, as one image can be involved into several triplet units. To overcome this problem, we develop an effective triplet generation scheme and an optimized gradient descent algorithm, making the computational load mainly depend on the number of original images instead of the number of triplets. On several challenging databases, our approach achieves very promising results and outperforms other state-of-the-art approaches. (C) 2015 Elsevier Ltd. All rights reserved.
C1 [Ding, Shengyong; Lin, Liang; Wang, Guangrun; Chao, Hongyang] Sun Yat Sen Univ, Guangzhou Higher Educ Mega Ctr, Guangzhou 510006, Guangdong, Peoples R China.
RP Lin, L (reprint author), Sun Yat Sen Univ, Guangzhou Higher Educ Mega Ctr, Guangzhou 510006, Guangdong, Peoples R China.
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61173081, 61173082]; Guangdong Science and
   Technology Program [2012B031500006]; Guangdong Natural Science
   FoundationNational Natural Science Foundation of Guangdong Province
   [S2013050014548, S2011020001215]; Special Project on Integration of
   Industry, Education and Research of Guangdong Province [2012B091000101];
   Program of Guangzhou Zhujiang Star of Science and Technology
   [2013J2200067]
FX This work was supported by the National Natural Science Foundation of
   China (No. 61173081 and No. 61173082), Guangdong Science and Technology
   Program (No. 2012B031500006), Guangdong Natural Science Foundation (No.
   S2013050014548 and No. S2011020001215), Special Project on Integration
   of Industry, Education and Research of Guangdong Province (No.
   2012B091000101), and Program of Guangzhou Zhujiang Star of Science and
   Technology (No. 2013J2200067).
CR Blitzer J., 2005, ADV NEURAL INFORM PR, P1473
   Davis J. V., 2007, P 24 INT C MACH LEAR, P209, DOI DOI 10.1145/1273496.1273523
   Farenzena M, 2010, PROC CVPR IEEE, P2360, DOI 10.1109/CVPR.2010.5539926
   Globerson A, 2005, ADV NEURAL INFORM PR, P451
   Gray D., 2007, PETS
   Gray D, 2008, LECT NOTES COMPUT SC, V5302, P262, DOI 10.1007/978-3-540-88682-2_21
   Jia Y., 2013, CAFFE OPEN SOURCE CO
   Kostinger M, 2012, PROC CVPR IEEE, P2288, DOI 10.1109/CVPR.2012.6247939
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   Layne R, 2012, LECT NOTES COMPUT SC, V7583, P402, DOI 10.1007/978-3-642-33863-2_40
   Li W., 2012, P AS C COMP VIS, P31, DOI DOI 10.1007/978-3-642-37331-2
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Li Z, 2013, PROC CVPR IEEE, P3610, DOI 10.1109/CVPR.2013.463
   Lin L, 2015, IEEE T PATTERN ANAL, V37, P959, DOI 10.1109/TPAMI.2014.2359888
   Lin L, 2015, IEEE T CIRC SYST VID, V25, P251, DOI 10.1109/TCSVT.2014.2313897
   Lin L, 2012, PATTERN RECOGN, V45, P231, DOI 10.1016/j.patcog.2011.06.011
   Lin LA, 2010, IEEE T PATTERN ANAL, V32, P1426, DOI 10.1109/TPAMI.2009.150
   Lin L, 2009, PATTERN RECOGN, V42, P1297, DOI 10.1016/j.patcog.2008.10.033
   Liu C., 2012, ECCV LECT NOTES COMP, V7583, P391
   Liu H, 2015, NEUROCOMPUTING, V151, P1283, DOI 10.1016/j.neucom.2014.11.002
   Ma B. P., 2012, BMVC
   Ma LY, 2014, IEEE T IMAGE PROCESS, V23, P3656, DOI 10.1109/TIP.2014.2331755
   Mignon A, 2012, PROC CVPR IEEE, P2666, DOI 10.1109/CVPR.2012.6247987
   Pedagadi S, 2013, PROC CVPR IEEE, P3318, DOI 10.1109/CVPR.2013.426
   Schwartz WR, 2009, SIBGRAPI, P322, DOI 10.1109/SIBGRAPI.2009.42
   Wang J., 2014, CVPR
   Wang T., 2014, EUR C COMP VIS IEEE
   Wang X, 2007, PROC SPIE, V6279, DOI 10.1117/12.725431
   Xiang SM, 2008, PATTERN RECOGN, V41, P3600, DOI 10.1016/j.patcog.2008.05.018
   Xing E P, 2002, ADV NEURAL INFORM PR, P505
   Xu YL, 2013, IEEE I CONF COMP VIS, P3152, DOI 10.1109/ICCV.2013.391
   Yi D., ABS14074979 CORR
   Zhao R, 2013, CVPR, P144
   Zhao R, 2013, IEEE I CONF COMP VIS, P2528, DOI 10.1109/ICCV.2013.314
   Zhao R, 2013, PROC CVPR IEEE, P3586, DOI 10.1109/CVPR.2013.460
   Zheng WS, 2011, PROC CVPR IEEE, P649, DOI 10.1109/CVPR.2011.5995598
NR 36
TC 242
Z9 253
U1 10
U2 122
PU ELSEVIER SCI LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND
SN 0031-3203
EI 1873-5142
J9 PATTERN RECOGN
JI Pattern Recognit.
PD OCT
PY 2015
VL 48
IS 10
BP 2993
EP 3003
DI 10.1016/j.patcog.2015.04.005
PG 11
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
SC Computer Science; Engineering
GA CL8TD
UT WOS:000357246100004
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Kim, S
   Choi, Y
   Lee, M
AF Kim, Sangwook
   Choi, Yonghwa
   Lee, Minho
TI Deep learning with support vector data description
SO NEUROCOMPUTING
LA English
DT Article
DE Support vector data description; Deep learning; Pattern recognition;
   Generalization
ID ALGORITHM; NETWORK
AB One of the most critical problems for machine learning methods is overfitting. The overfitting problem is a phenomenon in which the accuracy of the model on unseen data is poor whereas the training accuracy is nearly perfect. This problem is particularly severe in complex models that have a large set of parameters. In this paper, we propose a deep learning neural network model that adopts the support vector data description (SVDD). The SVDD is a variant of the support vector machine, which has high generalization performance by acquiring a maximal margin in one-class classification problems. The proposed model strives to obtain the representational power of deep learning. Generalization performance is maintained using the SVDD. The experimental results showed that the proposed model can learn multiclass data without severe overfitting problems. (C) 2015 Elsevier B.V. All rights reserved.
C1 [Kim, Sangwook; Choi, Yonghwa; Lee, Minho] Kyungpook Natl Univ, Sch Elect Engn, Taegu 702701, South Korea.
RP Lee, M (reprint author), Kyungpook Natl Univ, Sch Elect Engn, 1370 Sankyuk Dong, Taegu 702701, South Korea.
EM mholee@gmail.com
FU ICT R&D program of MSIP/IITP [10041826]; Industrial Strategic Technology
   Development Program - Ministry of Knowledge Economy (MKE, Korea)
   [10044009]
FX This work was partly supported by the ICT R&D program of MSIP/IITP.
   [10041826, Development of emotional features sensing, diagnostics and
   distribution s/w platform for measurement of multiple intelligence from
   young children] and by the Industrial Strategic Technology Development
   Program [10044009] funded by the Ministry of Knowledge Economy (MKE,
   Korea) (50%).
CR ACKLEY DH, 1985, COGNITIVE SCI, V9, P147
   Amari S, 1997, IEEE T NEURAL NETWOR, V8, P985, DOI 10.1109/72.623200
   Arel I., 2010, COMPUTATIONAL INTELL, V5, P13, DOI DOI 10.1109/MCI.2010.938364
   Banerjee A., 2007, P IEEE ICIP, pIV
   Bartlett P, 1999, ADVANCES IN KERNEL METHODS, P43
   Bengio Y., 2007, LARGE SCALE KERNEL M, V34, P1
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Bishop CM, 2006, PATTERN RECOGNITION
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411
   Erhan D, 2010, J MACH LEARN RES, V11, P625
   FUKUSHIMA K, 1980, BIOL CYBERN, V36, P193, DOI 10.1007/BF00344251
   Ge ZQ, 2011, J PROCESS CONTR, V21, P949, DOI 10.1016/j.jprocont.2011.02.004
   GOLUB GH, 1979, TECHNOMETRICS, V21, P215, DOI 10.1080/00401706.1979.10489751
   Goodfellow I. J., 2013, ARXIV13024389
   Hinton G., 2010, MOMENTUM, V9, P926
   Hinton G. E, 2012, ARXIV12070580
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Le Roux N, 2008, NEURAL COMPUT, V20, P1631, DOI 10.1162/neco.2008.04-07-510
   LeCun Y., 1995, HDB BRAIN THEORY NEU, P3361
   LeCun Y., 1989, CONNECTIONS PERSPECT
   Lee SW, 2006, PATTERN RECOGN, V39, P1809, DOI 10.1016/j.patcog.2006.04.033
   Lichman M., UCI MACHINE LEARNING
   Lucas DD, 2013, GEOSCI MODEL DEV, V6, P1157, DOI 10.5194/gmd-6-1157-2013
   MANGASARIAN OL, 1995, OPER RES, V43, P570, DOI 10.1287/opre.43.4.570
   Moody J., 1995, ADV NEURAL INFORM PR, V4, P950
   NOWLAN SJ, 1992, NEURAL COMPUT, V4, P473, DOI 10.1162/neco.1992.4.4.473
   Sangwook Kim, 2013, Neural Information Processing. 20th International Conference, ICONIP 2013. Proceedings: LNCS 8226, P458, DOI 10.1007/978-3-642-42054-2_57
   Scholkopf B., 1995, P 1 INT C KNOWL DISC
   Shawe-Taylor J, 1998, IEEE T INFORM THEORY, V44, P1926, DOI 10.1109/18.705570
   Sigillito V., 1990, PIMA INDIANS DIABETE
   Smolensky Paul, 1986, INFORM PROCESSING DY
   Tax D., 2014, DDTOOLS DATA DESCRIT
   Tax DMJ, 2004, MACH LEARN, V54, P45, DOI 10.1023/B:MACH.0000008084.60811.49
   Tax DMJ, 2002, J MACH LEARN RES, V2, P155, DOI 10.1162/15324430260185583
   TESAURO G, 1992, MACH LEARN, V8, P257, DOI 10.1023/A:1022624705476
   Utgoff PE, 2002, NEURAL COMPUT, V14, P2497, DOI 10.1162/08997660260293319
   VALIANT LG, 1984, COMMUN ACM, V27, P1134, DOI 10.1145/1968.1972
   Vapnik V., 1974, THEORY PATTERN RECOG
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Wan L., 2013, P 30 INT C MACH LEAR, P1058
   Weston J., 1998, CSDTR9804 U LOND
NR 41
TC 24
Z9 29
U1 4
U2 109
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0925-2312
EI 1872-8286
J9 NEUROCOMPUTING
JI Neurocomputing
PD OCT 1
PY 2015
VL 165
BP 111
EP 117
DI 10.1016/j.neucom.2014.09.086
PG 7
WC Computer Science, Artificial Intelligence
SC Computer Science
GA CL2BD
UT WOS:000356747700014
DA 2020-02-19
ER

PT J
AU Bai, J
   Wu, Y
   Zhang, JM
   Chen, FQ
AF Bai, Jing
   Wu, Yan
   Zhang, Junming
   Chen, Fuqiang
TI Subset based deep learning for RGB-D object recognition
SO NEUROCOMPUTING
LA English
DT Article
DE RGB-D object recognition; Subset based feature extracting; Sparse
   auto-encoder; Recursive neural networks; Deep learning
ID AUTOENCODER
AB RGB-D camera can easily record both color and depth images and previous works have proved that combining them together could dramatically improve the RGB-D based object recognition accuracy. In this paper, a new method based on a subset approach was introduced to learn higher level features from the raw data. The raw RGB and depth images were divided into several subsets according to their shapes and colors, guaranteeing that any two different objects in each subset are nearly not similar. Then a RGB-Subset-Sparse auto-encoder was trained to extract features from RGB images and a Depth-Subset-Sparse auto-encoder was trained to extract features from depth images for each subset. Then the learned features were transmitted to recursive neural networks (RNNs) to reduce the dimensionality of the features and learn robust hierarchical feature representations. The feature representations learned from RGB images and depth images were concatenated as the final features and then sent to a softmax classifier for classification. The proposed method is evaluated on three benchmark RGB-D datasets, RGB-D dataset of Lai et al., 2D3D dataset of Browatzki et al. and Aharon dataset of Aharon et al. Compared with other methods, ours achieves state-of-the-art performance on the first two datasets. Furthermore, to validate the generalization of our subset approach, we also do some extra experiments of applying the subsets approach to several previous works, these accuracies improved significantly. (C) 2015 Elsevier B.V. All rights reserved.
C1 [Bai, Jing; Wu, Yan; Zhang, Junming; Chen, Fuqiang] Tongji Univ, Coll Elect & Informat Engn, Shanghai 201804, Peoples R China.
RP Wu, Y (reprint author), Tongji Univ, Coll Elect & Informat Engn, Shanghai 201804, Peoples R China.
EM yanwu@tongji.edu.cn
CR Andrew Bagnell J., 2008, P NEUR INF PROC SYST, P113
   Andrew Ng, 2011, CS294A LECT NOTES, P72
   Baccouche M., 2005, NETWORKS, V18, P602
   BARHILLEL A, 2011, P 2011 IEEE INT C CO, P65
   Blum M, 2012, IEEE INT CONF ROBOT, P1298, DOI 10.1109/ICRA.2012.6225188
   Bo L., 2013, EXPT ROBOTICS, P387, DOI DOI 10.1007/978-3-319-00065-7
   Bo LF, 2011, IEEE INT C INT ROBOT, P821, DOI 10.1109/IROS.2011.6048717
   Browatzki B., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P1189, DOI 10.1109/ICCVW.2011.6130385
   Cheng YH, 2014, INT C PATT RECOG, P2377, DOI 10.1109/ICPR.2014.412
   Deng J, 2013, INT CONF AFFECT, P511, DOI 10.1109/ACII.2013.90
   Duong A. T., 2014, STUDIES COMPUTATIONA, P133
   Hu DK, 2013, APPL MECH MATER, V433-435, P334, DOI 10.4028/www.scientific.net/AMM.433-435.334
   Krizhevsky A., 2009, TECHNICAL REPORT
   Lai K, 2011, IEEE INT CONF ROBOT, P1817
   LeCun Y, 2004, PROC CVPR IEEE, P97
   Liou CY, 2014, NEUROCOMPUTING, V139, P84, DOI 10.1016/j.neucom.2013.09.055
   POLLACK JB, 1990, ARTIF INTELL, V46, P77, DOI 10.1016/0004-3702(90)90005-K
   Shu Michelle, 2013, P 3 NIPS WORKSH MACH
   Socher R., 2012, ADV NEURAL INFORM PR, V3, P665, DOI DOI 10.1002/2014GB005021
   Socher R., 2011, P 28 INT C MACH LEAR, P129, DOI DOI 10.1007/978-3-540-87479-9
   Socher R., 2011, ADV NEURAL INFORM PR, P801
   Socher R., 2011, P C EMP METH NAT LAN, P151
   Yang YL, 2011, PROC IEEE MICR ELECT, P79, DOI 10.1109/MEMSYS.2011.5734366
NR 23
TC 36
Z9 39
U1 2
U2 63
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0925-2312
EI 1872-8286
J9 NEUROCOMPUTING
JI Neurocomputing
PD OCT 1
PY 2015
VL 165
BP 280
EP 292
DI 10.1016/j.neucom.2015.03.017
PG 13
WC Computer Science, Artificial Intelligence
SC Computer Science
GA CL2BD
UT WOS:000356747700032
DA 2020-02-19
ER

PT J
AU Espi, M
   Fujimoto, M
   Kinoshita, K
   Nakatani, T
AF Espi, Miquel
   Fujimoto, Masakiyo
   Kinoshita, Keisuke
   Nakatani, Tomohiro
TI Exploiting spectro-temporal locality in deep learning based acoustic
   event detection
SO EURASIP JOURNAL ON AUDIO SPEECH AND MUSIC PROCESSING
LA English
DT Article
DE Acoustic event detection; Local spectro-temporal characterization;
   Feature extraction; Time-frequency resolution; Convolution neural
   networks
AB In recent years, deep learning has not only permeated the computer vision and speech recognition research fields but also fields such as acoustic event detection (AED). One of the aims of AED is to detect and classify non-speech acoustic events occurring in conversation scenes including those produced by both humans and the objects that surround us. In AED, deep learning has enabled modeling of detail-rich features, and among these, high resolution spectrograms have shown a significant advantage over existing predefined features (e.g., Mel-filter bank) that compress and reduce detail. In this paper, we further asses the importance of feature extraction for deep learning-based acoustic event detection. AED, based on spectrogram-input deep neural networks, exploits the fact that sounds have "global" spectral patterns, but sounds also have "local" properties such as being more transient or smoother in the time-frequency domain. These can be exposed by adjusting the time-frequency resolution used to compute the spectrogram, or by using a model that exploits locality leading us to explore two different feature extraction strategies in the context of deep learning: (1) using multiple resolution spectrograms simultaneously and analyzing the overall and event-wise influence to combine the results, and (2) introducing the use of convolutional neural networks (CNN), a state of the art 2D feature extraction model that exploits local structures, with log power spectrogram input for AED. An experimental evaluation shows that the approaches we describe outperform our state-of-the-art deep learning baseline with a noticeable gain in the CNN case and provides insights regarding CNN-based spectrogram characterization for AED.
C1 [Espi, Miquel; Fujimoto, Masakiyo; Kinoshita, Keisuke; Nakatani, Tomohiro] NTT Corp, NTT Commun Sci Labs, Keihanna Sci City, Kyoto 6190237, Japan.
RP Espi, M (reprint author), NTT Corp, NTT Commun Sci Labs, 2-4 Hikaridai, Keihanna Sci City, Kyoto 6190237, Japan.
EM espi.miquel@lab.ntt.co.jp
CR Bergstra J., 2010, P PYTH SCI COMP C SC, P3
   Canton-Ferrer C., 2009, 2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P81, DOI 10.1109/CVPR.2009.5204264
   Chang SY, 2014, INTERSPEECH, P905
   ESPI M, 2014, HSCMA, P117, DOI DOI 10.1109/HSCMA.2014.6843263
   Espi M, 2012, INT CONF ACOUST SPEE, P4293, DOI 10.1109/ICASSP.2012.6288868
   Gencoglu O, 2014, EUR SIGNAL PR CONF, P506
   Giannoulis D., 2013, P IEEE WORKSH APPL S, P1, DOI DOI 10.1109/WASPAA.2013.6701819
   Heittola T, 2013, EURASIP J AUDIO SPEE, DOI 10.1186/1687-4722-2013-1
   Hinton G., 2010, MOMENTUM, V9, P926
   Hirsch H., AURORA 4
   Hori T, 2012, IEEE T AUDIO SPEECH, V20, P499, DOI 10.1109/TASL.2011.2164527
   Imoto K, 2013, INTERSPEECH, P2608
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Mohamed AR, 2012, IEEE T AUDIO SPEECH, V20, P14, DOI 10.1109/TASL.2011.2109382
   Mostefa D, 2007, LANG RESOUR EVAL, V41, P389, DOI 10.1007/s10579-007-9054-4
   Ozerov A, 2011, 2011 IEEE WORKSHOP ON APPLICATIONS OF SIGNAL PROCESSING TO AUDIO AND ACOUSTICS (WASPAA), P257, DOI 10.1109/ASPAA.2011.6082285
   Sainath TN, 2015, NEURAL NETWORKS, V64, P39, DOI 10.1016/j.neunet.2014.08.005
   Simard PY, 2003, PROC INT CONF DOC, P958
   Thomas Samuel, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P2519, DOI 10.1109/ICASSP.2014.6854054
   Xugang Lu, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P6255, DOI 10.1109/ICASSP.2014.6854807
   Zhang HM, 2015, INT CONF ACOUST SPEE, P559, DOI 10.1109/ICASSP.2015.7178031
   Zhuang XD, 2010, PATTERN RECOGN LETT, V31, P1543, DOI 10.1016/j.patrec.2010.02.005
NR 22
TC 36
Z9 36
U1 0
U2 25
PU SPRINGEROPEN
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 1687-4722
J9 EURASIP J AUDIO SPEE
JI EURASIP J. Audio Speech Music Process.
PD SEP 14
PY 2015
AR 26
DI 10.1186/s13636-015-0069-2
PG 12
WC Acoustics; Engineering, Electrical & Electronic
SC Acoustics; Engineering
GA CR6NZ
UT WOS:000361465500001
OA DOAJ Gold
DA 2020-02-19
ER

PT J
AU Zhao, XM
   Shi, XG
   Zhang, SQ
AF Zhao, Xiaoming
   Shi, Xugan
   Zhang, Shiqing
TI Facial Expression Recognition via Deep Learning
SO IETE TECHNICAL REVIEW
LA English
DT Article
DE Deep belief networks; Deep learning; Facial expression recognition;
   Feature learning; Multi-layer perceptron; Unsupervised
ID FACE RECOGNITION; PATTERN; REPRESENTATION
AB Deep learning is a newly-emerged machine learning theory, and has received extensive attentions in pattern recognition, signal processing, computer vision, etc. Deep belief networks (DBNs) is a representative method of deep learning and has a strong ability of unsupervised feature learning. In this paper, by combining DBNs with multi-layer perceptron (MLP), a new method of facial expression recognition based on deep learning is proposed. The proposed method integrates the DBNs's advantage of unsupervised feature learning with the MLP's classification advantage. Experimental results on two benchmarking facial expression databases, i.e., the JAFFE database and the Cohn-Kanade database, demonstrate the promising performance of the proposed method for facial expression recognition, outperforming the other state-of-the-art classification methods such as the nearest neighbour, MLP, support vector machine, the nearest subspace, as well as sparse representation-based classification.
C1 [Zhao, Xiaoming; Shi, Xugan; Zhang, Shiqing] Taizhou Univ, Inst Image Proc & Pattern Recognit, Taizhou 318000, Zhejiang, Peoples R China.
   [Shi, Xugan] Zhejiang Sci Tech Univ, Sch Automat Control Mech, Hangzhou 310018, Peoples R China.
RP Zhao, XM (reprint author), Taizhou Univ, Inst Image Proc & Pattern Recognit, Taizhou 318000, Zhejiang, Peoples R China.
EM tzxyzxm@163.com; zhangshiqing111@126.com; tzczsq@163.com
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61272261, 61203257]
FX This work is supported by National Natural Science Foundation of China
   [grant number 61203257], [grant number 61272261].
CR Ahsan T, 2013, IETE TECH REV, V30, P47, DOI 10.4103/0256-4602.107339
   Arel I, 2010, IEEE COMPUT INTELL M, V5, P13, DOI 10.1109/MCI.2010.938364
   Bashyal S, 2008, ENG APPL ARTIF INTEL, V21, P1056, DOI 10.1016/j.engappai.2007.11.010
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Brahnam S., 2014, LOCAL BINARY PATTERN, P1
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411
   COVER TM, 1967, IEEE T INFORM THEORY, V13, P21, DOI 10.1109/TIT.1967.1053964
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Freund Yoav, 1994, UCSCCRL9425
   Gu WF, 2012, PATTERN RECOGN, V45, P80, DOI 10.1016/j.patcog.2011.05.006
   Hinton G., 2012, LNCS, V7700, P599, DOI DOI 10.1007/978-3-642-35289-8_32
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Jabid T, 2010, ETRI J, V32, P784, DOI 10.4218/etrij.10.1510.0132
   Kanade T., 2000, Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), P46, DOI 10.1109/AFGR.2000.840611
   Lee KC, 2005, IEEE T PATTERN ANAL, V27, P684, DOI 10.1109/TPAMI.2005.92
   Lyons M, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P200, DOI 10.1109/AFGR.1998.670949
   McCulloch Warren S., 1943, BULL MATH BIOPHYS, V5, P115, DOI 10.1007/BF02478259
   Mohamed AR, 2012, IEEE T AUDIO SPEECH, V20, P14, DOI 10.1109/TASL.2011.2109382
   Mohammadi MR, 2014, J VIS COMMUN IMAGE R, V25, P1082, DOI 10.1016/j.jvcir.2014.03.006
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Pearl J, 1988, PROBABILISTIC REASON
   Rabiner L. R., 1986, IEEE ASSP Magazine, V3, P4, DOI 10.1109/MASSP.1986.1165342
   Sarikaya R, 2014, IEEE-ACM T AUDIO SPE, V22, P778, DOI 10.1109/TASLP.2014.2303296
   Serre T, 2007, PROG BRAIN RES, V165, P33, DOI 10.1016/S0079-6123(06)65004-8
   Shan C., 2005, P IEEE INT C IM PROC, V2, P370
   Shan CF, 2009, IMAGE VISION COMPUT, V27, P803, DOI 10.1016/j.imavis.2008.08.005
   Siddiqi MH, 2014, IETE TECH REV, V31, P277, DOI 10.1080/02564602.2014.944588
   Tian Y., 2011, HDB FACE RECOGNITION, P487, DOI DOI 10.1007/978-0-85729-932-1_19
   Wright J, 2010, P IEEE, V98, P1031, DOI 10.1109/JPROC.2010.2044470
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Yu D, 2011, IEEE SIGNAL PROC MAG, V28, P145, DOI 10.1109/MSP.2010.939038
   Zavaschi THH, 2013, EXPERT SYST APPL, V40, P646, DOI 10.1016/j.eswa.2012.07.074
   Zhang SQ, 2012, SENSORS-BASEL, V12, P3747, DOI 10.3390/s120303747
   Zhang ZY, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P454, DOI 10.1109/AFGR.1998.670990
   Zhao GY, 2007, IEEE T PATTERN ANAL, V29, P915, DOI 10.1109/TPAMI.2007.1110
   Zhao XM, 2012, EURASIP J ADV SIG PR, P1, DOI 10.1186/1687-6180-2012-20
NR 37
TC 22
Z9 26
U1 2
U2 68
PU TAYLOR & FRANCIS LTD
PI ABINGDON
PA 2-4 PARK SQUARE, MILTON PARK, ABINGDON OR14 4RN, OXON, ENGLAND
SN 0256-4602
EI 0974-5971
J9 IETE TECH REV
JI IETE Tech. Rev.
PD SEP 3
PY 2015
VL 32
IS 5
BP 347
EP 355
DI 10.1080/02564602.2015.1017542
PG 9
WC Engineering, Electrical & Electronic; Telecommunications
SC Engineering; Telecommunications
GA CS6GW
UT WOS:000362177500004
DA 2020-02-19
ER

PT J
AU Wang, LH
   Zhao, GG
   Sun, DH
AF Wang, Longhui
   Zhao, Guoguang
   Sun, Donghong
TI Modeling Documents with Event Model
SO ALGORITHMS
LA English
DT Article
DE deep learning; neurolinguistics; topic model; Event Model; document
   retrieval
AB Currently deep learning has made great breakthroughs in visual and speech processing, mainly because it draws lessons from the hierarchical mode that brain deals with images and speech. In the field of NLP, a topic model is one of the important ways for modeling documents. Topic models are built on a generative model that clearly does not match the way humans write. In this paper, we propose Event Model, which is unsupervised and based on the language processing mechanism of neurolinguistics, to model documents. In Event Model, documents are descriptions of concrete or abstract events seen, heard, or sensed by people and words are objects in the events. Event Model has two stages: word learning and dimensionality reduction. Word learning is to learn semantics of words based on deep learning. Dimensionality reduction is the process that representing a document as a low dimensional vector by a linear mode that is completely different from topic models. Event Model achieves state-of-the-art results on document retrieval tasks.
C1 [Wang, Longhui; Zhao, Guoguang; Sun, Donghong] Tsinghua Univ, Beijing 100000, Peoples R China.
   [Wang, Longhui] Shandong Univ, Sch Mech Engn, Jinan 250061, Peoples R China.
RP Zhao, GG (reprint author), Tsinghua Univ, Beijing 100000, Peoples R China.
EM henryalink@126.com; zhaoinchina@gmail.com; sundh@cernet.edu.cn
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61272427]
FX This research was supported by National Natural Science Foundation of
   China (Grant No. 61272427).
CR Bear M.F., 2004, NEUROSCIENCE EXPLORI, V3, P620
   Bengio Y, 2003, J MACH LEARN RES, V3, P1137, DOI 10.1162/153244303322533223
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Boyd-Graber Jordan, 2014, HDB MIXED MEMBERSHIP
   Chen X, 2014, LEARNING RECURRENT V
   Fang H, 2014, CAPTIONS VISUAL CONC
   Frome A., 2013, P NEUR INF PROC SYST
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Mikolov T., 2013, EFFICIENT ESTIMATION
   Salakhutdinov R., 2009, P NIPS2009 VANC BC C
   Srivastava N., 2013, MODELING DOCUMENTS D
   Sun Zhi-jun, 2012, Application Research of Computers, V29, P2806, DOI 10.3969/j.issn.1001-3695.2012.08.002
   Tang J., 2014, P 31 INT C MACH LEAR, V32, P190
   Vinyals Oriol, 2014, SHOW TELL NEURAL IMA
NR 15
TC 1
Z9 1
U1 0
U2 5
PU MDPI AG
PI BASEL
PA ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
SN 1999-4893
J9 ALGORITHMS
JI Algorithms
PD SEP
PY 2015
VL 8
IS 3
BP 562
EP 572
DI 10.3390/a8030562
PG 11
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods
SC Computer Science
GA CV3HT
UT WOS:000364150500015
OA DOAJ Gold
DA 2020-02-19
ER

PT J
AU Chalasani, R
   Principe, JC
AF Chalasani, Rakesh
   Principe, Jose C.
TI Context Dependent Encoding Using Convolutional Dynamic Networks
SO IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
LA English
DT Article
DE Context; deep learning; dynamic models; empirical priors; object
   recognition
ID ALGORITHM; MODEL; TIME; CODE
AB Perception of sensory signals is strongly influenced by their context, both in space and time. In this paper, we propose a novel hierarchical model, called convolutional dynamic networks, that effectively utilizes this contextual information, while inferring the representations of the visual inputs. We build this model based on a predictive coding framework and use the idea of empirical priors to incorporate recurrent and top-down connections. These connections endow the model with contextual information coming from temporal as well as abstract knowledge from higher layers. To perform inference efficiently in this hierarchical model, we rely on a novel scheme based on a smoothing proximal gradient method. When trained on unlabeled video sequences, the model learns a hierarchy of stable attractors, representing low-level to high-level parts of the objects. We demonstrate that the model effectively utilizes contextual information to produce robust and stable representations for object recognition in video sequences, even in case of highly corrupted inputs.
C1 [Chalasani, Rakesh] AnalytXbook Inc, Boston, MA 02109 USA.
   [Chalasani, Rakesh; Principe, Jose C.] Univ Florida, Dept Elect & Comp Engn, Gainesville, FL 32611 USA.
RP Chalasani, R (reprint author), AnalytXbook Inc, Boston, MA 02109 USA.
EM rakeshch@ufl.edu; principe@cnel.ufl.edu
FU Office of Naval ResearchOffice of Naval Research [N000141010375]
FX This work was supported by the Office of Naval Research under Grant
   N000141010375.
CR Bar M, 2004, NAT REV NEUROSCI, V5, P617, DOI 10.1038/nrn1476
   Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Boureau YL, 2010, PROC CVPR IEEE, P2559, DOI 10.1109/CVPR.2010.5539963
   Cadieu C. F., 2008, ADV NEURAL INFORM PR, P209
   Cevikalp H, 2010, PROC CVPR IEEE, P2567, DOI 10.1109/CVPR.2010.5539965
   Chalasani R., 2013, P WORKSH INT C LEARN
   Chalasani R., 2013, P INT JOINT C NEUR N, DOI [10.1109/ijcnn.2013.6706854, DOI 10.1109/IJCNN.2013.6706854]
   Chen B., 2011, P 28 INT C MACH LEAR, P361
   Chen X, 2012, ANN APPL STAT, V6, P719, DOI 10.1214/11-AOAS514
   Chen YC, 2012, LECT NOTES COMPUT SC, V7577, P766, DOI 10.1007/978-3-642-33783-3_55
   Dalal N, 2005, PROC CVPR IEEE, P886
   Denil M., 2012, RECKLESSLY APPROXIMA
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Felleman DJ, 1991, CEREB CORTEX, V1, P1, DOI 10.1093/cercor/1.1.1
   Friston KJ, 2009, NEURAL NETWORKS, V22, P1093, DOI 10.1016/j.neunet.2009.07.023
   Friston KJ, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.1000081
   Friston KJ, 2005, PHILOS T R SOC B, V360, P815, DOI 10.1098/rstb.2005.1622
   George D, 2005, IEEE IJCNN, P1812
   Gilbert CD, 2013, NAT REV NEUROSCI, V14, P350, DOI 10.1038/nrn3476
   Goodfellow I., 2009, ADV NEURAL INFORM PR, V22, P646
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hu Y, 2011, PROC CVPR IEEE, P121, DOI 10.1109/CVPR.2011.5995500
   Hyvarinen A, 2001, VISION RES, V41, P2413, DOI 10.1016/S0042-6989(01)00114-6
   Jarrett K, 2009, IEEE I CONF COMP VIS, P2146, DOI 10.1109/ICCV.2009.5459469
   Kavukcuoglu K., 2010, ADV NEURAL INFORM PR, V23, P1090
   Kiebel SJ, 2009, PLOS COMPUT BIOL, V5, DOI 10.1371/journal.pcbi.1000464
   Kiebel SJ, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.1000209
   Lazebnik S., 2006, P IEEE INT C COMP VI, V2, P2169, DOI DOI 10.1109/CVPR.2006.68
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee H., 2007, ADV NEURAL INFORM PR, P873
   Lee H., 2009, P 26 ANN INT C MACH, P609, DOI DOI 10.1145/1553374.1553453
   Lee KC, 2005, COMPUT VIS IMAGE UND, V99, P303, DOI 10.1016/j.cviu.2005.02.002
   Lee TS, 2003, J OPT SOC AM A, V20, P1434, DOI 10.1364/JOSAA.20.001434
   Li FF, 2007, COMPUT VIS IMAGE UND, V106, P59, DOI 10.1016/j.cviu.2005.09.012
   Lowe D., 1999, P INT C COMP VIS, V2, P1150, DOI [10.1109/ICCV.1999.790410, DOI 10.1109/ICCV.1999.790410]
   Minyoung K., 2008, IEEE C COMP VIS PATT, V1, P1, DOI DOI 10.1109/CVPR.2008.4587572
   Mobahi H., 2009, P 26 ANN INT C MACH, P737
   Nene S. A., 1996, CUCS00596
   Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5
   Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0
   Raina R., 2007, LEARNING, P759, DOI DOI 10.1145/1273496.1273592
   Rao RPN, 1999, NAT NEUROSCI, V2, P79, DOI 10.1038/4580
   Rao RPN, 1997, NEURAL COMPUT, V9, P721, DOI 10.1162/neco.1997.9.4.721
   Rigamonti R, 2011, PROC CVPR IEEE, P1545, DOI 10.1109/CVPR.2011.5995313
   Salakhutdinov R, 2012, NEURAL COMPUT, V24, P1967, DOI 10.1162/NECO_a_00311
   Sanderson C., 2008, BIOMETRIC PERSON REC, P4
   Schwartz O, 2007, NAT REV NEUROSCI, V8, P522, DOI 10.1038/nrn2155
   Vemulapalli R, 2013, PROC CVPR IEEE, P1782, DOI 10.1109/CVPR.2013.233
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Wang RP, 2012, PROC CVPR IEEE, P2496, DOI 10.1109/CVPR.2012.6247965
   Wang RP, 2009, PROC CVPR IEEE, P429, DOI 10.1109/CVPRW.2009.5206850
   Wersing H, 2003, NEURAL COMPUT, V15, P1559, DOI 10.1162/089976603321891800
   Wiskott L, 2002, NEURAL COMPUT, V14, P715, DOI 10.1162/089976602317318938
   Yan KL, 2005, NEURAL COMPUT, V17, P397, DOI 10.1162/0899766053011474
   Zeiler MD, 2011, IEEE I CONF COMP VIS, P2018, DOI 10.1109/ICCV.2011.6126474
   Zeiler MD, 2010, PROC CVPR IEEE, P2528, DOI 10.1109/CVPR.2010.5539957
   Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x
   Zou W.Y., 2012, ADV NEURAL INFORM PR, P3212
NR 61
TC 4
Z9 4
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 2162-237X
EI 2162-2388
J9 IEEE T NEUR NET LEAR
JI IEEE Trans. Neural Netw. Learn. Syst.
PD SEP
PY 2015
VL 26
IS 9
BP 1992
EP 2004
DI 10.1109/TNNLS.2014.2360060
PG 13
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Computer Science, Theory & Methods; Engineering,
   Electrical & Electronic
SC Computer Science; Engineering
GA CQ2NH
UT WOS:000360437300012
PM 25376046
DA 2020-02-19
ER

PT J
AU Katsaggelos, AK
   Bahaadini, S
   Molina, R
AF Katsaggelos, Aggelos K.
   Bahaadini, Sara
   Molina, Rafael
TI Audiovisual Fusion: Challenges and New Approaches
SO PROCEEDINGS OF THE IEEE
LA English
DT Article
DE Audiovisual (AV) fusion; deep learning (DL); machine learning;
   multimodal analysis; multiview learning; stream asynchrony
ID STREAM WEIGHT ESTIMATION; SOURCE SEPARATION; SPEECH RECOGNITION; SPEAKER
   LOCALIZATION; GRAPHICAL MODEL; EVENT DETECTION; AUDIO; TRACKING;
   FEATURES; DATABASE
AB In this paper, we review recent results on audiovisual (AV) fusion. We also discuss some of the challenges and report on approaches to address them. One important issue in AV fusion is how the modalities interact and influence each other. This review will address this question in the context of AV speech processing, and especially speech recognition, where one of the issues is that the modalities both interact but also sometimes appear to desynchronize from each other. An additional issue that sometimes arises is that one of the modalities may be missing at test time, although it is available at training time; for example, it may be possible to collect AV training data while only having access to audio at test time. We will review approaches to address this issue from the area of multiview learning, where the goal is to learn a model or representation for each of the modalities separately while taking advantage of the rich multimodal training data. In addition to multiview learning, we also discuss the recent application of deep learning (DL) toward AV fusion. We finally draw conclusions and offer our assessment of the future in the area of AV fusion.
C1 [Katsaggelos, Aggelos K.; Bahaadini, Sara] Northwestern Univ, Dept Elect Engn & Comp Sci, Evanston, IL 60208 USA.
   [Molina, Rafael] Univ Granada, Dept Ciencias Computac & IA, E-18071 Granada, Spain.
RP Katsaggelos, AK (reprint author), Northwestern Univ, Dept Elect Engn & Comp Sci, Evanston, IL 60208 USA.
EM aggk@eecs.northwestern.edu
RI Katsaggelos, Aggelos K/B-7233-2009; Soriano, Rafael Molina/B-1849-2012
OI Soriano, Rafael Molina/0000-0003-4694-8588
FU U.S. Department of EnergyUnited States Department of Energy (DOE)
   [DE-NA0002520]; Spanish Ministry of Economy and Competitiveness
   [TIN2013-43880-R]
FX This work was supported in part by the U.S. Department of Energy under
   Grant DE-NA0002520 and by the Spanish Ministry of Economy and
   Competitiveness under Project TIN2013-43880-R.
CR Abdelaziz A. H., 2014, P IEEE INT C AC SPEE, DOI [10.1109/ICASSP.2014.6853853, DOI 10.1109/ICASSP.2014.6853853]
   Abdelaziz AH, 2014, INTERSPEECH, P1144
   Adams W., 1900, EURASIP J ADV SIG PR, V2003, P170
   Aleksc P., 2003, P WORKS MULT US AUTH, P80
   Aleksic P. S., 2009, VISUAL SPEECH RECOGN, P39
   Aleksic PS, 2006, P IEEE, V94, P2025, DOI 10.1109/JPROC.2006.886017
   Aleksic PS, 2002, EURASIP J APPL SIG P, V2002, P1213, DOI 10.1155/S1110865702206162
   ALEKSIC PS, 2003, [No title captured], P481
   Anderson B. D., 2012, OPTIMAL FILTERING
   Andrew G., 2013, P 30 INT C MACH LEAR, P1247
   [Anonymous], 2009, PERFORMANCE EVALUATI
   Atrey PK, 2010, MULTIMEDIA SYST, V16, P345, DOI 10.1007/s00530-010-0182-0
   Atrey PK, 2006, MULTIMEDIA SYST, V12, P239, DOI 10.1007/s00530-006-0063-8
   Bahaadini S., 2014, P INTERSPEECH, P2454
   Bailly-Bailliere E, 2003, LECT NOTES COMPUT SC, V2688, P625
   Barnard M, 2014, IEEE T MULTIMEDIA, V16, P864, DOI 10.1109/TMM.2014.2301977
   Beal MJ, 2003, IEEE T PATTERN ANAL, V25, P828, DOI 10.1109/TPAMI.2003.1206512
   Beddor PS, 2002, J PHONETICS, V30, P591, DOI 10.1006/jpho.2002.0177
   Beddor PS, 1999, J ACOUST SOC AM, V106, P2868, DOI 10.1121/1.428111
   Bengio S., 2004, Information Fusion, V5, P81, DOI 10.1016/j.inffus.2003.04.001
   Bengio S, 2003, LECT NOTES COMPUT SC, V2688, P770
   Benoit C., 1992, J COMMUNICATIONS, V43, P32
   Bilmes JA, 2005, IEEE SIGNAL PROC MAG, V22, P89, DOI 10.1109/MSP.2005.1511827
   Blum A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P92, DOI 10.1145/279943.279962
   Brand M, 1997, PROC CVPR IEEE, P994, DOI 10.1109/CVPR.1997.609450
   Bredin H., 2007, EURASIP J APPL SIG P, V2007, P179
   BREGLER C, 1994, INT CONF ACOUST SPEE, P669
   Bundy A., 1984, CATALOGUE ARTIFICIAL
   Campbell M., 2007, P TRECVID
   Casanovas A. L., 2011, THESIS EPFL LAUSANNE
   Casanovas A. Llagostera, 2012, IMAGE VIDEO RETRIEVA
   Casanovas AL, 2010, IEEE T MULTIMEDIA, V12, P358, DOI 10.1109/TMM.2010.2050650
   Cech J., 2013, P IEEE INT C HUM ROB, DOI [10.1109/HUMANOIDS.2013.7029977, DOI 10.1109/HUMANOIDS.2013.7029977]
   Chaisorn L, 2003, WORLD WIDE WEB, V6, P187, DOI 10.1023/A:1023622605600
   Chao Sui, 2012, 2012 46th Asilomar Conference on Signals, Systems and Computers, P1609, DOI 10.1109/ACSSC.2012.6489302
   Chaudhuri K., 2009, P 26 ANN INT C MACH, P129, DOI [10.1145/1553374.1553391, DOI 10.1145/1553374.1553391]
   Chen C., 1976, PATTERN RECOGNITION
   Chetty G., 2006, P NICTA HCSNET MULT, V57, P17
   Chibelushi C., 1996, SPEECH IMAGE PROCESS
   CHOW YL, 1990, INT CONF ACOUST SPEE, P701, DOI 10.1109/ICASSP.1990.115863
   Christoudias C. M., 2006, P 8 INT C MULT INT, P84
   Cooke M, 2006, J ACOUST SOC AM, V120, P2421, DOI 10.1121/1.2229005
   Corradini A., 2005, NATO SCI SER COMPUT, V198, P223
   Crisan D, 2002, IEEE T SIGNAL PROCES, V50, P736, DOI 10.1109/78.984773
   Cutler R, 2000, 2000 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, PROCEEDINGS VOLS I-III, P1589, DOI 10.1109/ICME.2000.871073
   Dean D. B., 2008, THESIS QUEENSLAND U
   Dean D, 2010, COMPUT SPEECH LANG, V24, P136, DOI 10.1016/j.csl.2009.03.007
   Deng L., 2014, DEEP LEARNING METHOD
   Dielmann A, 2007, IEEE T MULTIMEDIA, V9, P25, DOI 10.1109/TMM.2006.886337
   Dupont S, 2000, IEEE T MULTIMEDIA, V2, P141, DOI 10.1109/6046.865479
   Essid S., 2012, MULTIMODAL MUSIC PRO, P37
   Estellers V, 2012, IEEE T AUDIO SPEECH, V20, P1145, DOI 10.1109/TASL.2011.2172427
   FISHER J, 2000, ADV NEURAL INFORM PR, P772
   Fox N.A., 2005, P 5 INT C AUD VID BA
   Freund Y., 1999, Journal of Japanese Society for Artificial Intelligence, V14, P771
   Garcia-Salicetti S, 2003, LECT NOTES COMPUT SC, V2688, P845
   Garg A, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL III, PROCEEDINGS, P605
   Garofolo J., 2008, P AAAI FALL S MULT I, P3
   Gatica-Perez D, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 3, PROCEEDINGS, P25
   Gehrig T, 2005, IEEE WORK APPL SIG, P118, DOI 10.1109/ASPAA.2005.1540183
   Glodek M, 2011, LECT NOTES COMPUT SC, V6975, P359, DOI 10.1007/978-3-642-24571-8_47
   Gravier G, 2002, INT CONF ACOUST SPEE, P853
   Gurban Mihai, 2008, P 10 INT C MULT INT, P237
   Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814
   Heckmann M, 2002, EURASIP J APPL SIG P, V2002, P1260, DOI 10.1155/S1110865702206150
   HENKE W, 1966, THESIS MASSACHUSETTS
   Hernando J, 1997, INT CONF ACOUST SPEE, P1267, DOI 10.1109/ICASSP.1997.596176
   Hershey J, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL V, PROCEEDINGS, P649
   Hershey J., 2000, ADV NEURAL INFORM PR, V12
   Holzapfel Hartwig, 2004, P 6 INT C MULT INT, P175, DOI [10.1145/1027933.1027964, DOI 10.1145/1027933.1027964]
   Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.2307/2333955
   Hsu H., 2004, P IEEE INT C MULT EX, V2, P1091
   Huang J, 2013, INT CONF ACOUST SPEE, P7596, DOI 10.1109/ICASSP.2013.6639140
   Ivanov Y., 2005, ERROR WEIGHTED CLASS
   Iyengar G, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I, PROCEEDINGS, P329
   IYENGAR G, 2003, P 11 ACM INT C MULT, P255, DOI DOI 10.1145/957013.957065
   Jaffre G., 2006, INT WORKSH MULT MOD
   KANAK A, 2003, P IEEE INT C AC SPEE, P377
   KEATING P, 1988, PHONOLOGY, V5, P275, DOI DOI 10.1017/S095267570000230X
   Kilic V, 2015, IEEE T MULTIMEDIA, V17, P186, DOI 10.1109/TMM.2014.2377515
   Kim Y, 2013, INT CONF ACOUST SPEE, P3687, DOI 10.1109/ICASSP.2013.6638346
   Kittler J, 1998, PATTERN ANAL APPL, V1, P18, DOI 10.1007/BF01238023
   Konar A, 2015, EMOTION RECOGNITION: A PATTERN ANALYSIS APPROACH, P1, DOI 10.1002/9781118910566
   Krose B. J. A., 2006, P ACM C MULT INT, P201
   Lahat D, 2014, EUR SIGNAL PR CONF, P101
   Lai P L, 2000, Int J Neural Syst, V10, P365, DOI 10.1142/S012906570000034X
   Lathoud G, 2005, LECT NOTES COMPUT SC, V3361, P182
   Lee Bowon, 2004, P INTERSPEECH, P2489
   Lee H., 2008, ADV NEURAL INFORM PR, V20, P873
   Li BT, 2003, MULTIMEDIA SYST, V8, P512, DOI 10.1007/s00530-002-0069-9
   Liu Q., 2011, P SENS SIGN PROC DEF, P1, DOI DOI 10.1109/ICICS.2011.6174265
   Liu QJ, 2014, IEEE T MULTIMEDIA, V16, P1610, DOI 10.1109/TMM.2014.2322824
   Liu QJ, 2013, IEEE T SIGNAL PROCES, V61, P5520, DOI 10.1109/TSP.2013.2277834
   Livescu K, 2009, 2009 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION & UNDERSTANDING (ASRU 2009), P82, DOI 10.1109/ASRU.2009.5373462
   Loh AP, 2004, I C CONT AUTOMAT ROB, P1569
   Lucey S, 2001, PROCEEDINGS OF 2001 INTERNATIONAL SYMPOSIUM ON INTELLIGENT MULTIMEDIA, VIDEO AND SPEECH PROCESSING, P551, DOI 10.1109/ISIMP.2001.925455
   LUO RC, 1989, IEEE T SYST MAN CYB, V19, P901, DOI 10.1109/21.44007
   Lv G, 2007, IEEE INT SYM MULTIM, P37, DOI 10.1109/ISM.2007.21
   Makkook M., 2007, MULTIMODAKL SENSOR F
   Maragos P, 2008, MULTIMED SYST APPL, P1, DOI 10.1007/978-0-387-76316-3
   Marcheret E, 2007, INT CONF ACOUST SPEE, P945
   Martinez H. P., 2011, P 13 INT C MULT INT, P3
   Martinez Hector P, 2014, P 16 INT C MULT INT, P34
   McCowan I, 2005, IEEE T PATTERN ANAL, V27, P305, DOI 10.1109/TPAMI.2005.49
   MCGURK H, 1976, NATURE, V264, P746, DOI 10.1038/264746a0
   Messer Kieron, 1999, 2 INT C AUD VID BAS, V964, P965
   Metallinou A, 2012, IEEE T AFFECT COMPUT, V3, P184, DOI 10.1109/T-AFFC.2011.40
   Mihelic F., 2008, SPEECH RECOGNITION T
   Morency LP, 2010, AUTON AGENT MULTI-AG, V20, P70, DOI 10.1007/s10458-009-9092-y
   Narayanan S, 2000, IEEE T SPEECH AUDI P, V8, P328, DOI 10.1109/89.841215
   Nefian AV, 2002, EURASIP J APPL SIG P, V2002, P1274, DOI 10.1155/S1110865702206083
   Nefian AV, 2002, INT CONF ACOUST SPEE, P2013
   Nesime T., 2004, P 1 INT WORKSH DAT M, P24
   Ngiam J, 2011, IEEE INT C MACH LEAR, V28, P689, DOI DOI 10.1145/2647868
   Nickel K, 2005, P 7 INT C MULT INT I, P61
   Nock H. J., 2002, P 10 ACM INT C MULT, P303
   Nock HJ, 2003, LECT NOTES COMPUT SC, V2728, P488
   Noda K, 2015, APPL INTELL, V42, P722, DOI 10.1007/s10489-014-0629-7
   Noda K, 2014, ROBOT AUTON SYST, V62, P721, DOI 10.1016/j.robot.2014.03.003
   Patterson EK, 2002, INT CONF ACOUST SPEE, P2017
   Pigeon S, 1997, LECT NOTES COMPUT SC, V1206, P403
   POTAMIANOS G, 1998, ACOUST SPEECH SIG PR, P3733
   Potamianos G., 2004, ISSUES VISUAL AUDIO, V22, P23
   Quattoni A, 2007, IEEE T PATTERN ANAL, V29, P1848, DOI 10.1109/TPAMI.2007.1124
   Rajavel R, 2012, J SIGNAL PROCESS SYS, V68, P83, DOI 10.1007/s11265-011-0578-x
   Ribeiro M. I., 2004, KALMAN EXTENDED KALM, V43, P43
   Rivet B, 2007, IEEE T AUDIO SPEECH, V15, P96, DOI 10.1109/TASL.2006.872619
   Rivet B, 2014, IEEE SIGNAL PROC MAG, V31, P125, DOI 10.1109/MSP.2013.2296173
   Saenko K, 2006, IEEE W SP LANG TECH, P154, DOI 10.1109/SLT.2006.326841
   Saenko K, 2009, IEEE T PATTERN ANAL, V31, P1700, DOI 10.1109/TPAMI.2008.303
   Sanderson C., 2002, THE VIDTIMIT DATABAS
   Sargin M. E., 2006, P IEEE INT C AC SPEE, DOI [10.1109/ICASSP.2006.1660095, DOI 10.1109/ICASSP.2006.1660095]
   Sargin ME, 2007, IEEE T MULTIMEDIA, V9, P1396, DOI 10.1109/TMM.2007.906583
   Shah M, 2014, IEEE INT SYMP CIRC S, P754, DOI 10.1109/ISCAS.2014.6865245
   Shao X, 2008, SPEECH COMMUN, V50, P337, DOI 10.1016/j.specom.2007.11.002
   Shiell D. J., 2009, VISUAL SPEECH RECOGN, P1
   Shivappa ST, 2010, P IEEE, V98, P1692, DOI 10.1109/JPROC.2010.2057231
   Slaney M., 2000, ADV NEURAL INFORM PR, P814
   Snoek C. G. M., 2002, Proceedings 2002 IEEE International Conference on Multimedia and Expo (Cat. No.02TH8604), P21, DOI 10.1109/ICME.2002.1035364
   Snoek C. G. M., 2005, 13th Annual ACM International Conference on Multimedia, P399
   Sohn Kihyuk, 2014, ADV NEURAL INFORM PR, P2141
   Soonkyu Lee, 2002, PRICAI 2002: Trends in Artificial Intelligence. 7th Pacific Rim International Conference on Artificial Intelligence. Proceedings (Lecture Notes in Artificial Intelligence Vol.2417), P563
   Srivastava N., 2012, ADV NEURAL INFORM PR, P2222, DOI DOI 10.1109/CVPR.2013.49
   Stewart D, 2014, IEEE T CYBERNETICS, V44, P175, DOI 10.1109/TCYB.2013.2250954
   Strobel N, 2001, IEEE SIGNAL PROC MAG, V18, P22, DOI 10.1109/79.911196
   Talantzis F, 2006, 2006 IEEE WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P243, DOI 10.1109/MMSP.2006.285306
   Terry L., 2008, P INT C PATT REC, DOI [10.1109/ICPR.2008.4761927, DOI 10.1109/ICPR.2008.4761927]
   Terry L., 2011, THESIS NE U EVANSTON
   Terry L. H., 2010, P INTERSPEECH, P2682
   Terry LH, 2008, IEEE IMAGE PROC, P1316, DOI 10.1109/ICIP.2008.4712005
   Vermaak J, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P741, DOI 10.1109/ICCV.2001.937600
   Waibel A., 1990, READINGS SPEECH RECO
   Wang Y, 2000, IEEE SIGNAL PROC MAG, V17, P12, DOI 10.1109/79.888862
   Wu CH, 2014, APSIPA TRANS SIGNAL, V3, DOI 10.1017/ATSIP.2014.11
   Xu H, 2006, ACM T MULTIM COMPUT, V2, P44, DOI 10.1145/1126004.1126007
   Yan R., 2004, P 12 ANN ACM INT C M, P548
   Yehia H, 1998, SPEECH COMMUN, V26, P23, DOI 10.1016/S0167-6393(98)00048-X
   Yi Wu, 2004, P 12 ANN ACM INT C M, P572, DOI DOI 10.1145/1027527.1027665
   Zeng ZH, 2009, IEEE T PATTERN ANAL, V31, P39, DOI 10.1109/TPAMI.2008.52
   Zhao Y, 2012, INT J ADV ROBOT SYST, V9, DOI 10.5772/54000
   Zhiyong Wu, 2005, Advances in Biometrics. International Conference, ICB 2006. Proceedings (Lecture Notes in Computer Science Vol.3832), P493
   Zhou ZH, 2014, IMAGE VISION COMPUT, V32, P590, DOI 10.1016/j.imavis.2014.06.004
   Zotkin DN, 2002, EURASIP J APPL SIG P, V2002, P1154, DOI 10.1155/S1110865702206058
   Zou X., 2005, P IEEE COMP SOC C CO, V3, P4
   ZUE V, 1990, SPEECH COMMUN, V9, P351, DOI 10.1016/0167-6393(90)90010-7
NR 165
TC 29
Z9 30
U1 1
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 0018-9219
EI 1558-2256
J9 P IEEE
JI Proc. IEEE
PD SEP
PY 2015
VL 103
IS 9
SI SI
BP 1635
EP 1653
DI 10.1109/JPROC.2015.2459017
PG 19
WC Engineering, Electrical & Electronic
SC Engineering
GA CQ2YW
UT WOS:000360469500012
DA 2020-02-19
ER

PT J
AU Zhou, HM
   Huang, GB
   Lin, ZP
   Wang, H
   Soh, YC
AF Zhou, Hongming
   Huang, Guang-Bin
   Lin, Zhiping
   Wang, Han
   Soh, Yeng Chai
TI Stacked Extreme Learning Machines
SO IEEE TRANSACTIONS ON CYBERNETICS
LA English
DT Article
DE Deep learning; eigenvalue; extreme learning machine (ELM); feature
   mapping; principal component analysis (PCA); support vector machines
   (SVMs)
ID ALGORITHM
AB Extreme learning machine (ELM) has recently attracted many researchers' interest due to its very fast learning speed, good generalization ability, and ease of implementation. It provides a unified solution that can be used directly to solve regression, binary, and multiclass classification problems. In this paper, we propose a stacked ELMs (S-ELMs) that is specially designed for solving large and complex data problems. The S-ELMs divides a single large ELM network into multiple stacked small ELMs which are serially connected. The S-ELMs can approximate a very large ELM network with small memory requirement. To further improve the testing accuracy on big data problems, the ELM autoencoder can be implemented during each iteration of the S-ELMs algorithm. The simulation results show that the S-ELMs even with random hidden nodes can achieve similar testing accuracy to support vector machine (SVM) while having low memory requirements. With the help of ELM autoencoder, the S-ELMs can achieve much better testing accuracy than SVM and slightly better accuracy than deep belief network (DBN) with much faster training speed.
C1 [Zhou, Hongming; Huang, Guang-Bin; Lin, Zhiping; Wang, Han; Soh, Yeng Chai] Nanyang Technol Univ, Sch Elect & Elect Engn, Nanyang 639798, Singapore.
RP Zhou, HM (reprint author), Nanyang Technol Univ, Sch Elect & Elect Engn, Nanyang 639798, Singapore.
EM hmzhou@ntu.edu.sg
RI Wang, Han/A-5016-2011; Soh, Yeng Chai/A-5014-2011
OI Wang, Han/0000-0001-5448-9903; Soh, Yeng Chai/0000-0003-0624-2302
FU Singapore Academic Research Fund (AcRF) Tier 1 [RG 22/08 (M52040128)];
   Singapore's National Research FoundationSingapore National Research
   Foundation [NRF2011NRF-CRP001-090]
FX This work was supported in part by the grant from Singapore Academic
   Research Fund (AcRF) Tier 1 under Project RG 22/08 (M52040128), and in
   part by the Singapore's National Research Foundation under Grant
   NRF2011NRF-CRP001-090. This paper was recommended by Associate Editor X.
   Zeng.
CR [Anonymous], 2013, DIRECTORY PAGE TOP50
   [Anonymous], 2000, WHAT IS GPU COMPUTIN
   [Anonymous], 2007, NVIDIA CUDA ZONE
   Borthakur D., 2007, HADOOP DISTRIBUTED F
   BOURLARD H, 1988, BIOL CYBERN, V59, P291, DOI 10.1007/BF00332918
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Dean J., 2004, P 6 C S OP SYST DES, P10, DOI DOI 10.HTTP://DL.ACM.0RG/CITATI0N.CFM?
   Deng L., 2011, P INTERSPEECH, P2285
   Erhan D, 2010, J MACH LEARN RES, V11, P625
   Fung G., 2001, KDD-2001. Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P77
   Ghemawat S., 2003, Operating Systems Review, V37, P29
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Huang G.-B., 2010, EXTREME LEARNING MAC
   Huang GB, 2004, IEEE IJCNN, P985
   Huang GB, 2008, NEUROCOMPUTING, V71, P3460, DOI 10.1016/j.neucom.2007.10.008
   Huang GB, 2007, NEUROCOMPUTING, V70, P3056, DOI 10.1016/j.neucom.2007.02.009
   Huang GB, 2006, NEUROCOMPUTING, V70, P489, DOI 10.1016/j.neucom.2005.12.126
   Huang GB, 2006, IEEE T NEURAL NETWOR, V17, P879, DOI 10.1109/TNN.2006.875977
   Huang GB, 2012, IEEE T SYST MAN CY B, V42, P513, DOI 10.1109/TSMCB.2011.2168604
   HULL JJ, 1994, IEEE T PATTERN ANAL, V16, P550, DOI 10.1109/34.291440
   Japkowicz N, 2000, NEURAL COMPUT, V12, P531, DOI 10.1162/089976600300015691
   Jolliffe I., 1986, PRINCIPAL COMPONENT
   Kassel R., 2013, OCR DATASET
   Kasun LLC, 2013, IEEE INTELL SYST, V28, P31
   LeCun Y, 2004, PROC CVPR IEEE, P97
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y., 1998, MNIST DATABASE HANDW
   Lee Y.-J., 2001, P SIAM INT C DAT MIN, P7
   Liang NY, 2006, IEEE T NEURAL NETWOR, V17, P1411, DOI 10.1109/TNN.2006.880583
   Platt JC, 1999, ADVANCES IN KERNEL METHODS, P185
   Platt J, 1991, NEURAL COMPUT, V3, P213, DOI 10.1162/neco.1991.3.2.213
   Rao C.R., 1971, GEN INVERSE MATRICES
   Rumelhart D.E., 1986, PARALLEL DISTRIBUTED, V1, P318, DOI DOI 10.1016/B978-1-4832-1446-7.50035-2
   Salakhutdinov R., 2013, TRAINING DEEP AUTOEN
   Salakhutdinov Ruslan, 2010, INT C ART INT STAT, P693
   Serre D., 2002, MATRICES THEORY APPL
   Smith L., 2002, TUTORIAL PRINCIPAL C
   Smola Alex J., 2000, P 17 INT C MACH LEAR, P911
   Tsang IW, 2005, J MACH LEARN RES, V6, P363
   Vapnik V. N., 1998, STAT LEARNING THEORY
   Williams CKI, 2001, ADV NEUR IN, V13, P682
NR 42
TC 46
Z9 53
U1 5
U2 67
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 2168-2267
EI 2168-2275
J9 IEEE T CYBERNETICS
JI IEEE T. Cybern.
PD SEP
PY 2015
VL 45
IS 9
BP 2013
EP 2025
DI 10.1109/TCYB.2014.2363492
PG 13
WC Automation & Control Systems; Computer Science, Artificial Intelligence;
   Computer Science, Cybernetics
SC Automation & Control Systems; Computer Science
GA CP6SR
UT WOS:000360019000025
PM 25361517
DA 2020-02-19
ER

PT J
AU John, V
   Yoneda, K
   Liu, Z
   Mita, S
AF John, Vijay
   Yoneda, Keisuke
   Liu, Zheng
   Mita, Seiichi
TI Saliency Map Generation by the Convolutional Neural Network for
   Real-Time Traffic Light Detection Using Template Matching
SO IEEE TRANSACTIONS ON COMPUTATIONAL IMAGING
LA English
DT Article
DE Traffic Light Detection; Convolutional Neural Network; DBSCAN; Saliency
   Maps
ID PEDESTRIAN DETECTION; SIGN RECOGNITION; ROBUST; REPRESENTATION; VIDEO
AB A critical issue in autonomous vehicle navigation and advanced driver assistance systems (ADAS) is the accurate real-time detection of traffic lights. Typically, vision-based sensors are used to detect the traffic light. However, the detection of traffic lights using computer vision, image processing, and learning algorithms is not trivial. The challenges include appearance variations, illumination variations, and reduced appearance information in low illumination conditions. To address these challenges, we present a visual camera-based real-time traffic light detection algorithm, where we identify the spatially constrained regionof-interest in the image containing the traffic light. Given, the identified region-of-interest, we achieve high traffic light detection accuracy with few false positives, even in adverse environments. To perform robust traffic light detection in varying conditions with few false positives, the proposed algorithm consists of two steps, an offline saliency map generation and a real-time traffic light detection. In the offline step, a convolutional neural network, i.e., a deep learning framework, detects and recognizes the traffic lights in the image using region-of-interest information provided by an onboard GPS sensor. The detected traffic light information is then used to generate the saliency maps with a modified multidimensional density-based spatial clustering of applications with noise (M-DBSCAN) algorithm. The generated saliency maps are indexed using the vehicle GPS information. In the real-time step, traffic lights are detected by retrieving relevant saliencymaps and performing template matching by using colour information. The proposed algorithm is validated with the datasets acquired in varying conditions and different countries, e.g., USA, Japan, and France. The experimental results report a high detection accuracy with negligible false positives under varied illumination conditions. More importantly, an average computational time of 10 ms/frame is achieved. A detailed parameter analysis is conducted and the observations are summarized and reported in this paper.
C1 [John, Vijay; Liu, Zheng] Toyota Technol Inst, Intelligent Informat Proc Lab, Nagoya, Aichi 4688511, Japan.
   [Yoneda, Keisuke; Mita, Seiichi] Toyota Technol Inst, Res Ctr Smart Vehicles, Nagoya, Aichi 4688511, Japan.
RP John, V (reprint author), Toyota Technol Inst, Intelligent Informat Proc Lab, Nagoya, Aichi 4688511, Japan.
EM vijayjohn@toyota-ti.ac.jp; yoneda@toyota-ti.ac.jp;
   zhengliu@toyota-ti.ac.jp; smita@toyota-ti.ac.jp
RI Liu, Zheng/D-8678-2016
OI Liu, Zheng/0000-0002-7241-3483
FU Toyota Technological Institute through the Research Centre of Smart
   Vehicles
FX The work is supported by the Toyota Technological Institute through the
   Research Centre of Smart Vehicles. The authors would also like to thank
   Dr. Greg Shakhnarovich at Toyota Technological Institute Chicago for
   helpful discussion.
CR Alvarez JM, 2012, LECT NOTES COMPUT SC, V7578, P376, DOI 10.1007/978-3-642-33786-4_28
   Boureau YL, 2011, IEEE I CONF COMP VIS, P2651, DOI 10.1109/ICCV.2011.6126555
   Chiang CC, 2011, INT J INNOV COMPUT I, V7, P6919
   Chung Y. C., 2002, J TAIWAN NORMAL U MA, V47, P67, DOI DOI 10.6301/JNTNU.2002.47(1).04
   Chunhe Y., 2013, ADV INTELL SOFT COMP, V163, P745
   Chunzhao Guo, 2010, 2010 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2010), P5543, DOI 10.1109/IROS.2010.5650695
   Cieslak DA, 2012, DATA MIN KNOWL DISC, V24, P136, DOI 10.1007/s10618-011-0222-1
   Costa M, 2014, TRANSPORT RES F-TRAF, V23, P147, DOI 10.1016/j.trf.2014.01.003
   de Charette R, 2009, 2009 IEEE-RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, P333, DOI 10.1109/IROS.2009.5353941
   de Charette R, 2009, IEEE INT VEH SYM, P358, DOI 10.1109/IVS.2009.5164304
   Diaz-Cabrera M., 2012, P INT C INT TRANSP S, P315
   Dollar P., 2010, P BRIT MACHINE VISIO, P1, DOI DOI 10.5244/C.24.68
   Dollar P, 2012, IEEE T PATTERN ANAL, V34, P743, DOI 10.1109/TPAMI.2011.155
   Enzweiler M, 2009, IEEE T PATTERN ANAL, V31, P2179, DOI 10.1109/TPAMI.2008.260
   Ester M., 1996, KDD-96 Proceedings. Second International Conference on Knowledge Discovery and Data Mining, P226
   Fairfield N., 2011, 2011 IEEE International Conference on Robotics and Automation (ICRA 2011), P5421, DOI 10.1109/ICRA.2011.5980164
   Fang CY, 2003, PROC CVPR IEEE, P750
   Fang YJ, 2004, IEEE T VEH TECHNOL, V53, P1679, DOI 10.1109/TVT.2004.834875
   Gevers T, 1999, PATTERN RECOGN, V32, P453, DOI 10.1016/S0031-3203(98)00036-3
   Gong JW, 2010, IEEE INT VEH SYM, P431, DOI 10.1109/IVS.2010.5548083
   Guo CZ, 2011, IEEE INT VEH SYM, P715, DOI 10.1109/IVS.2011.5940502
   Hwang TH, 2006, LECT NOTES COMPUT SC, V4319, P682
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   John V, 2014, 2014 IEEE 17TH INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS (ITSC), P2286, DOI 10.1109/ITSC.2014.6958056
   Kim H. K., 2013, WORLD ACAD SCI ENG T, V7, P454
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Levinson Jesse, 2011, 2011 IEEE International Conference on Robotics and Automation, P5784
   Liu HP, 2014, INFORM SCIENCES, V266, P75, DOI 10.1016/j.ins.2014.01.010
   Liu Q, 2013, INFRARED PHYS TECHN, V60, P288, DOI 10.1016/j.infrared.2013.06.003
   Long Q., 2014, P BRIT MACH VIS C
   Lutz Priese, 1993, P INT VEH S TOK JAP, P95
   Niknejad HT, 2012, IEEE T INTELL TRANSP, V13, P748, DOI 10.1109/TITS.2012.2187894
   Niknejad HT, 2011, IEEE INT C INT ROBOT, P4442, DOI 10.1109/IROS.2011.6048502
   Omachi M, 2009, 2009 2ND IEEE INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND INFORMATION TECHNOLOGY, VOL 2, P284, DOI 10.1109/ICCSIT.2009.5234518
   Park Jin-Hyung, 2009, INT J SIGNAL PROCESS, V2, P1
   Qi B, 2014, IEEE COMPUT SOC CONF, P274, DOI 10.1109/CVPRW.2014.49
   Ruta A, 2010, PATTERN RECOGN, V43, P416, DOI 10.1016/j.patcog.2009.05.018
   Sermanet P., 2014, P INT C LEARN REPR, P1, DOI DOI 10.1016/J.VISRES.2006.11.009
   Sermanet P, 2013, PROC CVPR IEEE, P3626, DOI 10.1109/CVPR.2013.465
   Sermanet P, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P2809, DOI 10.1109/IJCNN.2011.6033589
   Shen YH, 2009, IEEE INT VEH SYM, P521, DOI 10.1109/IVS.2009.5164332
   Siogkas George, 2012, Proceedings of the International Conference on Computer Vision Theory and Applications. VISAPP 2012, P620
   Szlam A, 2012, LECT NOTES COMPUT SC, V7576, P200, DOI 10.1007/978-3-642-33715-4_15
   Yang X., 2008, INT J INFORM ACQUISI, V5, P149
   Yelal M R, 2006, P IEEE INT C VID SIG, P67, DOI DOI 10.1109/AVSS.2006.34
   Yu CH, 2010, 2010 IEEE 10TH INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING PROCEEDINGS (ICSP2010), VOLS I-III, P821, DOI 10.1109/ICOSP.2010.5655934
   Yu Z, 2007, IEEE SYMP COMP COMMU, P1, DOI 10.1109/ICEMI.2007.4350404
   Zakir U, 2010, P INT C COMP GRAPH I, P72
   Zaklouta F, 2014, ROBOT AUTON SYST, V62, P16, DOI 10.1016/j.robot.2012.07.019
NR 49
TC 20
Z9 21
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 2333-9403
J9 IEEE T COMPUT IMAG
JI IEEE Trans. Comput. Imaging
PD SEP
PY 2015
VL 1
IS 3
BP 159
EP 173
DI 10.1109/TCI.2015.2480006
PG 15
WC Engineering, Electrical & Electronic; Imaging Science & Photographic
   Technology
SC Engineering; Imaging Science & Photographic Technology
GA V3N8G
UT WOS:000218424900002
DA 2020-02-19
ER

PT J
AU Sprechmann, P
   Bronstein, AM
   Sapiro, G
AF Sprechmann, P.
   Bronstein, A. M.
   Sapiro, G.
TI Learning Efficient Sparse and Low Rank Models
SO IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
LA English
DT Article
DE Parsimonious modeling; sparse and low-rank models; NMF; deep learning;
   real-time implementations; big data; proximal methods
ID SINGING-VOICE SEPARATION; THRESHOLDING ALGORITHM; CONVERGENCE;
   REGRESSION; SHRINKAGE; SELECTION
AB Parsimony, including sparsity and low rank, has been shown to successfully model data in numerous machine learning and signal processing tasks. Traditionally, such modeling approaches rely on an iterative algorithm that minimizes an objective function with parsimony-promoting terms. The inherently sequential structure and data-dependent complexity and latency of iterative optimization constitute a major limitation in many applications requiring real-time performance or involving large-scale data. Another limitation encountered by these modeling techniques is the difficulty of their inclusion in discriminative learning scenarios. In this work, we propose to move the emphasis from the model to the pursuit algorithm, and develop a process-centric view of parsimonious modeling, in which a learned deterministic fixed-complexity pursuit process is used in lieu of iterative optimization. We show a principled way to construct learnable pursuit process architectures for structured sparse and robust low rank models, derived from the iteration of proximal descent algorithms. These architectures learn to approximate the exact parsimonious representation at a fraction of the complexity of the standard optimization methods. We also show that appropriate training regimes allow to naturally extend parsimonious models to discriminative settings. State-of-the-art results are demonstrated on several challenging problems in image and audio processing with several orders of magnitude speed-up compared to the exact optimization algorithms.
C1 [Sprechmann, P.; Sapiro, G.] Duke Univ, Dept Elect & Comp Engn, Durham, NC 27708 USA.
   [Bronstein, A. M.] Tel Aviv Univ, Sch Elect Engn, IL-69978 Tel Aviv, Israel.
RP Sprechmann, P (reprint author), Duke Univ, ECE Dept, Durham, NC 27708 USA.
EM pablo.sprechmann@duke.edu; bron@eng.tau.ac.il; guillermo.sapiro@duke.edu
FU US National Science Foundation (NSF)National Science Foundation (NSF);
   ONROffice of Naval Research; NGA; DARPAUnited States Department of
   DefenseDefense Advanced Research Projects Agency (DARPA); AFOSRUnited
   States Department of DefenseAir Force Office of Scientific Research
   (AFOSR); ARO; BSFUS-Israel Binational Science Foundation; ERCEuropean
   Research Council (ERC) [335491]
FX This work was partially supported by the US National Science Foundation
   (NSF), ONR, NGA, DARPA, AFOSR, ARO, and BSF. Pablo Sprechmann is the
   corresponding author. Alex Bronstein is supported by the ERC StG 335491
   (RAPID). The authors would like to thank Dr. Alexey Castrodad for his
   insightful discussion and for providing the action recognition code and
   experimental setting.
CR Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   Bach F, 2012, OPTIMIZATION FOR MACHINE LEARNING, P19
   Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Bertsekas DP, 1999, NONLINEAR PROGRAMMIN
   Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16
   Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970
   Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Castrodad A, 2012, INT J COMPUT VISION, V100, P1, DOI 10.1007/s11263-012-0534-7
   Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010
   COHEN JE, 1993, LINEAR ALGEBRA APPL, V190, P149, DOI 10.1016/0024-3795(93)90224-C
   Daubechies I, 2004, COMMUN PUR APPL MATH, V57, P1413, DOI 10.1002/cpa.20042
   FRIEDMAN J, 2010, NOTE GROUP LASSO SPA
   Goodfellow I., 2009, ADV NEURAL INFORM PR, V22, P646
   Gregor K., 2010, P 27 INT C MACH LEAR, P399
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hsu CL, 2010, IEEE T AUDIO SPEECH, V18, P310, DOI 10.1109/TASL.2009.2026503
   Huang PS, 2012, INT CONF ACOUST SPEE, P57, DOI 10.1109/ICASSP.2012.6287816
   JACOB L, 2009, P 26 ANN INT C MACH, P433, DOI DOI 10.1145/1553374.1553431
   Jarrett K, 2009, IEEE I CONF COMP VIS, P2146, DOI 10.1109/ICCV.2009.5459469
   Jenatton R, 2011, J MACH LEARN RES, V12, P2297
   Jolliffe I, 2002, PRINCIPAL COMPONENT
   Kavukcuoglu K., 2010, ARXIV10103467
   Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565
   Li YY, 2009, INVERSE PROBL IMAG, V3, P487, DOI 10.3934/ipi.2009.3.487
   Lijun Zhang, 2011, Frontiers of Electrical and Electronic Engineering in China, V6, P192, DOI 10.1007/s11460-011-0128-0
   Mairal J., 2009, P 26 ANN INT C MACH, P689, DOI DOI 10.1145/1553374.1553463
   Mardani M, 2013, IEEE T SIGNAL PROCES, V61, P5374, DOI 10.1109/TSP.2013.2279080
   Mateos G., 2011, ARXIVORG1111788
   Nesterov Y, 2013, MATH PROGRAM, V140, P125, DOI 10.1007/s10107-012-0629-5
   Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0
   Peng YG, 2010, PROC CVPR IEEE, P763, DOI 10.1109/CVPR.2010.5540138
   Randen T, 1999, IEEE T PATTERN ANAL, V21, P291, DOI 10.1109/34.761261
   Recht B, 2013, MATH PROGRAM COMPUT, V5, P201, DOI 10.1007/s12532-013-0053-8
   Recht B, 2010, SIAM REV, V52, P471, DOI 10.1137/070697835
   Sprechmann P., 2012, P 29 INT C MACH LEAR, V1, P615
   Sprechmann P., 2012, ISMIR, P67
   Sprechmann P, 2013, P ADV NEUR INF PROC, P908
   Sprechmann P, 2011, INT CONF ACOUST SPEE, P5816
   Sprechmann P, 2011, IEEE T SIGNAL PROCES, V59, P4183, DOI 10.1109/TSP.2011.2157912
   Srebro N., 2005, P 18 ANN C LEARN THE, P599
   Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267
   Tseng P, 2001, J OPTIMIZ THEORY APP, V109, P475, DOI 10.1023/A:1017501703105
   VAPNIK VN, 1971, THEOR PROBAB APPL+, V16, P264, DOI 10.1137/1116025
   Vincent E, 2006, IEEE T AUDIO SPEECH, V14, P1462, DOI 10.1109/TSA.2005.858005
   Xu H, 2012, IEEE T INFORM THEORY, V58, P3047, DOI 10.1109/TIT.2011.2173156
   Yadong Mu, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2609, DOI 10.1109/CVPR.2011.5995369
   Yuan M, 2006, J ROY STAT SOC B, V68, P49, DOI 10.1111/j.1467-9868.2005.00532.x
   Zhao P, 2009, ANN STAT, V37, P3468, DOI 10.1214/07-AOS584
NR 48
TC 47
Z9 47
U1 0
U2 44
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 0162-8828
EI 1939-3539
J9 IEEE T PATTERN ANAL
JI IEEE Trans. Pattern Anal. Mach. Intell.
PD SEP
PY 2015
VL 37
IS 9
BP 1821
EP 1833
DI 10.1109/TPAMI.2015.2392779
PG 13
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
SC Computer Science; Engineering
GA CO5RQ
UT WOS:000359216600007
PM 26353129
OA Bronze
DA 2020-02-19
ER

PT J
AU Sokolov, Y
   Kozma, R
   Werbos, LD
   Werbos, PJ
AF Sokolov, Yury
   Kozma, Robert
   Werbos, Ludmilla D.
   Werbos, Paul J.
TI Complete stability analysis of a heuristic approximate dynamic
   programming control design
SO AUTOMATICA
LA English
DT Article
DE Adaptive dynamic programming; Action-Dependent Heuristic Dynamic
   Programming; Adaptive control; Adaptive critic; Neural network; Gradient
   descent; Lyapunov function
ID SYSTEMS
AB This paper provides new stability results for Action-Dependent Heuristic Dynamic Programming (ADHDP), using a control algorithm that iteratively improves an internal model of the external world in the autonomous system based on its continuous interaction with the environment. We extend previous results for ADHDP control to the case of general multi-layer neural networks with deep learning across all layers. In particular, we show that the introduced control approach is uniformly ultimately bounded (UUB) under specific conditions on the learning rates, without explicit constraints on the temporal discount factor. We demonstrate the benefit of our results to the control of linear and nonlinear systems, including the cart-pole balancing problem. Our results show significantly improved learning and control performance as compared to the state-of-the-art. (C) 2015 Elsevier Ltd. All rights reserved.
C1 [Sokolov, Yury] Univ Memphis, Dept Math Sci, Math, Memphis, TN 38152 USA.
   [Kozma, Robert] Univ Memphis, Dept Math Sci, Memphis, TN 38152 USA.
   [Werbos, Ludmilla D.; Werbos, Paul J.] IntControl LLC, Arlington, VA USA.
   [Werbos, Ludmilla D.; Werbos, Paul J.] Univ Memphis, CLION, Memphis, TN 38152 USA.
RP Sokolov, Y (reprint author), Univ Memphis, Dept Math Sci, Math, Memphis, TN 38152 USA.
EM ysokolov@memphis.edu; rkozma@memphis.edu; l.dalmat@gmail.edu;
   werbos@ieee.org
RI Kozma, Robert/C-6365-2013
OI Kozma, Robert/0000-0001-7011-5768; Werbos, Paul/0000-0002-1288-3387
FU National Science Foundation (NSF)National Science Foundation (NSF)
   [DMS-13-11165]
FX This research was supported in part by National Science Foundation (NSF)
   to Robert Kozma in the CRCNS Program, grant #DMS-13-11165. The material
   in this paper was partially presented at the 2013 International Joint
   Conference on Awareness Science and Technology, November 2-4, 2013,
   Aizu-Wakamatsu, Japan. This paper was recommended for publication in
   revised form by Associate Editor Raul Ordonez under the direction of
   Editor Miroslav Krstic.
CR Abu-Khalaf M, 2005, AUTOMATICA, V41, P779, DOI 10.1016/j.automatica.2004.11.034
   Al-Tamimi A, 2007, AUTOMATICA, V43, P473, DOI 10.1016/j.automatica.2006.09.019
   BARRON AR, 1993, IEEE T INFORM THEORY, V39, P930, DOI 10.1109/18.256500
   BARRON AR, 1994, MACH LEARN, V14, P115, DOI 10.1023/A:1022650905902
   BARTO AG, 1983, IEEE T SYST MAN CYB, V13, P834, DOI 10.1109/TSMC.1983.6313077
   Bertsekas D P, 1996, NEURODYNAMIC PROGRAM
   Bryson A. E., 1975, APPL OPTIMAL CONTROL
   He H., 2011, SELF ADAPTIVE SYSTEM
   Lendaris GG, 2009, NEURAL NETWORKS, V22, P822, DOI 10.1016/j.neunet.2009.06.021
   Lewis FL, 2012, REINFORCEMENT LEARNI
   Liu F, 2012, NEURAL NETWORKS, V32, P229, DOI 10.1016/j.neunet.2012.02.005
   MICHEL A, 2008, [No title captured]
   Powell W., 2011, APPROXIMATE DYNAMIC
   Prokhorov DV, 1997, IEEE T NEURAL NETWOR, V8, P997, DOI 10.1109/72.623201
   Sarangapani J., 2006, NEURAL NETWORK CONTR
   Si J., 2004, HDB LEARNING APPROXI
   Sokolov Y, 2013, 2013 INTERNATIONAL JOINT CONFERENCE ON AWARENESS SCIENCE AND TECHNOLOGY & UBI-MEDIA COMPUTING (ICAST-UMEDIA), P41, DOI 10.1109/ICAwST.2013.6765406
   VALENTI M, 2007, THESIS MIT
   Venayagamoorthy GK, 2003, IEEE T IND APPL, V39, P382, DOI 10.1109/TIA.2003.809438
   Vrabie Draguna, 2009, Proceedings 2009 International Joint Conference on Neural Networks (IJCNN 2009 - Atlanta), P3224, DOI 10.1109/IJCNN.2009.5178964
   Wang D, 2012, AUTOMATICA, V48, P1825, DOI 10.1016/j.automatica.2012.05.049
   Werbos P., 1974, THESIS HARVARD U CAM
   Werbos P. J., 1992, HDB INTELLIGENT CONT
   Werbos P.J., 2012, STABLE ADAPTIVE CONT
   WERBOS PJ, 1990, NEURAL NETWORKS, V3, P179, DOI 10.1016/0893-6080(90)90088-3
   WHITE DA, 1992, HDB INTELLIGENT CONT, pCH3
   Zhang H, 2013, COMMUN CONTROL ENG, P1, DOI 10.1007/978-1-4471-4757-2
   Zhang HG, 2011, AUTOMATICA, V47, P207, DOI 10.1016/j.automatica.2010.10.033
   [张吉烈 Zhang Jilie], 2013, [自动化学报, Acta Automatica Sinica], V39, P142
NR 29
TC 42
Z9 43
U1 2
U2 33
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0005-1098
EI 1873-2836
J9 AUTOMATICA
JI Automatica
PD SEP
PY 2015
VL 59
BP 9
EP 18
DI 10.1016/j.automatica.2015.06.001
PG 10
WC Automation & Control Systems; Engineering, Electrical & Electronic
SC Automation & Control Systems; Engineering
GA CO3AN
UT WOS:000359028700002
OA Other Gold
DA 2020-02-19
ER

PT J
AU Zhao, XY
   Li, X
   Zhang, ZF
AF Zhao, Xueyi
   Li, Xi
   Zhang, Zhongfei
TI Multimedia Retrieval via Deep Learning to Rank
SO IEEE SIGNAL PROCESSING LETTERS
LA English
DT Article
DE Deep neural network; joint learning; latent variable; learning to rank;
   structural SVM
AB Many existing learning-to-rank approaches are incapable of effectively modeling the intrinsic interaction relationships between the feature-level and ranking-level components of a ranking model. To address this problem, we propose a novel joint learning-to-rank approach called Deep Latent Structural SVM (DL-SSVM), which jointly learns deep neural networks and latent structural SVM (connected by a set of latent feature grouping variables) to effectively model the interaction relationships at two levels (i.e., feature-level and ranking-level). To make the joint learning problem easier to optimize, we present an effective auxiliary variable-based alternating optimization approach with respect to deep neural network learning and structural latent SVM learning. Experimental results on several challenging datasets have demonstrated the effectiveness of the proposed learning to rank approach in real-world information retrieval.
C1 [Zhao, Xueyi; Zhang, Zhongfei] Zhejiang Univ, Dept Informat Sci & Elect Engn, Hangzhou 310003, Zhejiang, Peoples R China.
   [Li, Xi] Zhejiang Univ, Coll Comp Sci, Hangzhou 310003, Zhejiang, Peoples R China.
RP Li, X (reprint author), Zhejiang Univ, Coll Comp Sci, Hangzhou 310003, Zhejiang, Peoples R China.
EM xuey-izhao@zju.edu.cn; xilizju@zju.edu.cn; zhongfei@zju.edu.cn
RI Li, Xi/L-1234-2013
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61472353]; China Knowledge Centre for Engineering
   Sciences and Technology; Fundamental Research Funds for the Central
   UniversitiesFundamental Research Funds for the Central Universities;
   National Basic Research Program of ChinaNational Basic Research Program
   of China [2012CB316400]; Zhejiang Provincial Engineering Center on Media
   Data Cloud Processing and Analysis; U.S. National Science
   FoundationNational Science Foundation (NSF) [CCF-1017828]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61472353, the China Knowledge Centre for
   Engineering Sciences and Technology, the Fundamental Research Funds for
   the Central Universities, the National Basic Research Program of China
   under Grant 2012CB316400, and by the Zhejiang Provincial Engineering
   Center on Media Data Cloud Processing and Analysis. The work of Z. Zhang
   was also supported by the U.S. National Science Foundation under Grant
   CCF-1017828.
CR Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016
   Burges CJ, 2010, LEARNING, V11, P23
   Can EF, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P1035, DOI 10.1145/2600428.2609503
   Cao Z., 2007, P 24 INT C MACH LEAR, P129, DOI DOI 10.1145/1273496.1273513
   Chen C., 2012, P 20 ACM INT C MULT, P713, DOI DOI 10.1145/2393347.2396294
   Chen D., 2011, P NIPS WORKSH OPT MA
   CLEMENCON S., 2014, P INT C MACH LEARN, P343
   Coates Adam, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P561, DOI 10.1007/978-3-642-35289-8_30
   Feng J., 2011, P IEEE C COMP VIS PA, P2609
   Freund Y, 2004, J MACH LEARN RES, V4, P933, DOI 10.1162/1532443041827916
   Fujiwara Y., 2013, P ASS ADV ARTIF INTE
   Gao W., 2013, P INT JOINT C ART IN, P1337
   Joachims T., 2002, P 8 ACM SIGKDD INT C, P133, DOI DOI 10.1145/775047.775067
   Jun Xu, 2007, 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P391
   Leng C, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P1031
   Pan W., 2013, P 23 INT JOINT C ART, V13, P2691
   Sharmanska V, 2013, IEEE I CONF COMP VIS, P825, DOI 10.1109/ICCV.2013.107
   Weston J., 2012, ARXIV12064603
   Xia Fen, 2008, P 25 INT C MACH LEAR, P1192, DOI DOI 10.1145/1390156.1390306.ACCESSED:2012
   Yisong Yue, 2007, 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P271
   Yu C. N. J., 2009, P 26 ANN INT C MACH, P1169, DOI DOI 10.1145/1553374.1553523
   Zheng L, 2013, IEEE SIGNAL PROC LET, V20, P391, DOI 10.1109/LSP.2013.2249513
   Zhou Y, 2013, IEEE SIGNAL PROC LET, V20, P335, DOI 10.1109/LSP.2013.2246513
NR 24
TC 7
Z9 11
U1 2
U2 26
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1070-9908
EI 1558-2361
J9 IEEE SIGNAL PROC LET
JI IEEE Signal Process. Lett.
PD SEP
PY 2015
VL 22
IS 9
BP 1487
EP 1491
DI 10.1109/LSP.2015.2410134
PG 5
WC Engineering, Electrical & Electronic
SC Engineering
GA CN7FW
UT WOS:000358600600013
DA 2020-02-19
ER

PT J
AU Droniou, A
   Ivaldi, S
   Sigaud, O
AF Droniou, Alain
   Ivaldi, Serena
   Sigaud, Olivier
TI Deep unsupervised network for multimodal perception, representation and
   classification
SO ROBOTICS AND AUTONOMOUS SYSTEMS
LA English
DT Article
DE Unsupervised learning; Multimodal perception; Deep learning;
   Developmental robotics
ID INTEGRATION; DIMENSIONALITY; ARCHITECTURES; RECOGNITION
AB In this paper, we tackle the problem of multimodal learning for autonomous robots. Autonomous robots interacting with humans in an evolving environment need the ability to acquire knowledge from their multiple perceptual channels in an unsupervised way. Most of the approaches in the literature exploit engineered methods to process each perceptual modality. In contrast, robots should be able to acquire their own features from the raw sensors, leveraging the information elicited by interaction with their environment: learning from their sensorimotor experience would result in a more efficient strategy in a life-long perspective. To this end, we propose an architecture based on deep networks, which is used by the humanoid robot iCub to learn a task from multiple perceptual modalities (proprioception, vision, audition). By structuring high-dimensional, multimodal information into a set of distinct sub-manifolds in a fully unsupervised way, it performs a substantial dimensionality reduction by providing both a symbolic representation of data and a fine discrimination between two similar stimuli. Moreover, the proposed network is able to exploit multimodal correlations to improve the representation of each modality alone. (C) 2014 Elsevier B.V. All rights reserved.
C1 [Droniou, Alain; Ivaldi, Serena; Sigaud, Olivier] Univ Paris 06, Sorbonne Univ, ISIR, UMR7222, F-75005 Paris, France.
   [Droniou, Alain; Ivaldi, Serena; Sigaud, Olivier] CNRS, Inst Syst Intelligents & Robot, UMR7222, Paris, France.
   [Ivaldi, Serena] Tech Univ Darmstadt, Intelligent Autonomous Syst Lab, FB Informat, Darmstadt, Germany.
   [Ivaldi, Serena] Inria, F-54600 Villers Les Nancy, France.
RP Droniou, A (reprint author), Univ Paris 06, Inst Syst Intelligents & Robot, Pyramide T55-65,CC 173-4 Pl Jussieu, F-75005 Paris, France.
EM droniou@isir.upmc.fr; serena.ivaldi@inria.fr; olivier.sigaud@upmc.fr
RI Peters, Jan/P-6027-2019
OI Peters, Jan/0000-0002-5266-8091
CR Baranes A., 2011, P 2011 IEEE INT C DE, V2, P1
   Barsalou L.W., 1997, CONCEPTUAL STRUCTURE
   Barsalou LW, 1999, BEHAV BRAIN SCI, V22, P577, DOI 10.1017/S0140525X99532147
   BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181
   Bengio Y., 2012, ARXIV, V1206, P1, DOI DOI 10.1145/1756006.1756025
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Botvinick M, 1998, NATURE, V391, P756, DOI 10.1038/35784
   Calandra Roberto, 2012, Artificial Neural Networks and Machine Learning - ICANN 2012. 22nd International Conference on Artificial Neural Networks, P379, DOI 10.1007/978-3-642-33266-1_47
   Cayton L, 2005, TECHNICAL REPORT
   Ciliberto C, 2013, IEEE INT C INT ROBOT, P3759, DOI 10.1109/IROS.2013.6696893
   Ciresan Dan Claudiu, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P581, DOI 10.1007/978-3-642-35289-8_31
   Damasio AR, 1989, NEURAL COMPUT, V1, P123, DOI 10.1162/neco.1989.1.1.123
   DAMASIO AR, 1990, TRENDS NEUROSCI, V13, P95, DOI 10.1016/0166-2236(90)90184-C
   DAMASIO AR, 1989, COGNITION, V33, P25, DOI 10.1016/0010-0277(89)90005-X
   de Sa VR, 1998, NEURAL COMPUT, V10, P1097, DOI 10.1162/089976698300017368
   Delalleau O., 2011, NIPS, P666
   DESA V, 1997, PSYCHOL LEARN MOTIV, V36, P309
   Erhan D., 2009, AISTAT, V5
   Falchier A, 2002, J NEUROSCI, V22, P5749
   Fort A, 2002, COGNITIVE BRAIN RES, V14, P20, DOI 10.1016/S0926-6410(02)00058-7
   Freeman E, 2003, CURR BIOL, V13, P985, DOI 10.1016/S0960-9822(03)00333-6
   French R.M., 1991, ENCY COGN SCI
   Giard MH, 1999, J COGNITIVE NEUROSCI, V11, P473, DOI 10.1162/089892999563544
   Glorot X., 2010, JLMR P TRACK, p[249, 2010], DOI DOI 10.1177/1753193409103364.
   Goldstone RL, 1998, COGNITION, V65, P231, DOI 10.1016/S0010-0277(97)00047-4
   Goldstone RL, 2010, WIRES COGN SCI, V1, P69, DOI 10.1002/wcs.26
   Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947
   Hermans  M., 2013, ADV NEURAL INFORM PR, V1, P190
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   HORNIK K, 1989, NEURAL NETWORKS, V2, P359, DOI 10.1016/0893-6080(89)90020-8
   HUBERT L, 1985, J CLASSIF, V2, P193, DOI 10.1007/BF01908075
   Ivaldi S., 2011, 2011 11th IEEE-RAS International Conference on Humanoid Robots (Humanoids 2011), P521, DOI 10.1109/Humanoids.2011.6100813
   Ivaldi S, 2014, IEEE T AUTON MENT DE, V6, P56, DOI 10.1109/TAMD.2013.2280614
   Jain AK, 1999, ACM COMPUT SURV, V31, P264, DOI 10.1145/331499.331504
   Johnsson M, 2009, IJCCI 2009: PROCEEDINGS OF THE INTERNATIONAL JOINT CONFERENCE ON COMPUTATIONAL INTELLIGENCE, P363
   JOYCE D, 2003, [No title captured], P147
   KOHONEN T, 1982, BIOL CYBERN, V43, P59, DOI 10.1007/BF00337288
   Kouropteva O, 2005, PATTERN RECOGN, V38, P1764, DOI 10.1016/j.patcog.2005.04.006
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lallee S, 2013, ADAPT BEHAV, V21, P274, DOI 10.1177/1059712313488423
   Law MHC, 2006, IEEE T PATTERN ANAL, V28, P377, DOI 10.1109/TPAMI.2006.56
   LeCun Y, 2010, IEEE INT SYMP CIRC S, P253, DOI 10.1109/ISCAS.2010.5537907
   Lee DD, 2001, ADV NEUR IN, V13, P556
   Lee H., 2009, P 26 ANN INT C MACH, P609, DOI DOI 10.1145/1553374.1553453
   Lee H, 2006, ADV NEURAL INFORM PR
   Lefort M., 2010, BICS
   Lemaignan S, 2010, IEEE INT C INT ROBOT, P3548, DOI 10.1109/IROS.2010.5649547
   Lyubova N., 2012, IEEE IJCNN
   MacQueen J, 1967, P 5 BERK S MATH STAT, V1, P281, DOI DOI 10.1234/12345678
   Mangin O, 2013, IEEE 3 JOINT INT C D, P1
   Martens J., 2010, P 27 INT C MACH LEAR, V951, P2010
   MCGURK H, 1976, NATURE, V264, P746, DOI 10.1038/264746a0
   Memisevic R., 2010, ADV NEURAL INFORM PR, P1603
   Memisevic R, 2010, NEURAL COMPUT, V22, P1473, DOI 10.1162/neco.2010.01-09-953
   Meyer K, 2009, TRENDS NEUROSCI, V32, P376, DOI 10.1016/j.tins.2009.04.002
   Montesano L, 2008, IEEE T ROBOT, V24, P15, DOI 10.1109/TRO.2007.914848
   Morse AF, 2010, COGNITION IN FLUX, P1362
   Morse AF, 2010, IEEE T AUTON MENT DE, V2, P325, DOI 10.1109/TAMD.2010.2087020
   Nakamura Tomoaki, 2011, IEEE International Conference on Robotics and Automation, P6233
   Nakamura T, 2009, 2009 IEEE-RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, P3943, DOI 10.1109/IROS.2009.5354736
   Narayanan H., 2010, ADV NEURAL INFORM PR
   Natale L, 2013, INTRINSICALLY MOTIVA, P433, DOI DOI 10.1007/978-3-642-32375-1_17
   Ng AY, 2002, ADV NEUR IN, V14, P849
   Ngiam J, 2011, IEEE INT C MACH LEAR, V28, P689, DOI DOI 10.1145/2647868
   O'Hara S., 2011, ARXIV11013354
   Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7
   Pape L, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P1191, DOI 10.1109/IJCNN.2011.6033359
   Paplinski AP, 2005, LECT NOTES ARTIF INT, V3801, P81
   Poon H., 2011, P 27 C UNC ART INT, p[337, 346]
   Reed S., 2013, NIPS WORKSH
   Ridge B, 2010, IEEE INT CONF ROBOT, P5047, DOI 10.1109/ROBOT.2010.5509544
   Rifai S., 2011, ADV NEURAL INFORM PR, P2294
   Rifai S, 2011, LECT NOTES ARTIF INT, V6912, P645, DOI 10.1007/978-3-642-23783-6_41
   Salah R, 2011, P 28 INT C MACH LEAR, P833, DOI 10.1016/j.wasman.2010.10.006
   Salman A, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P103, DOI 10.1109/IJCNN.2011.6033207
   Schneider A, 2009, 2009 IEEE-RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, P243, DOI 10.1109/IROS.2009.5354648
   Smith L, 2005, ARTIF LIFE, V11, P13, DOI 10.1162/1064546053278973
   Socher R., 2011, ADV NEURAL INFORM PR, P801
   Stuhlsatz A., 2010, P INT JOINT C NEUR N, P1
   Taylor GW, 2011, J MACH LEARN RES, V12, P1025
   Tikhanoff V, 2011, IEEE T AUTON MENT DE, V3, P17, DOI 10.1109/TAMD.2010.2100390
   Torralba A, 2011, ADV NEURAL INFORM PR, P2061
   Ugur E., 2009, EPIROB
   Vavrecka M., 2013, COGN COMPUT, P1
   Vincent P, 2008, P 25 INT C MACH LEAR, P1096, DOI DOI 10.1145/1390156.1390294
   Waibel M., 2011, IEEE ROBOT AUTOM MAG
   WARD JH, 1963, J AM STAT ASSOC, V58, P236, DOI 10.2307/2282967
   Wermter S, 2004, ROBOT AUTON SYST, V47, P171, DOI 10.1016/j.robot.2004.03.011
   Yu C, 2004, PROCEEDING OF THE NINETEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND THE SIXTEENTH CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE, P488
   Zhao HT, 2006, IEEE T SYST MAN CY B, V36, P873, DOI 10.1109/TSMCB.2006.870645
NR 90
TC 16
Z9 17
U1 2
U2 39
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0921-8890
EI 1872-793X
J9 ROBOT AUTON SYST
JI Robot. Auton. Syst.
PD SEP
PY 2015
VL 71
SI SI
BP 83
EP 98
DI 10.1016/j.robot.2014.11.005
PG 16
WC Automation & Control Systems; Computer Science, Artificial Intelligence;
   Robotics
SC Automation & Control Systems; Computer Science; Robotics
GA CL7IP
UT WOS:000357146000009
DA 2020-02-19
ER

PT J
AU Lerouge, J
   Herault, R
   Chatelain, C
   Jardin, F
   Modzelewski, R
AF Lerouge, J.
   Herault, R.
   Chatelain, C.
   Jardin, F.
   Modzelewski, R.
TI IODA: An input/output deep architecture for image labeling
SO PATTERN RECOGNITION
LA English
DT Article
DE Deep learning architectures; Deep neural network; Image labeling;
   Machine learning; Medical imaging; Sarcopenia
ID PROGNOSTIC-FACTOR; SIGN RECOGNITION; NETWORK
AB In this paper, we propose a deep neural network (DNN) architecture called Input Output Deep Architecture (IODA) for solving the problem of image labeling. IODA directly links a whole image to a whole label map, assigning a label to each pixel using a single neural network forward step. Instead of designing a handcrafted a priori model on labels (such as an atlas in the medical domain), we propose to automatically learn the dependencies between labels. The originality of IODA is to transpose DNN input pre-training trick to the output space, in order to learn a high level representation of labels. It allows a fast image labeling inside a fully neural network framework, without the need of any preprocessing such as feature designing or output coding.
   In this paper, IODA is applied on both a toy texture problem and a real-world medical image dataset, showing promising results. We provide an open source implementation of IODA.(1,2) (C) 2015 Elsevier Ltd. All rights reserved.
C1 [Lerouge, J.; Herault, R.; Chatelain, C.; Modzelewski, R.] Normandie Univ, INSA Rouen, LITIS, EA 4108,FR CNRS 9638, F-76800 St Etienne Du Rouvray, France.
   [Modzelewski, R.] Henri Becquerel Canc Ctr, Dept Nucl Med, F-76000 Rouen, France.
   [Modzelewski, R.] Rouen Univ Hosp, F-76000 Rouen, France.
   [Jardin, F.] Henri Becquerel Canc Ctr, Dept Clin Hematol, F-76000 Rouen, France.
   [Jardin, F.] Henri Becquerel Canc Ctr, INSERM Unit U918, F-76000 Rouen, France.
RP Lerouge, J (reprint author), Normandie Univ, INSA Rouen, LITIS, EA 4108,FR CNRS 9638, F-76800 St Etienne Du Rouvray, France.
EM julien.lerouge@insa-rouen.fr; romain.herault@insa-rouen.fr;
   clement.chatelain@insa-rouen.fr; romain.modzelewski@chb.unicancer.fr
OI Chatelain, Clement/0000-0001-8377-0630
FU project LeMon [ANR-11-JS02-010]
FX This work has been partly supported by the ANR-11-JS02-010 project
   LeMon.
CR Barlas P, 2014, 2014 11TH IAPR INTERNATIONAL WORKSHOP ON DOCUMENT ANALYSIS SYSTEMS (DAS 2014), P46, DOI 10.1109/DAS.2014.39
   Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bergstra J., 2010, P PYTH SCI COMP C SC, P3
   BESAG J, 1986, J ROY STAT SOC B MET, V48, P259
   Blaschko MB, 2008, LECT NOTES COMPUT SC, V5302, P2, DOI 10.1007/978-3-540-88682-2_2
   BOURLARD H, 1988, BIOL CYBERN, V59, P291, DOI 10.1007/BF00332918
   CHOU PB, 1990, INT J COMPUT VISION, V4, P185, DOI 10.1007/BF00054995
   Chum O., 2007, CVPR, P1
   Chung H., 2009, P SOC PHOTO-OPT INS, V7261
   Ciresan DC, 2010, NEURAL COMPUT, V22, P3207, DOI 10.1162/NECO_a_00052
   Csurka G, 2011, INT J COMPUT VISION, V95, P198, DOI 10.1007/s11263-010-0344-8
   Deng L, 2013, INT CONF ACOUST SPEE, P8599, DOI 10.1109/ICASSP.2013.6639344
   Ferrari V, 2008, IEEE T PATTERN ANAL, V30, P36, DOI 10.1109/TPAMI.2007.1144
   Glorot X., 2010, P INT C ART INT STAT
   GOSHTASBY A, 1986, PATTERN RECOGN, V19, P459, DOI 10.1016/0031-3203(86)90044-0
   Graves A, 2009, IEEE T PATTERN ANAL, V31, P855, DOI 10.1109/TPAMI.2008.137
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Ion A, 2014, INT J COMPUT VISION, V107, P40, DOI 10.1007/s11263-013-0663-7
   Kohli P, 2009, INT J COMPUT VISION, V82, P302, DOI 10.1007/s11263-008-0202-0
   Krahenbuhl P., 2011, ADV NEURAL INFORM PR, P109
   Labbe B., 2009, ICMLA, P6
   Lafferty J.D., 2001, P INT C MACH LEARN, V18, P282
   Lanic H, 2014, LEUKEMIA LYMPHOMA, V55, P817, DOI 10.3109/10428194.2013.816421
   Li Stan Z., 2009, MARKOV RANDOM FIELD
   Maintz J B, 1998, Med Image Anal, V2, P1, DOI 10.1016/S1361-8415(01)80026-8
   Bascon SM, 2010, COMPUT VIS IMAGE UND, V114, P373, DOI 10.1016/j.cviu.2009.12.002
   Martin L, 2013, J CLIN ONCOL, V31, P1539, DOI 10.1200/JCO.2012.45.2722
   Nicolas S, 2006, INT C PATT RECOG, P292
   Papavassiliou V, 2010, PATTERN RECOGN, V43, P369, DOI 10.1016/j.patcog.2009.05.007
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   Rumelhart D.E., 1986, PARALLEL DISTRIBUTED, V1, P318, DOI DOI 10.1016/B978-1-4832-1446-7.50035-2
   Ruta A, 2010, PATTERN RECOGN, V43, P416, DOI 10.1016/j.patcog.2009.05.018
   Sarikaya R, 2014, IEEE-ACM T AUDIO SPE, V22, P778, DOI 10.1109/TASLP.2014.2303296
   Srivastava Nitish, 2013, THESIS
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Wang HZ, 2013, IEEE T PATTERN ANAL, V35, P611, DOI 10.1109/TPAMI.2012.143
   Weston J, 2008, P 25 INT C MACH LEAR, P1168, DOI DOI 10.1145/1390156.1390303
   Weston J., 2002, NIPS, P873
   Zitova B, 2003, IMAGE VISION COMPUT, V21, P977, DOI 10.1016/S0262-8856(03)00137-9
NR 40
TC 12
Z9 12
U1 1
U2 24
PU ELSEVIER SCI LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND
SN 0031-3203
EI 1873-5142
J9 PATTERN RECOGN
JI Pattern Recognit.
PD SEP
PY 2015
VL 48
IS 9
BP 2847
EP 2858
DI 10.1016/j.patcog.2015.03.017
PG 12
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
SC Computer Science; Engineering
GA CK3JM
UT WOS:000356112400008
DA 2020-02-19
ER

PT J
AU Zhao, ZJ
   Zhang, XB
   Fang, YC
AF Zhao, Zhenjie
   Zhang, Xuebo
   Fang, Yongchun
TI Stacked Multilayer Self-Organizing Map for Background Modeling
SO IEEE TRANSACTIONS ON IMAGE PROCESSING
LA English
DT Article
DE Background modeling; representative learning; self-organizing map
ID MOTION DETECTION; NETWORK
AB In this paper, a new background modeling method called stacked multilayer self-organizing map background model (SMSOM-BM) is proposed, which presents several merits such as strong representative ability for complex scenarios, easy to use, and so on. In order to enhance the representative ability of the background model and make the parameters learned automatically, the recently developed idea of representative learning (or deep learning) is elegantly employed to extend the existing single-layer self-organizing map background model to a multilayer one (namely, the proposed SMSOM-BM). As a consequence, the SMSOM-BM gains several merits including strong representative ability to learn background model of challenging scenarios, and automatic determination for most network parameters. More specifically, every pixel is modeled by a SMSOM, and spatial consistency is considered at each layer. By introducing a novel over-layer filtering process, we can train the background model layer by layer in an efficient manner. Furthermore, for real-time performance consideration, we have implemented the proposed method using NVIDIA CUDA platform. Comparative experimental results show superior performance of the proposed approach.
C1 [Zhao, Zhenjie; Zhang, Xuebo; Fang, Yongchun] Nankai Univ, Inst Robot & Automat Informat Syst, Tianjin 300071, Peoples R China.
   [Zhao, Zhenjie; Zhang, Xuebo; Fang, Yongchun] Nankai Univ, Tianjin Key Lab Intelligent Robot, Tianjin 300071, Peoples R China.
RP Zhang, XB (reprint author), Nankai Univ, Inst Robot & Automat Informat Syst, Tianjin 300071, Peoples R China.
EM doublec@mail.nankai.edu.cn; zhangxuebo@nankai.edu.cn;
   fangyc@nankai.edu.cn
FU Tianjin Natural Science FoundationNatural Science Foundation of Tianjin
   [13JCQNJC03200]; Specialized Research Fund for the Doctoral Program of
   Higher Education of ChinaSpecialized Research Fund for the Doctoral
   Program of Higher Education (SRFDP) [20120031120040]; National Natural
   Science Foundation of ChinaNational Natural Science Foundation of China
   [61203333, 61325017]
FX This work was supported in part by the Tianjin Natural Science
   Foundation under Grant 13JCQNJC03200, in part by the Specialized
   Research Fund for the Doctoral Program of Higher Education of China
   under Grant 20120031120040, and in part by the National Natural Science
   Foundation of China under Grant 61203333 and Grant 61325017. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Sam Hasinoff.
CR Babacan SD, 2012, IEEE T SIGNAL PROCES, V60, P3964, DOI 10.1109/TSP.2012.2197748
   Barnich O, 2011, IEEE T IMAGE PROCESS, V20, P1709, DOI 10.1109/TIP.2010.2101613
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Bouwmans Thierry, 2009, Recent Patents on Computer Science, V2, P223, DOI 10.2174/1874479610902030223
   Bouwmans T., 2008, RECENT PAT COMPUT SC, V1, P219, DOI DOI 10.2174/2213275910801030219
   Bouwmans T, 2014, COMPUT SCI REV, V11-12, P31, DOI 10.1016/j.cosrev.2014.04.001
   Bouwmans T, 2014, COMPUT VIS IMAGE UND, V122, P22, DOI 10.1016/j.cviu.2013.11.009
   Brutzer S., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P1937, DOI 10.1109/CVPR.2011.5995508
   Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Caseiro R, 2011, IEEE I CONF COMP VIS, P1, DOI 10.1109/ICCV.2011.6126218
   Cevher V, 2008, LECT NOTES COMPUT SC, V5303, P155, DOI 10.1007/978-3-540-88688-4_12
   Chacon Mario I M, 2009, Proceedings 2009 International Joint Conference on Neural Networks (IJCNN 2009 - Atlanta), P474, DOI 10.1109/IJCNN.2009.5178632
   Chacon-Murguia MI, 2012, IEEE T IND ELECTRON, V59, P3286, DOI 10.1109/TIE.2011.2106093
   Culibrk D, 2007, IEEE T NEURAL NETWOR, V18, P1614, DOI 10.1109/TNN.2007.896861
   Ding XH, 2011, IEEE T IMAGE PROCESS, V20, P3419, DOI 10.1109/TIP.2011.2156801
   El Baf F, 2008, LECT NOTES COMPUT SC, V5358, P772, DOI 10.1007/978-3-540-89639-5_74
   Elgammal A., 2000, COMPUTER VISION ECCV, P751, DOI DOI 10.1007/3-540-45053-X_48
   Goyette N., 2012, IEEE COMP SOC C COMP, P1, DOI DOI 10.1109/CVPRW.2012.6238919
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hofmann M., 2012, P IEEE COMP SOC C CO, P38, DOI DOI 10.1109/CVPRW.2012.6238925
   Palorno EJ, 2009, LECT NOTES COMPUT SC, V5863, P743, DOI 10.1007/978-3-642-10677-4_85
   Kim K, 2005, REAL-TIME IMAGING, V11, P172, DOI 10.1016/j.rti.2004.12.004
   KOHONEN T, 1982, BIOL CYBERN, V43, P59, DOI 10.1007/BF00337288
   Lanza A, 2011, IEEE T PATTERN ANAL, V33, P1894, DOI 10.1109/TPAMI.2011.42
   Li LY, 2004, IEEE T IMAGE PROCESS, V13, P1459, DOI 10.1109/TIP.2004.836169
   Luque R. M., 2010, EUR S ART NEUR NETW, P423
   Maddalena L., 2012, P IEEE COMP SOC C CO, P21, DOI DOI 10.1109/CVPRW.2012.6238922
   Maddalena L, 2008, IEEE T IMAGE PROCESS, V17, P1168, DOI 10.1109/TIP.2008.924285
   Oliver NM, 2000, IEEE T PATTERN ANAL, V22, P831, DOI 10.1109/34.868684
   Radke RJ, 2005, IEEE T IMAGE PROCESS, V14, P294, DOI 10.1109/TIP.2004.838698
   Rai NK, 2012, P 7 INT C BIOINSP CO, P453
   Ramirez JL, 2013, ISRN COMBIN, DOI DOI 10.1155/2013/759641.ARTICLE
   Rauber A, 2002, IEEE T NEURAL NETWOR, V13, P1331, DOI 10.1109/TNN.2002.804221
   Schick A., 2012, P IEEE COMP SOC C CO, P27
   Shimada A, 2013, PROC CVPR IEEE, P1979, DOI 10.1109/CVPR.2013.258
   Stauffer C., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P246, DOI 10.1109/CVPR.1999.784637
   Toyama K., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P255, DOI 10.1109/ICCV.1999.791228
   Van Droogenbroeck M., 2012, COMP VIS PATT REC WO, P32, DOI DOI 10.1109/CVPRW.2012.6238924
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Woo H, 2010, IEEE T IMAGE PROCESS, V19, P2838, DOI 10.1109/TIP.2010.2050644
   Wren CR, 1997, IEEE T PATTERN ANAL, V19, P780, DOI 10.1109/34.598236
   Yamazaki M, 2006, LECT NOTES COMPUT SC, V3852, P467
   Zhao ZJ, 2012, COMM COM INF SC, V346, P177
   Zivkovic Z, 2004, INT C PATT RECOG, P28, DOI 10.1109/ICPR.2004.1333992
NR 45
TC 14
Z9 14
U1 0
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1057-7149
EI 1941-0042
J9 IEEE T IMAGE PROCESS
JI IEEE Trans. Image Process.
PD SEP
PY 2015
VL 24
IS 9
BP 2841
EP 2850
DI 10.1109/TIP.2015.2427519
PG 10
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
SC Computer Science; Engineering
GA CJ8NR
UT WOS:000355760300007
PM 25935034
DA 2020-02-19
ER

PT J
AU Xiong, FF
   Meyer, BT
   Moritz, N
   Rehr, R
   Anemuller, J
   Gerkmann, T
   Doclo, S
   Goetze, S
AF Xiong, Feifei
   Meyer, Bernd T.
   Moritz, Niko
   Rehr, Robert
   Anemueller, Joern
   Gerkmann, Timo
   Doclo, Simon
   Goetze, Stefan
TI Front-end technologies for robust ASR in reverberant
   environments-spectral enhancement-based dereverberation and auditory
   modulation filterbank features
SO EURASIP JOURNAL ON ADVANCES IN SIGNAL PROCESSING
LA English
DT Article
DE Automatic speech recognition; Dereverberation; Auditory modulation
   filterbank; Deep neural network; REVERB challenge
ID BOTTLE-NECK FEATURES; SPEECH; MODEL; RECOGNITION
AB This paper presents extended techniques aiming at the improvement of automatic speech recognition (ASR) in single-channel scenarios in the context of the REVERB (REverberant Voice Enhancement and Recognition Benchmark) challenge. The focus is laid on the development and analysis of ASR front-end technologies covering speech enhancement and feature extraction. Speech enhancement is performed using a joint noise reduction and dereverberation system in the spectral domain based on estimates of the noise and late reverberation power spectral densities (PSDs). To obtain reliable estimates of the PSDs-even in acoustic conditions with positive direct-to-reverberation energy ratios (DRRs)-we adopt the statistical model of the room impulse response explicitly incorporating DRRs, as well in combination with a novel proposed joint estimator for the reverberation time T-60 and the DRR. The feature extraction approach is inspired by processing strategies of the auditory system, where an amplitude modulation filterbank is applied to extract the temporal modulation information. These techniques were shown to improve the REVERB baseline in our previous work. Here, we investigate if similar improvements are obtained when using a state-of-the-art ASR framework, and to what extent the results depend on the specific architecture of the back-end. Apart from conventional Gaussian mixture model (GMM)-hidden Markov model (HMM) back-ends, we consider subspace GMM (SGMM)-HMMs as well as deep neural networks in a hybrid system. The speech enhancement algorithm is found to be helpful in almost all conditions, with the exception of deep learning systems in matched training-test conditions. The auditory feature type improves the baseline for all system architectures. The relative word error rate reduction achieved by combining our front-end techniques with current back-ends is 52.7% on average with the REVERB evaluation test set compared to our original REVERB result.
C1 [Xiong, Feifei; Moritz, Niko; Doclo, Simon; Goetze, Stefan] Fraunhofer Inst Digital Media Technol IDMT, Project Grp Hearing Speech & Audio Technol HSA, Oldenburg, Germany.
   [Meyer, Bernd T.; Rehr, Robert; Anemueller, Joern; Gerkmann, Timo; Doclo, Simon] Carl von Ossietzky Univ Oldenburg, Dept Med Phys & Acoust, D-26111 Oldenburg, Germany.
   [Xiong, Feifei; Meyer, Bernd T.; Moritz, Niko; Rehr, Robert; Anemueller, Joern; Gerkmann, Timo; Doclo, Simon; Goetze, Stefan] Carl von Ossietzky Univ Oldenburg, Cluster Excellence Hearing4All, D-26111 Oldenburg, Germany.
RP Xiong, FF (reprint author), Fraunhofer Inst Digital Media Technol IDMT, Project Grp Hearing Speech & Audio Technol HSA, Oldenburg, Germany.
EM feifei.xiong@idmt.fraunhofer.de
RI Gerkmann, Timo/I-3353-2014
OI Gerkmann, Timo/0000-0002-8678-4699; Doclo, Simon/0000-0002-3392-2381
FU EUEuropean Union (EU) [ITN-GA-2012-316969]; DFG-Cluster of
   ExcellenceGerman Research Foundation (DFG) [EXC 1077/1]; MWK PhD Program
   "Signals and Cognition"
FX The research leading to these results has received funding from the EU
   Seventh Framework Programme project Dereverberation and Reverberation of
   Audio, Music, and Speech (DREAMS) under grant agreement
   ITN-GA-2012-316969, from the DFG-Cluster of Excellence EXC 1077/1
   "Hearing4all", and from the MWK PhD Program "Signals and Cognition".
CR ATAL BS, 1974, J ACOUST SOC AM, V55, P1304, DOI 10.1121/1.1914702
   Baker JK, 1975, STOCHASTIC MODELING
   Breithaupt C., 2008, ITG C VOIC COMM SPRA
   Breithaupt C, 2008, THESIS RUHR U BOCHUM
   Breithaupt C, 2008, INT CONF ACOUST SPEE, P4037, DOI 10.1109/ICASSP.2008.4518540
   Breithaupt C, 2008, INT CONF ACOUST SPEE, P4897, DOI 10.1109/ICASSP.2008.4518755
   Cauchi B, 2014, P REVERB CHALL JOINT
   Cauchi B, 2015, EURASIP J ADV SIG PR, DOI 10.1186/s13634-015-0242-x
   Dau T, 1997, J ACOUST SOC AM, V102, P2892, DOI 10.1121/1.420344
   DAVIS SB, 1980, IEEE T ACOUST SPEECH, V28, P357, DOI 10.1109/TASSP.1980.1163420
   Delcroix M, 2014, P REVERB CHALL LIN P
   Eaton J, 2013, INT CONF ACOUST SPEE, P161, DOI 10.1109/ICASSP.2013.6637629
   EPHRAIM Y, 1984, IEEE T ACOUST SPEECH, V32, P1109, DOI 10.1109/TASSP.1984.1164453
   Garofalo J, 2007, LINGUISTIC DATA LCON
   Gerkmann T, 2009, IEEE T SIGNAL PROCES, V57, P4165, DOI 10.1109/TSP.2009.2025795
   Gibson M, 2006, INTERSPEECH 2006 AND 9TH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE PROCESSING, VOLS 1-5, P2406
   Gopinath RA, 1998, INT CONF ACOUST SPEE, P661, DOI 10.1109/ICASSP.1998.675351
   Grezl F, 2008, INT CONF ACOUST SPEE, P4729, DOI 10.1109/ICASSP.2008.4518713
   Grezl F, 2007, INT CONF ACOUST SPEE, P757
   Habets E., 2007, THESIS U EINDHOVEN E
   Habets EAP, 2009, IEEE SIGNAL PROC LET, V16, P770, DOI 10.1109/LSP.2009.2024791
   Haykin S., 2008, NEURAL NETWORKS LEAR
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   JUANG BH, 1986, IEEE T INFORM THEORY, V32, P307
   Kinoshita K., 2013, IEEE WORK APPL SIG
   Kodrasi I, 2013, IEEE T AUDIO SPEECH, V21, P1879, DOI 10.1109/TASL.2013.2260743
   Kuttruff H., 2000, ROOM ACOUSTICS
   LANGNER G, 1988, J NEUROPHYSIOL, V60, P1799
   Lebart K, 2001, ACUSTICA, V87, P359
   Li JY, 2012, IEEE W SP LANG TECH, P131, DOI 10.1109/SLT.2012.6424210
   Lincoln M, 2005, 2005 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), P357
   Maas R, 2012, INT CONF ACOUST SPEE, P297, DOI 10.1109/ICASSP.2012.6287875
   Martin R, 2001, IEEE T SPEECH AUDI P, V9, P504, DOI 10.1109/89.928915
   Mesgarani N, 2007, INT CONF ACOUST SPEE, P765
   Meutzner H., 2013, P CHIME 2013 VANC CA, P7
   Meyer B, 2011, P INT, P1269
   Meyer BT, 2011, SPEECH COMMUN, V53, P753, DOI 10.1016/j.specom.2010.07.002
   Mohamed AR, 2012, IEEE T AUDIO SPEECH, V20, P14, DOI 10.1109/TASL.2011.2109382
   Moritz N, 2015, IEEE-ACM T AUDIO SPE, V23, P1926, DOI 10.1109/TASLP.2015.2456420
   Moritz N, 2011, INT CONF ACOUST SPEE, P5492
   Muller KE, 2001, NUMER MATH, V90, P179, DOI 10.1007/s002110100285
   Nakatani T, 2010, IEEE T AUDIO SPEECH, V18, P1717, DOI 10.1109/TASL.2010.2052251
   Povey D., 2011, IEEE WORKSH AUT SPEE
   Povey D, 2008, INT CONF ACOUST SPEE, P4057, DOI 10.1109/ICASSP.2008.4518545
   Povey D, 2011, INT CONF ACOUST SPEE, P4460
   Povey D, 2011, COMPUT SPEECH LANG, V25, P404, DOI 10.1016/j.csl.2010.06.003
   ROBINSON T, 1995, INT CONF ACOUST SPEE, P81, DOI 10.1109/ICASSP.1995.479278
   RUMELHART DE, 1986, [No title captured], V1
   Schadler MR, 2012, J ACOUST SOC AM, V131, P4134, DOI 10.1121/1.3699200
   SCHROEDE.MR, 1965, J ACOUST SOC AM, V37, P409, DOI 10.1121/1.1909343
   Sehr A., 2009, THESIS FRIEDRICH ALE
   Sehr A, 2010, IEEE T AUDIO SPEECH, V18, P1676, DOI 10.1109/TASL.2010.2050511
   Seltzer ML, 2013, INT CONF ACOUST SPEE, P7398, DOI 10.1109/ICASSP.2013.6639100
   Tachioka Y, 2014, P REVERB CHALL DUAL
   TUSKE Z, 2014, P INTERSPEECH, P890
   Vesely K, 2013, P INTERSPEECH, P2345
   Weninger F, 2014, P REVERB CHALL T MER
   Wolfel M., 2009, DISTANT SPEECH RECOG
   Xiong F, 2014, P REVERB CHALL ROB A
   Xiong FF, 2014, INT CONF ACOUST SPEE
   Xiong FF, 2015, INT CONF ACOUST SPEE, P5043, DOI 10.1109/ICASSP.2015.7178931
   Xiong FF, 2013, INT CONF ACOUST SPEE, P443, DOI 10.1109/ICASSP.2013.6637686
   Yoshioka T, 2012, IEEE SIGNAL PROC MAG, V29, P114, DOI 10.1109/MSP.2012.2205029
   Young Steve, 2009, HTK BOOK HTK VERSION
   Yu D, 2011, P INTERSPEECH, P237
   Yu D, 2013, P ICLR FEAT LEARN DE
NR 66
TC 1
Z9 1
U1 0
U2 15
PU SPRINGEROPEN
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 1687-6180
J9 EURASIP J ADV SIG PR
JI EURASIP J. Adv. Signal Process.
PD AUG 5
PY 2015
AR 70
DI 10.1186/s13634-015-0256-4
PG 18
WC Engineering, Electrical & Electronic
SC Engineering
GA CQ4LU
UT WOS:000360577200002
OA DOAJ Gold
DA 2020-02-19
ER

PT J
AU Zhao, R
   Mao, KZ
AF Zhao, Rui
   Mao, Kezhi
TI Semi-Random Projection for Dimensionality Reduction and Extreme Learning
   Machine in High-Dimensional Space
SO IEEE COMPUTATIONAL INTELLIGENCE MAGAZINE
LA English
DT Article
AB Random Projection (RP) is a popular technique for dimensionality reduction because of its high computational efficiency. However, RP may not yield highly discriminative low-dimensional space to produce best pattern classification performance since the random transformation matrix of RP is independent of data. In this paper, we propose a Semi-Random Projection (SRP) framework, which takes the merit of random feature sampling of RP, but employs learning mechanism in the determination of the transformation matrix. One advantage of SRP is that it achieves a good balance between computational complexity and classification accuracy. Another advantage of SRP is that multiple SRP modules can be stacked to form a deep learning architecture for compact and robust feature learning. In addition, based on the insight on the relationship between RP and Extreme Learning Machine (ELM), the SRP is applied to ELM to derive Partially Connected ELM (PC-ELM). The hidden nodes of PC-ELM are more discriminative and hence a smaller number of nodes are needed. Experiments on two real-world text corpus, i.e., 20 News-groups and Farms Ads., verify the effectiveness and efficiency of the proposed SRP. Experimental results also show that PC-ELM outperforms ELM for high-dimensional data.
C1 [Zhao, Rui; Mao, Kezhi] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
RP Zhao, R (reprint author), Nanyang Technol Univ, Sch Elect & Elect Engn, Nanyang Ave, Singapore 639798, Singapore.
RI /AAD-1562-2020
CR Bache K., 2013, UCI MACHINE LEARNING
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Beyer K, 1999, LECT NOTES COMPUT SC, V1540, P217
   BINGHAM E, 2001, P 7 ACM SIGKDD INT C, P245, DOI DOI 10.1145/502512.502546
   Cambria E, 2015, PROCEEDINGS OF THE TWENTY-NINTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, P508
   Cambria E, 2013, IEEE INTELL SYST, V28, P30, DOI 10.1109/MIS.2013.140
   Chen M., 2012, ARXIV12064683
   Chen X., 2011, P SIAM INT C DAT MIN, P474
   Fukunaga K., 1990, INTRO STAT PATTERN R
   Hinton GE, 2007, TRENDS COGN SCI, V11, P428, DOI 10.1016/j.tics.2007.09.004
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Huang GB, 2004, IEEE IJCNN, P985
   Huang GB, 2006, NEUROCOMPUTING, V70, P489, DOI 10.1016/j.neucom.2005.12.126
   Jolliffe I., 2005, PRINCIPAL COMPONENT
   Kamber M., 2006, DATA MINING SE ASIA
   Li P., 2006, P 12 ACM SIGKDD INT, P287
   Martinez AM, 2001, IEEE T PATTERN ANAL, V23, P228, DOI 10.1109/34.908974
   SALTON G, 1975, COMMUN ACM, V18, P613, DOI 10.1145/361219.361220
   Sorensen D. C., 1997, IMPLICITLY RESTARTED
   Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Yan SC, 2007, IEEE T PATTERN ANAL, V29, P40, DOI 10.1109/TPAMI.2007.250598
   Zou H, 2006, J COMPUT GRAPH STAT, V15, P265, DOI 10.1198/106186006X113430
NR 23
TC 12
Z9 13
U1 1
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1556-603X
EI 1556-6048
J9 IEEE COMPUT INTELL M
JI IEEE Comput. Intell. Mag.
PD AUG
PY 2015
VL 10
IS 3
BP 30
EP 41
DI 10.1109/MCI.2015.2437316
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA CN5SO
UT WOS:000358490600005
DA 2020-02-19
ER

PT J
AU Kaburlasos, VG
   Papakostas, GA
AF Kaburlasos, Vassilis G.
   Papakostas, George A.
TI Learning Distributions of Image Features by Interactive Fuzzy Lattice
   Reasoning in Pattern Recognition Applications
SO IEEE COMPUTATIONAL INTELLIGENCE MAGAZINE
LA English
DT Article
ID SIMILARITY MEASURES; APPROXIMATION
AB This paper describes the recognition of image patterns based on novel representation learning techniques by considering higher-level (meta-) representations of numerical data in a mathematical lattice. In particular, the interest here focuses on lattices of (Type-1) Intervals' Numbers (INs), where an IN represents a distribution of image features including orthogonal moments. A neural classifier, namely fuzzy lattice reasoning (flr) fuzzy-ART-MAP (FAM), or flrFAM for short, is described for learning distributions of INs; hence, Type-2 INs emerge. Four benchmark image pattern recognition applications are demonstrated. The results obtained by the proposed techniques compare well with the results obtained by alternative methods from the literature. Furthermore, due to the isomorphism between the lattice of INs and the lattice of fuzzy numbers, the proposed techniques are straightforward applicable to Type-1 and/or Type-2 fuzzy systems. The far-reaching potential for deep learning in big data applications is also discussed.
C1 [Kaburlasos, Vassilis G.; Papakostas, George A.] Eastern Macedonia & Thrace Inst Technol, HMI Lab, Dept Comp & Informat Engn, Agios Loukas 65404, Kavala, Greece.
RP Kaburlasos, VG (reprint author), Eastern Macedonia & Thrace Inst Technol, HMI Lab, Dept Comp & Informat Engn, Agios Loukas 65404, Kavala, Greece.
EM vgkabs@teikav.edu.gr
OI Kaburlasos, Vassilis/0000-0002-1639-0627
CR Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Cambria E, 2013, IEEE INTELL SYST, V28, P30, DOI 10.1109/MIS.2013.140
   Cross V, 2013, INT J APPROX REASON, V54, P861, DOI 10.1016/j.ijar.2013.03.003
   Dou YL, 2012, APPL SOFT COMPUT, V12, P1621, DOI 10.1016/j.asoc.2012.03.013
   Esmi E., 2015, FUZZY SETS SYST
   Esmi E, 2015, IEEE T FUZZY SYST, V23, P313, DOI 10.1109/TFUZZ.2014.2312131
   Hofmann D, 2015, NEUROCOMPUTING, V147, P96, DOI 10.1016/j.neucom.2013.11.044
   Hsu CW, 2002, IEEE T NEURAL NETWOR, V13, P415, DOI 10.1109/72.991427
   Huang GB, 2006, IEEE T NEURAL NETWOR, V17, P879, DOI 10.1109/TNN.2006.875977
   Kaburlasos V. G, 2006, STUDIES COMPUTATIONA, V27
   Kaburlasos V. G., 2013, P IEEE INT C FUZZ SY
   Kaburlasos VG, 2014, IEEE T FUZZY SYST, V22, P531, DOI 10.1109/TFUZZ.2013.2263807
   Kaburlasos VG, 2014, INFORM FUSION, V16, P68, DOI 10.1016/j.inffus.2011.04.003
   Kaburlasos VG, 2013, IEEE T NEUR NET LEAR, V24, P1526, DOI 10.1109/TNNLS.2012.2237038
   Kaburlasos VG, 2012, J MATH IMAGING VIS, V42, P118, DOI 10.1007/s10851-011-0301-3
   Lyons M, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P200, DOI 10.1109/AFGR.1998.670949
   Maiora J, 2014, NEUROCOMPUTING, V126, P71, DOI 10.1016/j.neucom.2013.01.051
   Mendel J. M., 2013, P IEEE INT C FUZZ SY
   Miezianko R., IEEE OTCBVS WS SERIE
   Papadakis SE, 2014, J MULT-VALUED LOG S, V22, P561
   Papadakis SE, 2010, INFORM SCIENCES, V180, P5060, DOI 10.1016/j.ins.2010.03.023
   Papakostas GA, 2015, NEUROCOMPUTING, V150, P37, DOI 10.1016/j.neucom.2014.02.076
   Papakostas GA, 2013, PATTERN RECOGN LETT, V34, P1609, DOI 10.1016/j.patrec.2013.05.015
   Papakostas G.A., 2014, MOMENTS MOMENT INVAR
   Papakostas GA, 2014, IEEE INT FUZZY SYST, P39, DOI 10.1109/FUZZ-IEEE.2014.6891674
   Triesch J, 1996, PROCEEDINGS OF THE SECOND INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, P170, DOI 10.1109/AFGR.1996.557260
   Tsougenis ED, 2013, IEEE CONF IMAGING SY, P101, DOI 10.1109/IST.2013.6729671
   Valle ME, 2013, PATTERN RECOGN LETT, V34, P1589, DOI 10.1016/j.patrec.2013.03.034
NR 28
TC 17
Z9 17
U1 1
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1556-603X
EI 1556-6048
J9 IEEE COMPUT INTELL M
JI IEEE Comput. Intell. Mag.
PD AUG
PY 2015
VL 10
IS 3
BP 42
EP 51
DI 10.1109/MCI.2015.2437318
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA CN5SO
UT WOS:000358490600006
DA 2020-02-19
ER

PT J
AU Han, JW
   Zhang, DW
   Hu, XT
   Guo, L
   Ren, JC
   Wu, F
AF Han, Junwei
   Zhang, Dingwen
   Hu, Xintao
   Guo, Lei
   Ren, Jinchang
   Wu, Feng
TI Background Prior-Based Salient Object Detection via Deep Reconstruction
   Residual
SO IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY
LA English
DT Article
DE Background prior; deep reconstruction residual; salient object
   detection; stacked denoising autoencoder (SDAE)
ID VISUAL SALIENCY; CONTRAST; AUTOENCODERS; FRAMEWORK
AB Detection of salient objects from images is gaining increasing research interest in recent years as it can substantially facilitate a wide range of content-based multimedia applications. Based on the assumption that foreground salient regions are distinctive within a certain context, most conventional approaches rely on a number of hand-designed features and their distinctiveness is measured using local or global contrast. Although these approaches have been shown to be effective in dealing with simple images, their limited capability may cause difficulties when dealing with more complicated images. This paper proposes a novel framework for saliency detection by first modeling the background and then separating salient objects from the background. We develop stacked denoising autoencoders with deep learning architectures to model the background where latent patterns are explored and more powerful representations of data are learned in an unsupervised and bottom-up manner. Afterward, we formulate the separation of salient objects from the background as a problem of measuring reconstruction residuals of deep autoencoders. Comprehensive evaluations of three benchmark datasets and comparisons with nine state-of-the-art algorithms demonstrate the superiority of this paper.
C1 [Han, Junwei; Zhang, Dingwen; Hu, Xintao; Guo, Lei] Northwestern Polytech Univ, Sch Automat, Xian 710072, Peoples R China.
   [Ren, Jinchang] Univ Strathclyde, Dept Elect & Elect Engn, Glasgow G1 1XQ, Lanark, Scotland.
   [Wu, Feng] Univ Sci & Technol China, Sch Informat Sci, Hefei 230026, Peoples R China.
RP Han, JW (reprint author), Northwestern Polytech Univ, Sch Automat, Xian 710072, Peoples R China.
EM junweihan2010@gmail.com
RI zhang, dingwen/S-9447-2017
OI zhang, dingwen/0000-0001-8369-8886; Ren, Jinchang/0000-0001-6116-3194
FU National Science Foundation of ChinaNational Natural Science Foundation
   of China [61103061, 91120005, 61473231]
FX This work was supported by the National Science Foundation of China
   under Grants 61103061, 91120005, and 61473231. This paper was
   recommended by Associate Editor A. Signoroni.
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Alpert S., 2007, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2007.383017
   Andrew Ng, 2011, CS294A LECT NOTES, P72
   Bengio Y., 2013, NEURAL NETWORKS TRIC
   Bengio Y., 2007, LARGE SCALE KERNEL M, V34
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Borji A, 2012, LECT NOTES COMPUT SC, V7573, P414, DOI 10.1007/978-3-642-33709-3_30
   Borji A, 2012, PROC CVPR IEEE, P438, DOI 10.1109/CVPR.2012.6247706
   Chang KY, 2011, IEEE I CONF COMP VIS, P914, DOI 10.1109/ICCV.2011.6126333
   Chen T, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618470
   Erhan D, 2010, J MACH LEARN RES, V11, P625
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   Goferman S, 2012, IEEE T PATTERN ANAL, V34, P1915, DOI 10.1109/TPAMI.2011.272
   Goodfellow I. J., 2013, JMLR W CP, P1319
   Han B., 2011, P ACM INT C MULT, P1117
   Han JW, 2013, IEEE T CIRC SYST VID, V23, P2009, DOI 10.1109/TCSVT.2013.2242594
   Harel J, 2007, ADV NEURAL INF PROCE, P545, DOI DOI 10.7551/mitpress/7503.003.0073
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hou X., 2007, IEEE C COMP VIS PATT, V2007, P1, DOI DOI 10.1109/CVPR.2007.383267
   Hou X., 2008, P ADV NEURAL INFORM, P681
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jia YQ, 2013, IEEE I CONF COMP VIS, P1761, DOI 10.1109/ICCV.2013.221
   Jiang H., 2011, BMVC, V6, P7, DOI DOI 10.5244/C.25.110.(4)
   Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462
   Kim W, 2014, IEEE T CIRC SYST VID, V24, P646, DOI 10.1109/TCSVT.2013.2290579
   Lang CY, 2012, IEEE T IMAGE PROCESS, V21, P1327, DOI 10.1109/TIP.2011.2169274
   Li Y, 2009, 2009 16TH IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOLS 1-6, P3093, DOI 10.1109/ICIP.2009.5414465
   Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70
   Margolin R., 2014, P IEEE C COMP VIS PA, P1
   Margolin R, 2013, PROC CVPR IEEE, P1139, DOI 10.1109/CVPR.2013.151
   Martin DR, 2004, IEEE T PATTERN ANAL, V26, P530, DOI 10.1109/TPAMI.2004.1273918
   Min Chen, 2011, IEEE INFOCOM 2011 - IEEE Conference on Computer Communications Workshops, P409, DOI 10.1109/INFCOMW.2011.5928847
   Movahedi V., 2010, P IEEE C COMP VIS PA, P49, DOI DOI 10.1109/CVPRW.2010.5543739
   Qian XL, 2013, PATTERN RECOGN LETT, V34, P1270, DOI 10.1016/j.patrec.2013.04.009
   Seo HJ, 2009, J VISION, V9, DOI 10.1167/9.12.15
   Shao L, 2014, IEEE T NEUR NET LEAR, V25, P2303, DOI 10.1109/TNNLS.2014.2308519
   Shen XH, 2012, PROC CVPR IEEE, P853, DOI 10.1109/CVPR.2012.6247758
   Shin HC, 2013, IEEE T PATTERN ANAL, V35, P1930, DOI 10.1109/TPAMI.2012.277
   Siva P, 2013, PROC CVPR IEEE, P3238, DOI 10.1109/CVPR.2013.416
   Sun J, 2011, IEEE I CONF COMP VIS, P1511, DOI 10.1109/ICCV.2011.6126409
   Sun LT, 2014, IEEE T CIRC SYST VID, V24, P780, DOI 10.1109/TCSVT.2013.2290573
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Wang N., 2012, EUR S ART NEUR NETW, P287
   Wang Q, 2013, IEEE T CYBERNETICS, V43, P660, DOI 10.1109/TSMCB.2012.2214210
   Wei YC, 2012, LECT NOTES COMPUT SC, V7574, P29, DOI 10.1007/978-3-642-33712-3_3
   Xie YL, 2013, IEEE T IMAGE PROCESS, V22, P1689, DOI 10.1109/TIP.2012.2216276
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Zhang LY, 2008, J VISION, V8, DOI 10.1167/8.7.32
   Zhen XT, 2014, INFORM SCIENCES, V281, P295, DOI 10.1016/j.ins.2014.05.021
   Zhu F, 2014, INT J COMPUT VISION, V109, P42, DOI 10.1007/s11263-014-0703-y
NR 50
TC 229
Z9 230
U1 10
U2 109
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1051-8215
EI 1558-2205
J9 IEEE T CIRC SYST VID
JI IEEE Trans. Circuits Syst. Video Technol.
PD AUG
PY 2015
VL 25
IS 8
BP 1309
EP 1321
DI 10.1109/TCSVT.2014.2381471
PG 13
WC Engineering, Electrical & Electronic
SC Engineering
GA CO5QK
UT WOS:000359213400005
OA Green Accepted
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Huang, Y
   Wu, RW
   Sun, Y
   Wang, W
   Ding, XH
AF Huang, Yue
   Wu, Ruiwen
   Sun, Ye
   Wang, Wei
   Ding, Xinghao
TI Vehicle Logo Recognition System Based on Convolutional Neural Networks
   With a Pretraining Strategy
SO IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS
LA English
DT Article
DE Convolutional neural networks (CNNs); deep learning; pretraining;
   vehicle logo recognition (VLR)
AB Since a vehicle logo is the clearest indicator of a vehicle manufacturer, most vehicle manufacturer recognition (VMR) methods are based on vehicle logo recognition. Logo recognition can be still a challenge due to difficulties in precisely segmenting the vehicle logo in an image and the requirement for robustness against various imaging situations simultaneously. In this paper, a convolutional neural network (CNN) system has been proposed for VMR that removes the requirement for precise logo detection and segmentation. In addition, an efficient pretraining strategy has been introduced to reduce the high computational cost of kernel training in CNN-based systems to enable improved real-world applications. A data set containing 11 500 logo images belonging to 10 manufacturers, with 10 000 for training and 1500 for testing, is generated and employed to assess the suitability of the proposed system. An average accuracy of 99.07% is obtained, demonstrating the high classification potential and robustness against various poor imaging situations.
C1 [Huang, Yue; Wu, Ruiwen; Sun, Ye; Ding, Xinghao] Xiamen Univ, Dept Commun Engn, Sch Informat Sci & Engn, Xiamen 361005, Peoples R China.
   [Wang, Wei] Xiamen Univ, Sch Informat Sci & Engn, Dept Elect Engn, Xiamen 361005, Peoples R China.
RP Ding, XH (reprint author), Xiamen Univ, Dept Commun Engn, Sch Informat Sci & Engn, Xiamen 361005, Peoples R China.
EM dxh@xmu.edu.cn
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [30900328, 61172179, 61103121, 71103150, 81301278];
   National Key Technology Research and Development ProgramNational Key
   Technology R&D Program [2012BAI07B06]; Fundamental Research Funds for
   the Central UniversitiesFundamental Research Funds for the Central
   Universities [2013121023]; Research Fund for the Doctoral Program of
   Higher EducationResearch Fund for the Doctoral Program of Higher
   Education of China (RFDP) [20120121120043]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 30900328, Grant 61172179, Grant
   61103121, Grant 71103150, and Grant 81301278; by the National Key
   Technology Research and Development Program under Grant 2012BAI07B06; by
   the Fundamental Research Funds for the Central Universities under Grant
   2013121023; and by the Research Fund for the Doctoral Program of Higher
   Education under Grant 20120121120043. The Associate Editor for this
   paper was P. Cerri. (Corresponding author: Xinghao Ding.)
CR Ahmed A, 2008, LECT NOTES COMPUT SC, V5304, P69, DOI 10.1007/978-3-540-88690-7_6
   Bai HL, 2004, INT C PATT RECOG, P831, DOI 10.1109/ICPR.2004.1334387
   Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153
   Dai SJ, 2009, 2009 WRI WORLD CONGRESS ON SOFTWARE ENGINEERING, VOL 3, PROCEEDINGS, P18, DOI 10.1109/WCSE.2009.263
   Dlagnekov L., 2005, RECOGNIZING CARS
   Erhan D, 2010, J MACH LEARN RES, V11, P625
   Fan JL, 2010, IEEE T NEURAL NETWOR, V21, P1610, DOI 10.1109/TNN.2010.2066286
   Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231
   Figueiredo L, 2001, 2001 IEEE INTELLIGENT TRANSPORTATION SYSTEMS - PROCEEDINGS, P1206, DOI 10.1109/ITSC.2001.948835
   Gonzalez R.C., 2002, DIGITAL IMAGE PROCES
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   HUBEL DH, 1959, J PHYSIOL-LONDON, V148, P574, DOI 10.1113/jphysiol.1959.sp006308
   Lawrence S, 1997, IEEE T NEURAL NETWOR, V8, P98, DOI 10.1109/72.554195
   LeCun Yann, 1990, ADV NEURAL INFORM PR, P396, DOI DOI 10.1111/DSU.12130
   Petrovic V.S., 2004, P BRIT MACH VIS C, P1
   Psyllos AP, 2010, IEEE T INTELL TRANSP, V11, P322, DOI 10.1109/TITS.2010.2042714
   Psyllos P., 2013, P IEEE INT C VEH EL, P24
   Sam K.-T., 2012, INT P COMP SCI INF T, P91
   Sermanet P, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P2809, DOI 10.1109/IJCNN.2011.6033589
   WANG YL, 2007, [No title captured], P691
   Yu SY, 2013, 2013 10TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS 2013), P353, DOI 10.1109/AVSS.2013.6636665
   Zheng D, 2005, PATTERN RECOGN LETT, V26, P2431, DOI 10.1016/j.patrec.2005.04.014
   2009, IMAGES DATABASE
NR 23
TC 40
Z9 47
U1 2
U2 72
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1524-9050
EI 1558-0016
J9 IEEE T INTELL TRANSP
JI IEEE Trans. Intell. Transp. Syst.
PD AUG
PY 2015
VL 16
IS 4
BP 1951
EP 1960
DI 10.1109/TITS.2014.2387069
PG 10
WC Engineering, Civil; Engineering, Electrical & Electronic; Transportation
   Science & Technology
SC Engineering; Transportation
GA CO6FW
UT WOS:000359253600030
DA 2020-02-19
ER

PT J
AU Nie, LQ
   Wang, M
   Zhang, LM
   Yan, SC
   Zhang, B
   Chua, TS
AF Nie, Liqiang
   Wang, Meng
   Zhang, Luming
   Yan, Shuicheng
   Zhang, Bo
   Chua, Tat-Seng
TI Disease Inference from Health-Related Questions via Sparse Deep Learning
SO IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING
LA English
DT Article
DE Community-based health services; question answering; disease inference;
   deep learning
AB Automatic disease inference is of importance to bridge the gap between what online health seekers with unusual symptoms need and what busy human doctors with biased expertise can offer. However, accurately and efficiently inferring diseases is non-trivial, especially for community-based health services due to the vocabulary gap, incomplete information, correlated medical concepts, and limited high quality training samples. In this paper, we first report a user study on the information needs of health seekers in terms of questions and then select those that ask for possible diseases of their manifested symptoms for further analytic. We next propose a novel deep learning scheme to infer the possible diseases given the questions of health seekers. The proposed scheme is comprised of two key components. The first globally mines the discriminant medical signatures from raw features. The second deems the raw features and their signatures as input nodes in one layer and hidden nodes in the subsequent layer, respectively. Meanwhile, it learns the inter-relations between these two layers via pre-training with pseudo-labeled data. Following that, the hidden nodes serve as raw features for the more abstract signature mining. With incremental and alternative repeating of these two components, our scheme builds a sparsely connected deep architecture with three hidden layers. Overall, it well fits specific tasks with fine-tuning. Extensive experiments on a real-world dataset labeled by online doctors show the significant performance gains of our scheme.
C1 [Nie, Liqiang; Zhang, Luming; Chua, Tat-Seng] Natl Univ Singapore, Sch Comp, Singapore 117548, Singapore.
   [Yan, Shuicheng] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 117548, Singapore.
   [Wang, Meng] Hefei Univ Technol, Hefei, Peoples R China.
   [Zhang, Bo] Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
RP Nie, LQ (reprint author), Natl Univ Singapore, Sch Comp, Singapore 117548, Singapore.
EM nieliqiang@gmail.com; eric.mengwang@gmail.com; zglumg@gmail.com;
   eleyans@nus.edu.sg; dcszb@tsinghua.edu.cn; chuats@comp.nus.edu.sg
CR Akgul C.B., 2009, P ACM INT C IM VID R, P34
   [Anonymous], 2013, ONL HLTH RES ECL PAT
   Aronson AR, 2010, J AM MED INFORM ASSN, V17, P229, DOI 10.1136/jamia.2009.002733
   Batal I., 2008, P AM MED INFORM ASS, P29
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Cartright M.-A., 2011, P 34 INT ACM SIGIR C, P65, DOI DOI 10.1145/2009916.2009929
   Chen Y., 2012, P 24 INT C COMP LING, P561
   Cline RJW, 2001, HEALTH EDUC RES, V16, P671, DOI 10.1093/her/16.6.671
   Davis D.A., 2008, P 17 ACM C INF KNOWL, P769, DOI DOI 10.1145/1458082.1458185
   Doan Son, 2010, Proc Int Conf Comput Ling, V2010, P259
   Erhan D, 2010, J MACH LEARN RES, V11, P625
   Fakoor R., 2013, INT C MACH LEARN ATL
   Fox S., 2013, HLTH ONLINE 2013
   Galle M, 2013, SIGIR'13: THE PROCEEDINGS OF THE 36TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH & DEVELOPMENT IN INFORMATION RETRIEVAL, P1053
   Ghumbre S., 2011, INT C COMP SCI INF T, P84
   H Liu, 2010, ICML, P671
   Huang XJ, 2009, PROCEEDINGS 32ND ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P307, DOI 10.1145/1571941.1571995
   Khosla A, 2010, P 16 ACM SIGKDD INT, P183
   King B., 2011, PARASITOLOGY, P1
   Koopman B., 2011, P 34 INT ACM SIGIR C, P1139, DOI DOI 10.1145/2009916.2010088
   Krogh A., 1992, ADV NEURAL INFORMATI, P950
   Lee H., 2009, P 26 ANN INT C MACH, P609, DOI DOI 10.1145/1553374.1553453
   Limsopatham N, 2013, SIGIR'13: THE PROCEEDINGS OF THE 36TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH & DEVELOPMENT IN INFORMATION RETRIEVAL, P833
   Liu HR, 2013, IEEE T PATTERN ANAL, V35, P2131, DOI 10.1109/TPAMI.2013.16
   Luo G., 2008, P 31 ANN INT ACM SIG, P3
   Nie L, 2012, P 20 ACM INT C MULT, P59, DOI DOI 10.1145/2393347.2393363
   Nie L, 2014, MED INF RETR WORKSH, P24
   Nie L., 2011, P 34 INT ACM SIGIR C, P695, DOI DOI 10.1145/2009916.2010010
   Nie LQ, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P1245, DOI 10.1145/2600428.2611176
   Nie LQ, 2015, IEEE T KNOWL DATA EN, V27, P396, DOI 10.1109/TKDE.2014.2330813
   Nie LQ, 2014, ACM T INFORM SYST, V32, DOI 10.1145/2559157
   Nie LQ, 2013, IEEE T MULTIMEDIA, V15, P426, DOI 10.1109/TMM.2012.2229971
   Penny KI, 2012, J CLIN NURS, V21, P2722, DOI 10.1111/j.1365-2702.2011.03854.x
   Shouman M., 2011, P 9 AUSTR DAT MIN C, V12, P23
   Sindhwani V., 2006, Proceedings of the Twenty-Ninth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P477, DOI 10.1145/1148170.1148253
   Sondhi P., 2012, P 18 ACM SIGKDD INT, P1167
   Voorhees E. M., 2012, P TEXT RETRIEVAL C, P1
   Wang F., 2012, P 18 ACM SIGKDD INT, P453
   Wang F, 2013, IEEE T PATTERN ANAL, V35, P272, DOI 10.1109/TPAMI.2012.111
   Wang TD, 2008, CHI 2008: 26TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS VOLS 1 AND 2, CONFERENCE PROCEEDINGS, P457
   Warrens MJ, 2010, ADV DATA ANAL CLASSI, V4, P271, DOI 10.1007/s11634-010-0073-4
   White Ryen W., 2012, Proceedings of the 35th Annual International ACM SIGIR Conference on Research & Development in Information Retrieval (SIGIR 2012), P265, DOI 10.1145/2348283.2348322
   Yang SH, 2011, P 14 INT C ART INT S, P823
   Yi Zhang, 2007, 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P747, DOI 10.1145/1277741.1277889
   ZHANG D, 2006, P 12 ACM SIGKDD INT, P474, DOI DOI 10.1145/1150402.1150455
   Zhao YL, 2014, ACM T INTEL SYST TEC, V5, DOI 10.1145/2532439
   Zhou J., 2011, P 17 ACM SIGKDD INT, P814, DOI DOI 10.1145/2020408.2020549
   Zhou Jiayu, 2012, KDD, V2012, P1095
   Zhou T. C., 2012, P 21 INT C COMP WORL, P783, DOI DOI 10.1145/2187980.2188201
   Zhu DQ, 2013, SIGIR'13: THE PROCEEDINGS OF THE 36TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH & DEVELOPMENT IN INFORMATION RETRIEVAL, P1025
NR 51
TC 71
Z9 74
U1 1
U2 68
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1041-4347
EI 1558-2191
J9 IEEE T KNOWL DATA EN
JI IEEE Trans. Knowl. Data Eng.
PD AUG
PY 2015
VL 27
IS 8
BP 2107
EP 2119
DI 10.1109/TKDE.2015.2399298
PG 13
WC Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Engineering, Electrical & Electronic
SC Computer Science; Engineering
GA CM4YL
UT WOS:000357692600008
DA 2020-02-19
ER

PT J
AU Liu, QH
   Hu, XN
   Ye, M
   Cheng, XQ
   Li, F
AF Liu, Qihe
   Hu, Xiaonan
   Ye, Mao
   Cheng, Xianqiong
   Li, Fan
TI Gas Recognition under Sensor Drift by Using Deep Learning
SO INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS
LA English
DT Article; Proceedings Paper
CT 8th International Conference on Intelligent Systems and Knowledge
   Engineering (ISKE)
CY NOV 20-23, 2013
CL Shenzhen, PEOPLES R CHINA
SP Shenzhen Univ, Sci China Press, Chinese Acad Sci, IEEE Computat Intelligence Soc, Chinese Assoc Artificial Intelligence, State Key Lab Complex Elect Syst Simulat, Sci & Technol Integrated Informat Syst Lab, SW Jiaotong Univ, Univ Technol
ID COMPENSATION
AB Machine olfaction is an intelligent system that combines a cross-sensitivity chemical sensor array and an effective pattern recognition algorithm for the detection, identification, or quantification of various odors. Data collected by the sensor array are the multivariate time series signals with a complex structure, and these signals become more difficult to analyze due to sensor drift. In this work, we focus on improving the classification performance under sensor drift by using the deep learning method, which is popular nowadays. Compared with other methods, our method can effectively tackle sensor drift by automatically extracting features, thus not only removing the complexity of designing the hand-made features but also making it pervasive for a variety of application in machine olfaction. Our experimental results show that the deep learning method can learn the features that are more robust to drift than the original input and achieves high classification accuracy. (C) 2015 Wiley Periodicals, Inc.
C1 [Liu, Qihe; Hu, Xiaonan; Ye, Mao; Li, Fan] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Ctr Robot, Key Lab NeuroInformat,Minist Educ, Chengdu 610054, Peoples R China.
   [Cheng, Xianqiong] Chengdu Univ Technol, Coll Geophys, Chengdu, Peoples R China.
RP Liu, QH (reprint author), Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Ctr Robot, Key Lab NeuroInformat,Minist Educ, Chengdu 610054, Peoples R China.
EM qiheliu@uestc.edu.cn
RI Ye, Mao/K-3012-2019
OI Ye, Mao/0000-0001-9253-1332
FU Fundamental Research Funds for the Central UniversitiesFundamental
   Research Funds for the Central Universities [ZYGX2012J086,
   ZYGX2012J079]; National Natural Science Foundation of ChinaNational
   Natural Science Foundation of China [61375038]
FX We would like to thank Alexander Vergara, San Diego, and Ramon Huerta
   for their providing public gas sensor drift data. This work is supported
   in part by the Fundamental Research Funds for the Central Universities
   (numbers ZYGX2012J086 and ZYGX2012J079) and in part by the National
   Natural Science Foundation of China (number 61375038).
CR AIGNER R, 1994, SENSOR ACTUAT B-CHEM, V18, P143, DOI 10.1016/0925-4005(94)87073-X
   Artursson T, 2000, J CHEMOMETR, V14, P711, DOI 10.1002/1099-128X(200009/12)14:5/6<711::AID-CEM607>3.0.CO;2-4
   Bengio Y, 2006, ADV NEURAL INFORM PR, P153
   Bruins M, 2009, EUR J CLIN MICROBIOL, V28, P775, DOI 10.1007/s10096-009-0700-1
   Chang C. C., LIBSVM LIB SUPPORT V
   Coates A, 2011, PROC INT CONF DOC, P440, DOI 10.1109/ICDAR.2011.95
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411
   De Vito S, 2012, IEEE SENS J, V12, P3215, DOI 10.1109/JSEN.2012.2192425
   GARDNER JW, 1999, ELECT NOSES PRINCIPL
   GOPEL W, 1994, SENSOR ACTUAT B-CHEM, V18, P1, DOI 10.1016/0925-4005(94)87049-7
   Gutierrez-Osuna R., 2000, P 7 INT S OLF EL NOS, P147
   Gutierrez-Osuna R, 2002, IEEE SENS J, V2, P189, DOI 10.1109/JSEN.2002.800688
   Haugen JE, 2000, ANAL CHIM ACTA, V407, P23, DOI 10.1016/S0003-2670(99)00784-9
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hinton GE, 2010, MOMENTUM, V9, P3
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Jaitly N, 2011, INT CONF ACOUST SPEE, P5884
   Klinkenberg R, 2000, P 17 INT C MACH LEAR, P487, DOI DOI 10.1007/978-3-540-44871-6_130
   Kusy B, 2009, 2009 INTERNATIONAL CONFERENCE ON INFORMATION PROCESSING IN SENSOR NETWORKS (IPSN 2009), P109
   Langkvist M, 2011, P NIPS 2011 WORKSH D
   Le Q. V., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3361, DOI 10.1109/CVPR.2011.5995496
   Le Q.V., 2011, P 28 INT C MACH LEAR, P265
   Liu QH, 2014, IEEE SENS J, V14, P657, DOI 10.1109/JSEN.2013.2285919
   Llobet E, 1997, SENSOR ACTUAT B-CHEM, V41, P13, DOI 10.1016/S0925-4005(97)80272-9
   Muezzinoglu MK, 2009, SENSOR ACTUAT B-CHEM, V137, P507, DOI 10.1016/j.snb.2008.10.065
   Nair V, 2006, ADV NEURAL INFORM PR, V19, P1339
   Pearce TC, 2003, HDB MACHINE OLFACTIO, P134
   Romain AC, 2010, SENSOR ACTUAT B-CHEM, V146, P502, DOI 10.1016/j.snb.2009.12.027
   Roth M, 1996, SENSOR ACTUAT B-CHEM, V36, P358, DOI 10.1016/S0925-4005(97)80096-2
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Scholkopf B., 2006, ADV NEURAL INFORM PR, P1345
   Trincavelli M, 2010, IEEE T BIO-MED ENG, V57, P2884, DOI 10.1109/TBME.2010.2049492
   Vergara A, 2012, SENSOR ACTUAT B-CHEM, V166, P320, DOI 10.1016/j.snb.2012.01.074
   Wulsin D, 2011, J NEURAL ENG, V8, P1741
   YAMAZOE N, 1991, SENSOR ACTUAT B-CHEM, V5, P7, DOI 10.1016/0925-4005(91)80213-4
NR 35
TC 20
Z9 20
U1 2
U2 74
PU WILEY
PI HOBOKEN
PA 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
SN 0884-8173
EI 1098-111X
J9 INT J INTELL SYST
JI Int. J. Intell. Syst.
PD AUG
PY 2015
VL 30
IS 8
SI SI
BP 907
EP 922
DI 10.1002/int.21731
PG 16
WC Computer Science, Artificial Intelligence
SC Computer Science
GA CK2AV
UT WOS:000356012600005
DA 2020-02-19
ER

PT J
AU Liu, J
   Liu, BY
   Lu, HG
AF Liu, Jing
   Liu, Bingyuan
   Lu, Hanging
TI Detection guided deconvolutional network for hierarchical feature
   learning
SO PATTERN RECOGNITION
LA English
DT Article
DE Image representation; Deep leaning; Object recognition
ID OBJECT RECOGNITION; RECEPTIVE-FIELDS; ALGORITHM; MODELS; LEVEL
AB Deep learning models have gained significant interest as a way of building hierarchical image representation. However, current models still perform far behind human vision system because of the lack of selective property, the lack of high-level guidance for learning and the weakness to learn from few examples. To address these problems, we propose a detection-guided hierarchical learning algorithm for image representation. First, we train a multi-layer deconvolutional network in an unsupervised bottom-up scheme. During the training process, we use each raw image as an input, and decompose an image using multiple alternating layers of non-negative convolutional sparse coding and max-pooling. Inspired from the observation that the filters in top layer can be selectively activated by different high-level structures of images, i.e., one or partial filters should correspond to a particular object class, we update the filters in network by minimizing the reconstruction errors of the corresponding feature maps with respect to certain object detection maps obtained by a set of pre-trained detectors. With the fine-tuned network, we can extract the features of given images in a purely unsupervised way with no need of detectors. We evaluate the proposed feature representation on the task of object recognition, for which an SVM classifier with spatial pyramid matching kernel is used. Experiments on the datasets of PASCAL VOC 2007, Caltech-101 and Caltech-256 demonstrate that our approach outperforms some recent hierarchical feature descriptors as well as classical hand-crafted features. (C) 2015 Elsevier Ltd. All rights reserved.
C1 [Liu, Jing; Liu, Bingyuan; Lu, Hanging] CASIA, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
RP Liu, J (reprint author), CASIA, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
EM jliu@nlpr.ia.ac.cn
FU 973 ProgramNational Basic Research Program of China [2012CB316304];
   National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61272329, 61472422, 61332016]
FX This work was supported by 973 Program (2012CB316304) and National
   Natural Science Foundation of China (61272329, 61472422, 61332016).
CR Ahmed A, 2008, LECT NOTES COMPUT SC, V5304, P69, DOI 10.1007/978-3-540-88690-7_6
   Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Boureau YL, 2010, PROC CVPR IEEE, P2559, DOI 10.1109/CVPR.2010.5539963
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76
   Dalal N, 2005, PROC CVPR IEEE, P886
   DESIMONE R, 1984, J NEUROSCI, V4, P2051
   Everingham M., 2007, PASCAL VISUAL OBJECT
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Griffin G., 2007, 7694 CALTECH
   Harzallah H, 2009, IEEE I CONF COMP VIS, P237, DOI 10.1109/ICCV.2009.5459257
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI 10.1007/978-3-319-10578-9_23
   Hinton G.E., 2012, CORRABS12070580
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hoyer PO, 2003, NEUROCOMPUTING, V52-4, P547, DOI 10.1016/S0925-2312(02)00782-8
   Huang F, 2007, B ENTOMOL RES, V97, P1, DOI DOI 10.1787/HEMP-V19-ART3-EN
   HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837
   Kavukcuoglu K., 2010, ADV NEURAL INFORM PR, V23, P1090
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   Lazebnik S., 2006, P IEEE INT C COMP VI, V2, P2169, DOI DOI 10.1109/CVPR.2006.68
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee H., 2009, P 26 ANN INT C MACH, P609, DOI DOI 10.1145/1553374.1553453
   Li FF, 2007, COMPUT VIS IMAGE UND, V106, P59, DOI 10.1016/j.cviu.2005.09.012
   Li L.-J., 2010, P ADV NEUR INF PROC, P1378
   Quiroga RQ, 2005, NATURE, V435, P1102, DOI 10.1038/nature03687
   Riesenhuber M, 1999, NAT NEUROSCI, V2, P1019
   Serre T, 2007, IEEE T PATTERN ANAL, V29, P411, DOI 10.1109/TPAMI.2007.56
   Song Z, 2011, PROC CVPR IEEE, P1585, DOI 10.1109/CVPR.2011.5995330
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   van de Sande KEA, 2010, IEEE T PATTERN ANAL, V32, P1582, DOI 10.1109/TPAMI.2009.154
   van Gemert JC, 2008, LECT NOTES COMPUT SC, V5304, P696, DOI 10.1007/978-3-540-88690-7_52
   Wan L., 2013, P 30 INT C MACH LEAR, P1058
   Yang JC, 2009, PROC CVPR IEEE, P1794, DOI 10.1109/CVPRW.2009.5206757
   Yu K, 2011, PROC CVPR IEEE, P1713, DOI 10.1109/CVPR.2011.5995732
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zeiler MD, 2011, IEEE I CONF COMP VIS, P2018, DOI 10.1109/ICCV.2011.6126474
   Zeiler MD, 2010, PROC CVPR IEEE, P2528, DOI 10.1109/CVPR.2010.5539957
   Zeiler Matthew D., 2013, INT C LEARN REPR ICL
NR 37
TC 7
Z9 8
U1 0
U2 38
PU ELSEVIER SCI LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND
SN 0031-3203
EI 1873-5142
J9 PATTERN RECOGN
JI Pattern Recognit.
PD AUG
PY 2015
VL 48
IS 8
BP 2645
EP 2655
DI 10.1016/j.patcog.2015.02.002
PG 11
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
SC Computer Science; Engineering
GA CI2MW
UT WOS:000354582700024
DA 2020-02-19
ER

PT J
AU Mimura, M
   Sakai, S
   Kawahara, T
AF Mimura, Masato
   Sakai, Shinsuke
   Kawahara, Tatsuya
TI Reverberant speech recognition combining deep neural networks and deep
   autoencoders augmented with a phone-class feature
SO EURASIP JOURNAL ON ADVANCES IN SIGNAL PROCESSING
LA English
DT Article
DE Reverberant speech recognition; Deep Neural Networks (DNN); Deep
   Autoencoder (DAE)
ID ALGORITHM
AB We propose an approach to reverberant speech recognition adopting deep learning in the front-end as well as back-end of a reverberant speech recognition system, and a novel method to improve the dereverberation performance of the front-end network using phone-class information. At the front-end, we adopt a deep autoencoder (DAE) for enhancing the speech feature parameters, and speech recognition is performed in the back-end using DNN-HMM acoustic models trained on multi-condition data. The system was evaluated through the ASR task in the Reverb Challenge 2014. The DNN-HMM system trained on the multi-condition training set achieved a conspicuously higher word accuracy compared to the MLLR-adapted GMM-HMM system trained on the same data. Furthermore, feature enhancement with the deep autoencoder contributed to the improvement of recognition accuracy especially in the more adverse conditions. While the mapping between reverberant and clean speech in DAE-based dereverberation is conventionally conducted only with the acoustic information, we presume the mapping is also dependent on the phone information. Therefore, we propose a new scheme (pDAE), which augments a phone-class feature to the standard acoustic features as input. Two types of the phone-class feature are investigated. One is the hard recognition result of monophones, and the other is a soft representation derived from the posterior outputs of monophone DNN. The augmented feature in either type results in a significant improvement (7-8% relative) from the standard DAE.
C1 [Mimura, Masato; Sakai, Shinsuke; Kawahara, Tatsuya] Kyoto Univ, Acad Ctr Comp & Media Studies, Sakyo Ku, Kyoto 6068501, Japan.
RP Mimura, M (reprint author), Kyoto Univ, Acad Ctr Comp & Media Studies, Sakyo Ku, Kyoto 6068501, Japan.
EM mimura@ar.media.kyoto-u.ac.jp
RI Kawahara, Tatsuya/AAE-4682-2020
OI Kawahara, Tatsuya/0000-0002-2686-2296
FU JST CREST programJapan Science & Technology Agency (JST)Core Research
   for Evolutional Science and Technology (CREST); ERATO program
FX This work was supported by JST CREST and ERATO programs.
CR Bell P.J., 2012, P SLT, P324
   Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153
   Bishop CM, 1995, NEURAL NETWORKS PATT
   Dahl GE, 2012, IEEE T AUDIO SPEECH, V20, P30, DOI 10.1109/TASL.2011.2134090
   Delcroix M., 2006, ICASSP, V1
   Deng L, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 3 AND 4, P1692
   Du J, 2014, INTERSPEECH, P616
   Feng  X., 2014, P ICASSP, P1778
   Gannot S, 2003, EURASIP J APPL SIG P, V2003, P1074, DOI 10.1155/S1110865703305049
   Gomez R, 2010, IEEE T AUDIO SPEECH, V18, P1708, DOI 10.1109/TASL.2010.2052610
   GURELLI MI, 1995, IEEE T SIGNAL PROCES, V43, P134, DOI 10.1109/78.365293
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Ishii T, 2013, INTERSPEECH, P3479
   Karanasou Penny, 2014, INTERSPEECH, P616
   Kinoshita K., 2013, IEEE WORK APPL SIG
   Kinoshita K, 2009, IEEE T AUDIO SPEECH, V17, P1, DOI 10.1109/TASL.2008.2009015
   LEGGETTER CJ, 1995, COMPUT SPEECH LANG, V9, P171, DOI 10.1006/csla.1995.0010
   Lu XG, 2013, INTERSPEECH, P436
   Mimura M., 2014, HSCMA
   Mnih V, 2009, TECH REP UTML TR
   Mohamed AR, 2012, IEEE T AUDIO SPEECH, V20, P14, DOI 10.1109/TASL.2011.2109382
   Morgan N, 2012, IEEE T AUDIO SPEECH, V20, P7, DOI 10.1109/TASL.2011.2116010
   Rosenberg A. E., 1994, ICSLP 94. 1994 International Conference on Spoken Language Processing, P1835
   Sankar A, 1996, IEEE T SPEECH AUDI P, V4, P190, DOI 10.1109/89.496215
   Saon G, 2013, 2013 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING (ASRU), P55, DOI 10.1109/ASRU.2013.6707705
   Seide F., 2011, P INTERSPEECH, P437
   Seltzer ML, 2013, INT CONF ACOUST SPEE, P7398, DOI 10.1109/ICASSP.2013.6639100
   Sivaram GSVS, 2012, IEEE T AUDIO SPEECH, V20, P23, DOI 10.1109/TASL.2011.2129510
   Vincent P., 2008, P 25 INT C MACH LEAR, P1096, DOI DOI 10.1145/1390156.1390294
   Weninger F, 2014, P ICASSP, P4656
   Wu MY, 2006, IEEE T AUDIO SPEECH, V14, P774, DOI 10.1109/TSA.2005.858066
NR 33
TC 9
Z9 9
U1 1
U2 20
PU SPRINGEROPEN
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 1687-6180
J9 EURASIP J ADV SIG PR
JI EURASIP J. Adv. Signal Process.
PD JUL 23
PY 2015
AR 62
DI 10.1186/s13634-015-0246-6
PG 13
WC Engineering, Electrical & Electronic
SC Engineering
GA CN3JF
UT WOS:000358321100001
OA DOAJ Gold, Green Published
DA 2020-02-19
ER

PT J
AU Jia, K
   Sun, L
   Gao, SH
   Song, Z
   Shi, BE
AF Jia, Kui
   Sun, Lin
   Gao, Shenghua
   Song, Zhan
   Shi, Bertram E.
TI Laplacian Auto-Encoders: An explicit learning of nonlinear data manifold
SO NEUROCOMPUTING
LA English
DT Article
DE Auto-encoders; Deep learning; Manifold learning; Image classification
ID DIMENSIONALITY REDUCTION; IMAGE; EIGENMAPS
AB A key factor contributing to the success of many auto-encoders based deep learning techniques is the implicit consideration of the underlying data manifold in their training criteria. In this paper, we aim to make this consideration more explicit by training auto-encoders completely from the manifold learning perspective. We propose a novel unsupervised manifold learning method termed Laplacian Auto-Encoders (LAEs). Starting from a general regularized function learning framework, LAE regularizes training of auto-encoders so that the learned encoding function has the locality-preserving property for data points on the manifold. By exploiting the analog relation between the graph Laplacian and the Laplace-Beltrami operator on the continuous manifold, we derive discrete approximations of the first- and higher-order auto-encoder regularizers that can be applied in practical scenarios, where only data points sampled from the distribution on the manifold are available. Our proposed LAE has potentially better generalization capability, due to its explicit respect of the underlying data manifold. Extensive experiments on benchmark visual classification datasets show that LAE consistently outperforms alternative auto-encoders recently proposed in deep learning literature, especially when training samples are relatively scarce: (C) 2015 Elsevier B.V. All rights reserved.
C1 [Jia, Kui] Univ Macau, Fac Sci & Technol, Dept Elect & Comp Engn, Macau, Peoples R China.
   [Jia, Kui] Adv Digital Sci Ctr, Singapore 138632, Singapore.
   [Sun, Lin; Shi, Bertram E.] Hong Kong Univ Sci & Technol, Dept Elect & Comp Engn, Kowloon, Hong Kong, Peoples R China.
   [Gao, Shenghua] ShanghaiTech Univ, Sch Informat Sci & Technol, Shanghai, Peoples R China.
   [Song, Zhan] Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China.
RP Jia, K (reprint author), Adv Digital Sci Ctr, 08-10 Connexis North Tower,1 Fusionopolis Way, Singapore 138632, Singapore.
EM kuijia@umac.mo; lsunece@ust.hk; gaoshh@shanghaitech.edu.cn;
   zhan.song@siat.ac.cn; eebert@ee.ust.hk
OI Sun, Lin/0000-0002-2466-4632; Shi, Bertram/0000-0001-9167-7495
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61202158]; Singapore's Agency for Science,
   Technology and Research (A*STAR)Agency for Science Technology & Research
   (ASTAR)
FX This work is partially supported by the National Natural Science
   Foundation of China (Grant no. 61202158) and the research grant for the
   Human Sixth Sense Programme at the Advanced Digital Sciences Center from
   Singapore's Agency for Science, Technology and Research (A*STAR).
CR Belkin M, 2005, LECT NOTES COMPUT SC, V3559, P486, DOI 10.1007/11503415_33
   Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317
   Bengio Y., 2003, ADV NEURAL INFORM PR
   Bengio Y., 2013, ADV NEURAL INFORM PR
   Bengio Y, 2006, ADV NEURAL INFORM PR, P153
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16
   Cayton L, 2005, TECHNICAL REPORT
   Donoho DL, 2003, P NATL ACAD SCI USA, V100, P5591, DOI 10.1073/pnas.1031596100
   Goodfellow I. J., 2013, INT C MACH LEARN
   Hein M, 2005, LECT NOTES COMPUT SC, V3559, P470, DOI 10.1007/11503415_32
   Hinton G. E., 2012, CORR
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Krizhevsky A., 2009, TECHNICAL REPORT
   Lafon SS, 2004, THESIS YALE U
   LeCun Y, 1998, LECT NOTES COMPUT SC, V1524, P9
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Liao Y., ARXIV13120786
   Liu W., 2010, P 27 INT C MACH LEAR, P679
   Liu WF, 2014, COMPUT VIS IMAGE UND, V118, P50, DOI 10.1016/j.cviu.2013.03.007
   Liu WF, 2013, IEEE T IMAGE PROCESS, V22, P2676, DOI 10.1109/TIP.2013.2255302
   Narayanan H, 2010, ADV NEURAL INFORM PR, P1786
   Ranzato M., 2007, J MACH LEARN RES, V2, P371
   Rifai S., 2012, INT C MACH LEARN
   Rifai S, 2011, LECT NOTES ARTIF INT, V6912, P645, DOI 10.1007/978-3-642-23783-6_41
   Rifai Salah, 2011, ADV NEURAL INFORM PR
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Russakovsky, 2014, ARXIVARXIV14090575
   Salah R, 2011, P 28 INT C MACH LEAR, P833, DOI 10.1016/j.wasman.2010.10.006
   Schaul T., 2013, INT C MACH LEARN ICM
   Shaw B., 2009, P 26 ANN INT C MACH, P937, DOI DOI 10.1145/1553374.1553494
   Talwalkar Ameet, 2008, COMPUTER VISION PATT
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
   Tikhonov A.N., 1977, SOLUTIONS ILL POSED
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Vladymyrov Max, 2013, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2013. Proceedings: LNCS 8190, P256, DOI 10.1007/978-3-642-40994-3_17
   Wan L., 2013, INT C MACH LEARN
   Weinberger KQ, 2004, PROC CVPR IEEE, P988
   Weston J, 2008, P 25 INT C MACH LEAR, P1168, DOI DOI 10.1145/1390156.1390303
   Williams CKI, 2001, ADV NEUR IN, V13, P682
   Yu J, 2011, PROC CVPR IEEE
   Yu J, 2014, IEEE T IMAGE PROCESS, V23, P2019, DOI 10.1109/TIP.2014.2311377
   Yu J, 2012, IEEE T IMAGE PROCESS, V21, P3262, DOI 10.1109/TIP.2012.2190083
NR 45
TC 25
Z9 27
U1 0
U2 38
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0925-2312
EI 1872-8286
J9 NEUROCOMPUTING
JI Neurocomputing
PD JUL 21
PY 2015
VL 160
BP 250
EP 260
DI 10.1016/j.neucom.2015.02.023
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA CH6IA
UT WOS:000354139100022
DA 2020-02-19
ER

PT J
AU Ma, XR
   Geng, J
   Wang, HY
AF Ma, Xiaorui
   Geng, Jie
   Wang, Hongyu
TI Hyperspectral image classification via contextual deep learning
SO EURASIP JOURNAL ON IMAGE AND VIDEO PROCESSING
LA English
DT Article
DE Hyperspectral image classification; Contextual deep learning;
   Multinomial logistic regression (MLR); Supervised classification
ID SPECTRAL-SPATIAL CLASSIFICATION; REPRESENTATIONS; FRAMEWORK
AB Because the reliability of feature for every pixel determines the accuracy of classification, it is important to design a specialized feature mining algorithm for hyperspectral image classification. We propose a feature learning algorithm, contextual deep learning, which is extremely effective for hyperspectral image classification. On the one hand, the learning-based feature extraction algorithm can characterize information better than the pre-defined feature extraction algorithm. On the other hand, spatial contextual information is effective for hyperspectral image classification. Contextual deep learning explicitly learns spectral and spatial features via a deep learning architecture and promotes the feature extractor using a supervised fine-tune strategy. Extensive experiments show that the proposed contextual deep learning algorithm is an excellent feature learning algorithm and can achieve good performance with only a simple classifier.
C1 [Ma, Xiaorui; Geng, Jie; Wang, Hongyu] Dalian Univ Technol, Fac Elect Informat & Elect Engn, Dalian, Peoples R China.
RP Wang, HY (reprint author), Dalian Univ Technol, Fac Elect Informat & Elect Engn, Linggong Rd, Dalian, Peoples R China.
EM xrui.ma@gmail.com; gengjie@mail.dlut.edu.cn; whyu@dlut.edu.cn
CR Camps-Valls G, 2005, IEEE T GEOSCI REMOTE, V43, P1351, DOI 10.1109/TGRS.2005.846154
   Camps-Valls G, 2014, IEEE SIGNAL PROC MAG, V31, P45, DOI 10.1109/MSP.2013.2279179
   Chang C.-I., 2007, HYPERSPECTRAL DATA E
   Chen Y, 2014, IEEE J-STARS, P1
   Chen Y, 2013, IEEE T GEOSCI REMOTE, V51, P217, DOI 10.1109/TGRS.2012.2201730
   Cheriyadat AM, 2014, IEEE T GEOSCI REMOTE, V52, P439, DOI 10.1109/TGRS.2013.2241444
   Ghamisi P, 2015, IEEE T GEOSCI REMOTE, V53, P2335, DOI 10.1109/TGRS.2014.2358934
   Ghamisi P, 2014, IEEE J-STARS, V7, P2147, DOI 10.1109/JSTARS.2014.2298876
   Ghamisi P, 2014, IEEE T GEOSCI REMOTE, V52, P5771, DOI 10.1109/TGRS.2013.2292544
   Gu YF, 2012, IEEE T GEOSCI REMOTE, V50, P2852, DOI 10.1109/TGRS.2011.2176341
   Hinton GE, 1993, ADV NEURAL INFORM PR, P3
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   HUGHES GF, 1968, IEEE T INFORM THEORY, V14, P55, DOI 10.1109/TIT.1968.1054102
   Huiming Xie, 2014, 2014 IEEE Geoscience and Remote Sensing Symposium. (IGARSS). Proceedings, P2818, DOI 10.1109/IGARSS.2014.6947062
   Landgrebe David A, 2005, SIGNAL THEORY METHOD, V29
   Li CH, 2012, IEEE T GEOSCI REMOTE, V50, P784, DOI 10.1109/TGRS.2011.2162246
   Li JY, 2014, IEEE GEOSCI REMOTE S, V11, P1409, DOI 10.1109/LGRS.2013.2294241
   Li J, 2013, IEEE T GEOSCI REMOTE, V51, P4816, DOI 10.1109/TGRS.2012.2230268
   Li J, 2013, IEEE T GEOSCI REMOTE, V51, P844, DOI 10.1109/TGRS.2012.2205263
   Li J, 2011, IEEE T GEOSCI REMOTE, V49, P3947, DOI 10.1109/TGRS.2011.2128330
   Melgani F, 2004, IEEE T GEOSCI REMOTE, V42, P1778, DOI 10.1109/TGRS.2004.831865
   Moser G, 2013, IEEE T GEOSCI REMOTE, V51, P2734, DOI 10.1109/TGRS.2012.2211882
   Moser G, 2013, P IEEE, V101, P631, DOI 10.1109/JPROC.2012.2211551
   Song BQ, 2014, IEEE T GEOSCI REMOTE, V52, P5122, DOI 10.1109/TGRS.2013.2286953
   Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244
   Tarabalka Y., 2010, THESIS U ICELAND
   Tarabalka Y, 2010, IEEE GEOSCI REMOTE S, V7, P736, DOI 10.1109/LGRS.2010.2047711
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Willett RM, 2014, IEEE SIGNAL PROC MAG, V31, P116, DOI 10.1109/MSP.2013.2279507
   Yu Dong, 2010, NIPS 2010 WORKSH DEE
   Zhang F, 2015, IEEE T GEOSCI REMOTE, V53, P2175, DOI 10.1109/TGRS.2014.2357078
   Zhang Q, 2015, IEEE T GEOSCI REMOTE, V53, P261, DOI 10.1109/TGRS.2014.2321405
   Zhang QS, 2013, PROCEEDINGS OF 2013 IEEE INTERNATIONAL CONFERENCE ON GREY SYSTEMS AND INTELLIGENT SERVICES (GSIS), P16, DOI 10.1109/GSIS.2013.6714730
   Zhang Y, 2014, IEEE J-STARS, P1
   Zhou Y., 2014, IEEE T IND ELECTRON, P1, DOI DOI 10.1109/EDSSC.2014.7061083
NR 35
TC 70
Z9 73
U1 4
U2 116
PU SPRINGEROPEN
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 1687-5176
EI 1687-5281
J9 EURASIP J IMAGE VIDE
JI EURASIP J. Image Video Process.
PD JUL 14
PY 2015
AR 20
DI 10.1186/s13640-015-0071-8
PG 12
WC Engineering, Electrical & Electronic; Imaging Science & Photographic
   Technology
SC Engineering; Imaging Science & Photographic Technology
GA CN7JP
UT WOS:000358610800001
OA DOAJ Gold
DA 2020-02-19
ER

PT J
AU Padmanabhan, J
   Premkumar, MJJ
AF Padmanabhan, Jayashree
   Premkumar, Melvin Jose Johnson
TI Machine Learning in Automatic Speech Recognition: A Survey
SO IETE TECHNICAL REVIEW
LA English
DT Article
DE Automatic speech recognition; Gaussian mixture models; Hidden Markov
   models; Machine learning; Support vector machines
ID FEATURES; MODELS
AB Over the past few decades, there has been tremendous development in machine learning paradigms used in automatic speech recognition (ASR) for home automation to space exploration. Though commercial speech recognizers are available for certain well-defined applications like dictation and transcription, many issues in ASR like recognition in noisy environments, multilingual recognition, and multi-modal recognition are yet to be addressed effectively. A comprehensive review of common machine learning techniques like artificial neural networks, support vector machines, and Gaussian mixture models along with hidden Markov models employed in ASR is provided. A thorough review on the recent developments in deep learning which has provided significant improvements in ASR performance, along with its relevance in the future of ASR, is also presented.
C1 [Padmanabhan, Jayashree] Anna Univ, MIT, Dept Comp Technol, Chennai 600025, Tamil Nadu, India.
   [Premkumar, Melvin Jose Johnson] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.
RP Padmanabhan, J (reprint author), Anna Univ, MIT, Dept Comp Technol, Chennai 600025, Tamil Nadu, India.
EM pjshree12@gmail.com; jmelvinjose73@yahoo.com
CR Baker J., 1976, SPEECH RECOGNITION, P297
   Baker JM, 2009, IEEE SIGNAL PROC MAG, V26, P75, DOI 10.1109/MSP.2009.932166
   BAUM LE, 1966, ANN MATH STAT, V37, P1554, DOI 10.1214/aoms/1177699147
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Bilmes JA, 2006, IEICE T INF SYST, VE89D, P869, DOI 10.1093/ietisy/e89-d.3.869
   BOURLARD H, 1989, [No title captured], V1, P502
   Chandra-Sekhar C., 2003, P WORKSH SPOK LANG P, P79
   Chen WY, 1996, NEURAL NETWORKS, V9, P655, DOI 10.1016/0893-6080(95)00140-9
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411
   Dahl GE, 2012, IEEE T AUDIO SPEECH, V20, P30, DOI 10.1109/TASL.2011.2134090
   Dahl GE, 2013, INT CONF ACOUST SPEE, P8609, DOI 10.1109/ICASSP.2013.6639346
   DAVIS KH, 1952, J ACOUST SOC AM, V24, P637, DOI 10.1121/1.1906946
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1
   DENG L, 1991, IEEE T SIGNAL PROCES, V39, P1677, DOI 10.1109/78.134406
   DENG L, 2012, P ICASSP, P2133
   Dudley H, 1939, J FRANKL INST, V227, P0739, DOI 10.1016/S0016-0032(39)90816-1
   Dudley WH, 1939, BELL LABS RECORD, V18, P122
   Ech-Cherif A, 2002, ICONIP'02: PROCEEDINGS OF THE 9TH INTERNATIONAL CONFERENCE ON NEURAL INFORMATION PROCESSING, P2507
   Elman L., 1988, FINDING STRUCTURE TI
   FRANCO H, 1994, COMPUT SPEECH LANG, V8, P211, DOI 10.1006/csla.1994.1010
   FRANCO H, 1995, P EUROSPEECH MADR, P1681
   FRY DB, 1959, J BRIT I RADIO ENG, V19, P211
   Gales MJF, 1996, IEEE T SPEECH AUDI P, V4, P352, DOI 10.1109/89.536929
   Ganapathiraju A, 2004, IEEE T SIGNAL PROCES, V52, P2348, DOI 10.1109/TSP.2004.831018
   Ganapathiraju A., 2000, P INT C SPOK LANG PR, V4, P504
   Graves Alex, 2014, P 31 INT C MACH LEAR, P1764, DOI DOI 10.1145/1143844.1143891
   Grezl F, 2007, INT CONF ACOUST SPEE, P757
   Hennebert J., 1997, P EUROSPEECH, V4, P1951
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   HOCHBERG MM, 1995, P IEEE INT C AC SPEE, P69
   Imseng D, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 1-2, P278
   Ishii Takaaki, 2013, P INTERSPEECH, P3512
   Jaitley N., 2011, ADV NEURAL INF PROCE, V24
   Jaitly N, 2012, APPL PRETRAINED DEEP
   Jayanna HS, 2009, IETE TECH REV, V26, P181, DOI 10.4103/0256-4602.50702
   JELINEK F, 1976, P IEEE, V64, P532, DOI 10.1109/PROC.1976.10159
   JORDAN MI, 1986, 8604 UCSD
   JUANG BH, 1986, IEEE T INFORM THEORY, V32, P307
   Landauer T., 1987, P 9 ANN C COGN SCI S, P531
   Lee C.H., 2004, P INTERSPEECH, P109
   Maas A.L., 2013, P ICML WORKSH DEEP L
   Maas Andrew L., 2014, ARXIV1408
   Mitchell T., 1997, MACHINE LEARNING
   Mohamed A.R., 2010, P INTERSPEECH, P2846
   Morgan N, 2005, IEEE SIGNAL PROC MAG, V22, P81, DOI 10.1109/MSP.2005.1511826
   MORGAN N, 1990, INT CONF ACOUST SPEE, P413, DOI 10.1109/ICASSP.1990.115720
   MORGAN N, 1991, INT CONF ACOUST SPEE, P49, DOI 10.1109/ICASSP.1991.150275
   OLSON HF, 1956, J ACOUST SOC AM, V28, P1072, DOI 10.1121/1.1908561
   Padrell-Sendra J., 2006, P 14 EUR SIGN PROC C, P434
   Pati D, 2010, IETE TECH REV, V27, P138, DOI 10.4103/0256-4602.60167
   PEELING SM, 1988, SPEECH COMMUN, V7, P403, DOI 10.1016/0167-6393(88)90057-X
   Platt JC, 1999, ADVANCES IN KERNEL METHODS, P185
   Povey D, 2010, INT CONF ACOUST SPEE, P4330, DOI 10.1109/ICASSP.2010.5495662
   Powell MJD, 1987, ALGORITHMS APPROXIMA, P143
   Premkumar M. J. J., 2013, P INT LYON FRANC, P1012
   Rabiner L. R., 1993, FUNDAMENTALS SPEECH
   RABINER LR, 1979, IEEE T ACOUST SPEECH, V27, P336, DOI 10.1109/TASSP.1979.1163259
   Renals S, 1994, IEEE T SPEECH AUDI P, V2, P161, DOI 10.1109/89.260359
   Richard MD, 1991, NEURAL COMPUT, V3, P461, DOI 10.1162/neco.1991.3.4.461
   Robinson T., 1991, Computer Speech and Language, V5, P259, DOI 10.1016/0885-2308(91)90010-N
   Sahoo SK, 2012, IETE TECH REV, V29, P54, DOI 10.4103/0256-4602.93139
   Sainath TN, 2013, IEEE T AUDIO SPEECH, V21, P2267, DOI 10.1109/TASL.2013.2284378
   Seide F., 2011, P INTERSPEECH, P437
   Singer E., 1992, ICASSP-92: 1992 IEEE International Conference on Acoustics, Speech and Signal Processing (Cat. No.92CH3103-9), P629, DOI 10.1109/ICASSP.1992.225830
   Singh S, 2014, IETE TECH REV, V31, P34, DOI 10.1080/02564602.2014.890840
   Stadermann J., 2004, P INT C SPOK LANG PR, P661
   Stolcke A, 2006, INT CONF ACOUST SPEE, P321
   Toth L, 2008, INTERSPEECH 2008: 9TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2008, VOLS 1-5, P2695
   Vincent P, 2008, P 25 INT C MACH LEAR, P1096, DOI DOI 10.1145/1390156.1390294
   Vintsyuk T. K., 1968, Cybernetics, V4, P52
   Vu N. T, 2012, P SLTU CAP TOWN S AF, P90
   WAIBEL A, 1989, IEEE T ACOUST SPEECH, V37, P328, DOI 10.1109/29.21701
   Watrous R. L., 1987, IEEE First International Conference on Neural Networks, P381
   WILPON JG, 1990, IEEE T ACOUST SPEECH, V38, P1870, DOI 10.1109/29.103088
   Wollmer Martin, 2013, PROBABILISTIC ASR FE
   Yan YH, 1997, INT CONF ACOUST SPEE, P3241, DOI 10.1109/ICASSP.1997.595483
   Young S. J., 1989, TIKEN PASSING CONCEP
   Young SJ, 1994, P WORKSH HUM LANG TE, P307, DOI DOI 10.3115/1075812.1075885
   YU D, 1994, P ICSLP YOK, P1503
   Zhang SX, 2010, IEEE SIGNAL PROC LET, V17, P945, DOI 10.1109/LSP.2010.2077626
NR 81
TC 13
Z9 16
U1 5
U2 40
PU TAYLOR & FRANCIS LTD
PI ABINGDON
PA 2-4 PARK SQUARE, MILTON PARK, ABINGDON OR14 4RN, OXON, ENGLAND
SN 0256-4602
EI 0974-5971
J9 IETE TECH REV
JI IETE Tech. Rev.
PD JUL 4
PY 2015
VL 32
IS 4
BP 240
EP 251
DI 10.1080/02564602.2015.1010611
PG 12
WC Engineering, Electrical & Electronic; Telecommunications
SC Engineering; Telecommunications
GA CP2YK
UT WOS:000359743700002
DA 2020-02-19
ER

PT J
AU Gu, F
   Florez-Revuelta, F
   Monekosso, D
   Remagnino, P
AF Gu, Feng
   Florez-Revuelta, Francisco
   Monekosso, Dorothy
   Remagnino, Paolo
TI Marginalised Stacked Denoising Autoencoders for Robust Representation of
   Real-Time Multi-View Action Recognition
SO SENSORS
LA English
DT Article
DE deep learning; marginalised stacked denoising autoencoders; bag of
   words; multiple kernel learning; multi-view action recognition
ID SYSTEM
AB Multi-view action recognition has gained a great interest in video surveillance, human computer interaction, and multimedia retrieval, where multiple cameras of different types are deployed to provide a complementary field of views. Fusion of multiple camera views evidently leads to more robust decisions on both tracking multiple targets and analysing complex human activities, especially where there are occlusions. In this paper, we incorporate the marginalised stacked denoising autoencoders (mSDA) algorithm to further improve the bag of words (BoWs) representation in terms of robustness and usefulness for multi-view action recognition. The resulting representations are fed into three simple fusion strategies as well as a multiple kernel learning algorithm at the classification stage. Based on the internal evaluation, the codebook size of BoWs and the number of layers of mSDA may not significantly affect recognition performance. According to results on three multi-view benchmark datasets, the proposed framework improves recognition performance across all three datasets and outputs record recognition performance, beating the state-of-art algorithms in the literature. It is also capable of performing real-time action recognition at a frame rate ranging from 33 to 45, which could be further improved by using more powerful machines in future applications.
C1 [Gu, Feng; Monekosso, Dorothy; Remagnino, Paolo] Univ Kingston, Sch Comp & Informat Syst, Kingston Upon Thames KT1 2EE, Surrey, England.
   [Florez-Revuelta, Francisco] Univ Kingston, Fac Sci Engn & Comp, Kingston Upon Thames KT1 2EE, Surrey, England.
RP Gu, F (reprint author), Univ Kingston, Sch Comp & Informat Syst, Penrhyn Rd, Kingston Upon Thames KT1 2EE, Surrey, England.
EM F.Gu@kingston.ac.uk; F.Florez@kingston.ac.uk;
   D.Monekosso@kingston.ac.uk; P.Remagnino@kingston.ac.uk
RI Remagnino, Paolo/K-1829-2012; Florez-Revuelta, Francisco/J-3370-2013;
   Florez, Francisco/AAH-9180-2019
OI Remagnino, Paolo/0000-0002-9168-7746; Florez-Revuelta,
   Francisco/0000-0002-3391-711X; Monekosso, Dorothy/0000-0001-7322-5911
FU Ambient Assisted Living Joint Programme; Innovate UK under project
   "BREATHE-Platform for self-assessment and efficient management for
   informal caregivers" [AAL-JP-2012-5-045]
FX This work has been supported by the Ambient Assisted Living Joint
   Programme and Innovate UK under project "BREATHE-Platform for
   self-assessment and efficient management for informal caregivers"
   (AAL-JP-2012-5-045). The funders had no role in study design, data
   collection and analysis, decision to publish, or preparation of the
   manuscript.
CR Chaaraoui AA, 2014, SENSORS-BASEL, V14, P8895, DOI 10.3390/s140508895
   Chaaraoui AA, 2013, PATTERN RECOGN LETT, V34, P1799, DOI 10.1016/j.patrec.2013.01.021
   Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153
   Bergstra J., 2010, P PYTH SCI COMP C SC, P3
   Bishop CM, 2006, PATTERN RECOGNITION
   Blitzer J., 2006, P 2006 C EMP METH NA, P120
   Bobick AF, 2001, IEEE T PATTERN ANAL, V23, P257, DOI 10.1109/34.910878
   Burghouts G, 2013, 2013 10TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS 2013), P250, DOI 10.1109/AVSS.2013.6636648
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chen M, 2012, PROCEEDINGS OF THE 6TH INTERNATIONAL CONFERENCE ON APAC 2011
   Chen M., 2012, P LEARN WORKSH UT UT
   Cheng Z., 2012, P ECCV 2012 WORKSH C
   Cherla Srikanth, 2008, 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P1, DOI 10.1109/CVPRW.2008.4563179
   Cilla R, 2014, EXPERT SYST, V31, P354, DOI 10.1111/exsy.12040
   Cilla R, 2012, NEUROCOMPUTING, V75, P78, DOI 10.1016/j.neucom.2011.03.051
   Dauphin Y N, 2011, P 28 INT C MACH LEAR
   Feng Gu, 2014, Ambient Assisted Living and Daily Activities. 6th International Work-Conference, IWAAL 2014. Proceedings: LNCS 8868, P26, DOI 10.1007/978-3-319-13105-4_5
   Glorot X., 2011, P 28 INT C MACH LEAR
   Gonen M, 2011, J MACH LEARN RES, V12, P2211
   Hashemi SM, 2016, MULTIMED TOOLS APPL, V75, P6755, DOI 10.1007/s11042-015-2606-5
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Holte MB, 2012, IEEE J-STSP, V6, P553, DOI 10.1109/JSTSP.2012.2193556
   Iosifidis A., 2012, 2012 INT JOINT C NEU, P1
   Iosifidis A, 2012, COMPUT VIS IMAGE UND, V116, P347, DOI 10.1016/j.cviu.2011.08.008
   Kittler J, 1998, IEEE T PATTERN ANAL, V20, P226, DOI 10.1109/34.667881
   Laptev I, 2008, PROC CVPR IEEE, P3222
   Larochelle H, 2009, J MACH LEARN RES, V10, P1
   Linsker R, 2008, ADV NEURAL INFORM PR
   MacKay D. J. C., 2003, INFORM THEORY INFERE
   Mosabbeb EA, 2013, SENSORS-BASEL, V13, P8750, DOI 10.3390/s130708750
   Ni B., 2011, P IEEE WORKSH CONS D
   Varma M., 2009, P INT C MACH LEARN I
   Vedaldi  A., 2009, P INT C COMP VIS ICC
   Vincent P, 2008, P 25 INT C MACH LEAR, P1096, DOI DOI 10.1145/1390156.1390294
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Wang H., 2013, P IEEE INT C COMP VI
   Weinland D., 2010, P EUR C COMP VIS CRE
   Weinland D, 2006, COMPUT VIS IMAGE UND, V104, P249, DOI 10.1016/j.cviu.2006.07.013
   Xu Z., 2012, P 21 ACM C INF KNOWL
   Yan P., 2008, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2008.4587737
   Zhu F, 2013, PATTERN RECOGN LETT, V34, P20, DOI 10.1016/j.patrec.2012.04.016
NR 41
TC 8
Z9 9
U1 0
U2 11
PU MDPI
PI BASEL
PA ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
EI 1424-8220
J9 SENSORS-BASEL
JI Sensors
PD JUL
PY 2015
VL 15
IS 7
BP 17209
EP 17231
DI 10.3390/s150717209
PG 23
WC Chemistry, Analytical; Engineering, Electrical & Electronic; Instruments
   & Instrumentation
SC Chemistry; Engineering; Instruments & Instrumentation
GA CS0YN
UT WOS:000361788200122
PM 26193271
OA DOAJ Gold, Green Accepted, Green Published
DA 2020-02-19
ER

PT J
AU Varga, ED
   Beretka, SF
   Noce, C
   Sapienza, G
AF Varga, Ervin D.
   Beretka, Sandor F.
   Noce, Christian
   Sapienza, Gianluca
TI Robust Real-Time Load Profile Encoding and Classification Framework for
   Efficient Power Systems Operation
SO IEEE TRANSACTIONS ON POWER SYSTEMS
LA English
DT Article
DE Classification algorithms; load modeling; multidimensional systems;
   multi-layer neural network; multilevel systems; real-time systems;
   unsupervised learning
ID NEAREST-NEIGHBOR
AB Neatly represented and properly classified load profiles are fundamental to many control optimization techniques of modern power systems, especially in a distribution area. This paper presents a novel load profile management software framework for boosting the efficiency of power systems operation. The proposed framework encodes and classifies load profiles in real-time. Imperfections as well as time-shifts in the input (measured power consumption levels) are tolerated by the suggested system, thus always providing accurate, fast and reliable output. The framework's fully component based structure allows easy customizations of the encoding as well as the classification engines. The default encoding engine is based on an artificial neural network, a variant known as a deep learning auto-encoder comprised from stacked sparse auto-encoders. The default classifier engine is based on an implementation of a locality sensitive hashing algorithm. The developed methodology was tested on the real case of a set of anonymous customers supplied by a power distribution company. The paper also contains an elaboration about the experiences gained during the design, implementation and testing phase of this system as well as a detailed engineering use case of the framework's applicability.
C1 [Varga, Ervin D.] Univ Novi Sad, Fac Tech Sci, Novi Sad 21000, Serbia.
   [Beretka, Sandor F.] Schneider Elect DMS NS, Novi Sad 21000, Serbia.
   [Noce, Christian; Sapienza, Gianluca] ENEL SpA, Milan, Italy.
RP Varga, ED (reprint author), Univ Novi Sad, Fac Tech Sci, Novi Sad 21000, Serbia.
EM e.varga@ieee.org; sandor.beretka@gmail.com; christian.noce@enel.com;
   gianluca.sapienza@enel.com
RI Varga, Ervin/P-4679-2017
OI Varga, Ervin/0000-0002-3954-4078
FU Collegium Talentum (Tatabanya, Studium ter 1, Hungary); Expro I.T.
   Consulting Ltd. (Kikinda, Svetosavska 43, I/2, Serbia)
FX This work was supported in part by Collegium Talentum (Tatabanya,
   Studium ter 1, Hungary) and Expro I.T. Consulting Ltd. (Kikinda,
   Svetosavska 43, I/2, Serbia). Paper no. TPWRS-00233- 2014.
CR Andoni A, 2006, ANN IEEE SYMP FOUND, P459
   [Anonymous], 2012, PJM MANUAL 19 LOAD F
   Baldi Pierre, 2012, J MACH LEARN RES P T, V27, P37
   Beretka S., 2013, P INT C REN EN RES A
   Beretka SF, 2013, AFRICON
   Bobric EC, 2009, ADV ELECTR COMPUT EN, V9, P63, DOI 10.4316/aece.2009.01011
   Broder A. Z., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P327, DOI 10.1145/276698.276781
   Broder AZ, 1998, COMPRESSION AND COMPLEXITY OF SEQUENCES 1997 - PROCEEDINGS, P21, DOI 10.1109/SEQUEN.1997.666900
   Catalao JPS, 2011, IEEE T SUSTAIN ENERG, V2, P50, DOI 10.1109/TSTE.2010.2076359
   Chicco G, 2006, IEEE T POWER SYST, V21, P933, DOI 10.1109/TPWRS.2006.873122
   Chicco G, 2004, IEEE T POWER SYST, V19, P1232, DOI 10.1109/TPWRS.2004.826810
   Chicco G, 2012, ENERGY, V42, P68, DOI 10.1016/j.energy.2011.12.031
   Dahl G., P IASTED INT C PAR D, P220
   Datar M, 2004, P 20 ANN S COMP GEOM, P253, DOI DOI 10.1145/997817.997857
   Depuru SSSR, 2011, IEEE POW ENER SOC GE
   Depuru SSSR, 2012, P N AM POW S NAPS
   Ernst B, 2007, IEEE POWER ENERGY M, V5, P78, DOI 10.1109/MPE.2007.906306
   Guthikonda SM, 2005, KOHONEN SELF ORG MAP
   Haykin S., 2009, NEURAL NETWORKS LEAR
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Indyk P., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P604, DOI 10.1145/276698.276876
   Kabouris J, 2010, IEEE T SUSTAIN ENERG, V1, P107, DOI 10.1109/TSTE.2010.2050348
   Kalaitzakis K, 2002, ELECTR POW SYST RES, V63, P185, DOI 10.1016/S0378-7796(02)00123-2
   Kok K., 2011, FINAL ARCHITECTURE S
   Koponen P., 2011, LOAD RESPONSE MODELI
   Li XH, 2011, IEEE T POWER SYST, V26, P932, DOI 10.1109/TPWRS.2010.2070882
   Li XL, 2010, IEEE T IND ELECTRON, V57, P3639, DOI 10.1109/TIE.2009.2027926
   Lo KL, 2005, P 5 WSEAS INT C POW, V2005, P212
   Meldorf M, 2009, OIL SHALE, V26, P243, DOI 10.3176/oil.2009.3S.07
   Mutanen A., 2010, CUSTOMER CLASSIFICAT
   Ng A., CS294A LECT NOTES
   Pan J, 2012, PROC INT CONF DATA, P378, DOI 10.1109/ICDE.2012.40
   Pauleve L, 2010, PATTERN RECOGN LETT, V31, P1348, DOI 10.1016/j.patrec.2010.04.004
   Rajaraman A., 2010, MINING MASSIVE DATAS
   Tsekouras GJ, 2007, IEEE T POWER SYST, V22, P1120, DOI 10.1109/TPWRS.2007.901287
   Tuytelaars T, 2007, IEEE I CONF COMP VIS, P754
   Yunzhi Wang, 2011, International Journal of Advanced Pervasive and Ubiquitous Computing, V3, P39, DOI 10.4018/japuc.2011040106
NR 37
TC 24
Z9 28
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 0885-8950
EI 1558-0679
J9 IEEE T POWER SYST
JI IEEE Trans. Power Syst.
PD JUL
PY 2015
VL 30
IS 4
BP 1897
EP 1904
DI 10.1109/TPWRS.2014.2354552
PG 8
WC Engineering, Electrical & Electronic
SC Engineering
GA CK9AK
UT WOS:000356531600023
DA 2020-02-19
ER

PT J
AU Liu, BY
   Liu, J
   Lu, HG
AF Liu, Bingyuan
   Liu, Jing
   Lu, Hanging
TI Learning representative and discriminative image representation by deep
   appearance and spatial coding
SO COMPUTER VISION AND IMAGE UNDERSTANDING
LA English
DT Article
DE Image classification; Deep learning; Structured sparsity
ID CLASSIFICATION; LEVEL
AB How to build a suitable image representation remains a critical problem in computer vision. Traditional Bag-of-Feature (BoF) based models build image representation by the pipeline of local feature extraction, feature coding and spatial pooling. However, three major shortcomings hinder the performance, i.e., the limitation of hand-designed features, the discrimination loss in local appearance coding and the lack of spatial information. To overcome the above limitations, in this paper, we propose a generalized BoF-based framework, which is hierarchically learned by exploring recently developed deep learning methods. First, with raw images as input, we densely extract local patches and learn local features by stacked Independent Subspace Analysis network. The learned features are then transformed to appearance codes by sparse Restricted Boltzmann Machines. Second, we perform spatial max-pooling on a set of over-complete spatial regions, which is generated by covering various spatial distributions, to incorporate more flexible spatial information. Third, a structured sparse Auto-encoder is proposed to explore the region representations into the image-level signature. To learn the proposed hierarchy, we layerwise pre-train the network in unsupervised manner, followed by supervised fine-tuning with image labels. Extensive experiments on different benchmarks, i.e., UIUC-Sports, Caltech-101, Caltech-256, Scene-15 and MIT Indoor-67, demonstrate the effectiveness of our proposed model. (C) 2015 Elsevier Inc. All rights reserved.
C1 [Liu, Bingyuan; Liu, Jing; Lu, Hanging] CASIA, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
RP Liu, J (reprint author), CASIA, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
EM jliu@nlpr.ia.ac.cn
FU 973 ProgramNational Basic Research Program of China [2012CB316304];
   National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61272329, 61472422, 61332016, 61273034]
FX This work was supported by 973 Program (2012CB316304) and National
   Natural Science Foundation of China (61272329, 61472422, 61332016 and
   61273034).
CR Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   Bengio S., 2009, NIPS, P676
   Boiman O, 2008, PROC CVPR IEEE, P1992, DOI 10.1109/CVPR.2008.4587598
   Boureau YL, 2011, IEEE I CONF COMP VIS, P2651, DOI 10.1109/ICCV.2011.6126555
   Boureau YL, 2010, PROC CVPR IEEE, P2559, DOI 10.1109/CVPR.2010.5539963
   Chatfield K., 2011, BMVC
   Chen XY, 2011, IEEE I CONF COMP VIS, P834, DOI 10.1109/ICCV.2011.6126323
   Csurka G., 2004, ECCV, P1
   Dalal N, 2005, PROC CVPR IEEE, P886
   Dixit M, 2011, PROC CVPR IEEE, P937, DOI 10.1109/CVPR.2011.5995674
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Fei-Fei L., 2011, CVPR, P524
   Gao SH, 2010, PROC CVPR IEEE, P3555, DOI 10.1109/CVPR.2010.5539943
   Goh H, 2012, LECT NOTES COMPUT SC, V7576, P298, DOI 10.1007/978-3-642-33715-4_22
   Griffin G., 2007, 7694 CAL I TECHN
   Harada T, 2011, PROC CVPR IEEE, P1617, DOI 10.1109/CVPR.2011.5995691
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Huang F, 2007, B ENTOMOL RES, V97, P1, DOI DOI 10.1787/HEMP-V19-ART3-EN
   Hyvrinen A., 2009, NATURAL IMAGE STAT
   Jenatton R., 2010, ICML, P2297
   Jia YQ, 2012, PROC CVPR IEEE, P3370, DOI 10.1109/CVPR.2012.6248076
   Jiashi Feng, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2697, DOI 10.1109/CVPR.2011.5995370
   Juneja M, 2013, PROC CVPR IEEE, P923, DOI 10.1109/CVPR.2013.124
   Jurie F, 2005, IEEE I CONF COMP VIS, P604
   Kavukcuoglu K., 2010, ADV NEURAL INFORM PR, V23, P1090
   Lazebnik S., 2006, P IEEE INT C COMP VI, V2, P2169, DOI DOI 10.1109/CVPR.2006.68
   Le Q. V., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3361, DOI 10.1109/CVPR.2011.5995496
   Lee H., 2009, P 26 ANN INT C MACH, P609, DOI DOI 10.1145/1553374.1553453
   Li FF, 2007, COMPUT VIS IMAGE UND, V106, P59, DOI 10.1016/j.cviu.2005.09.012
   Li L.-J., 2010, P ADV NEUR INF PROC, P1378
   Li L. -J., 2010, TRENDS TOPICS COMPUT, P57, DOI DOI 10.1007/978-3-642-35749-7
   Li LJ, 2007, IEEE I CONF COMP VIS, P345
   Lin D, 2014, PROC CVPR IEEE, P3726, DOI 10.1109/CVPR.2014.476
   Liu D., 2008, CVPR
   Liu LQ, 2011, IEEE I CONF COMP VIS, P2486, DOI 10.1109/ICCV.2011.6126534
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mairal J., 2010, ADV NEURAL INFORM PR, P1558
   Morioka N., 2010, BMVC
   Morioka N, 2010, LECT NOTES COMPUT SC, V6311, P692, DOI 10.1007/978-3-642-15549-9_50
   Perronnin F, 2006, LECT NOTES COMPUT SC, V3954, P464
   Perronnin F., 2007, CVPR
   Quattoni A., 2006, CVPR, P413
   Razavian A., 2014, CVPR 2014 WORKSH
   Savarese S., 2006, CVPR, V2, P2033
   Schmitz M., 2008, P IEEE INT S PAR DIS, P1, DOI DOI 10.1109/CVPR.2008.4587367
   Sharma G., 2011, BMVC
   Sharma G, 2012, PROC CVPR IEEE, P3506, DOI 10.1109/CVPR.2012.6248093
   Todorovic S., 2008, CVPR
   van Gemert JC, 2010, IEEE T PATTERN ANAL, V32, P1271, DOI 10.1109/TPAMI.2009.132
   Wang JJ, 2010, PROC CVPR IEEE, P3360, DOI 10.1109/CVPR.2010.5540018
   Xie LX, 2014, PROC CVPR IEEE, P3734, DOI 10.1109/CVPR.2014.477
   Yang JC, 2010, PROC CVPR IEEE, P3517, DOI 10.1109/CVPR.2010.5539958
   Yang JC, 2009, PROC CVPR IEEE, P1794, DOI 10.1109/CVPRW.2009.5206757
   Yu K, 2011, PROC CVPR IEEE, P1713, DOI 10.1109/CVPR.2011.5995732
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zeiler MD, 2011, IEEE I CONF COMP VIS, P2018, DOI 10.1109/ICCV.2011.6126474
   Zheng YB, 2012, LECT NOTES COMPUT SC, V7576, P172, DOI 10.1007/978-3-642-33715-4_13
   Zhou X., 2008, P 2 INT C SIGN PROC, P1, DOI DOI 10.1109/GL0C0M.2008.ECP.573
   Zhou X, 2009, IEEE I CONF COMP VIS, P1971, DOI 10.1109/ICCV.2009.5459435
   Zhou X, 2010, LECT NOTES COMPUT SC, V6315, P141, DOI 10.1007/978-3-642-15555-0_11
NR 61
TC 4
Z9 4
U1 0
U2 25
PU ACADEMIC PRESS INC ELSEVIER SCIENCE
PI SAN DIEGO
PA 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
SN 1077-3142
EI 1090-235X
J9 COMPUT VIS IMAGE UND
JI Comput. Vis. Image Underst.
PD JUL
PY 2015
VL 136
SI SI
BP 23
EP 31
DI 10.1016/j.cviu.2015.03.006
PG 9
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
SC Computer Science; Engineering
GA CK3LE
UT WOS:000356116800004
DA 2020-02-19
ER

PT J
AU Lu, JW
   Liong, VE
   Wang, G
   Moulin, P
AF Lu, Jiwen
   Liong, Venice Erin
   Wang, Gang
   Moulin, Pierre
TI Joint Feature Learning for Face Recognition
SO IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY
LA English
DT Article
DE Face recognition; feature learning; joint learning; deep learning
ID REPRESENTATION; PATTERNS; IMAGE; SAMPLE; MODEL; SCALE; VERIFICATION;
   MAGNITUDES; EIGENFACES; HISTOGRAM
AB This paper presents a new joint feature learning (JFL) approach to automatically learn feature representation from raw pixels for face recognition. Unlike many existing face recognition systems, where conventional feature descriptors, such as local binary patterns and Gabor features, are used for face representation, we propose an unsupervised feature learning method to learn hierarchical feature representation. Since different face regions have different physical characteristics, we propose to use different feature dictionaries to represent them, and to learn multiple yet related feature projection matrices for these regions simultaneously. Hence position-specific discriminative information can be exploited for face representation. Having learned these feature projections for different face regions, we perform spatial pooling for face patches within each region to enhance the representative power of the learned features. Moreover, we stack our JFL model into a deep architecture to exploit hierarchical information for feature representation and further improve the recognition performance. Experimental results on five widely used face data sets show the effectiveness of our proposed approach.
C1 [Lu, Jiwen; Liong, Venice Erin; Wang, Gang; Moulin, Pierre] Adv Digital Sci Ctr, Singapore 138632, Singapore.
   [Wang, Gang] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
   [Moulin, Pierre] Univ Illinois, Dept Elect & Comp Engn, Champaign, IL 61820 USA.
RP Lu, JW (reprint author), Adv Digital Sci Ctr, Singapore 138632, Singapore.
EM jiwen.lu@adsc.com.sg; venice.l@adsc.com.sg; wanggang@ntu.edu.sg;
   moulin@ifp.uiuc.edu
RI Wang, Gang/B-7027-2013; Lu, Jiwen/C-5291-2009
OI Lu, Jiwen/0000-0002-6121-5529
FU Human Cyber Security Systems Program within the Advanced Digital
   Sciences Center, Singapore, through the Agency for Science, Technology
   and Research, Singapore
FX This work was supported by the Human Cyber Security Systems Program
   within the Advanced Digital Sciences Center, Singapore, through the
   Agency for Science, Technology and Research, Singapore. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. Zhenan Sun.
CR Ahonen T, 2004, LECT NOTES COMPUT SC, V3021, P469
   Arasu S., 2013, P IEEE 6 INT C BIOM, P1, DOI DOI 10.1109/BTAS.2013.6712721
   Argyriou A., 2007, NEURAL INFORM PROCES, V19, P41
   Barkan O, 2013, IEEE I CONF COMP VIS, P1960, DOI 10.1109/ICCV.2013.246
   Bartlett MS, 2002, IEEE T NEURAL NETWOR, V13, P1450, DOI 10.1109/TNN.2002.804287
   Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228
   Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153
   Beveridge J, 2013, BIOM THEOR APPL SYST, P1
   Cao Q, 2013, IEEE I CONF COMP VIS, P2408, DOI 10.1109/ICCV.2013.299
   Cao ZM, 2010, PROC CVPR IEEE, P2707, DOI 10.1109/CVPR.2010.5539992
   Chai ZH, 2014, IEEE T INF FOREN SEC, V9, P14, DOI 10.1109/TIFS.2013.2290064
   Chen D, 2013, PROC CVPR IEEE, P3025, DOI 10.1109/CVPR.2013.389
   Cui Z, 2013, PROC CVPR IEEE, P3554, DOI 10.1109/CVPR.2013.456
   Deng WH, 2014, PATTERN RECOGN, V47, P3738, DOI 10.1016/j.patcog.2014.06.020
   Deng WH, 2012, PATTERN RECOGN, V45, P4438, DOI 10.1016/j.patcog.2012.06.010
   Deng WH, 2012, IEEE T PATTERN ANAL, V34, P1864, DOI 10.1109/TPAMI.2012.30
   Deniz O, 2011, PATTERN RECOGN LETT, V32, P1598, DOI 10.1016/j.patrec.2011.01.004
   Fan JL, 2010, IEEE T NEURAL NETWOR, V21, P1610, DOI 10.1109/TNN.2010.2066286
   Gao W, 2008, IEEE T SYST MAN CY A, V38, P149, DOI 10.1109/TSMCA.2007.909557
   Gui J, 2014, IEEE T IMAGE PROCESS, V23, P3126, DOI 10.1109/TIP.2014.2326001
   Gui J, 2014, IEEE T CIRC SYST VID, V24, P211, DOI 10.1109/TCSVT.2013.2273652
   Gui J, 2012, PATTERN RECOGN, V45, P2884, DOI 10.1016/j.patcog.2012.02.005
   Gui J, 2010, ARTIF INTELL MED, V50, P181, DOI 10.1016/j.artmed.2010.05.004
   Gui J, 2010, NEUROCOMPUTING, V73, P2696, DOI 10.1016/j.neucom.2010.04.017
   He XF, 2005, IEEE T PATTERN ANAL, V27, P328, DOI 10.1109/TPAMI.2005.55
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hu JL, 2014, PROC CVPR IEEE, P1875, DOI 10.1109/CVPR.2014.242
   Huang G. B., 2014, UMCS2014003
   Huang G. B., 2007, 0749 U MASS DEP COMP
   Huang GB, 2012, PROC CVPR IEEE, P2518, DOI 10.1109/CVPR.2012.6247968
   Hyvarinen A, 2009, COMPUT IMAGING VIS, V39, P151
   KUMAR A, 2012, POW IND C 2012 IEEE, P1
   Le Q. V., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3361, DOI 10.1109/CVPR.2011.5995496
   Le Q. V., 2011, ADV NEURAL INFORM PR, P1017
   Lee H., 2006, ADV NEURAL INF PROCE, P801
   Lei Z, 2014, IEEE T PATTERN ANAL, V36, P289, DOI 10.1109/TPAMI.2013.112
   Lei Z, 2011, IEEE T IMAGE PROCESS, V20, P247, DOI 10.1109/TIP.2010.2060207
   Li HX, 2013, PROC CVPR IEEE, P3499, DOI 10.1109/CVPR.2013.449
   Liu CJ, 2002, IEEE T IMAGE PROCESS, V11, P467, DOI 10.1109/TIP.2002.999679
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu JQ, 2014, EARTHQ ENG ENG VIB, V13, P13, DOI 10.1007/s11803-014-0208-2
   Lu JW, 2013, IEEE T PATTERN ANAL, V35, P39, DOI 10.1109/TPAMI.2012.70
   Maturana D, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P470, DOI 10.1109/FG.2011.5771444
   Maturana D, 2010, P 10 AS C COMP VIS, V6495, P618
   Mendez-Vazquez H., 2013, P ICB JUN, P1, DOI DOI 10.1109/ICB.2013.6612990
   Meng X, 2006, INT C PATT RECOG, P536
   Meyers E, 2008, INT J COMPUT VISION, V76, P93, DOI 10.1007/s11263-007-0058-8
   Vu NS, 2012, IEEE T IMAGE PROCESS, V21, P1352, DOI 10.1109/TIP.2011.2166974
   Nguyen Hieu V., 2010, Computer Vision - ACCV 2010. 10th Asian Conference on Computer Vision. Revised Selected Papers, P709, DOI 10.1007/978-3-642-19309-5_55
   Ouyang WL, 2012, PROC CVPR IEEE, P3258, DOI 10.1109/CVPR.2012.6248062
   Phillips PJ, 2000, IEEE T PATTERN ANAL, V22, P1090, DOI 10.1109/34.879790
   Salah R, 2011, P 28 INT C MACH LEAR, P833, DOI 10.1016/j.wasman.2010.10.006
   Schwartz WR, 2012, IEEE T IMAGE PROCESS, V21, P2245, DOI 10.1109/TIP.2011.2176951
   Seo HJ, 2011, IEEE T INF FOREN SEC, V6, P1275, DOI 10.1109/TIFS.2011.2159205
   Sharma A, 2012, PROC CVPR IEEE, P2160, DOI 10.1109/CVPR.2012.6247923
   Sun Y, 2014, ADV NEURAL INFORM PR, V60, P1988
   Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244
   Tan XY, 2006, PATTERN RECOGN, V39, P1725, DOI 10.1016/j.patcog.2006.03.013
   TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71
   ul Hussain S, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.99
   VERSCHAE R, 2008, [No title captured], P1
   Vu N.-S., 2011, P INT JOINT C BIOM, P1
   Vu NS, 2013, IEEE T INF FOREN SEC, V8, P295, DOI 10.1109/TIFS.2012.2224866
   Wolf L, 2013, PROC CVPR IEEE, P3523, DOI 10.1109/CVPR.2013.452
   Wolf L, 2011, PROC CVPR IEEE, P529, DOI 10.1109/CVPR.2011.5995566
   Xie SF, 2009, SIGNAL PROCESS, V89, P2333, DOI 10.1016/j.sigpro.2009.02.016
   Yi D, 2013, PROC CVPR IEEE, P3539, DOI 10.1109/CVPR.2013.454
   Zhang BH, 2007, IEEE T IMAGE PROCESS, V16, P57, DOI 10.1109/TIP.2006.884956
   Zhang BC, 2010, IEEE T IMAGE PROCESS, V19, P533, DOI 10.1109/TIP.2009.2035882
   Zhang WC, 2005, IEEE I CONF COMP VIS, P786
NR 70
TC 43
Z9 47
U1 1
U2 40
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1556-6013
EI 1556-6021
J9 IEEE T INF FOREN SEC
JI IEEE Trans. Inf. Forensic Secur.
PD JUL
PY 2015
VL 10
IS 7
BP 1371
EP 1383
DI 10.1109/TIFS.2015.2408431
PG 13
WC Computer Science, Theory & Methods; Engineering, Electrical & Electronic
SC Computer Science; Engineering
GA CI7PK
UT WOS:000354956100005
DA 2020-02-19
ER

PT J
AU Motlicek, P
   Imseng, D
   Potard, B
   Garner, PN
   Himawan, I
AF Motlicek, Petr
   Imseng, David
   Potard, Blaise
   Garner, Philip N.
   Himawan, Ivan
TI Exploiting foreign resources for DNN-based ASR
SO EURASIP JOURNAL ON AUDIO SPEECH AND MUSIC PROCESSING
LA English
DT Article
DE Automatic speech recognition; Deep learning for speech; Acoustic model
   adaptation; Semi-supervised training
ID SPEECH; ALGORITHM; FEATURES
AB Manual transcription of audio databases for the development of automatic speech recognition (ASR) systems is a costly and time-consuming process. In the context of deriving acoustic models adapted to a specific application, or in low-resource scenarios, it is therefore essential to explore alternatives capable of improving speech recognition results. In this paper, we investigate the relevance of foreign data characteristics, in particular domain and language, when using this data as an auxiliary data source for training ASR acoustic models based on deep neural networks (DNNs). The acoustic models are evaluated on a challenging bilingual database within the scope of the MediaParl project. Experimental results suggest that in-language (but out-of-domain) data is more beneficial than in-domain (but out-of-language) data when employed in either supervised or semi-supervised training of DNNs. The best performing ASR system, an HMM/GMM acoustic model that exploits DNN as a discriminatively trained feature extractor outperforms the best performing HMM/DNN hybrid by about 5 % relative (in terms of WER). An accumulated relative gain with respect to the MFCC-HMM/GMM baseline is about 30 % WER.
C1 [Motlicek, Petr; Imseng, David; Potard, Blaise; Garner, Philip N.; Himawan, Ivan] Idiap Res Inst, Martigny, Switzerland.
RP Motlicek, P (reprint author), Idiap Res Inst, Rue Marconi 19, Martigny, Switzerland.
EM motlicek@idiap.ch; dimseng@idiap.ch; bpotard@idiap.ch; garner@idiap.ch;
   ihimawan@idiap.ch
OI Himawan, Ivan/0000-0003-3848-244X; Garner, Philip/0000-0002-0814-1348
FU Samsung Electronics Co. Ltd., South Korea; Eurostars Programme by
   Eureka; European CommunityEuropean Community (EC); EC FP7 "Speaker
   Identification integrated project" [607784]
FX The work was supported by Samsung Electronics Co. Ltd., South Korea,
   under the project "Multi-Lingual and Cross-Lingual Adaptation for
   Automatic Speech Recognition". The work was also partially supported by
   Eurostars Programme powered by Eureka and the European Community under
   the project "D-Box: A generic dialog box for multi-lingual
   conversational applications", and by EC FP7 "Speaker Identification
   integrated project" under grant agreement no. 607784. The authors would
   like to express their gratitude to the MediaParl project, funded by the
   Parliament Service of the State of Valais, Switzerland for their
   financial support and for providing access to the audio-video
   recordings.
CR Athineos M, 2003, P ASRU, P261
   Cohen J, 2007, 2007 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING, VOLS 1 AND 2, P237, DOI 10.1109/ASRU.2007.4430115
   Dahl GE, 2012, IEEE T AUDIO SPEECH, V20, P30, DOI 10.1109/TASL.2011.2134090
   Erhan D., 2009, J MACHINE LEARNING R, V5, P153
   Erhan D, 2010, J MACH LEARN RES, V11, P625
   Gales MJF, 1998, COMPUT SPEECH LANG, V12, P75, DOI 10.1006/csla.1998.0043
   GALES MJF, 1997, CUEDFINFENGTR291
   Galliano S., 2006, P INT C LANG RES EV
   Ganapathy Sriram, 2009, 2009 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA), P341, DOI 10.1109/ASPAA.2009.5346495
   Ganapathy S, 2009, J ACOUST SOC AM, V125, pEL8, DOI 10.1121/1.3040022
   Ghoshal A, 2013, INT CONF ACOUST SPEE, P7319, DOI 10.1109/ICASSP.2013.6639084
   Grezl F., 2011, 2011 IEEE Workshop on Automatic Speech Recognition & Understanding (ASRU), P359, DOI 10.1109/ASRU.2011.6163958
   Grezl F, 2009, INTERSPEECH 2009: 10TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2009, VOLS 1-5, P2915
   Hermansky H, 2003, ASRU'03: 2003 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING ASRU '03, P255, DOI 10.1109/ASRU.2003.1318450
   Hermansky H, 2000, INT CONF ACOUST SPEE, P1635, DOI 10.1109/ICASSP.2000.862024
   Hinton G. E., 2010, 2010003 UTML TR
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   HOCHBERG MM, 1995, INT CONF ACOUST SPEE, P69, DOI 10.1109/ICASSP.1995.479275
   Huang JT, 2013, INT CONF ACOUST SPEE, P7304, DOI 10.1109/ICASSP.2013.6639081
   Huang Y., 2013, P INTERSPEECH, P2360
   Imseng David, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P2322, DOI 10.1109/ICASSP.2014.6854014
   IMSENG D, 2012, P IEEE WORKSH SPOK L, P263
   IMSENG D, 2013, P IEEE WORKSH AUT SP, P332
   Imseng D, 2014, SPEECH COMMUN, V56, P142, DOI 10.1016/j.specom.2013.01.007
   Kingsbury BED, 1997, INT CONF ACOUST SPEE, P1259, DOI 10.1109/ICASSP.1997.596174
   Koehn P, 2005, MT SUMMIT, P79, DOI DOI 10.3115/1626355.1626380
   Kusumoto A, 2005, SPEECH COMMUN, V45, P101, DOI 10.1016/j.specom.2004.06.003
   Lamel L. F., 1991, EUROSPEECH 91. 2nd European Conference on Speech Communication and Technology Proceedings, P505
   Lee C.-H., 1993, ICASSP-93. 1993 IEEE International Conference on Acoustics, Speech, and Signal Processing (Cat. No.92CH3252-4), P558, DOI 10.1109/ICASSP.1993.319368
   Lyu DC, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 3 AND 4, P1986
   Matsoukas S, 1997, DARPA SPEECH REC WOR
   Mohamed A, 2009, NIPS WORKSH DEEP LEA
   Morgan N, 1994, CONTINUOUS SPEECH RE
   Ngoc Thang Vu, 2010, Proceedings 2010 IEEE Spoken Language Technology Workshop (SLT 2010), P183, DOI 10.1109/SLT.2010.5700848
   Novak J. R., 2012, P INT, P2526
   Ochiai Tsubasa, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P6349, DOI 10.1109/ICASSP.2014.6854826
   Perennou G., 1986, ICASSP 86 Proceedings. IEEE-IECEJ-ASJ International Conference on Acoustics, Speech and Signal Processing (Cat. No.86CH2243-4), P325
   Povey D, 2002, INT CONF ACOUST SPEE, P105
   POVEY D, 2005, P INTERSPEECH, P2977
   POVEY D, 2005, P ICASSP, P961
   Povey D, 2008, INT CONF ACOUST SPEE, P4057, DOI 10.1109/ICASSP.2008.4518545
   Rabiner L.R, 1986, IEEE ASSP MAGAZINE, V3, P5
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   Schiel F., 2013, AUSSPRACHE LEXIKON P
   Swietojanski P, 2013, P IEEE WORKSH AUT SP, DOI [10.1109/ASRU.2013.6707744, DOI 10.1109/ASRU.2013.6707744]
   SWIETOJANSKI P, 2012, P SLT, P246
   Swietojanski P., 2014, P IEEE WORKSH SPOK L
   THOMAS S, 2013, P ICASSP, P6704
   Thomas S, 2012, INT CONF ACOUST SPEE, P4269, DOI 10.1109/ICASSP.2012.6288862
   Thomas S, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 1-2, P877
   VESELY K, 2013, P ASRU, P267
   Vesely K, 2013, INTERSPEECH, P2344
   Vesely K, 2012, IEEE W SP LANG TECH, P336, DOI 10.1109/SLT.2012.6424246
   Wells J., 2013, SAMPA COMPUTER READA
   Young SJ, 1994, P WORKSH HUM LANG TE, P307, DOI DOI 10.3115/1075812.1075885
   Yu Dong, 2010, NIPS 2010 WORKSH DEE
NR 56
TC 3
Z9 3
U1 0
U2 11
PU SPRINGEROPEN
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 1687-4722
J9 EURASIP J AUDIO SPEE
JI EURASIP J. Audio Speech Music Process.
PD JUN 26
PY 2015
AR 17
DI 10.1186/s13636-015-0058-5
PG 10
WC Acoustics; Engineering, Electrical & Electronic
SC Acoustics; Engineering
GA CL9TR
UT WOS:000357321500001
OA DOAJ Gold
DA 2020-02-19
ER

PT J
AU Zhu, SH
   Li, XX
   Shen, SH
AF Zhu, Songhao
   Li, Xiangxiang
   Shen, Shuhan
TI Multimodal deep network learning-based image annotation
SO ELECTRONICS LETTERS
LA English
DT Article
AB Multilabel image annotation is one of the most important open problems in the computer vision field. Unlike existing works that usually use conventional visual features to annotate images, features based on deep learning have shown potential to achieve outstanding performance. A multimodal deep learning framework is proposed, which aims to optimally integrate multiple deep neural networks pre-trained with convolutional neural networks. In particular, the proposed framework explores a unified two-stage learning scheme that consists of (i) learning to fune-tune the parameters of the deep neural network with respect to each individual modality and (ii) learning to find the optimal combination of diverse modalities simultaneously in a coherent process. Experiments conducted on a variety of public datasets.
C1 [Zhu, Songhao; Li, Xiangxiang; Shen, Shuhan] Nanjing Univ Posts & Telecommun, Sch Automat, Nanjing 210046, Jiangsu, Peoples R China.
RP Zhu, SH (reprint author), Nanjing Univ Posts & Telecommun, Sch Automat, Xianlin Campus, Nanjing 210046, Jiangsu, Peoples R China.
EM zhush@njupt.edu.cn
FU Postdoctoral Foundation of ChinaChina Postdoctoral Science Foundation
   [2014M550297]; Postdoctoral Foundation of Jiangsu Province [1302087B]
FX This work was supported by the Postdoctoral Foundation of China (No.
   2014M550297), and the Postdoctoral Foundation of Jiangsu Province (No.
   1302087B).
CR BIANCHI N, 2006, [No title captured]
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Kiros R., 2012, IEEE C NEUR INF PROC, V25, P917
   Zhang ML, 2007, PATTERN RECOGN, V40, P2038, DOI 10.1016/j.patcog.2006.12.019
NR 5
TC 3
Z9 3
U1 1
U2 29
PU INST ENGINEERING TECHNOLOGY-IET
PI HERTFORD
PA MICHAEL FARADAY HOUSE SIX HILLS WAY STEVENAGE, HERTFORD SG1 2AY, ENGLAND
SN 0013-5194
EI 1350-911X
J9 ELECTRON LETT
JI Electron. Lett.
PD JUN 11
PY 2015
VL 51
IS 12
BP 905
EP 906
DI 10.1049/el.2015.0258
PG 2
WC Engineering, Electrical & Electronic
SC Engineering
GA CJ7KT
UT WOS:000355674600013
DA 2020-02-19
ER

PT J
AU Nguyen, N
   Yoshitaka, A
AF Ngoc Nguyen
   Yoshitaka, Atsuo
TI Human Interaction Recognition Using Hierarchical Invariant Features
SO INTERNATIONAL JOURNAL OF SEMANTIC COMPUTING
LA English
DT Article
DE Independent subspace analysis; human interaction recognition;
   convolutional network; pooling
ID GRAPHS; MODEL
AB Human interaction recognition has been widely studied because it has great scientific importance and many potential practical applications. However, recognizing human interactions is also very challenging especially in realistic environments where the background is dynamic or has varying lighting conditions. Most existing methods rely on either spatio-temporal local features (i.e. SIFT) or human poses, or human joints to model human interactions. As a result, they are not fully unsupervised processes because they require either hand-designed features or human detection results. Motivated by the recent success of deep learning networks, we investigate a three-layer convolutional network which uses the Independent Subspace Analysis (ISA) algorithm to learn hierarchical invariant features from videos. Using the invariant features learned by the ISA, we build a bag-of-features (BOF) model to recognize human interactions. We also evaluate the performance of our approach and the effectiveness of hierarchical invariant features on video sequences of the UT-Interaction dataset which contain both interacting persons and irrelevant pedestrians in the scenes. The dataset imposes several challenging factors including moving backgrounds, clutter scenes, scales and camera jitters. Experimental results show that our three-layer convolutional ISA network is able to learn features which are effective to represent complex activities such as human interactions in realistic environments.
C1 [Ngoc Nguyen; Yoshitaka, Atsuo] Japan Adv Inst Sci & Technol, Sch Informat Sci, Nomi, Ishikawa 9231292, Japan.
RP Nguyen, N (reprint author), Japan Adv Inst Sci & Technol, Sch Informat Sci, Nomi, Ishikawa 9231292, Japan.
EM ngocnguyen@jaist.ac.jp; ayoshi@jaist.ac.jp
CR Amer MR, 2011, IEEE I CONF COMP VIS, P786, DOI 10.1109/ICCV.2011.6126317
   Blank M, 2005, IEEE I CONF COMP VIS, P1395
   Brendel W, 2011, IEEE I CONF COMP VIS, P778, DOI 10.1109/ICCV.2011.6126316
   Dalal N, 2005, PROC CVPR IEEE, P886
   Dollar P., 2005, Proceedings. 2nd Joint IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance (VS-PETS) (IEEE Cat. No. 05EX1178), P65
   Dong Z, 2011, 2011 FIRST ASIAN CONFERENCE ON PATTERN RECOGNITION (ACPR), P77, DOI 10.1109/ACPR.2011.6166533
   Gaidon A, 2014, INT J COMPUT VISION, V107, P219, DOI 10.1007/s11263-013-0677-1
   Gaur U, 2011, IEEE I CONF COMP VIS, P2595, DOI 10.1109/ICCV.2011.6126548
   Grauman K, 2005, IEEE I CONF COMP VIS, P1458
   Hyvarinen A, 2009, COMPUT IMAGING VIS, V39, P1
   Kong Y, 2012, LECT NOTES COMPUT SC, V7572, P300, DOI 10.1007/978-3-642-33718-5_22
   Laptev I, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P432
   Laptev I, 2008, PROC CVPR IEEE, P3222
   Le Q. V., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3361, DOI 10.1109/CVPR.2011.5995496
   Marszalek M., 2009, P IEEE C COMP VIS PA, P2929, DOI DOI 10.1109/CVPR.2009.5206557
   Meng LX, 2012, INT C PATT RECOG, P609
   Raptis M, 2013, PROC CVPR IEEE, P2650, DOI 10.1109/CVPR.2013.342
   Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7
   Ryoo M. S., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P672, DOI 10.1109/ICCVW.2011.6130307
   Ryoo MS, 2010, LECT NOTES COMPUT SC, V6388, P270, DOI 10.1007/978-3-642-17711-8_28
   Ryoo MS, 2009, IEEE I CONF COMP VIS, P1593, DOI 10.1109/ICCV.2009.5459361
   Schuldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462
   Scovanner P., 2007, P 15 INT C MULT, P357, DOI DOI 10.1145/1291233.1291311
   Sefidgar Y. S., 2015, COMPUTER VISION IMAG
   Sun L, 2014, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2014.336
   Taylor GW, 2010, LECT NOTES COMPUT SC, V6316, P140, DOI 10.1007/978-3-642-15567-3_11
   Vahdat A., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P1729, DOI 10.1109/ICCVW.2011.6130458
   Waltisberg D., 2010, P ICPR CONT SEM DESC, P309
   Wang H., 2010, P BRIT MACH VIS C, P1
   Willems G, 2008, LECT NOTES COMPUT SC, V5303, P650, DOI 10.1007/978-3-540-88688-4_48
   Yu Kong, 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P1, DOI 10.1109/ICME.2012.67
   Yu T., 2010, P BRIT MACH VIS C, P1
   Yun Kiwon, 2012, P IEEE COMP SOC C CO, P28, DOI DOI 10.1109/CVPRW.2012.6239234
NR 33
TC 0
Z9 0
U1 0
U2 3
PU WORLD SCIENTIFIC PUBL CO PTE LTD
PI SINGAPORE
PA 5 TOH TUCK LINK, SINGAPORE 596224, SINGAPORE
SN 1793-351X
EI 1793-7108
J9 INT J SEMANT COMPUT
JI Int. J. Semant. Comput.
PD JUN
PY 2015
VL 9
IS 2
SI SI
BP 169
EP 191
DI 10.1142/S1793351X15400024
PG 23
WC Computer Science, Artificial Intelligence
SC Computer Science
GA CT1MM
UT WOS:000362562500003
DA 2020-02-19
ER

PT J
AU Yu, XG
   Ding, W
   Zeng, ZZ
   Leong, HW
AF Yu, Xinguo
   Ding, Wan
   Zeng, Zhizhong
   Leong, Hon Wai
TI Reading Digital Video Clocks
SO INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE
LA English
DT Article
DE Digit localization; digit recognition; digital video clock; second-pixel
   periodicity; deep learning; conditional random field
ID TEXT DETECTION; LOCALIZATION; IMAGES; DETECT; TIME
AB This paper presents an algorithm for reading digital video clocks reliably and quickly. Reading digital clocks from videos is difficult due to the challenges such as color variety, font diversity, noise, and low resolution. The proposed algorithm overcomes these challenges by using the novel methods derived from the domain knowledge. This algorithm first localizes the digits of a digital video clock and then recognizes the digits representing the time of digital video clock. It is a robust three-step algorithm. The first step is an efficient procedure that directly identifies the region of the second digit at a very low computational cost, which replaces the traditional tedious image processing procedure of identifying the second digit region. The success of the first step mainly leverages on the novel second-pixel periodicity method. Using the acquired second digit region as input, the second step is a clock digit localization procedure. It first acquires the colors of the digits of the digital video clock and performs the color conversion. Then it localizes the remaining clock digits. Finally, the last step is a clock digit recognition procedure. It first employs an enhanced digit-sequence recognition method to robustly recognize the digits on the second; it then adopts a deep learning procedure to recognize the remaining digits. The proposed algorithm is tested on a prepared benchmark of 1000 videos that is publicly available and the experimental results show that it can read digital video clocks with a 100% accuracy at a low computational cost.
C1 [Yu, Xinguo; Ding, Wan; Zeng, Zhizhong] Cent China Normal Univ, Natl Engn Res Ctr E Learning, Wuhan 430079, Peoples R China.
   [Leong, Hon Wai] Natl Univ Singapore, Dept Comp Sci, Singapore 117417, Singapore.
RP Zeng, ZZ (reprint author), Cent China Normal Univ, Natl Engn Res Ctr E Learning, 152 Luoyu Rd, Wuhan 430079, Peoples R China.
EM xgyu@mail.ccnu.edu.cn; dingwan_cn@mails.ccnu.edu.cn;
   zzzeng@mail.ccnu.edu.cn; leonghw@comp.nus.edu.sg
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61272206]; National Key Technology Research and
   Development ProgramNational Key Technology R&D Program [2013BAH18F02,
   2013BAH72B01]; Humanities and Social Sciences Foundation of the Ministry
   of Education [14YJAZH005]; Ministry of Education and China Mobile
   [MCM20130601]; 111 Plan: Collaboration Innovation Base on Educational
   Digital Media and Visualization; Natural Science Foundation of Hubei
   Province, ChinaNatural Science Foundation of Hubei Province [2014CFB661]
FX This work is partially supported by National Natural Science Foundation
   of China (No. 61272206); National Key Technology Research and
   Development Program (No. 2013BAH18F02, No. 2013BAH72B01); Research funds
   from the Humanities and Social Sciences Foundation of the Ministry of
   Education (No. 14YJAZH005), Research funds from Ministry of Education
   and China Mobile (MCM20130601); 111 Plan: Collaboration Innovation Base
   on Educational Digital Media and Visualization; Natural Science
   Foundation of Hubei Province, China (No. 2014CFB661).
CR Bu F, 2008, LECT NOTES COMPUT SC, V5353, P306
   Covavisaruch N, 2004, CISST '04: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON IMAGING SCIENCE, SYSTEMS, AND TECHNOLOGY, P173
   Epshtein B, 2010, PROC CVPR IEEE, P2963, DOI 10.1109/CVPR.2010.5540041
   Hanif Shehzad Muhammad, 2009, 2009 10th International Conference on Document Analysis and Recognition (ICDAR), P1, DOI 10.1109/ICDAR.2009.172
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee JJ, 2011, PROC INT CONF DOC, P429, DOI 10.1109/ICDAR.2011.93
   Li Y., 2006, ICASSP 2006, VII, P653
   Li YQ, 2006, INT C PATT RECOG, P128
   Lyu MR, 2005, IEEE T CIRC SYST VID, V15, P243, DOI 10.1109/TCSVT.2004.841653
   Mishra A, 2012, PROC CVPR IEEE, P2687, DOI 10.1109/CVPR.2012.6247990
   Neumann L, 2013, IEEE I CONF COMP VIS, P97, DOI 10.1109/ICCV.2013.19
   Neumann L, 2012, PROC CVPR IEEE, P3538, DOI 10.1109/CVPR.2012.6248097
   Pan YF, 2011, IEEE T IMAGE PROCESS, V20, P800, DOI 10.1109/TIP.2010.2070803
   Sermanet Pierre, 2009, 2009 21st IEEE International Conference on Tools with Artificial Intelligence (ICTAI 2009), P693, DOI 10.1109/ICTAI.2009.28
   Sha F, 2003, HLT-NAACL 2003: HUMAN LANGUAGE TECHNOLOGY CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE MAIN CONFERENCE, P213
   Shahab A, 2011, PROC INT CONF DOC, P1491, DOI 10.1109/ICDAR.2011.296
   Shi CZ, 2014, PATTERN RECOGN, V47, P2853, DOI 10.1016/j.patcog.2014.03.023
   Shi CZ, 2013, PATTERN RECOGN LETT, V34, P107, DOI 10.1016/j.patrec.2012.09.019
   Shivakumara P, 2011, IEEE T PATTERN ANAL, V33, P412, DOI 10.1109/TPAMI.2010.166
   Smith R, 2007, PROC INT CONF DOC, P629
   Wang K, 2011, IEEE I CONF COMP VIS, P1457, DOI 10.1109/ICCV.2011.6126402
   Wang T, 2012, INT C PATT RECOG, P3304
   Weinman JJ, 2009, IEEE T PATTERN ANAL, V31, P1733, DOI 10.1109/TPAMI.2009.38
   Xu C., 2006, ACM INT C MULT, P224
   Yao C, 2012, PROC CVPR IEEE, P1083, DOI 10.1109/CVPR.2012.6247787
   Ye QX, 2005, IMAGE VISION COMPUT, V23, P565, DOI 10.1016/j.imavis.2005.01.004
   Yi CC, 2012, IEEE T IMAGE PROCESS, V21, P4256, DOI 10.1109/TIP.2012.2199327
   Yi CC, 2011, IEEE T IMAGE PROCESS, V20, P2594, DOI 10.1109/TIP.2011.2126586
   Yin P, 2002, 2002 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL II, PROCEEDINGS, P73
   Yu X., 2008, IEEE POW EN SOC GEN, P1
   Yu XG, 2012, INT C PATT RECOG, P1217
   Yu XG, 2009, J VIS COMMUN IMAGE R, V20, P117, DOI 10.1016/j.jvcir.2008.12.004
   Zhang YF, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P2190
NR 33
TC 4
Z9 4
U1 0
U2 10
PU WORLD SCIENTIFIC PUBL CO PTE LTD
PI SINGAPORE
PA 5 TOH TUCK LINK, SINGAPORE 596224, SINGAPORE
SN 0218-0014
EI 1793-6381
J9 INT J PATTERN RECOGN
JI Int. J. Pattern Recognit. Artif. Intell.
PD JUN
PY 2015
VL 29
IS 4
AR 1555006
DI 10.1142/S021800141555006X
PG 21
WC Computer Science, Artificial Intelligence
SC Computer Science
GA CI5UI
UT WOS:000354824900008
DA 2020-02-19
ER

PT J
AU Chen, YS
   Zhao, X
   Jia, XP
AF Chen, Yushi
   Zhao, Xing
   Jia, Xiuping
TI Spectral-Spatial Classification of Hyperspectral Data Based on Deep
   Belief Network
SO IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE
   SENSING
LA English
DT Article; Proceedings Paper
CT 6th Workshop on Hyperspectral Image and Signal Processing - Evolution in
   Remote Sensing (WHISPERS)
CY JUN 24-27, 2014
CL Lausanne, SWITZERLAND
DE Deep belief network (DBN); deep learning; feature extraction (FE);
   hyperspectral data classification; logistic regression (LR); restricted
   Boltzmann machine (RBM); support vector machine (SVM)
ID IMAGE CLASSIFICATION; DIMENSIONALITY REDUCTION; FEATURE-SELECTION;
   REPRESENTATION; ALGORITHM; URBAN
AB Hyperspectral data classification is a hot topic in remote sensing community. In recent years, significant effort has been focused on this issue. However, most of the methods extract the features of original data in a shallow manner. In this paper, we introduce a deep learning approach into hyperspectral image classification. A new feature extraction (FE) and image classification framework are proposed for hyperspectral data analysis based on deep belief network (DBN). First, we verify the eligibility of restricted Boltzmann machine (RBM) and DBN by the following spectral information-based classification. Then, we propose a novel deep architecture, which combines the spectral-spatial FE and classification together to get high classification accuracy. The framework is a hybrid of principal component analysis (PCA), hierarchical learning-based FE, and logistic regression (LR). Experimental results with hyperspectral data indicate that the classifier provide competitive solution with the state-of-the-art methods. In addition, this paper reveals that deep learning system has huge potential for hyperspectral data classification.
C1 [Chen, Yushi; Zhao, Xing] Harbin Inst Technol, Sch Elect & Informat Engn, Dept Informat Engn, Harbin 150001, Peoples R China.
   [Jia, Xiuping] Univ New S Wales, Sch Engn & Informat Technol, Sydney, NSW 1000, Australia.
RP Chen, YS (reprint author), Harbin Inst Technol, Sch Elect & Informat Engn, Dept Informat Engn, Harbin 150001, Peoples R China.
EM chenyushi@hit.edu.cn; zhaoxing@hit.edu.cn; x.jia@adfa.edu.au
OI Jia, Xiuping/0000-0001-9916-6382
FU Fundamental Research Funds for the Central UniversitiesFundamental
   Research Funds for the Central Universities [HIT.NSRIF.2013028];
   National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61301206, 61371180, 61471148]
FX This work was supported in part by the Fundamental Research Funds for
   the Central Universities under Grant HIT.NSRIF.2013028, and in part by
   the National Natural Science Foundation of China under Grant 61301206,
   Grant 61371180, and Grant 61471148). (Corresponding author: Yushi Chen.)
CR Ambikapathi A., 2012, SIGNAL, V1, P1
   Benediktsson JA, 2005, IEEE T GEOSCI REMOTE, V43, P480, DOI 10.1109/TGRS.2004.842478
   Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153
   Bengio Y., 2007, LARGE SCALE KERNEL M, V34
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bioucas-Dias JM, 2013, IEEE GEOSC REM SEN M, V1, P6, DOI 10.1109/MGRS.2013.2244672
   Bruce LM, 2002, IEEE T GEOSCI REMOTE, V40, P2331, DOI 10.1109/TGRS.2002.804721
   Camps-Valls G, 2005, IEEE T GEOSCI REMOTE, V43, P1351, DOI 10.1109/TGRS.2005.846154
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chang CI, 1999, IEEE T GEOSCI REMOTE, V37, P2631, DOI 10.1109/36.803411
   Chen H, 2003, IEE P-VIS IMAGE SIGN, V150, P153, DOI 10.1049/ip-vis:20030362
   Chen YS, 2014, IEEE J-STARS, V7, P1295, DOI 10.1109/JSTARS.2014.2307356
   Fauvel M, 2008, IEEE T GEOSCI REMOTE, V46, P3804, DOI 10.1109/TGRS.2008.922034
   Foody GM, 2004, IEEE T GEOSCI REMOTE, V42, P1335, DOI 10.1109/TGRS.2004.827257
   FUKUSHIMA K, 1980, BIOL CYBERN, V36, P193, DOI 10.1007/BF00344251
   HARSANYI JC, 1994, IEEE T GEOSCI REMOTE, V32, P779, DOI 10.1109/36.298007
   Hecht-Nielsen R, 1988, IEEE T NEURAL NETWOR, DOI [DOI 10.1109/IJCNN.1989.118638, 10.1016/0893-6080(88)90469-8]
   Hege K, 2003, P SOC PHOTO-OPT INS, V5159, P380, DOI 10.1117/12.506426
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Huang WH, 2014, IEEE T INTELL TRANSP, V15, P2191, DOI 10.1109/TITS.2014.2311123
   Jia XP, 2013, P IEEE, V101, P676, DOI 10.1109/JPROC.2012.2229082
   Jimenez LO, 1999, IEEE T GEOSCI REMOTE, V37, P2653, DOI 10.1109/36.803413
   Kruger N, 2013, IEEE T PATTERN ANAL, V35, P1847, DOI 10.1109/TPAMI.2012.272
   Lacar FM, 2001, INT GEOSCI REMOTE SE, P2875, DOI 10.1109/IGARSS.2001.978191
   Landgrebe D, 2002, IEEE SIGNAL PROC MAG, V19, P17, DOI 10.1109/79.974718
   Larochelle H., 2007, P 24 INT C MACH LEAR, P473, DOI DOI 10.1145/1273496.1273556
   Le Roux N, 2010, NEURAL COMPUT, V22, P2192, DOI 10.1162/neco.2010.08-09-1081
   Lee H., 2008, ADV NEURAL INFORM PR, V20, P873
   Li J, 2013, IEEE T GEOSCI REMOTE, V51, P844, DOI 10.1109/TGRS.2012.2205263
   Liu JJ, 2013, IEEE J-STARS, V6, P2462, DOI 10.1109/JSTARS.2013.2252150
   Malthus TJ, 2003, INT J REMOTE SENS, V24, P2805, DOI 10.1080/0143116031000066954
   Melgani F, 2004, IEEE T GEOSCI REMOTE, V42, P1778, DOI 10.1109/TGRS.2004.831865
   Mohamed Abdel-rahman, 2009, SCIENCE, P1, DOI DOI 10.4249/SCHOLARPEDIA.5947
   Plaza A., 2009, MACHINE LEARNING SIG, P1
   Qian YT, 2013, IEEE J-STARS, V6, P499, DOI 10.1109/JSTARS.2012.2232904
   Rajan S, 2008, IEEE T GEOSCI REMOTE, V46, P1231, DOI 10.1109/TGRS.2007.910220
   Richards J. A., 2013, REMOTE SENSING DIGIT
   Salakhutdinov R., 2008, ADV NEURAL INFORM PR, V20, P1249
   Salakhutdinov R, 2009, ARTIF INTELL, P448, DOI DOI 10.1109/CVPRW.2009.5206577
   Samadzadegan F, 2012, CAN J REMOTE SENS, V38, P139, DOI 10.5589/m12-022
   Santos AB, 2013, IEEE J-STARS, V6, P1450, DOI 10.1109/JSTARS.2013.2251969
   Serpico SB, 2001, IEEE T GEOSCI REMOTE, V39, P1360, DOI 10.1109/36.934069
   Sutskever I, 2008, NEURAL COMPUT, V20, P2629, DOI 10.1162/neco.2008.12-07-661
   Van der Meer F., 2004, INT J APPL EARTH OBS, V5, P55, DOI DOI 10.1016/J.JAG.2003.09.001
   Wang G, 2012, IEEE T PATTERN ANAL, V34, P2177, DOI 10.1109/TPAMI.2012.29
   Yu D., 2009, P NIPS WORKSH, P1
   Yu D, 2012, IEEE T AUDIO SPEECH, V20, P4, DOI 10.1109/TASL.2011.2173371
   Yuan HL, 2014, IEEE J-STARS, V7, P2174, DOI 10.1109/JSTARS.2014.2328601
   Yuen PWT, 2010, IMAGING SCI J, V58, P241, DOI 10.1179/174313110X12771950995716
   Zhang HY, 2014, IEEE J-STARS, V7, P2056, DOI 10.1109/JSTARS.2013.2264720
   Zhu Z, 2012, REMOTE SENS ENVIRON, V117, P72, DOI 10.1016/j.rse.2011.07.020
   Zuo Z, 2014, IEEE SIGNAL PROC LET, V21, P1159, DOI 10.1109/LSP.2014.2298888
NR 54
TC 331
Z9 356
U1 35
U2 272
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1939-1404
EI 2151-1535
J9 IEEE J-STARS
JI IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens.
PD JUN
PY 2015
VL 8
IS 6
SI SI
BP 2381
EP 2392
DI 10.1109/JSTARS.2015.2388577
PG 12
WC Engineering, Electrical & Electronic; Geography, Physical; Remote
   Sensing; Imaging Science & Photographic Technology
SC Engineering; Physical Geography; Remote Sensing; Imaging Science &
   Photographic Technology
GA CO6JW
UT WOS:000359264000006
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Hou, WL
   Gao, XB
   Tao, DC
   Li, XL
AF Hou, Weilong
   Gao, Xinbo
   Tao, Dacheng
   Li, Xuelong
TI Blind Image Quality Assessment via Deep Learning
SO IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
LA English
DT Article
DE Deep learning; image quality assessment (IQA); natural scene statistics
   (NSS); no reference
ID NATURAL SCENE STATISTICS; NEURAL-NETWORK; INFORMATION; FRAMEWORK
AB This paper investigates how to blindly evaluate the visual quality of an image by learning rules from linguistic descriptions. Extensive psychological evidence shows that humans prefer to conduct evaluations qualitatively rather than numerically. The qualitative evaluations are then converted into the numerical scores to fairly benchmark objective image quality assessment (IQA) metrics. Recently, lots of learning-based IQA models are proposed by analyzing the mapping from the images to numerical ratings. However, the learnt mapping can hardly be accurate enough because some information has been lost in such an irreversible conversion from the linguistic descriptions to numerical scores. In this paper, we propose a blind IQA model, which learns qualitative evaluations directly and outputs numerical scores for general utilization and fair comparison. Images are represented by natural scene statistics features. A discriminative deep model is trained to classify the features into five grades, corresponding to five explicit mental concepts, i.e., excellent, good, fair, poor, and bad. A newly designed quality pooling is then applied to convert the qualitative labels into scores. The classification framework is not only much more natural than the regression-based models, but also robust to the small sample size problem. Thorough experiments are conducted on popular databases to verify the model's effectiveness, efficiency, and robustness.
C1 [Hou, Weilong; Gao, Xinbo] Xidian Univ, Sch Elect Engn, Xian 710071, Shaanxi, Peoples R China.
   [Tao, Dacheng] Univ Technol Sydney, Ctr Quantum Computat & Intelligent Syst, Ultimo, NSW 2007, Australia.
   [Tao, Dacheng] Univ Technol Sydney, Fac Engn & Informat Technol, Ultimo, NSW 2007, Australia.
   [Li, Xuelong] Chinese Acad Sci, Xian Inst Opt & Precis Mech, State Key Lab Transient Opt & Photon, Ctr Opt IMagery Anal & Learning OPTIMAL, Xian 710119, Shaanxi, Peoples R China.
RP Hou, WL (reprint author), Xidian Univ, Sch Elect Engn, 2 South Taibai Rd, Xian 710071, Shaanxi, Peoples R China.
EM weilonghou@gmail.com; xbgao@ieee.org; dacheng.tao@uts.edu.au;
   xuelong_li@opt.ac.cn
RI Li, Xuelong/Z-3785-2019
OI Li, Xuelong/0000-0002-0019-4197
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61125204, 61125106, 61172146, 61101250, 61372130];
   Fundamental Research Funds for the Central UniversitiesFundamental
   Research Funds for the Central Universities [K5051202048, BDZ021403,
   JB149901]; Program for Changjiang Scholars and Innovative Research
   TeamProgram for Changjiang Scholars & Innovative Research Team in
   University (PCSIRT); University of China [IRT13088]; Shaanxi Innovative
   Research Team for Key Science and Technology [2012KCT-02]; Key Research
   Program, Chinese Academy of Sciences, Beijing, China [KGZD-EW-T03];
   Australian Research CouncilAustralian Research Council [FT-130101457,
   DP-120103730, LP-140100569]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61125204, Grant 61125106, Grant
   61172146, Grant 61101250, and Grant 61372130, in part by the Fundamental
   Research Funds for the Central Universities under Grant K5051202048,
   Grant BDZ021403, Grant JB149901, in part by the Program for Changjiang
   Scholars and Innovative Research Team with the University of China under
   Grant IRT13088, in part by the Shaanxi Innovative Research Team for Key
   Science and Technology under Grant 2012KCT-02, in part by the Key
   Research Program, Chinese Academy of Sciences, Beijing, China, under
   Grant KGZD-EW-T03, and in part by the Australian Research Council under
   Project FT-130101457, Project DP-120103730, and Project LP-140100569.
CR Arel I, 2010, IEEE COMPUT INTELL M, V5, P13, DOI 10.1109/MCI.2010.938364
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Bianchini M, 2014, IEEE T NEUR NET LEAR, V25, P1553, DOI 10.1109/TNNLS.2013.2293637
   BT. 500, 2002, BT500110602
   Callet P.L., 2006, SUBJECTIVE QUALITY A
   Collobert R., 2008, P 25 ICML, V25, P160, DOI [10.1145/1390156.1390177, DOI 10.1145/1390156.1390177]
   Gao XB, 2013, IEEE T NEUR NET LEAR, V24, P2013, DOI 10.1109/TNNLS.2013.2271356
   Gao XB, 2009, IEEE T IMAGE PROCESS, V18, P1409, DOI 10.1109/TIP.2009.2018014
   He LH, 2012, PROC CVPR IEEE, P1146, DOI 10.1109/CVPR.2012.6247795
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Horita Y., 2000, MICT IMAGE QUALITY E
   Hutchins E., 1995, COGNITION WILD
   Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105
   Li CF, 2011, IEEE T NEURAL NETWOR, V22, P793, DOI 10.1109/TNN.2011.2120620
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Mittal A, 2012, IEEE SIGNAL PROC LET, V19, P75, DOI 10.1109/LSP.2011.2179293
   Moghaddam B, 2007, IEEE I CONF COMP VIS, P2073
   Mohamed AR, 2011, INT CONF ACOUST SPEE, P5060
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Moorthy AK, 2010, IEEE SIGNAL PROC LET, V17, P513, DOI 10.1109/LSP.2010.2043888
   Ponomarenko N., 2009, ADV MOD RADIOELECTRO, V10, P30
   Romberg JK, 2001, IEEE T IMAGE PROCESS, V10, P1056, DOI 10.1109/83.931100
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Saad MA, 2010, IEEE SIGNAL PROC LET, V17, P583, DOI 10.1109/LSP.2010.2045550
   Shao L, 2014, IEEE T NEUR NET LEAR, V25, P2303, DOI 10.1109/TNNLS.2014.2308519
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Sheikh HR, 2005, IEEE T IMAGE PROCESS, V14, P2117, DOI 10.1109/TIP.2005.859389
   Sheikh HR, 2005, IEEE T IMAGE PROCESS, V14, P1918, DOI 10.1109/TIP.2005.854492
   Simoncelli EP, 2001, ANNU REV NEUROSCI, V24, P1193, DOI 10.1146/annurev.neuro.24.1.1193
   Tang HX, 2011, PROC CVPR IEEE, P305, DOI 10.1109/CVPR.2011.5995446
   Varadarajan S, 2008, IEEE IMAGE PROC, P401, DOI 10.1109/ICIP.2008.4711776
   VQEG, 2009, VAL RED REF NO REF O
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2002, IEEE IMAGE PROC, P477
   Xia YS, 2012, IEEE T NEUR NET LEAR, V23, P812, DOI 10.1109/TNNLS.2012.2184800
   Ye P, 2012, PROC CVPR IEEE, P1098, DOI 10.1109/CVPR.2012.6247789
   Yu JF, 2014, IEEE T NEUR NET LEAR, V25, P780, DOI 10.1109/TNNLS.2013.2281313
   Zhang KB, 2013, IEEE T NEUR NET LEAR, V24, P1648, DOI 10.1109/TNNLS.2013.2262001
   Zhong S., 2011, P 19 ACM INT C MULT, P343
   Zhong SH, 2010, IEEE IMAGE PROC, P1553, DOI 10.1109/ICIP.2010.5653807
NR 42
TC 158
Z9 168
U1 15
U2 121
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 2162-237X
EI 2162-2388
J9 IEEE T NEUR NET LEAR
JI IEEE Trans. Neural Netw. Learn. Syst.
PD JUN
PY 2015
VL 26
IS 6
BP 1275
EP 1286
DI 10.1109/TNNLS.2014.2336852
PG 12
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Computer Science, Theory & Methods; Engineering,
   Electrical & Electronic
SC Computer Science; Engineering
GA CI7PS
UT WOS:000354957000013
PM 25122842
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Noda, K
   Yamaguchi, Y
   Nakadai, K
   Okuno, HG
   Ogata, T
AF Noda, Kuniaki
   Yamaguchi, Yuki
   Nakadai, Kazuhiro
   Okuno, Hiroshi G.
   Ogata, Tetsuya
TI Audio-visual speech recognition using deep learning
SO APPLIED INTELLIGENCE
LA English
DT Article
DE Audio-visual speech recognition; Feature extraction; Deep learning;
   Multi-stream HMM
ID NEURAL-NETWORKS; EXTRACTION; FEATURES
AB Audio-visual speech recognition (AVSR) system is thought to be one of the most promising solutions for reliable speech recognition, particularly when the audio is corrupted by noise. However, cautious selection of sensory features is crucial for attaining high recognition performance. In the machine-learning community, deep learning approaches have recently attracted increasing attention because deep neural networks can effectively extract robust latent features that enable various recognition algorithms to demonstrate revolutionary generalization capabilities under diverse application conditions. This study introduces a connectionist-hidden Markov model (HMM) system for noise-robust AVSR. First, a deep denoising autoencoder is utilized for acquiring noise-robust audio features. By preparing the training data for the network with pairs of consecutive multiple steps of deteriorated audio features and the corresponding clean features, the network is trained to output denoised audio features from the corresponding features deteriorated by noise. Second, a convolutional neural network (CNN) is utilized to extract visual features from raw mouth area images. By preparing the training data for the CNN as pairs of raw images and the corresponding phoneme label outputs, the network is trained to predict phoneme labels from the corresponding mouth area input images. Finally, a multi-stream HMM (MSHMM) is applied for integrating the acquired audio and visual HMMs independently trained with the respective features. By comparing the cases when normal and denoised mel-frequency cepstral coefficients (MFCCs) are utilized as audio features to the HMM, our unimodal isolated word recognition results demonstrate that approximately 65 % word recognition rate gain is attained with denoised MFCCs under 10 dB signal-to-noise-ratio (SNR) for the audio signal input. Moreover, our multimodal isolated word recognition results utilizing MSHMM with denoised MFCCs and acquired visual features demonstrate that an additional word recognition rate gain is attained for the SNR conditions below 10 dB.
C1 [Noda, Kuniaki; Ogata, Tetsuya] Waseda Univ, Grad Sch Fundamental Sci & Engn, Tokyo 1698555, Japan.
   [Yamaguchi, Yuki; Okuno, Hiroshi G.] Kyoto Univ, Grad Sch Informat, Kyoto 6068501, Japan.
   [Nakadai, Kazuhiro] Honda Res Inst Japan Co Ltd, Saitama 3510114, Japan.
RP Noda, K (reprint author), Waseda Univ, Grad Sch Fundamental Sci & Engn, Tokyo 1698555, Japan.
EM kuniaki.noda@akane.waseda.jp; yamaguch@kuis.kyoto-u.ac.jp;
   nakadai@jp.honda-ri.com; okuno@kuis.kyoto-u.ac.jp; ogata@waseda.jp
RI Okuno, Hiroshi G/S-3130-2018
OI Okuno, Hiroshi G/0000-0002-8704-4318; Ogata, Tetsuya/0000-0001-7015-0379
FU JST PRESTO "Information Environment and Humans"; MEXTMinistry of
   Education, Culture, Sports, Science and Technology, Japan (MEXT)
   [24119003, 24220006];  [265114]
FX This work has been supported by JST PRESTO "Information Environment and
   Humans" and MEXT Grant-in-Aid for Scientific Research on Innovative
   Areas "Constructive Developmental Science" (24119003), Scientific
   Research (S) (24220006), and JSPS Fellows (265114).
CR Abdel-Hamid O., 2013, P 14 ANN C INT SPEEC
   Abdel-Hamid O, 2012, INT CONF ACOUST SPEE, P4277, DOI 10.1109/ICASSP.2012.6288864
   Aleksic PS, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL V, PROCEEDINGS, P917
   Barker J, 1999, P 14 INT C PHON SCI, P5
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Bourlard H, 1996, ICSLP 96 - FOURTH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE PROCESSING, PROCEEDINGS, VOLS 1-4, P426
   Bourlard H., 1996, MULTISTREAM SPEECH R
   Bourlard H., 1994, CONNECTIONIST SPEECH
   Brooke N. M., 1986, International Conference on Speech Input/Output; Techniques and Applications (Conf. Publ. No. 258), P104
   Coates A., 2013, P 30 INT C MACH LEAR, p1337~1345
   Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467
   Dahl GE, 2012, IEEE T AUDIO SPEECH, V20, P30, DOI 10.1109/TASL.2011.2134090
   Gurban Mihai, 2008, P 10 INT C MULT INT, P237
   Heckmann M., 2002, P INT C SPOK LANG PR, P1925
   Hermansky H, 2000, INT CONF ACOUST SPEE, P1635, DOI 10.1109/ICASSP.2000.862024
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Huang J, 2013, INT CONF ACOUST SPEE, P7596, DOI 10.1109/ICASSP.2013.6639140
   JANIN A, 1999, P 6 EUR C SPEECH COM
   Krizhevsky A., 2011, P 19 EUR S ART NEUR
   Krizhevsky A., 2012, ADV NEURAL INFORM PR
   Kusy B, 2009, 2009 INTERNATIONAL CONFERENCE ON INFORMATION PROCESSING IN SENSOR NETWORKS (IPSN 2009), P109
   Kuwabara H., 1989, ICASSP-89: 1989 International Conference on Acoustics, Speech and Signal Processing (IEEE Cat. No.89CH2673-2), P560, DOI 10.1109/ICASSP.1989.266488
   Lan Y., 2010, P INT C AUD VIS SPEE
   Le Q. V., 2012, P 29 INT C MACH LEAR, P81, DOI DOI 10.1109/MSP.2011.940881
   LeCun Y, 2004, PROC CVPR IEEE, P97
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee H., 2009, P 26 ANN INT C MACH, P609, DOI DOI 10.1145/1553374.1553453
   Lerner B, 1999, PATTERN RECOGN LETT, V20, P7, DOI 10.1016/S0167-8655(98)00120-2
   Luettin J, 1996, INT CONF ACOUST SPEE, P817, DOI 10.1109/ICASSP.1996.543246
   Maas AL, 2013, P 2 INT WORKSH MACH
   Martens J., 2010, P 27 INT C MACH LEAR, P735, DOI DOI 10.1155/2011/176802
   Matthews I, 2002, IEEE T PATTERN ANAL, V24, P198, DOI 10.1109/34.982900
   Matthews I., 2001, P IEEE INT C MULT EX
   Mohamed AR, 2012, IEEE T AUDIO SPEECH, V20, P14, DOI 10.1109/TASL.2011.2109382
   Ngiam J, 2011, P 28 INT C MACH LEAR
   NVIDIA Corporation, 2014, CUBLAS LIB VERS 6 0
   Palaz D, 2013, P 14 ANN C INT SPEEC
   PEARLMUTTER BA, 1994, NEURAL COMPUT, V6, P147, DOI 10.1162/neco.1994.6.1.147
   Renals S, 1994, IEEE T SPEECH AUDI P, V2, P161, DOI 10.1109/89.260359
   Robert-Ribes J., 1996, SPEECHREADING MAN MA, V150, P193
   Sainath TN, 2012, INT CONF ACOUST SPEE, P4153, DOI 10.1109/ICASSP.2012.6288833
   Scanlon P, 2001, 2001 IEEE FOURTH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P625, DOI 10.1109/MMSP.2001.962802
   Schraudolph NN, 2002, NEURAL COMPUT, V14, P1723, DOI 10.1162/08997660260028683
   Slaney M., 1998, AUDITORY TOOLBOX MAT
   Sutskever I., 2011, P 28 INT C MACH LEAR, P1017
   Vincent P, 2008, P 25 INT C MACH LEAR, P1096, DOI DOI 10.1145/1390156.1390294
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Xue Feng, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P1759, DOI 10.1109/ICASSP.2014.6853900
   Yehia H, 1998, SPEECH COMMUN, V26, P23, DOI 10.1016/S0167-6393(98)00048-X
   Young Steve, 2009, HTK BOOK HTK VERSION
NR 51
TC 110
Z9 115
U1 10
U2 99
PU SPRINGER
PI DORDRECHT
PA VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN 0924-669X
EI 1573-7497
J9 APPL INTELL
JI Appl. Intell.
PD JUN
PY 2015
VL 42
IS 4
BP 722
EP 737
DI 10.1007/s10489-014-0629-7
PG 16
WC Computer Science, Artificial Intelligence
SC Computer Science
GA CG4GP
UT WOS:000353243900008
OA Bronze
DA 2020-02-19
ER

PT J
AU Zhang, SH
   Yang, H
   Yin, ZP
AF Zhang, Shaohua
   Yang, Hua
   Yin, Zhouping
TI Multiple deep convolutional neural networks averaging for face alignment
SO JOURNAL OF ELECTRONIC IMAGING
LA English
DT Article
DE face alignment; facial landmark localization; face recognition; deep
   learning; convolutional neural network
ID POSE ESTIMATION; DIMENSIONALITY; REPRESENTATION; LOCALIZATION; MODELS
AB Face alignment is critical for face recognition, and the deep learning-based method shows promise for solving such issues, given that competitive results are achieved on benchmarks with additional benefits, such as dispensing with handcrafted features and initial shape. However, most existing deep learning-based approaches are complicated and quite time-consuming during training. We propose a compact face alignment method for fast training without decreasing its accuracy. Rectified linear unit is employed, which allows all networks approximately five times faster convergence than a tanh neuron. An eight learnable layer deep convolutional neural network (DCNN) based on local response normalization and a padding convolutional layer (PCL) is designed to provide reliable initial values during prediction. A model combination scheme is presented to further reduce errors, while showing that only two network architectures and hyperparameter selection procedures are required in our approach. A three-level cascaded system is ultimately built based on the DCNNs and model combination mode. Extensive experiments validate the effectiveness of our method and demonstrate comparable accuracy with state-of-the-art methods on BioID, labeled face parts in the wild, and Helen datasets. (C) 2015 SPIE and IS&T
C1 [Zhang, Shaohua; Yang, Hua; Yin, Zhouping] Huazhong Univ Sci & Technol, State Key Lab Digital Mfg Equipment & Technol, Sch Mech Sci & Engn, Wuhan 430074, Peoples R China.
RP Yang, H (reprint author), Huazhong Univ Sci & Technol, State Key Lab Digital Mfg Equipment & Technol, Sch Mech Sci & Engn, Luoyu Rd 1037, Wuhan 430074, Peoples R China.
EM huayang@hust.edu.cn
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [51327801, 51475193]
FX This research was partially supported by the National Natural Science
   Foundation of China (Grant Nos. 51327801 and 51475193). The authors
   would also like to thank Alex Krizhevsky from the University of Toronto,
   Yi Sun from the Chinese University of Hong Kong, Jie Zhang from the Key
   Lab of Intelligent Information Processing of the Chinese Academy of
   Sciences, and Li Wan from New York University for helpful suggestions
   and open resources.
CR [Anonymous], 2013, MICROSOFT RES FAC SD
   Belhumeur PN, 2011, PROC CVPR IEEE, P545, DOI 10.1109/CVPR.2011.5995602
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Burgos-Artizzu XP, 2013, IEEE I CONF COMP VIS, P1513, DOI 10.1109/ICCV.2013.191
   Cao C, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2462012
   Cao XD, 2014, INT J COMPUT VISION, V107, P177, DOI 10.1007/s11263-013-0667-3
   Chen D, 2013, PROC CVPR IEEE, P3025, DOI 10.1109/CVPR.2013.389
   Ciresan D, 2012, PROC CVPR IEEE, P3642, DOI 10.1109/CVPR.2012.6248110
   COOTES TF, 1995, COMPUT VIS IMAGE UND, V61, P38, DOI 10.1006/cviu.1995.1004
   Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467
   Cootes TF, 2012, LECT NOTES COMPUT SC, V7578, P278, DOI 10.1007/978-3-642-33786-4_21
   Cristinacce D, 2008, PATTERN RECOGN, V41, P3054, DOI 10.1016/j.patcog.2008.01.024
   Dantone M, 2012, PROC CVPR IEEE, P2578, DOI 10.1109/CVPR.2012.6247976
   Dollar P, 2010, PROC CVPR IEEE, P1078, DOI 10.1109/CVPR.2010.5540094
   Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451
   Glorot X., 2011, JMLR W CP
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Huang Gary B., 2007, 0749 U MASS
   Jesorsky O., 2001, 3 INT C AUD VID BAS
   Kazemi V, 2014, PROC CVPR IEEE, P1867, DOI 10.1109/CVPR.2014.241
   Krizhevsky A., 2014, CUDA CONVNET
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Le V, 2012, LECT NOTES COMPUT SC, V7574, P679, DOI 10.1007/978-3-642-33712-3_49
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LEE H, 2007, ADV NEURAL INFORM PR, V20
   Liang L, 2008, LECT NOTES COMPUT SC, V5303, P72, DOI 10.1007/978-3-540-88688-4_6
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Maas AL, 2013, ICML WORKSH DEEP LEA
   Matthews I, 2004, INT J COMPUT VISION, V60, P135, DOI 10.1023/B:VISI.0000029666.37597.d3
   Ozuysal M, 2010, IEEE T PATTERN ANAL, V32, P448, DOI 10.1109/TPAMI.2009.23
   Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7
   Osadchy M, 2007, J MACH LEARN RES, V8, P1197
   Pantic M, 2000, IEEE T PATTERN ANAL, V22, P1424, DOI 10.1109/34.895976
   Ren SQ, 2014, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR.2014.218
   Sagonas C, 2013, IEEE COMPUT SOC CONF, P896, DOI 10.1109/CVPRW.2013.132
   Saragih JM, 2011, INT J COMPUT VISION, V91, P200, DOI 10.1007/s11263-010-0380-4
   Snoek J., 2012, ADV NEURAL INFORM PR, P2951
   Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244
   Sun Y, 2013, PROC CVPR IEEE, P3476, DOI 10.1109/CVPR.2013.446
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Valstar M, 2010, PROC CVPR IEEE, P2729, DOI 10.1109/CVPR.2010.5539996
   Wu HS, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3475952
   Xiong XH, 2013, PROC CVPR IEEE, P532, DOI 10.1109/CVPR.2013.75
   Yan JJ, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P392, DOI 10.1109/ICCVW.2013.126
   Zhang JH, 2014, ELECTRON J QUAL THEO, P1, DOI [10.1007/978-3-319-10605-2_1, 10.14232/ejqtde.2014.1.50]
   Zhou EJ, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P386, DOI 10.1109/ICCVW.2013.58
   Zhu XX, 2012, PROC CVPR IEEE, P2879, DOI 10.1109/CVPR.2012.6248014
NR 48
TC 3
Z9 3
U1 0
U2 30
PU IS&T & SPIE
PI BELLINGHAM
PA 1000 20TH ST, BELLINGHAM, WA 98225 USA
SN 1017-9909
EI 1560-229X
J9 J ELECTRON IMAGING
JI J. Electron. Imaging
PD MAY
PY 2015
VL 24
IS 3
AR 033013
DI 10.1117/1.JEI.24.3.033013
PG 13
WC Engineering, Electrical & Electronic; Optics; Imaging Science &
   Photographic Technology
SC Engineering; Optics; Imaging Science & Photographic Technology
GA CP3ZM
UT WOS:000359821800013
DA 2020-02-19
ER

PT J
AU Li, SJ
   Liu, ZQ
   Chan, AB
AF Li, Sijin
   Liu, Zhi-Qiang
   Chan, Antoni B.
TI Heterogeneous Multi-task Learning for Human Pose Estimation with Deep
   Convolutional Neural Network
SO INTERNATIONAL JOURNAL OF COMPUTER VISION
LA English
DT Article
DE Human Pose Estimation; Deep Learning
AB We propose a heterogeneous multi-task learning framework for human pose estimation from monocular images using a deep convolutional neural network. In particular, we simultaneously learn a human pose regressor and sliding-window body-part and joint-point detectors in a deep network architecture. We show that including the detection tasks helps to regularize the network, directing it to converge to a good solution. We report competitive and state-of-art results on several datasets. We also empirically show that the learned neurons in the middle layer of our network are tuned to localized body parts.
C1 [Li, Sijin] City Univ Hong Kong, Dept Comp Sci, Hong Kong, Hong Kong, Peoples R China.
   [Liu, Zhi-Qiang] City Univ Hong Kong, SCM, Hong Kong, Hong Kong, Peoples R China.
   [Chan, Antoni B.] City Univ Hong Kong, Multimedia Software Engn Res Ctr MERC, Dept Comp Sci, Hong Kong, Hong Kong, Peoples R China.
   [Chan, Antoni B.] Multimedia Software Engn Res Ctr MERC, Shenzhen, Guangdong, Peoples R China.
RP Li, SJ (reprint author), City Univ Hong Kong, Dept Comp Sci, Hong Kong, Hong Kong, Peoples R China.
EM sijin.li@my.cityu.edu.hk; smzliu@cityu.edu.hk; abchan@cityu.edu.hk
RI CHAN, Antoni B./D-7858-2013
OI CHAN, Antoni B./0000-0002-2886-2513
FU Research Grants Council of the Hong Kong Special Administrative Region,
   ChinaHong Kong Research Grants Council [CityU 123212, CityU 110513, GRF
   9041574 (CityU 118810), GRF 9041905 (CityU 119313)]
FX A.B.C. was supported by the Research Grants Council of the Hong Kong
   Special Administrative Region, China (CityU 123212 and CityU 110513).
   This work was supported by the Research Grants Council of the Hong Kong
   Special Administrative Region, China, under GRF 9041574 (CityU 118810),
   GRF 9041905 (CityU 119313).
CR Bo LF, 2010, INT J COMPUT VISION, V87, P28, DOI 10.1007/s11263-008-0204-y
   Dalal N., 2005, IEEE C COMP VIS PATT
   Dantone M., 2013, IEEE C COMP VIS PATT
   Eichner M, 2012, INT J COMPUT VISION, V99, P190, DOI 10.1007/s11263-012-0524-9
   Eichner M., 2012, IEEE T PATTERN ANAL
   Eichner M., 2009, UPPER BODY DETECTOR
   Eichner M., 2010, EUR C COMP VIS
   Eichner M., 2009, P BRIT MACH VIS C, P1
   Evgeniou T, 2005, J MACH LEARN RES, V6, P615
   Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231
   Felzenszwalb PF, 2005, INT J COMPUT VISION, V61, P55, DOI 10.1023/B:VISI.0000042934.15159.49
   Gulcehrem C., 2013, INT C LEARN REPR
   Hara K., 2013, IEEE C COMP VIS PATT
   Jain A., 2014, INT C LEARN REPR
   Johnson MK, 2011, PROC CVPR IEEE
   Krizhevsky A., 2012, NEURAL INFORM PROCES
   Le Quoc V, 2012, INT C MACH LEARN
   Nair V., 2010, INT C MACH LEARN
   Pishchulin L, 2013, PROC CVPR IEEE, P588, DOI 10.1109/CVPR.2013.82
   Pishchulin Leonid, 2012, IEEE C COMP VIS PATT
   Rumelhart D. E., 1988, NEUROCOMPUTING FDN R, V323, P696, DOI DOI 10.1002/1520-6696(198907)25:3<239::AID-JHBS2300250311>3.0.CO;2-A
   Sapp B., 2010, EUR C COMP VIS
   Sapp B, 2013, IEEE C COMP VIS PATT
   Shotton J., 2011, IEEE C COMP VIS PATT
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Sun Yi, 2013, IEEE C COMP VIS PATT
   Toshev A, 2014, IEEE C COMP VIS PATT
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Weston  Jason, 2008, INT C MACH LEARN
   Yang X., 2009, NEURAL INFORM PROCES
   Yang Y., 2011, IEEE C COMP VIS PATT
   Yang Y, 2013, IEEE T PATTERN ANAL, V35, P2878, DOI 10.1109/TPAMI.2012.261
   Yu K., 2005, P 22 INT C MACH LEAR, P1012
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
NR 34
TC 30
Z9 33
U1 2
U2 56
PU SPRINGER
PI DORDRECHT
PA VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN 0920-5691
EI 1573-1405
J9 INT J COMPUT VISION
JI Int. J. Comput. Vis.
PD MAY
PY 2015
VL 113
IS 1
SI SI
BP 19
EP 36
DI 10.1007/s11263-014-0767-8
PG 18
WC Computer Science, Artificial Intelligence
SC Computer Science
GA CH6GX
UT WOS:000354135700003
DA 2020-02-19
ER

PT J
AU Zheng, Y
   Zemel, RS
   Zhang, YJ
   Larochelle, H
AF Zheng, Yin
   Zemel, Richard S.
   Zhang, Yu-Jin
   Larochelle, Hugo
TI A Neural Autoregressive Approach to Attention-based Recognition
SO INTERNATIONAL JOURNAL OF COMPUTER VISION
LA English
DT Article
DE Deep learning; Attention-based recognition; Neural networks; Neural
   autoregressive distribution estimator
AB Tasks that require the synchronization of perception and action are incredibly hard and pose a fundamental challenge to the fields of machine learning and computer vision. One important example of such a task is the problem of performing visual recognition through a sequence of controllable fixations; this requires jointly deciding what inference to perform from fixations and where to perform these fixations. While these two problems are challenging when addressed separately, they become even more formidable if solved jointly. Recently, a restricted Boltzmann machine (RBM) model was proposed that could learn meaningful fixation policies and achieve good recognition performance. In this paper, we propose an alternative approach based on a feed-forward, auto-regressive architecture, which permits exact calculation of training gradients (given the fixation sequence), unlike for the RBM model. On a problem of facial expression recognition, we demonstrate the improvement gained by this alternative approach. Additionally, we investigate several variations of the model in order to shed some light on successful strategies for fixation-based recognition.
C1 [Zheng, Yin; Zhang, Yu-Jin] Tsinghua Univ, Dept Elect Engn, Beijing 10084, Peoples R China.
   [Zemel, Richard S.] Univ Toronto, Dept Comp Sci, Toronto, ON M5S 3G4, Canada.
   [Larochelle, Hugo] Univ Sherbrooke, Dept Informat, Sherbrooke, PQ J1K 2R1, Canada.
RP Zheng, Y (reprint author), Tsinghua Univ, Dept Elect Engn, Beijing 10084, Peoples R China.
EM yzheng3xg@gmail.com; zemel@cs.toronto.edu; zhang-yj@tsinghua.edu.cn;
   hugo.larochelle@usherbrooke.ca
FU Natural Sciences and Engineering Research Council of CanadaNatural
   Sciences and Engineering Research Council of Canada; National Natural
   Science FoundationNational Natural Science Foundation of China
   [NNSF-61171118]; Ministry of Education of ChinaMinistry of Education,
   China [SRFDP-20110002110057]
FX This work was partially supported by the Natural Sciences and
   Engineering Research Council of Canada, the National Natural Science
   Foundation under Grants NNSF-61171118 and the Ministry of Education
   under Grants SRFDP-20110002110057 of China.
CR Bazzani L., 2011, P 28 INT C MACH LEAR, P937
   Butko NJ, 2010, IEEE T AUTON MENT DE, V2, P91, DOI 10.1109/TAMD.2010.2051029
   Dalal N, 2005, PROC CVPR IEEE, P886
   Denil M, 2012, NEURAL COMPUT, V24, P2151, DOI 10.1162/NECO_a_00312
   Erez T., 2011, AAAI
   Fazl A, 2009, COGNITIVE PSYCHOL, V58, P1, DOI 10.1016/j.cogpsych.2008.05.001
   Hinton G. E, 2012, ARXIV12070580
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462
   Kanan C., 2010, CVPR, P3
   Krause A., 2011, ADV NEURAL INFORM PR, P2447
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Larochelle H., 2012, ADV NEURAL INFORM PR, P2717
   Larochelle H., 2011, ARTIF INTELL, P29
   Larochelle H., 2008, P 25 INT C MACH LEAR, P536, DOI [10.1145/1390156.1390224, DOI 10.1145/1390156.1390224]
   Larochelle H., 2010, P ADV NEUR INF PROC, P1243
   Lazebnik S., 2006, CVPR
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mathe S., 2013, ADV NEURAL INFORM PR, P1923
   Min Chen, 2011, IEEE INFOCOM 2011 - IEEE Conference on Computer Communications Workshops, P409, DOI 10.1109/INFCOMW.2011.5928847
   Nair V., 2010, ICML
   Najemnik J, 2005, NATURE, V434, P387, DOI 10.1038/nature03390
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Rifai S, 2011, P 28 INT C MACH LEAR
   Schmidhuber J., 1991, International Journal of Neural Systems, V2, P125, DOI 10.1142/S012906579100011X
   SOUTHALL JPC, 1962, HELMHOLTZS TREATISE, V2
   Susskind Joshua, 2010, TORONTO FACE DATABAS
   Uria B., 2013, ADV NEURAL INFORM PR, V26, P2175
   Vincent P, 2008, P 25 INT C MACH LEAR, P1096, DOI DOI 10.1145/1390156.1390294
   Yang J., 2009, CVPR
NR 31
TC 8
Z9 8
U1 2
U2 18
PU SPRINGER
PI DORDRECHT
PA VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN 0920-5691
EI 1573-1405
J9 INT J COMPUT VISION
JI Int. J. Comput. Vis.
PD MAY
PY 2015
VL 113
IS 1
SI SI
BP 67
EP 79
DI 10.1007/s11263-014-0765-x
PG 13
WC Computer Science, Artificial Intelligence
SC Computer Science
GA CH6GX
UT WOS:000354135700006
DA 2020-02-19
ER

PT J
AU Huang, GB
   Bai, Z
   Lekamalage, L
   Kasun, C
   Vong, CM
AF Huang, Guang-Bin
   Bai, Zuo
   Lekamalage, Liyanaarachchi
   Kasun, Chamara
   Vong, Chi Man
TI Local Receptive Fields Based Extreme Learning Machine
SO IEEE COMPUTATIONAL INTELLIGENCE MAGAZINE
LA English
DT Article
ID NEURAL-NETWORK; FUNCTION APPROXIMATION; RECOGNITION; REGRESSION;
   REPRESENTATIONS; OPTIMIZATION; CHALLENGES; PREDICTION; ALGORITHM;
   NEURONS
AB Extreme learning machine (ELM), which was originally proposed for "generalized" single-hidden layer feedforward neural networks (SLFNs), provides efficient unified learning solutions for the applications of feature learning, clustering, regression and classification. Different from the common understanding and tenet that hidden neurons of neural networks need to be iteratively adjusted during training stage, ELM theories show that hidden neurons are important but need not be iteratively tuned. In fact, all the parameters of hidden nodes can be independent of training samples and randomly generated according to any continuous probability distribution. And the obtained ELM networks satisfy universal approximation and classification capability. The fully connected ELM architecture has been extensively studied. However, ELM with local connections has not attracted much research attention yet. This paper studies the general architecture of locally connected ELM, showing that: 1) ELM theories are naturally valid for local connections, thus introducing local receptive fields to the input layer; 2) each hidden node in ELM can be a combination of several hidden nodes (a subnetwork), which is also consistent with ELM theories. ELM theories may shed a light on the research of different local receptive fields including true biological receptive fields of which the exact shapes and formula may be unknown to human beings. As a specific example of such general architectures, random convolutional nodes and a pooling structure are implemented in this paper. Experimental results on the NORB dataset, a benchmark for object recognition, show that compared with conventional deep learning solutions, the proposed local receptive fields based ELM (ELM-LRF) reduces the error rate from 6.5% to 2.7% and increases the learning speed up to 200 times.
C1 [Huang, Guang-Bin; Bai, Zuo; Lekamalage, Liyanaarachchi; Kasun, Chamara] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
   [Vong, Chi Man] Univ Macau, Dept Comp & Informat Sci, Taipa, Peoples R China.
RP Huang, GB (reprint author), Nanyang Technol Univ, Sch Elect & Elect Engn, Nanyang Ave, Singapore 639798, Singapore.
RI Huang, Guang-Bin/A-5035-2011
OI Huang, Guang-Bin/0000-0002-2480-4965
CR Arnaldo I, 2015, IEEE COMPUT INTELL M, V10, P20, DOI 10.1109/MCI.2014.2369892
   Bai Z, 2014, IEEE T CYBERNETICS, V44, P1858, DOI 10.1109/TCYB.2014.2298235
   Barak O, 2013, J NEUROSCI, V33, P3844, DOI 10.1523/JNEUROSCI.2753-12.2013
   Bartlett PL, 1998, IEEE T INFORM THEORY, V44, P525, DOI 10.1109/18.661502
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Butcher JB, 2013, NEURAL NETWORKS, V38, P76, DOI 10.1016/j.neunet.2012.11.011
   Chen CLP, 2014, INFORM SCIENCES, V275, P314, DOI 10.1016/j.ins.2014.01.015
   Chen CLP, 1996, IEEE T NEURAL NETWOR, V7, P1220, DOI 10.1109/72.536316
   Chen CLP, 1999, IEEE T SYST MAN CY B, V29, P62, DOI 10.1109/3477.740166
   Chen CLP, 1998, NEUROCOMPUTING, V18, P11, DOI 10.1016/S0925-2312(97)00062-3
   Coates A., 2011, INT C ART INT STAT, V15, P215, DOI 10.1177/1753193410390845
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411
   Cox D, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P8, DOI 10.1109/FG.2011.5771385
   Frenay B., 2010, P 18 EUR S ART NEUR, P315
   Hawkins J., 2007, INTELLIGENCE
   Huang G, 2015, NEURAL NETWORKS, V61, P32, DOI 10.1016/j.neunet.2014.10.001
   Huang G, 2014, IEEE T CYBERNETICS, V44, P2405, DOI 10.1109/TCYB.2014.2307349
   Huang GB, 2006, IEEE T CIRCUITS-II, V53, P187, DOI 10.1109/TCSII.2005.857540
   Huang GB, 2004, IEEE IJCNN, P985
   Huang GB, 2008, NEUROCOMPUTING, V71, P3460, DOI 10.1016/j.neucom.2007.10.008
   Huang GB, 2007, NEUROCOMPUTING, V70, P3056, DOI 10.1016/j.neucom.2007.02.009
   Huang GB, 2006, NEUROCOMPUTING, V70, P489, DOI 10.1016/j.neucom.2005.12.126
   Huang GB, 2006, IEEE T NEURAL NETWOR, V17, P879, DOI 10.1109/TNN.2006.875977
   Huang GB, 2014, COGN COMPUT, V6, P376, DOI 10.1007/s12559-014-9255-2
   Huang GB, 2012, IEEE T SYST MAN CY B, V42, P513, DOI 10.1109/TSMCB.2011.2168604
   Huang GB, 2010, NEUROCOMPUTING, V74, P155, DOI 10.1016/j.neucom.2010.02.019
   HUBEL DH, 1968, J PHYSIOL-LONDON, V195, P215, DOI 10.1113/jphysiol.1968.sp008455
   IGELNIK B, 1995, IEEE T NEURAL NETWOR, V6, P1320, DOI 10.1109/72.471375
   Jarrett K, 2009, IEEE I CONF COMP VIS, P2146, DOI 10.1109/ICCV.2009.5459469
   Kara P, 2003, J NEUROSCI, V23, P8547
   Kasun LLC, 2013, IEEE INTELL SYST, V28, P31
   Lawrence S, 1997, IEEE T NEURAL NETWOR, V8, P98, DOI 10.1109/72.554195
   Le Q.V., 2010, ADV NEURAL INFORM PR, V23, P1279
   LeCun Y, 2004, PROC CVPR IEEE, P97
   Lee H., 2009, P 26 ANN INT C MACH, P609, DOI DOI 10.1145/1553374.1553453
   LEE TH, 1993, J ECONOMETRICS, V56, P269, DOI 10.1016/0304-4076(93)90122-L
   MARQUARDT DW, 1970, TECHNOMETRICS, V12, P591, DOI 10.2307/1267205
   Minhas R, 2012, IEEE T CIRC SYST VID, V22, P1529, DOI 10.1109/TCSVT.2011.2177182
   Nair V, 2009, ADV NEURAL INF PROCE, V22, P1339
   Nizar AH, 2008, IEEE T POWER SYST, V23, P946, DOI 10.1109/TPWRS.2008.926431
   Pal M, 2013, REMOTE SENS LETT, V4, P853, DOI 10.1080/2150704X.2013.805279
   Pao Y. H., 1989, ADAPTIVE PATTERN REC
   PAO YH, 1994, NEUROCOMPUTING, V6, P163, DOI 10.1016/0925-2312(94)90053-1
   Poggio T, 2001, 2001011 AI MIT
   RAHIMI A, 2008, P 46 ANN ALL C COMM, P555
   Ranzato M., 2006, ADV NEURAL INFORM PR, V19, P1137
   Rao CR, 1971, GEN INVERSE MATRICES, V7
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Saxe A., 2011, P 28 INT C MACH LEAR, P1089
   Scherer D, 2010, LECT NOTES COMPUT SC, V6354, P92, DOI 10.1007/978-3-642-15825-4_10
   SCHMIDT WF, 1992, 11TH IAPR INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION, PROCEEDINGS, VOL II, P1, DOI 10.1109/ICPR.1992.201708
   Song YD, 2012, J NEUROSCI METH, V210, P132, DOI 10.1016/j.jneumeth.2012.07.003
   Sosulski DL, 2011, NATURE, V472, P213, DOI 10.1038/nature09868
   Steinwart I, 2011, J MACH LEARN RES, V12, P141
   Stinchcombe MB, 1998, ECONOMET THEOR, V14, P295, DOI 10.1017/S0266466698143013
   White H., 1989, IJCNN: International Joint Conference on Neural Networks (Cat. No.89CH2765-6), P451, DOI 10.1109/IJCNN.1989.118281
   White H., 2006, HDB EC FORECASTING, P460
   Widrow B, 2013, NEURAL NETWORKS, V37, P180, DOI 10.1016/j.neunet.2012.09.020
   Yan Z, 2014, IEEE T NEUR NET LEAR, V25, P457, DOI 10.1109/TNNLS.2013.2275948
   Yang YM, 2012, IEEE T NEUR NET LEAR, V23, P1498, DOI 10.1109/TNNLS.2012.2202289
   You ZH, 2013, BMC BIOINFORMATICS, V14, DOI 10.1186/1471-2105-14-S8-S10
   Zhai YT, 2014, IEEE COMPUT INTELL M, V9, P14, DOI 10.1109/MCI.2014.2326099
   Zhang YW, 2011, CHEM ENG SCI, V66, P4702, DOI 10.1016/j.ces.2011.06.030
   Zheng WB, 2013, NEURAL COMPUT APPL, V22, P447, DOI 10.1007/s00521-011-0808-y
   Zhou Y., 2014, IEEE J SELE IN PRESS
   Zhou ZH, 2014, IEEE COMPUT INTELL M, V9, P62, DOI 10.1109/MCI.2014.2350953
NR 66
TC 176
Z9 199
U1 15
U2 148
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1556-603X
EI 1556-6048
J9 IEEE COMPUT INTELL M
JI IEEE Comput. Intell. Mag.
PD MAY
PY 2015
VL 10
IS 2
BP 18
EP 29
DI 10.1109/MCI.2015.2405316
PG 12
WC Computer Science, Artificial Intelligence
SC Computer Science
GA CF9RG
UT WOS:000352902900002
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Wu, Y
   Ji, Q
AF Wu, Yue
   Ji, Qiang
TI Discriminative Deep Face Shape Model for Facial Point Detection
SO INTERNATIONAL JOURNAL OF COMPUTER VISION
LA English
DT Article
DE Facial point detection; Restricted Boltzmann Machine; Deep learning
ID LOCALIZATION
AB Facial point detection is an active area in computer vision due to its relevance to many applications. It is a nontrivial task, since facial shapes vary significantly with facial expressions, poses or occlusion. In this paper, we address this problem by proposing a discriminative deep face shape model that is constructed based on an augmented factorized three-way Restricted Boltzmann Machines model. Specifically, the discriminative deep model combines the top-down information from the embedded face shape patterns and the bottom up measurements from local point detectors in a unified framework. In addition, along with the model, effective algorithms are proposed to perform model learning and to infer the true facial point locations from their measurements. Based on the discriminative deep face shape model, 68 facial points are detected on facial images in both controlled and "in-the-wild" conditions. Experiments on benchmark data sets show the effectiveness of the proposed facial point detection algorithm against state-of-the-art methods.
C1 [Wu, Yue; Ji, Qiang] Rensselaer Polytech Inst, Dept Elect Comp & Syst Engn, Troy, NY 12180 USA.
RP Ji, Q (reprint author), Rensselaer Polytech Inst, Dept Elect Comp & Syst Engn, 110 8th St, Troy, NY 12180 USA.
EM wuy9@rpi.edu; jiq@rpi.edu
FU US Army Research office [W911NF-12-C-0017]
FX This work is supported in part by a Grant from US Army Research office
   (W911NF-12-C-0017).
CR Baker S, 2004, INT J COMPUT VISION, V56, P221, DOI 10.1023/B:VISI.0000011205.11775.fd
   Belhumeur P. N., 2011, IEEE INT C COMP VIS
   Belhumeur PN, 2013, IEEE T PATTERN ANAL, V35, P2930, DOI 10.1109/TPAMI.2013.23
   COOTES TF, 1995, COMPUT VIS IMAGE UND, V61, P38, DOI 10.1006/cviu.1995.1004
   Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467
   Cristinacce D, 2008, PATTERN RECOGN, V41, P3054, DOI 10.1016/j.patcog.2008.01.024
   Dalal N, 2005, PROC CVPR IEEE, P886
   Eslami SMA, 2012, PROC CVPR IEEE, P406, DOI 10.1109/CVPR.2012.6247702
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Gross R, 2010, IMAGE VISION COMPUT, V28, P807, DOI 10.1016/j.imavis.2009.08.002
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Kae A, 2013, PROC CVPR IEEE, P2019, DOI 10.1109/CVPR.2013.263
   Krizhevsky Alex, 2010, INT C ART INT STAT, P621
   Le V, 2012, LECT NOTES COMPUT SC, V7574, P679, DOI 10.1007/978-3-642-33712-3_49
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Martinez B, 2013, IEEE T PATTERN ANAL, V35, P1149, DOI 10.1109/TPAMI.2012.205
   Matthews I, 2004, INT J COMPUT VISION, V60, P135, DOI 10.1023/B:VISI.0000029666.37597.d3
   Memisevic R, 2010, NEURAL COMPUT, V22, P1473, DOI 10.1162/neco.2010.01-09-953
   Mohamed AD, 2012, INT J NEUROPSYCHOPH, V15, P559, DOI 10.1017/S146114571100037X
   Sagonas C., 2013, P IEEE INT C COMP VI
   Sagonas C, 2013, IEEE COMPUT SOC CONF, P896, DOI 10.1109/CVPRW.2013.132
   Salakhutdinov R, 2009, ARTIF INTELL, P448, DOI DOI 10.1109/CVPRW.2009.5206577
   Saragih JM, 2011, INT J COMPUT VISION, V91, P200, DOI 10.1007/s11263-010-0380-4
   Smola AJ, 2004, STAT COMPUT, V14, P199, DOI 10.1023/B:STCO.0000035301.49549.88
   Sun Y, 2013, IEEE I CONF COMP VIS, P1489, DOI 10.1109/ICCV.2013.188
   Sun Y, 2013, PROC CVPR IEEE, P3476, DOI 10.1109/CVPR.2013.446
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Taylor GW, 2010, PROC CVPR IEEE, P631, DOI 10.1109/CVPR.2010.5540157
   Tieleman T., 2008, P 25 INT C MACH LEAR, P1064, DOI DOI 10.1145/1390156.1390290
   Tzimiropoulos G, 2013, IEEE I CONF COMP VIS, P593, DOI 10.1109/ICCV.2013.79
   Valstar M., 2010, IEEE INT C COMP VIS, P13
   Welling M, 2002, LECT NOTES COMPUT SC, V2415, P351
   Wu Y, 2013, PROC CVPR IEEE, P3452, DOI 10.1109/CVPR.2013.443
   Xiong X., 2013, IEEE INT C COMP VIS
   Zhou EJ, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P386, DOI 10.1109/ICCVW.2013.58
   Zhu XX, 2012, PROC CVPR IEEE, P2879, DOI 10.1109/CVPR.2012.6248014
NR 37
TC 27
Z9 29
U1 0
U2 0
PU SPRINGER
PI DORDRECHT
PA VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN 0920-5691
EI 1573-1405
J9 INT J COMPUT VISION
JI Int. J. Comput. Vis.
PD MAY
PY 2015
VL 113
IS 1
SI SI
BP 37
EP 53
DI 10.1007/s11263-014-0775-8
PG 17
WC Computer Science, Artificial Intelligence
SC Computer Science
GA CH6GX
UT WOS:000354135700004
DA 2020-02-19
ER

PT J
AU Cao, YQ
   Chen, Y
   Khosla, D
AF Cao, Yongqiang
   Chen, Yang
   Khosla, Deepak
TI Spiking Deep Convolutional Neural Networks for Energy-Efficient Object
   Recognition
SO INTERNATIONAL JOURNAL OF COMPUTER VISION
LA English
DT Article
DE Deep learning; Machine learning; Convolutional neural networks; Spiking
   neural networks; Neuromorphic circuits; Object recognition
ID RECEPTIVE FIELDS; VIEW-INVARIANT; REPRESENTATIONS; NEOCOGNITRON; MODELS
AB Deep-learning neural networks such as convolutional neural network (CNN) have shown great potential as a solution for difficult vision problems, such as object recognition. Spiking neural networks (SNN)-based architectures have shown great potential as a solution for realizing ultra-low power consumption using spike-based neuromorphic hardware. This work describes a novel approach for converting a deep CNN into a SNN that enables mapping CNN to spike-based hardware architectures. Our approach first tailors the CNN architecture to fit the requirements of SNN, then trains the tailored CNN in the same way as one would with CNN, and finally applies the learned network weights to an SNN architecture derived from the tailored CNN. We evaluate the resulting SNN on publicly available Defense Advanced Research Projects Agency (DARPA) Neovision2 Tower and CIFAR-10 datasets and show similar object recognition accuracy as the original CNN. Our SNN implementation is amenable to direct mapping to spike-based neuromorphic hardware, such as the ones being developed under the DARPA SyNAPSE program. Our hardware mapping analysis suggests that SNN implementation on such spike-based hardware is two orders of magnitude more energy-efficient than the original CNN implementation on off-the-shelf FPGA-based hardware.
C1 [Cao, Yongqiang; Chen, Yang; Khosla, Deepak] HRL Labs LLC, Malibu, CA 90265 USA.
RP Chen, Y (reprint author), HRL Labs LLC, 3011 Malibu Canyon Rd, Malibu, CA 90265 USA.
EM ychen@hrl.com
FU Defense Advanced Research Projects Agency Cognitive Technology Threat
   Warning System (CT2WS) program [W31P4Q-08-C-0264, HR0011-09-C-0001];
   SyNAPSE program [W31P4Q-08-C-0264, HR0011-09-C-0001]
FX This work was partially supported by the Defense Advanced Research
   Projects Agency Cognitive Technology Threat Warning System (CT2WS) and
   SyNAPSE programs (contracts W31P4Q-08-C-0264 and HR0011-09-C-0001). The
   views expressed in this document are those of the authors and do not
   reflect the official policy or position of the Department of Defense or
   the U.S. Government. We would like to thank Dr. Clement Farabet of New
   York University for providing the initial CNN structure on which the CNN
   model outlined in Fig. 1 is based; and the anonymous reviewers for their
   invaluable comments and recommendations that led to this revised
   manuscript.
CR Arthur J. V., 2012, INT JOINT C NEUR NET, P1, DOI [10.1109/ijcnn.2012.6252637, DOI 10.1109/IJCNN.2012.6252637]
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Cao YQ, 2012, NEURAL NETWORKS, V26, P75, DOI 10.1016/j.neunet.2011.10.010
   Cao YQ, 2011, NEURAL NETWORKS, V24, P1050, DOI 10.1016/j.neunet.2011.04.004
   Cassola A., 2013, P INT JOINT C NEUR N, P1, DOI [DOI 10.1109/IJCNN.2013.6707077, 10.1109/IJCNN.2013.6707077]
   Ciresan D, 2012, PROC CVPR IEEE, P3642, DOI 10.1109/CVPR.2012.6248110
   Collobert Ronan, 2011, BIGLEARN
   Cruz-Albrecht JM, 2012, IEEE T BIOMED CIRC S, V6, P246, DOI 10.1109/TBCAS.2011.2174152
   Farabet C., 2010, IEEE INT S CIRC SYST
   Farabet C, 2013, THESIS U PARIS EST
   Fazl A, 2009, COGNITIVE PSYCHOL, V58, P1, DOI 10.1016/j.cogpsych.2008.05.001
   Folowosele F, 2011, IEEE J EM SEL TOP C, V1, P516, DOI 10.1109/JETCAS.2012.2183409
   FUKUSHIMA K, 1988, NEURAL NETWORKS, V1, P119, DOI 10.1016/0893-6080(88)90014-7
   FUKUSHIMA K, 1980, BIOL CYBERN, V36, P193, DOI 10.1007/BF00344251
   Grossberg S, 2011, NEURAL NETWORKS, V24, P1036, DOI 10.1016/j.neunet.2011.04.001
   Grossberg S, 2009, J VISION, V9, DOI 10.1167/9.4.6
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton G, 2006, COGNITIVE SCI, V30, P725, DOI 10.1207/s15516709cog0000_76
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Ho N, 2013, CONVOLUTIONAL NEUR 3
   HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837
   HUBEL DH, 1959, J PHYSIOL-LONDON, V148, P574, DOI 10.1113/jphysiol.1959.sp006308
   Itti L, 2013, NEOVISION2 ANNOTATED
   Jarrett K, 2009, IEEE I CONF COMP VIS, P2146, DOI 10.1109/ICCV.2009.5459469
   Khosla D., 2013, P SPIE, V8713
   Khosla D., 2013, P SPIE, V8745
   Krizhevsky A., 2009, THESIS U TORONTO
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lin Min, 2014, INT C LEARN REPR ICL
   Masquelier T, 2007, PLOS COMPUT BIOL, V3, P247, DOI 10.1371/journal.pcbi.0030031
   Merolla P., 2011, IEEE CUST INT CIRC C, P1, DOI [DOI 10.1109/CICC.2011.6055294, 10.1109/CICC.2011.6055294]
   Perez-Carrasco J A, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P3085, DOI 10.1109/ICPR.2010.756
   Ranzato MA, 2007, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2007.383157
   Razavian A.S., 2014, DEEPVISION CVPR 2014
   Riesenhuber M, 1999, NAT NEUROSCI, V2, P1019
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Sermanet P., 2014, INT C LEARN REPR ICL, P1
   Serre T, 2007, IEEE T PATTERN ANAL, V29, P411, DOI 10.1109/TPAMI.2007.56
NR 40
TC 133
Z9 138
U1 14
U2 140
PU SPRINGER
PI DORDRECHT
PA VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN 0920-5691
EI 1573-1405
J9 INT J COMPUT VISION
JI Int. J. Comput. Vis.
PD MAY
PY 2015
VL 113
IS 1
SI SI
BP 54
EP 66
DI 10.1007/s11263-014-0788-3
PG 13
WC Computer Science, Artificial Intelligence
SC Computer Science
GA CH6GX
UT WOS:000354135700005
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Zang, D
   Chai, ZL
   Zhang, JQ
   Zhang, DD
   Cheng, JJ
AF Zang, Di
   Chai, Zhenliang
   Zhang, Junqi
   Zhang, Dongdong
   Cheng, Jiujun
TI Vehicle license plate recognition using visual attention model and deep
   learning
SO JOURNAL OF ELECTRONIC IMAGING
LA English
DT Article
DE license plate recognition; visual attention model; convolutional neural
   network; intelligent transportation system; deep learning
ID IMAGES
AB A vehicle's license plate is the unique feature by which to identify each individual vehicle. As an important research area of an intelligent transportation system, the recognition of vehicle license plates has been investigated for some decades. An approach based on a visual attention model and deep learning is proposed to handle the problem of Chinese car license plate recognition for traffic videos. We first use a modified visual attention model to locate the license plate, and then the license plate is segmented into seven blocks using a projection method. Two classifiers, which combine the advantages of convolutional neural network-based feature learning and support vector machine for multichannel processing, are designed to recognize Chinese characters, numbers, and alphabet letters, respectively. Experimental results demonstrate that the presented method can achieve high recognition accuracy and works robustly even under the conditions of illumination change and noise contamination. (C) The Authors.
C1 [Zang, Di; Chai, Zhenliang; Zhang, Junqi; Zhang, Dongdong; Cheng, Jiujun] Tongji Univ, Dept Comp Sci, Shanghai 201804, Peoples R China.
   [Zang, Di; Chai, Zhenliang; Zhang, Junqi; Zhang, Dongdong; Cheng, Jiujun] Minist Educ, Key Lab Embedded Syst & Serv Comp, Shanghai 201804, Peoples R China.
RP Zang, D (reprint author), Tongji Univ, Dept Comp Sci, 4800 Caoan Highway, Shanghai 201804, Peoples R China.
EM zangdi@tongji.edu.cn
RI Cheng, Jiujun/AAG-8193-2019
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61103071, 61103072, 61272271, 61472284]; Natural
   Science Foundation of Shanghai, ChinaNatural Science Foundation of
   Shanghai [12ZR1434000, 13ZR1443100]; Research Fund for the Doctoral
   Program of Higher Education of ChinaResearch Fund for the Doctoral
   Program of Higher Education of China (RFDP)Specialized Research Fund for
   the Doctoral Program of Higher Education (SRFDP) [20110072120065]
FX This work has been supported by National Natural Science Foundation of
   China (Nos. 61103071, 61103072, 61272271, 61472284), Natural Science
   Foundation of Shanghai, China (No. 12ZR1434000, 13ZR1443100) and
   Research Fund for the Doctoral Program of Higher Education of China (No.
   20110072120065).
CR Al-Ghaili AM, 2013, IEEE T VEH TECHNOL, V62, P26, DOI 10.1109/TVT.2012.2222454
   Amit Y, 2004, IEEE T PATTERN ANAL, V26, P1606, DOI 10.1109/TPAMI.2004.111
   Anagnostopoulos CNE, 2006, IEEE T INTELL TRANSP, V7, P377, DOI 10.1109/TITS.2006.880641
   Anagnostopoulos CNE, 2008, IEEE T INTELL TRANSP, V9, P377, DOI 10.1109/TITS.2008.922938
   ANDERSON JR, 2005, COGNITIVE PSYCHOL IT
   Arth C., 2007, COMP VIS PATT REC 20, P1, DOI DOI 10.1109/CVPR.2007.383412
   Ban SW, 2008, NEUROCOMPUTING, V71, P853, DOI 10.1016/j.neucom.2007.03.003
   Cecotti H, 2011, IEEE T PATTERN ANAL, V33, P433, DOI 10.1109/TPAMI.2010.125
   Chang SL, 2004, IEEE T INTELL TRANSP, V5, P42, DOI 10.1109/TITS.2004.825086
   Chen XY, 2014, IEEE GEOSCI REMOTE S, V11, P1797, DOI 10.1109/LGRS.2014.2309695
   Chen YN, 2006, INT C PATT RECOG, P552
   Choi SB, 2006, NEUROCOMPUTING, V69, P537, DOI 10.1016/j.neucom.2004.12.012
   COMELLI P, 1995, IEEE T VEH TECHNOL, V44, P790, DOI 10.1109/25.467963
   Duan Tran Due, 2005, P INT C COMP SCI RIV, P59
   Gendy S., 1997, Proceedings IEEE 31st Annual 1997 International Carnahan Conference on Security Technology (Cat. No.97CH36062), P209, DOI 10.1109/CCST.1997.626271
   Ghazal M., 2013, 1 INT C COMM SIGN PR, P1
   Hsu GS, 2013, IEEE T VEH TECHNOL, V62, P552, DOI 10.1109/TVT.2012.2226218
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Jiao JB, 2009, PATTERN RECOGN, V42, P358, DOI 10.1016/j.patcog.2008.08.016
   Kim SK, 1996, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, PROCEEDINGS - VOL II, P661, DOI 10.1109/ICIP.1996.560964
   Kseneman M, 2011, INFORM MIDEM, V41, P212
   Kusy B, 2009, 2009 INTERNATIONAL CONFERENCE ON INFORMATION PROCESSING IN SENSOR NETWORKS (IPSN 2009), P109
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lowe D., 1999, P INT C COMP VIS, V2, P1150, DOI [10.1109/ICCV.1999.790410, DOI 10.1109/ICCV.1999.790410]
   Lu XB, 2003, PROCEEDINGS OF 2003 INTERNATIONAL CONFERENCE ON NEURAL NETWORKS & SIGNAL PROCESSING, PROCEEDINGS, VOLS 1 AND 2, P1066
   MengZe Zheng, 2010, Proceedings 2010 IEEE International Conference on Intelligent Systems and Knowledge Engineering (ISKE 2010), P287, DOI 10.1109/ISKE.2010.5680862
   Niebur E., 1998, COMPUTATIONAL ARCHIT, P163
   Nomura S, 2005, PATTERN RECOGN, V38, P1961, DOI 10.1016/j.patcog.2005.01.026
   Nomura S, 2004, IEICE T INF SYST, VE87D, P1012
   Norouzi Mohammad, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2735, DOI 10.1109/CVPRW.2009.5206577
   OLSHAUSEN BA, 1993, J NEUROSCI, V13, P4700
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Qin Z, 2006, WCICA 2006: SIXTH WORLD CONGRESS ON INTELLIGENT CONTROL AND AUTOMATION, VOLS 1-12, CONFERENCE PROCEEDINGS, P8645
   Qiu Y., 2009, P INT C COMP INT SOF, P1
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Sheng H, 2009, IEEE INTEL TRANSP SY, V1, P17, DOI 10.1109/MITS.2010.935911
   Shi XF, 2005, LECT NOTES COMPUT SC, V3483, P1159
   ter Brugge MH, 1998, CNNA 98 - 1998 FIFTH IEEE INTERNATIONAL WORKSHOP ON CELLULAR NEURAL NETWORKS AND THEIR APPLICATIONS - PROCEEDINGS, P212, DOI 10.1109/CNNA.1998.685366
   TSOTSOS JK, 1995, ARTIF INTELL, V78, P507, DOI 10.1016/0004-3702(95)00025-9
   Wen Y, 2011, IEEE T INTELL TRANSP, V12, P830, DOI 10.1109/TITS.2011.2114346
   Wu QX, 2013, NEUROCOMPUTING, V116, P3, DOI 10.1016/j.neucom.2012.01.046
   Yanamura Y, 2003, IEEE IV2003: INTELLIGENT VEHICLES SYMPOSIUM, PROCEEDINGS, P243, DOI 10.1109/IVS.2003.1212916
   YiQing Liu, 2011, Proceedings 2011 International Conference on Information and Automation (ICIA 2011), P363, DOI 10.1109/ICINFA.2011.5949018
   Zhang HF, 2006, INT C PATT RECOG, P1102
   Zhihong Zhao, 2008, 2008 Pacific-Asia Workshop on Computational Intelligence and Industrial Application. PACIIA 2008, P27, DOI 10.1109/PACIIA.2008.196
   Zhou WG, 2012, IEEE T IMAGE PROCESS, V21, P4269, DOI 10.1109/TIP.2012.2199506
   Zhu WG, 2002, 2002 6TH INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING PROCEEDINGS, VOLS I AND II, P748, DOI 10.1109/ICOSP.2002.1181164
NR 48
TC 21
Z9 22
U1 2
U2 71
PU IS&T & SPIE
PI BELLINGHAM
PA 1000 20TH ST, BELLINGHAM, WA 98225 USA
SN 1017-9909
EI 1560-229X
J9 J ELECTRON IMAGING
JI J. Electron. Imaging
PD MAY
PY 2015
VL 24
IS 3
AR 033001
DI 10.1117/1.JEI.24.3.033001
PG 10
WC Engineering, Electrical & Electronic; Optics; Imaging Science &
   Photographic Technology
SC Engineering; Optics; Imaging Science & Photographic Technology
GA CP3ZM
UT WOS:000359821800001
OA Other Gold
DA 2020-02-19
ER

PT J
AU Feng, FX
   Li, RF
   Wang, XJ
AF Feng, Fangxiang
   Li, Ruifan
   Wang, Xiaojie
TI Deep correspondence restricted Boltzmann machine for cross-modal
   retrieval
SO NEUROCOMPUTING
LA English
DT Article
DE Cross-modal; RBM; Retrieval; Deep Learning; Multi-modal
AB The task of cross-modal retrieval, i.e., using a text query to search for images or vice versa, has received considerable attention with the rapid growth of multi-modal web data. Modeling the correlations between different modalities is the key to tackle this problem. In this paper, we propose a correspondence restricted Boltzmann machine (Corr-RBM) to map the original features of bimodal data, such as image and text in our setting, into a low-dimensional common space, in which the heterogeneous data are comparable. In our Corr-RBM, two RBMs built for image and text, respectively are connected at their individual hidden representation layers by a correlation loss function. A single objective function is constructed to trade off the correlation loss and likelihoods of both modalities. Through the optimization of this objective function, our Corr-RBM is able to capture the correlations between two modalities and learn the representation of each modality simultaneously. Furthermore, we construct two deep neural structures using Corr-RBM as the main building block for the task of cross-modal retrieval. A number of comparison experiments are performed on three public real-world data sets. All of our models show significantly better results than state-of-the-art models in both searching images via text query and vice versa. (C) 2014 Elsevier B.V. All rights reserved.
C1 [Feng, Fangxiang; Li, Ruifan; Wang, Xiaojie] Beijing Univ Posts & Telecommun, Sch Comp Sci, Beijing 100088, Peoples R China.
RP Li, RF (reprint author), Beijing Univ Posts & Telecommun, Sch Comp Sci, Beijing 100088, Peoples R China.
EM f.fangxiang@gmail.com; rfli@bupt.edu.cn; xjwang@bupt.edu.cn
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61273365]; National High Technology Research and
   Development Program of ChinaNational High Technology Research and
   Development Program of China [2012AA011103]; Discipline Building Plan in
   111 Base [B08004]; Fundamental Research Funds for the Central
   UniversitiesFundamental Research Funds for the Central Universities
   [2013RC0304]; Engineering Research Center of Information Networks,
   Ministry of Education
FX This work was partially supported by National Natural Science Foundation
   of China (no. 61273365), National High Technology Research and
   Development Program of China (no. 2012AA011103), Discipline Building
   Plan in 111 Base (no. B08004), the Fundamental Research Funds for the
   Central Universities (no. 2013RC0304) and Engineering Research Center of
   Information Networks, Ministry of Education.
CR Bird S, 2006, P COLING ACL INT PRE, P69, DOI DOI 10.3115/1225403.1225421
   Bosch A, 2007, IEEE I CONF COMP VIS, P1863
   Bronstein MM, 2010, PROC CVPR IEEE, P3594, DOI 10.1109/CVPR.2010.5539928
   Chua T., 2009, P ACM INT C IM VID R, P1, DOI DOI 10.1145/1646396
   Desjardins G., 2010, P 13 INT C ART INT S, V9, P145
   Feng FX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P7, DOI 10.1145/2647868.2654902
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hinton GE, 2009, ADV NEURAL INFORM PR, P1607
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.2307/2333955
   Huiskes M. J., 2010, P INT C MULT INF RET, P527, DOI DOI 10.1145/1743384.1743475
   Kim JW, 2012, INT J STEEL STRUCT, V12, P579, DOI 10.1007/s13296-012-4012-4
   Kumar S, 2011, P IJCAI, P1360, DOI DOI 10.5591/978-1-57735-516-8/IJCA111-230
   Manjunath BS, 2001, IEEE T CIRC SYST VID, V11, P703, DOI 10.1109/76.927424
   Manning C. D, 2008, INTRO INFORM RETRIEV
   Masci J, 2014, IEEE T PATTERN ANAL, V36, P824, DOI 10.1109/TPAMI.2013.225
   McFee B., 2009, P 26 ANN INT C MACH, P721
   Ngiam J, 2011, IEEE INT C MACH LEAR, V28, P689, DOI DOI 10.1145/2647868
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Rasiwasia N., 2010, P ACM MULT 2010, P251, DOI DOI 10.1145/1873951.1873987
   Salakhutdinov R., 2007, P INT C MACH LEARN, V24, P791, DOI DOI 10.1145/1273496.1273596
   Smolensky P, 1986, PARALLEL DISTRIBUTED, P194
   Srivastava N., 2012, INT C MACH LEARN REP
   Srivastava N., 2013, P 29 C UNC ART INT, P616
   Srivastava N., 2012, ADV NEURAL INFORM PR, V25, P2231
   Taylor GW, 2009, P 26 ANN INT C MACH, P1025, DOI DOI 10.1145/1553374.1553505
   Tieleman T., 2008, P 25 INT C MACH LEAR, P1064, DOI DOI 10.1145/1390156.1390290
   Tieleman T., 2009, P 26 ANN INT C MACH, P1033, DOI DOI 10.1145/1553374.1553506
   Wang W, 2014, PROC VLDB ENDOW, V7, P649, DOI 10.14778/2732296.2732301
   Welling M., 2004, NEUR INF PROC SYST N, P501
   Xu JG, 2014, NEUROCOMPUTING, V139, P328, DOI 10.1016/j.neucom.2014.02.024
   Zhu Xiaofeng, 2013, P 21 ACM INT C MULT, P143, DOI DOI 10.1145/2502081.2502107
NR 32
TC 27
Z9 32
U1 3
U2 31
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0925-2312
EI 1872-8286
J9 NEUROCOMPUTING
JI Neurocomputing
PD APR 22
PY 2015
VL 154
BP 50
EP 60
DI 10.1016/j.neucom.2014.12.020
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA CC1DR
UT WOS:000350081900006
DA 2020-02-19
ER

PT J
AU Li, ZH
   Fan, YY
   Liu, WH
AF Li, Zuhe
   Fan, Yangyu
   Liu, Weihua
TI The effect of whitening transformation on pooling operations in
   convolutional autoencoders
SO EURASIP JOURNAL ON ADVANCES IN SIGNAL PROCESSING
LA English
DT Article
DE Convolutional neural network; Sparse autoencoder; Image classification;
   Computer vision; Unsupervised learning; Deep learning
ID NETWORK
AB Convolutional autoencoders (CAEs) are unsupervised feature extractors for high-resolution images. In the pre-processing step, whitening transformation has widely been adopted to remove redundancy by making adjacent pixels less correlated. Pooling is a biologically inspired operation to reduce the resolution of feature maps and achieve spatial invariance in convolutional neural networks. Conventionally, pooling methods are mainly determined empirically in most previous work. Therefore, our main purpose is to study the relationship between whitening processing and pooling operations in convolutional autoencoders for image classification. We propose an adaptive pooling approach based on the concepts of information entropy to test the effect of whitening on pooling in different conditions. Experimental results on benchmark datasets indicate that the performance of pooling strategies is associated with the distribution of feature activations, which can be affected by whitening processing. This provides guidance for the selection of pooling methods in convolutional autoencoders and other convolutional neural networks.
C1 [Li, Zuhe; Fan, Yangyu; Liu, Weihua] Northwestern Polytech Univ, Sch Elect & Informat, Xian 710072, Peoples R China.
   [Li, Zuhe] Zhengzhou Univ Light Ind, Sch Comp & Commun Engn, Zhengzhou 450002, Peoples R China.
RP Li, ZH (reprint author), Northwestern Polytech Univ, Sch Elect & Informat, 127 West Youyi Rd, Xian 710072, Peoples R China.
EM zuheli@126.com
OI Li, Zuhe/0000-0002-2511-3226
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61202314]; China Postdoctoral Science
   FoundationChina Postdoctoral Science Foundation [2012 M521801, 2014
   T70937]; Science and Technology Innovation Engineering Program for
   Shaanxi Provincial Key Laboratories [2013SZS15-K02]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61202314, China Postdoctoral Science Foundation under
   Grant 2012 M521801, China Postdoctoral Science Foundation Special
   Project under Grant 2014 T70937, and the Science and Technology
   Innovation Engineering Program for Shaanxi Provincial Key Laboratories
   under Grant 2013SZS15-K02.
CR Bell AJ, 1997, ADV NEUR IN, V9, P831
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Boureau Y., 2010, P 27 INT C MACH LEAR, P111, DOI DOI 10.1016/J.NEUNET.2012.02.023
   Boureau YL, 2010, PROC CVPR IEEE, P2559, DOI 10.1109/CVPR.2010.5539963
   Cires D. C., 2011, P 22 INT JOINT C ART, V22, P1237, DOI DOI 10.5591/978-1-57735-516-8/IJCAI11-210
   Coates A., 2011, INT C ART INT STAT, V15, P215, DOI 10.1177/1753193410390845
   Goodfellow I., 2009, ADV NEURAL INFORM PR, V22, P646
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   HUBEL DH, 1959, J PHYSIOL-LONDON, V148, P574, DOI 10.1113/jphysiol.1959.sp006308
   Jarrett K, 2009, IEEE I CONF COMP VIS, P2146, DOI 10.1109/ICCV.2009.5459469
   Korekado K, 2003, LECT NOTES ARTIF INT, V2774, P169
   Krizhevsky, 2009, THESIS U TORONTO
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Le Q. V., 2011, ADV NEURAL INFORM PR, P1017
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Makhzani A., 2014, ARXIV14092752
   Masci J, 2011, LECT NOTES COMPUT SC, V6791, P52, DOI 10.1007/978-3-642-21735-7_7
   Ng AY, DEEP LEARNING
   Ranzato M., 2006, ADV NEURAL INFORM PR, V19, P1137
   Ranzato MA, 2007, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2007.383157
   Scherer D, 2010, LECT NOTES COMPUT SC, V6354, P92, DOI 10.1007/978-3-642-15825-4_10
   Sermanet P, 2013, PROC CVPR IEEE, P3626, DOI 10.1109/CVPR.2013.465
   SHANNON CE, 1948, BELL SYST TECH J, V27, P623, DOI 10.1002/j.1538-7305.1948.tb00917.x
   Sohn K., 2012, ARXIV12066418
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Zeiler Matthew D, 2013, ARXIV13013557
NR 26
TC 13
Z9 13
U1 0
U2 29
PU SPRINGEROPEN
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 1687-6180
J9 EURASIP J ADV SIG PR
JI EURASIP J. Adv. Signal Process.
PD APR 14
PY 2015
AR 37
DI 10.1186/s13634-015-0222-1
PG 11
WC Engineering, Electrical & Electronic
SC Engineering
GA CG3VM
UT WOS:000353207700001
OA DOAJ Gold
DA 2020-02-19
ER

PT J
AU Schulz, H
   Cho, K
   Raiko, T
   Behnke, S
AF Schulz, Hannes
   Cho, Kyunghyun
   Raiko, Tapani
   Behnke, Sven
TI Two-layer contractive encodings for learning stable nonlinear features
SO NEURAL NETWORKS
LA English
DT Article
DE Deep learning; Multi-layer perceptron; Two-layer contractive encoding;
   Linear transformation; Pretraining; Semi-supervised learning
AB Unsupervised learning of feature hierarchies is often a good strategy to initialize deep architectures for supervised learning. Most existing deep learning methods build these feature hierarchies layer by layer in a greedy fashion using either auto-encoders or restricted Boltzmann machines. Both yield encoders which compute linear projections of input followed by a smooth thresholding function. In this work, we demonstrate that these encoders fail to find stable features when the required computation is in the exclusive-or class. To overcome this limitation, we propose a two-layer encoder which is less restricted in the type of features it can learn. The proposed encoder is regularized by an extension of previous work on contractive regularization. This proposed two-layer contractive encoder potentially poses a more difficult optimization problem, and we further propose to linearly transform hidden neurons of the encoder to make learning easier. We demonstrate the advantages of the two-layer encoders qualitatively on artificially constructed datasets as well as commonly used benchmark datasets. We also conduct experiments on a semi-supervised learning task and show the benefits of the proposed two-layer encoders trained with the linear transformation of perceptrons. (C) 2014 Elsevier Ltd. All rights reserved.
C1 [Schulz, Hannes; Behnke, Sven] Univ Bonn, Comp Sci Inst 6, Autonomous Intelligent Syst, Bonn, Germany.
   [Cho, Kyunghyun; Raiko, Tapani] Aalto Univ, Sch Sci, Dept Informat & Comp Sci, Espoo, Finland.
RP Schulz, H (reprint author), Univ Bonn, Comp Sci Inst 6, Autonomous Intelligent Syst, Bonn, Germany.
EM schulz@ais.uni-bonn.de
RI Raiko, Tapani/E-7237-2012; Behnke, Sven/B-5509-2013
OI Raiko, Tapani/0000-0002-0321-304X; Behnke, Sven/0000-0002-5040-7525
FU Academy of Finland (Finnish Centre of Excellence in Computational
   Inference Research COIN) [251170]
FX This work was supported by the Academy of Finland (Finnish Centre of
   Excellence in Computational Inference Research COIN, 251170).
CR Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153
   Bengio Y, 2006, ADV NEURAL INFORM PR, P153
   Bengio Y, 2011, LECT NOTES ARTIF INT, V6925, P18, DOI 10.1007/978-3-642-24412-4_3
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Bergstra J, 2011, ADV NEURAL INFORM PR, V24, P2546
   Cho K., 2011, P 28 INT C MACH LEAR, P105
   Ciresan D, 2012, PROC CVPR IEEE, P3642, DOI 10.1109/CVPR.2012.6248110
   Dauphin Y., 2014, ARXIV14062572CSLG
   Erhan D., 2009, J MACHINE LEARNING R, V5, P153
   Erhan D, 2010, J MACH LEARN RES, V11, P625
   GALLAGER RG, 1962, IRE T INFORM THEOR, V8, P21, DOI 10.1109/TIT.1962.1057683
   Glorot X., 2010, JLMR P TRACK, p[249, 2010], DOI DOI 10.1177/1753193409103364.
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   HORNIK K, 1989, NEURAL NETWORKS, V2, P359, DOI 10.1016/0893-6080(89)90020-8
   Le Roux N, 2008, NEURAL COMPUT, V20, P1631, DOI 10.1162/neco.2008.04-07-510
   Minsky M, 1969, PERCEPTRONS
   Ngiam J, 2011, P 28 INT C MACH LEAR, P1105
   Raiko T., 2012, P INT C ART INT STAT, P924
   Rifai S, 2011, LECT NOTES ARTIF INT, V6912, P645, DOI 10.1007/978-3-642-23783-6_41
   Salah R, 2011, P 28 INT C MACH LEAR, P833, DOI 10.1016/j.wasman.2010.10.006
   Salakhutdinov R, 2009, ARTIF INTELL, P448, DOI DOI 10.1109/CVPRW.2009.5206577
   Scholkopf B., 2007, ADV NEURAL INFORM PR, V19, P1137, DOI DOI 10.7551/mitpress/7503.003.0147
   Schulz Hannes, 2012, Artificial Neural Networks and Machine Learning - ICANN 2012. Proceedings 22nd International Conference on Artificial Neural Networks, P620, DOI 10.1007/978-3-642-33269-2_78
   Spall JC, 1998, J HOPKINS APL TECH D, V19, P482
   Vatanen T., 2013, ARXIV13013476CSLG
NR 25
TC 14
Z9 14
U1 0
U2 15
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0893-6080
EI 1879-2782
J9 NEURAL NETWORKS
JI Neural Netw.
PD APR
PY 2015
VL 64
SI SI
BP 4
EP 11
DI 10.1016/j.neunet.2014.09.008
PG 8
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA CD2RL
UT WOS:000350926600002
PM 25292461
DA 2020-02-19
ER

PT J
AU Berglund, M
   Raiko, T
   Cho, K
AF Berglund, Mathias
   Raiko, Tapani
   Cho, Kyunghyun
TI Measuring the usefulness of hidden units in Boltzmann machines with
   mutual information
SO NEURAL NETWORKS
LA English
DT Article
DE Deep learning; Restricted Boltzmann machine; Deep Boltzmann machine;
   Pruning; Structural learning; Mutual information
ID GRADIENT
AB Restricted Boltzmann machines (RBMs) and deep Boltzmann machines (DBMs) are important models in deep learning, but it is often difficult to measure their performance in general, or measure the importance of individual hidden units in specific. We propose to use mutual information to measure the usefulness of individual hidden units in Boltzmann machines. The measure is fast to compute, and serves as an upper bound for the information the neuron can pass on, enabling detection of a particular kind of poor training results. We confirm experimentally that the proposed measure indicates how much the performance of the model drops when some of the units of an RBM are pruned away. We demonstrate the usefulness of the measure for early detection of poor training in DBMs. (C) 2014 Elsevier Ltd. All rights reserved.
C1 [Berglund, Mathias; Raiko, Tapani; Cho, Kyunghyun] Aalto Univ, Sch Sci, Dept Informat & Comp Sci, Espoo, Finland.
RP Berglund, M (reprint author), Aalto Univ, Sch Sci, Dept Informat & Comp Sci, Espoo, Finland.
EM mathias.berglund@aalto.fi
RI Raiko, Tapani/E-7237-2012
OI Raiko, Tapani/0000-0002-0321-304X
FU Academy of Finland (Finnish Centre of Excellence in Computational
   Inference Research COIN) [251170]
FX This work was supported by the Academy of Finland (Finnish Centre of
   Excellence in Computational Inference Research COIN, 251170).
CR Adams R. P., 2010, JMLR WORKSH C P, V9
   Berglund M., 2013, P 20 INT C NEUR INF
   Cho K., 2011, P 28 INT C MACH LEAR, P105
   Cho K., 2013, P 23 INT C ART NEUR
   Cho K, 2013, NEURAL COMPUT, V25, P805, DOI 10.1162/NECO_a_00397
   Desjardins G., 2011, ADV NEURAL INFORM PR, V24, P2501
   Engelbrecht AP, 2001, IEEE T NEURAL NETWOR, V12, P1386, DOI 10.1109/72.963775
   Glorot X., 2010, JLMR P TRACK, p[249, 2010], DOI DOI 10.1177/1753193409103364.
   Goodfellow  I., 2013, ADV NEURAL INFORM PR, P548
   Hinton G., 2010, MOMENTUM, V9, P926
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee H., 2009, P 26 ANN INT C MACH, P609, DOI DOI 10.1145/1553374.1553453
   REED R, 1993, IEEE T NEURAL NETWOR, V4, P740, DOI 10.1109/72.248452
   Salakhutdinov R, 2008, 2008002 UTML TR DEP
   Salakhutdinov R., 2007, P INT C MACH LEARN, V24, P791, DOI DOI 10.1145/1273496.1273596
   Salakhutdinov R, 2009, ARTIF INTELL, P448, DOI DOI 10.1109/CVPRW.2009.5206577
   Salakhutdinov R, 2012, NEURAL COMPUT, V24, P1967, DOI 10.1162/NECO_a_00311
   Smolensky P, 1986, PARALLEL DISTRIBUTED, P194
   Srivastava N., 2012, ADV NEURAL INFORM PR, V25, P2231
   Tieleman T., 2008, P 25 INT C MACH LEAR, P1064, DOI DOI 10.1145/1390156.1390290
   Zhou G., 2012, JMLR WORKSH C P, P1453
NR 23
TC 8
Z9 8
U1 1
U2 22
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0893-6080
EI 1879-2782
J9 NEURAL NETWORKS
JI Neural Netw.
PD APR
PY 2015
VL 64
SI SI
BP 12
EP 18
DI 10.1016/j.neunet.2014.09.004
PG 7
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA CD2RL
UT WOS:000350926600003
PM 25318376
DA 2020-02-19
ER

PT J
AU Kim, S
   Yu, Z
   Kil, RM
   Lee, M
AF Kim, Sangwook
   Yu, Zhibin
   Kil, Rhee Man
   Lee, Minho
TI Deep learning of support vector machines with class probability output
   networks
SO NEURAL NETWORKS
LA English
DT Article
DE Deep learning; Support vector machine; Class probability output network;
   Uncertainty measure
ID ALGORITHM
AB Deep learning methods endeavor to learn features automatically at multiple levels and allow systems to learn complex functions mapping from the input space to the output space for the given data. The ability to learn powerful features automatically is increasingly important as the volume of data and range of applications of machine learning methods continues to grow. This paper proposes a new deep architecture that uses support vector machines (SVMs) with class probability output networks (CPONs) to provide better generalization power for pattern classification problems. As a result, deep features are extracted without additional feature engineering steps, using multiple layers of the SVM classifiers with CPONs. The proposed structure closely approaches the ideal Bayes classifier as the number of layers increases. Using a simulation of classification problems, the effectiveness of the proposed method is demonstrated. (C) 2014 Elsevier Ltd. All rights reserved.
C1 [Kim, Sangwook; Yu, Zhibin; Lee, Minho] Kyungpook Natl Univ, Sch Elect Engn, Taegu 702701, South Korea.
   [Kil, Rhee Man] Sungkyunkwan Univ, Coll Informat & Commun Engn, Suwon 440746, Gyeonggi Do, South Korea.
RP Kil, RM (reprint author), Kyungpook Natl Univ, Sch Elect Engn, 1370 Sankyuk Dong, Taegu 702701, South Korea.
EM rmkil@skku.edu; mholee@knu.ac.kr
RI Yu, Zhibin/Z-1138-2019
OI Yu, Zhibin/0000-0003-4372-1767
FU ICT R&D program of MSIP/IITP [10041826]; Industrial Strategic Technology
   Development Program - Ministry of Knowledge Economy (MIKE, Korea)
   [10044009]
FX This work was partly supported by the ICT R&D program of MSIP/IITP.
   [10041826, Development of emotional features sensing, diagnostics and
   distribution s/w platform for measurement of multiple intelligence from
   young children] and by the Industrial Strategic Technology Development
   Program [10044009] funded by the Ministry of Knowledge Economy (MIKE,
   Korea) (50%).
CR Anthony M., 1997, COMPUTATIONAL LEARNI
   Arel I, 2010, IEEE COMPUT INTELL M, V5, P13, DOI 10.1109/MCI.2010.938364
   Bache K., 2013, UCI MACHINE LEARNING
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   BENTAL A, 1982, MATH PROGRAM STUD, V19, P39
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411
   FREUND Y, 1995, INFORM COMPUT, V121, P256, DOI 10.1006/inco.1995.1136
   FUKUSHIMA K, 1980, BIOL CYBERN, V36, P193, DOI 10.1007/BF00344251
   GUIGNARD M, 1969, SIAM J CONTROL, V7, P232, DOI 10.1137/0307016
   Hastie T., 1998, ADV NEURAL INFORM PR, V10
   Hinton G., 2010, MOMENTUM, V9, P926
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Kim HG, 2011, LECT NOTES COMPUT SC, V7064, P774, DOI 10.1007/978-3-642-24965-5_87
   Le Roux N, 2008, NEURAL COMPUT, V20, P1631, DOI 10.1162/neco.2008.04-07-510
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee H., 2008, ADV NEURAL INFORM PR, V20, P873
   Luenberger D.G., 1968, OPTIMIZATION VECTOR
   Metsis V., 2006, CEAS, P27
   Park WJ, 2009, IEEE T NEURAL NETWOR, V20, P1659, DOI 10.1109/TNN.2009.2029103
   Platt JC, 2000, ADV NEUR IN, P61
   Rohatgi V. K., 2001, INTRO PROBABILITY ST, P598
   Rosas H, 2010, IEEE T CONSUM ELECTR, V56, P2296, DOI 10.1109/TCE.2010.5681103
   Sangwook Kim, 2013, Neural Information Processing. 20th International Conference, ICONIP 2013. Proceedings: LNCS 8226, P458, DOI 10.1007/978-3-642-42054-2_57
   Schapire RE, 2003, LECT NOTES STAT, V171, P149
   Schapire RE, 1998, ANN STAT, V26, P1651
   Smolensky Paul, 1986, INFORM PROCESSING DY
   Tang Y., 2013, WORKSH CHALL REPR LE
   TESAURO G, 1992, MACH LEARN, V8, P257, DOI 10.1023/A:1022624705476
   Ting KM, 1999, J ARTIF INTELL RES, V10, P271, DOI 10.1613/jair.594
   Utgoff PE, 2002, NEURAL COMPUT, V14, P2497, DOI 10.1162/08997660260293319
   Vapnik V. N., 1998, STAT LEARNING THEORY
   Weston J., 1999, ESANN 99, V99, P61
   Wiering M., 2013, DEEP SUPPORT VECTOR
   WOLPERT DH, 1992, NEURAL NETWORKS, V5, P241, DOI 10.1016/S0893-6080(05)80023-1
NR 36
TC 30
Z9 32
U1 1
U2 97
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0893-6080
EI 1879-2782
J9 NEURAL NETWORKS
JI Neural Netw.
PD APR
PY 2015
VL 64
SI SI
BP 19
EP 28
DI 10.1016/j.neunet.2014.09.007
PG 10
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA CD2RL
UT WOS:000350926600004
PM 25304363
DA 2020-02-19
ER

PT J
AU Elfwing, S
   Uchibe, E
   Doya, K
AF Elfwing, S.
   Uchibe, E.
   Doya, K.
TI Expected energy-based restricted Boltzmann machine for classification
SO NEURAL NETWORKS
LA English
DT Article
DE Classification; Restricted Boltzmann machine; Expected energy; Free
   energy
ID DEEP
AB In classification tasks, restricted Boltzmann machines (RBMs) have predominantly been used in the first stage, either as feature extractors or to provide initialization of neural networks. In this study, we propose a discriminative learning approach to provide a self-contained RBM method for classification, inspired by free-energy based function approximation (FE-RBM), originally proposed for reinforcement learning. For classification, the FE-RBM method computes the output for an input vector and a class vector by the negative free energy of an RBM. Learning is achieved by stochastic gradient-descent using a mean-squared error training objective. In an earlier study, we demonstrated that the performance and the robustness of FE-RBM function approximation can be improved by scaling the free energy by a constant that is related to the size of network. In this study, we propose that the learning performance of RBM function approximation can be further improved by computing the output by the negative expected energy (EE-RBM), instead of the negative free energy. To create a deep learning architecture, we stack several RBMs on top of each other. We also connect the class nodes to all hidden layers to try to improve the performance even further. We validate the classification performance of EE-RBM using the MNIST data set and the NORB data set, achieving competitive performance compared with other classifiers such as standard neural networks, deep belief networks, classification RBMs, and support vector machines. The purpose of using the NORB data set is to demonstrate that EE-RBM with binary input nodes can achieve high performance in the continuous input domain. (C) 2014 The Authors. Published by Elsevier Ltd.
C1 [Elfwing, S.; Uchibe, E.; Doya, K.] Grad Univ, Okinawa Inst Sci & Technol, Okinawa 9040495, Japan.
RP Elfwing, S (reprint author), Grad Univ, Okinawa Inst Sci & Technol, 1919-1 Tancha, Okinawa 9040495, Japan.
EM elfwing@oist.jp; uchibe@oist.jp; doya@oist.jp
RI Elfwing, Stefan/G-2940-2015; Doya, Kenji/B-5841-2015
OI Elfwing, Stefan/0000-0001-6689-1000; Doya, Kenji/0000-0002-2446-6820
FU  [23120007];  [26120727]
FX This work was supported by Grant-in-Aid for Scientific Research on
   Innovative Areas: Prediction and Decision Making 23120007 and 26120727.
CR Bengio Y., 2006, P ADV NEUR INF PROC, V19
   Bengio Y., 2007, LARGE SCALE KERNEL M
   Ciresan DC, 2010, NEURAL COMPUT, V22, P3207, DOI 10.1162/NECO_a_00052
   Decoste D, 2002, MACH LEARN, V46, P161, DOI 10.1023/A:1012454411458
   Elfwing S, 2013, FRONT NEUROROBOTICS, V7, DOI 10.3389/fnbot.2013.00003
   Freund Y., 1992, ADV NEURAL INFORM PR, V4
   Goodfellow I. J., 2013, P ADV NEUR INF PROC, V26
   Goodfellow I. J., 2013, P INT C MACH LEARN I
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton G. E., 2012, ARXIV12070580CENE
   Hinton G. E., 2010, 2010003 UTML TR DEP
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hinton GE, 2007, PROG BRAIN RES, V165, P535, DOI 10.1016/S0079-6123(06)65034-6
   Larochelle H., 2008, P INT C MACH LEARN I
   Larochelle H, 2012, J MACH LEARN RES, V13, P643
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y., 2004, P CVPR2004
   Nair V., 2008, P ADV NEUR INF PROC, V21
   Ngiam J, 2011, P 28 INT C MACH LEAR, P1105
   P Smolensky, 1986, PARALLEL DISTRIBUTED, VI
   Raiko T., 2012, P INT C ART INT STAT, P924
   Salakhutdinov R, 2009, ARTIF INTELL, P448, DOI DOI 10.1109/CVPRW.2009.5206577
   Sallans B, 2004, J MACH LEARN RES, V5, P1063
   Schmah T., 2008, P ADV NEUR INF PROC, V21
   Sutton R. S, 1998, REINFORCEMENT LEARNI
NR 26
TC 16
Z9 18
U1 0
U2 44
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0893-6080
EI 1879-2782
J9 NEURAL NETWORKS
JI Neural Netw.
PD APR
PY 2015
VL 64
SI SI
BP 29
EP 38
DI 10.1016/j.neunet.2014.09.006
PG 10
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA CD2RL
UT WOS:000350926600005
PM 25318375
OA Other Gold
DA 2020-02-19
ER

PT J
AU Menotti, D
   Chiachia, G
   Pinto, A
   Schwartz, WR
   Pedrini, H
   Falcao, AX
   Rocha, A
AF Menotti, David
   Chiachia, Giovani
   Pinto, Allan
   Schwartz, William Robson
   Pedrini, Helio
   Falcao, Alexandre Xavier
   Rocha, Anderson
TI Deep Representations for Iris, Face, and Fingerprint Spoofing Detection
SO IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY
LA English
DT Article
DE Deep learning; convolutional networks; hyperparameter architecture
   optimization; filter weights learning; back-propagation; spoofing
   detection
ID LIVENESS DETECTION; TEXTURE CLASSIFICATION; CONTACT-LENSES; IMAGE;
   RECOGNITION; SEARCH
AB Biometrics systems have significantly improved person identification and authentication, playing an important role in personal, national, and global security. However, these systems might be deceived (or spoofed) and, despite the recent advances in spoofing detection, current solutions often rely on domain knowledge, specific biometric reading systems, and attack types. We assume a very limited knowledge about biometric spoofing at the sensor to derive outstanding spoofing detection systems for iris, face, and fingerprint modalities based on two deep learning approaches. The first approach consists of learning suitable convolutional network architectures for each domain, whereas the second approach focuses on learning the weights of the network via back propagation. We consider nine biometric spoofing benchmarks-each one containing real and fake samples of a given biometric modality and attack type- and learn deep representations for each benchmark by combining and contrasting the two learning approaches. This strategy not only provides better comprehension of how these approaches interplay, but also creates systems that exceed the best known results in eight out of the nine benchmarks. The results strongly indicate that spoofing detection systems based on convolutional networks can be robust to attacks already known and possibly adapted, with little effort, to image-based attacks that are yet to come.
C1 [Menotti, David; Chiachia, Giovani; Pinto, Allan; Pedrini, Helio; Falcao, Alexandre Xavier; Rocha, Anderson] Univ Estadual Campinas, Inst Comp, BR-13083852 Campinas, SP, Brazil.
   [Schwartz, William Robson] Univ Fed Minas Gerais, Dept Comp Sci, BR-31270010 Belo Horizonte, MG, Brazil.
RP Menotti, D (reprint author), Univ Fed Ouro Preto, Dept Comp, BR-35400000 Ouro Preto, Brazil.
EM menottid@gmail.com; chiachia@ic.unicamp.br; allan.pinto@ic.unicamp.br;
   william@dcc.ufmg.br; helio.pedrini@ic.unicamp.br; afalcao@ic.unicamp.br;
   anderson.rocha@ic.unicamp.br
RI Pinto, Allan/G-7922-2012; Menotti, David/W-6899-2019; Menotti,
   David/M-6205-2014; Pedrini, Helio/A-7556-2012; Falcao,
   Alexandre/F-8361-2012
OI Pinto, Allan/0000-0003-3765-8300; Menotti, David/0000-0003-2430-2030;
   Menotti, David/0000-0003-2430-2030; Pedrini, Helio/0000-0003-0125-630X;
   Schwartz, William/0000-0003-1449-8834
FU Sao Paulo Research FoundationFundacao de Amparo a Pesquisa do Estado de
   Sao Paulo (FAPESP) [2010/05647-4, 2011/22749-8, 2013/04172-0,
   2013/11359-0]; Federal University of Ouro Preto through Brazilian
   National Research Council [303673/2010-9, 304352/2012-8, 307113/2012-4,
   477662/2013-7, 487529/2013-8, 479070/2013-0, 477457/2013-4]; Minas
   Gerais Research Foundation [APQ-01806-13]; Coordination for the
   Improvement of Higher Education Personnel (CAPES) DeepEyes Project
FX This work was supported in part by the Sao Paulo Research Foundation
   under Grant 2010/05647-4, Grant 2011/22749-8, Grant 2013/04172-0, and
   Grant 2013/11359-0, in part by the Federal University of Ouro Preto
   through the Brazilian National Research Council under Grant
   303673/2010-9, Grant 304352/2012-8, Grant 307113/2012-4, Grant
   477662/2013-7, Grant 487529/2013-8, Grant 479070/2013-0, and Grant
   477457/2013-4, in part by the Minas Gerais Research Foundation under
   Grant APQ-01806-13, and in part by the Coordination for the Improvement
   of Higher Education Personnel (CAPES) DeepEyes Project. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. Stan Z. Li. (David Menotti and Giovani Chiachia
   contributed equally to this work.)
CR Anjos A., 2011, BIOM IJCB 2011 INT J, P1, DOI DOI 10.1109/IJCB.2011.6117503
   Bergstra J, 2013, P 30 INT C MACH LEAR, V28, pI
   Bergstra J., 2013, HYPERPARAMETER OPTIM
   Bergstra J., 2010, P 9 PYTH SCI C, P1
   Bergstra J, 2011, ADV NEURAL INFORM PR, V24, P2546
   Bergstra J, 2012, J MACH LEARN RES, V13, P281
   Bowyer KW, 2014, COMPUTER, V47, P96, DOI 10.1109/MC.2014.118
   Chakka M. M., 2011, BIOM IJCB 2011 INT J, p1~6
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chiachia G., 2014, HYPERPARAMETER OPTIM
   Chingovska I, 2013, INT CONF BIOMETR
   Chingovska  I., 2012, P INT C BIOM SPEC IN, P1, DOI DOI 10.1109/VTCFALL.2012.6399116
   Christian R., 2011, COMP VIS PATT REC WO, P23
   Ciresan D, 2012, NEURAL NETWORKS, V32, P333, DOI 10.1016/j.neunet.2012.02.023
   Ciresan DC, 2010, NEURAL COMPUT, V22, P3207, DOI 10.1162/NECO_a_00052
   Cox D, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P8, DOI 10.1109/FG.2011.5771385
   Czajka A, 2013, 2013 18TH INTERNATIONAL CONFERENCE ON METHODS AND MODELS IN AUTOMATION AND ROBOTICS (MMAR), P28
   da Silva Pinto A., 2012, 2012 XXV SIBGRAPI - Conference on Graphics, Patterns and Images (SIBGRAPI 2012), P221, DOI 10.1109/SIBGRAPI.2012.38
   DAUGMAN J, 2004, [No title captured]
   DAUGMAN J, 1999, [No title captured], P103
   de Freitas Pereira T., 2013, BIOM ICB 2013 INT C, P1, DOI DOI 10.1109/ICB.2013.6612981
   Doyle J., 2013, IEEE INT C BIOM THEO, P1, DOI DOI 10.1109/BTAS.2013.6712745
   Doyle J., 2014, NOTRE DAME IMAGE DAT
   Erdogmus Nesli, 2013, P IEEE 6 INT C BIOM, P1, DOI DOI 10.1109/BTAS.2013.6712688
   Fierrez J, 2007, PATTERN RECOGN, V40, P1389, DOI 10.1016/j.patcog.2006.10.014
   Galbally J., 2012, 2012 5th IAPR International Conference on Biometrics (ICB), P271, DOI 10.1109/ICB.2012.6199819
   Galbally J., 2007, DATABASE, V1, P1
   Galbally J., 2009, P IEEE INT C BIOM ID, P1
   Galbally J, 2014, IEEE T IMAGE PROCESS, V23, P710, DOI 10.1109/TIP.2013.2292332
   Galbally J, 2012, FUTURE GENER COMP SY, V28, P311, DOI 10.1016/j.future.2010.11.024
   GEISLER WS, 1992, VISION RES, V32, P1409, DOI 10.1016/0042-6989(92)90196-P
   Ghiani L., 2013, P IEEE 6 INT C BIOM, P1
   Ghiani L, 2012, INT C PATT RECOG, P537
   Gragnaniello Diego, 2013, Proceedings of the 2013 IEEE Workshop on Biometric Measurements and Systems for Security and Medical Applications (BIOMS), P46, DOI 10.1109/BIOMS.2013.6656148
   Gunther Manuel, 2012, Artificial Neural Networks and Machine Learning - ICANN 2012. Proceedings 22nd International Conference on Artificial Neural Networks, P411, DOI 10.1007/978-3-642-33269-2_52
   Huang XY, 2013, IEEE WORK APP COMP, P252, DOI 10.1109/WACV.2013.6475026
   Hyvarinen A, 2009, COMPUT IMAGING VIS, V39, P1
   Jarrett K, 2009, IEEE I CONF COMP VIS, P2146, DOI 10.1109/ICCV.2009.5459469
   Jia X., 2013, P IAPR INT C BIOM IC, P1
   Kanematsu Masashi, 2007, SICE '07. 46th SICE Annual Conference, P361
   Kannala J, 2012, INT C PATT RECOG, P1363
   Kathikeyan  T., 2012, P INT C COMP COMM AP, P1
   Kohli N., 2013, P 6 IAPR, P1, DOI DOI 10.1109/ICB.2013.6613021
   Komulainen J., 2013, P INT C BIOM, P1
   Kose N., 2013, P 18 INT C DIG SIGN, P1
   Kose N., 2013, AUT FAC GEST REC FG, P1
   Kose N, 2013, INT CONF ACOUST SPEE, P2357, DOI 10.1109/ICASSP.2013.6638076
   Krizhevsky A., 2012, CUDA CONVENT HIGH PE
   Krizhevsky A., 2012, ADV NEURAL INFORM PR
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee EC, 2006, LECT NOTES COMPUT SC, V3832, P397
   Lee JE, 2008, 2008 BIOMETRICS SYMPOSIUM (BSYM), P1, DOI [10.1109/PLASMA.2008.4591032, 10.1109/BSYM.2008.4655515]
   Lee TW, 2013, INT CONF ACOUST SPEE, P2367, DOI 10.1109/ICASSP.2013.6638078
   Marcialis GL, 2009, LECT NOTES COMPUT SC, V5716, P12, DOI 10.1007/978-3-642-04146-4_4
   Mato J., 2011, BIOM IJCB 2011 INT J, P1, DOI DOI 10.1109/IJCB.2011.6117510
   Monteiro J. C., 2004, COMPUTER VISION IMAG
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Ojansivu V, 2008, LECT NOTES COMPUT SC, V5099, P236, DOI 10.1007/978-3-540-69905-7_27
   Ouyang WL, 2013, IEEE I CONF COMP VIS, P2056, DOI 10.1109/ICCV.2013.257
   Pacut A, 2006, CAR C SECUR, P122, DOI 10.1109/CCST.2006.313440
   Pereira TD, 2014, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2014-2
   Pinto N, 2009, PLOS COMPUT BIOL, V5, DOI 10.1371/journal.pcbi.1000579
   PUDIL P, 1994, PATTERN RECOGN LETT, V15, P1119, DOI 10.1016/0167-8655(94)90127-9
   Ratha NK, 2001, LECT NOTES COMPUT SC, V2091, P223
   Rathgeb Christian, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P1217, DOI 10.1109/ICPR.2010.303
   Ross A. A., 2006, HDB MULTIBIOMETRICS, P91
   Ruiz-Albacete V, 2008, LECT NOTES COMPUT SC, V5372, P181, DOI 10.1007/978-3-540-89991-4_19
   Saxe A, 2011, P 28 INT C MACH LEAR, P1
   Schuckers S., 2013, LIVDET 2013 LIVMESS
   Schwarzenbach W, 2011, P IEEE INT JOINT C B, P1
   Sequeira AF, 2014, IEEE IJCNN, P3002, DOI 10.1109/IJCNN.2014.6889816
   Sequeira AF, 2014, PROCEEDINGS OF THE 2014 9TH INTERNATIONAL CONFERENCE ON COMPUTER VISION, THEORY AND APPLICATIONS (VISAPP 2014), VOL 3, P133
   Sequeira AF, 2014, PROCEEDINGS OF THE 2014 9TH INTERNATIONAL CONFERENCE ON COMPUTER VISION, THEORY AND APPLICATIONS (VISAPP 2014), VOL 3, P22
   Simonyan K., 2014, VERY DEEP CONVOLUTIO
   Sun ZN, 2014, IEEE T PATTERN ANAL, V36, P1120, DOI 10.1109/TPAMI.2013.234
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Tan XY, 2010, LECT NOTES COMPUT SC, V6316, P504
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Wardern P., 2014, SDK JETPACS DEEP BEL
   Wei Z., 2008, P 19 INT C PATT REC, P1, DOI DOI 10.1109/ICPR.2008.4761126
   Wiskott L, 1997, IEEE T PATTERN ANAL, V19, P775, DOI 10.1109/34.598235
   Yadav D, 2014, IEEE T INF FOREN SEC, V9, P851, DOI 10.1109/TIFS.2014.2313025
   Yambay D., 2011, P 5 IAPR INT C BIOM, P208
   Yi D, 2014, ADV COMPUT VIS PATT, P83, DOI 10.1007/978-1-4471-6524-8_5
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang L, 2012, IEEE IMAGE PROC, P81, DOI 10.1109/ICIP.2012.6466800
   Zhang WC, 2005, IEEE I CONF COMP VIS, P786
NR 87
TC 154
Z9 157
U1 11
U2 84
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1556-6013
EI 1556-6021
J9 IEEE T INF FOREN SEC
JI IEEE Trans. Inf. Forensic Secur.
PD APR
PY 2015
VL 10
IS 4
BP 864
EP 879
DI 10.1109/TIFS.2015.2398817
PG 16
WC Computer Science, Theory & Methods; Engineering, Electrical & Electronic
SC Computer Science; Engineering
GA CE3UB
UT WOS:000351753400008
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Lv, YS
   Duan, YJ
   Kang, WW
   Li, ZX
   Wang, FY
AF Lv, Yisheng
   Duan, Yanjie
   Kang, Wenwen
   Li, Zhengxi
   Wang, Fei-Yue
TI Traffic Flow Prediction With Big Data: A Deep Learning Approach
SO IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS
LA English
DT Article
DE Deep learning; stacked autoencoders (SAEs); traffic flow prediction
ID NEURAL-NETWORKS; MULTIVARIATE; MODELS; VOLUME; REGRESSION; ALGORITHM
AB Accurate and timely traffic flow information is important for the successful deployment of intelligent transportation systems. Over the last few years, traffic data have been exploding, and we have truly entered the era of big data for transportation. Existing traffic flow prediction methods mainly use shallow traffic prediction models and are still unsatisfying for many real-world applications. This situation inspires us to rethink the traffic flow prediction problem based on deep architecture models with big traffic data. In this paper, a novel deep-learning-based traffic flow prediction method is proposed, which considers the spatial and temporal correlations inherently. A stacked autoencoder model is used to learn generic traffic flow features, and it is trained in a greedy layerwise fashion. To the best of our knowledge, this is the first time that a deep architecture model is applied using autoencoders as building blocks to represent traffic flow features for prediction. Moreover, experiments demonstrate that the proposed method for traffic flow prediction has superior performance.
C1 [Lv, Yisheng; Duan, Yanjie; Kang, Wenwen; Wang, Fei-Yue] Chinese Acad Sci, Inst Automat, State Key Lab Management & Control Complex Syst, Beijing 100190, Peoples R China.
   [Li, Zhengxi] North China Univ Technol, Beijing 100144, Peoples R China.
RP Lv, YS (reprint author), Chinese Acad Sci, Inst Automat, State Key Lab Management & Control Complex Syst, Beijing 100190, Peoples R China.
EM yisheng.lv@ia.ac.cn; duanyanjie2012@ia.ac.cn; kangwenwen2012@ia.ac.cn;
   lzx@ncut.edu.cn; feiyue@ieee.org
RI Lv, Yisheng/N-3426-2015
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61233001, 61203166, 71232006, 61104054, 61273326]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61233001, 61203166, 71232006, 61104054,
   and 61273326. The Associate Editor for this paper was J. Zhang.
CR Ahmed M. S., 1979, TRANSPORT RES REC, V722, P1
   Ben-Akiva M., 1995, URBAN TRAFFIC NETWOR, P83
   Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Caltrans, 2014, PERF MEAS SYST PEMS
   Cetin M, 2006, TRANSPORT RES REC, P23
   Chan KY, 2012, IEEE T INTELL TRANSP, V13, P644, DOI 10.1109/TITS.2011.2174051
   Chang H, 2012, IET INTELL TRANSP SY, V6, P292, DOI 10.1049/iet-its.2011.0123
   Chen CLP, 2014, INFORM SCIENCES, V275, P314, DOI 10.1016/j.ins.2014.01.015
   Chen CY, 2012, TRANSPORT RES C-EMER, V22, P103, DOI 10.1016/j.trc.2011.12.006
   Chung E., 2001, 24 AUSTR TRANSP RES
   Collobert R., 2008, P 25 ICML, V25, P160, DOI [10.1145/1390156.1390177, DOI 10.1145/1390156.1390177]
   Comert G, 2013, IEEE T INTELL TRANSP, V14, P1360, DOI 10.1109/TITS.2013.2260540
   DAVIS GA, 1991, J TRANSP ENG-ASCE, V117, P178, DOI 10.1061/(ASCE)0733-947X(1991)117:2(178)
   Dia H, 2001, EUR J OPER RES, V131, P253, DOI 10.1016/S0377-2217(00)00125-9
   Dimitriou L, 2008, TRANSPORT RES C-EMER, V16, P554, DOI 10.1016/j.trc.2007.11.003
   DOUGHERTY M, 1995, TRANSPORT RES C-EMER, V3, P247, DOI 10.1016/0968-090X(95)00009-8
   Duncan G., 1997, P IEE C STRAT CONTR
   ElFaouzi NE, 1996, TRANSPORTATION AND TRAFFIC THEORY, P41
   Ghosh B, 2009, IEEE T INTELL TRANSP, V10, P246, DOI 10.1109/TITS.2009.2021448
   Goodfellow I. J, 2013, ARXIV13126082
   HAMED MM, 1995, J TRANSP ENG-ASCE, V121, P249, DOI 10.1061/(ASCE)0733-947X(1995)121:3(249)
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Huval B, 2013, ARXIV13126885
   Jeong YS, 2013, IEEE T INTELL TRANSP, V14, P1700, DOI 10.1109/TITS.2013.2267735
   Jin F, 2008, IEEE IJCNN, P1897, DOI 10.1109/IJCNN.2008.4634057
   Kamarianakis Y, 2003, TRANSPORT RES REC, P74, DOI 10.3141/1857-09
   Kirby HR, 1997, INT J FORECASTING, V13, P43, DOI 10.1016/S0169-2070(96)00699-1
   Kumar K, 2013, PROCD SOC BEHV, V104, P755, DOI 10.1016/j.sbspro.2013.11.170
   LEE S, 1999, TRANSPORT RES REC, V1678, P179, DOI DOI 10.3141/1678-22
   Levin M., 1980, TRANSPORT RES REC, P47
   Li L., 2006, IEE Proceedings Intelligent Transport Systems, V153, P33, DOI 10.1049/ip-its:20055009
   Lippi M, 2013, IEEE T INTELL TRANSP, V14, P871, DOI 10.1109/TITS.2013.2247040
   OKUTANI I, 1984, TRANSPORT RES B-METH, V18, P1, DOI 10.1016/0191-2615(84)90002-X
   Palm R.B., 2012, PREDICTION CANDIDATE
   Park B, 1998, TRANSPORT RES REC, P39, DOI 10.3141/1651-06
   Ran B, 2000, INT J TECHNOL MANAGE, V20, P326, DOI 10.1504/IJTM.2000.002870
   Shin HC, 2013, IEEE T PATTERN ANAL, V35, P1930, DOI 10.1109/TPAMI.2012.277
   Smith BL, 2002, TRANSPORT RES C-EMER, V10, P303, DOI 10.1016/S0968-090X(02)00009-8
   Smith BL, 1997, J TRANSP ENG-ASCE, V123, P261, DOI 10.1061/(ASCE)0733-947X(1997)123:4(261)
   Stathopoulos A, 2003, TRANSPORT RES C-EMER, V11, P121, DOI 10.1016/S0968-090X(03)00004-4
   Sun HY, 2003, TRANSPORT RES REC, P143, DOI 10.3141/1836-18
   Sun SL, 2011, IEEE T INTELL TRANSP, V12, P466, DOI 10.1109/TITS.2010.2093575
   Sun SL, 2006, IEEE T INTELL TRANSP, V7, P124, DOI 10.1109/TITS.2006.869623
   Tahmasbi R, 2014, IEEE T INTELL TRANSP, V15, P250, DOI 10.1109/TITS.2013.2278614
   Tan MC, 2009, IEEE T INTELL TRANSP, V10, P60, DOI 10.1109/TITS.2008.2011693
   Van Hinsbergen C. P., 2007, ITS WORLD C BEIJ CHI
   vanderVoort M, 1996, TRANSPORT RES C-EMER, V4, P307, DOI 10.1016/S0968-090X(97)82903-8
   Vlahogianni EI, 2005, TRANSPORT RES C-EMER, V13, P211, DOI 10.1016/j.trc.2005.04.007
   Vlahogianni EI, 2004, TRANSPORT REV, V24, P533, DOI 10.1080/0144164042000195072
   Williams BM, 1998, TRANSPORT RES REC, P132, DOI 10.3141/1644-14
   Williams BM, 2003, J TRANSP ENG-ASCE, V129, P664, DOI 10.1061/(ASCE)0733-947X(2003)129:6(664)
   Williams BM, 2001, TRANSPORT RES REC, P194, DOI 10.3141/1776-25
   Yang F, 2004, TRANSPORT RES REC, P1, DOI 10.3141/1879-01
   Yin HB, 2002, TRANSPORT RES C-EMER, V10, P85, DOI 10.1016/S0968-090X(01)00004-3
   Zargari SA, 2012, EXPERT SYST, V29, P124, DOI 10.1111/j.1468-0394.2010.00567.x
   Zhang JP, 2011, IEEE T INTELL TRANSP, V12, P1624, DOI 10.1109/TITS.2011.2158001
   Zhang N, 2008, IEEE INTELL SYST, V23, P19, DOI 10.1109/MIS.2008.101
   Zheng WZ, 2006, J TRANSP ENG, V132, P114, DOI 10.1061/(ASCE)0733-947X(2006)132:2(114)
   Zhong M, 2005, J COMPUT CIVIL ENG, V19, P94, DOI 10.1061/(ASCE)0887-3801(2005)19:1(94)
NR 61
TC 663
Z9 691
U1 27
U2 27
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1524-9050
EI 1558-0016
J9 IEEE T INTELL TRANSP
JI IEEE Trans. Intell. Transp. Syst.
PD APR
PY 2015
VL 16
IS 2
BP 865
EP 873
DI 10.1109/TITS.2014.2345663
PG 9
WC Engineering, Civil; Engineering, Electrical & Electronic; Transportation
   Science & Technology
SC Engineering; Transportation
GA CF1CZ
UT WOS:000352282500029
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Hayat, M
   Bennamoun, M
   An, SJ
AF Hayat, Munawar
   Bennamoun, Mohammed
   An, Senjian
TI Deep Reconstruction Models for Image Set Classification
SO IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
LA English
DT Article
DE Image set classification; deep learning; auto-encoders; video based face
   recognition; object recognition
ID FACE RECOGNITION; APPEARANCE
AB Image set classification finds its applications in a number of real-life scenarios such as classification from surveillance videos, multi-view camera networks and personal albums. Compared with single image based classification, it offers more promises and has therefore attracted significant research attention in recent years. Unlike many existing methods which assume images of a set to lie on a certain geometric surface, this paper introduces a deep learning framework which makes no such prior assumptions and can automatically discover the underlying geometric structure. Specifically, a Template Deep Reconstruction Model (TDRM) is defined whose parameters are initialized by performing unsupervised pre-training in a layer-wise fashion using Gaussian Restricted Boltzmann Machines (GRBMs). The initialized TDRM is then separately trained for images of each class and class-specific DRMs are learnt. Based on the minimum reconstruction errors from the learnt class-specific models, three different voting strategies are devised for classification. Extensive experiments are performed to demonstrate the efficacy of the proposed framework for the tasks of face and object recognition from image sets. Experimental results show that the proposed method consistently outperforms the existing state of the art methods.
C1 [Hayat, Munawar; Bennamoun, Mohammed; An, Senjian] Univ Western Australia, Sch Comp Sci & Software Engn, Perth, WA 6009, Australia.
RP Hayat, M (reprint author), Univ Western Australia, Sch Comp Sci & Software Engn, Perth, WA 6009, Australia.
EM munawar.hayat@research.uwa.edu.au; mohammed.bennamoun@uwa.edu.au;
   senjian.an@uwa.edu.au
RI Bennamoun, Mohammed/C-2789-2013
OI Bennamoun, Mohammed/0000-0002-6603-3257; An,
   Senjian/0000-0002-1758-6824; Hayat, Munawar/0000-0002-2706-5985
FU SIRF scholarship from the University of Western Australia (UWA);
   ARCFondation ARC pour la Recherche sur le CancerAustralian Research
   Council [DPI10102166]
FX This work is supported by SIRF scholarship from the University of
   Western Australia (UWA) and ARC grant DPI10102166.
CR Arandjelovic O, 2005, PROC CVPR IEEE, P581
   Bengio Yoshua, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P437, DOI 10.1007/978-3-642-35289-8_26
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Carreira-Perpinan M.A., 2005, ARTIF INTELL STAT, P17
   Cevikalp H, 2010, PROC CVPR IEEE, P2567, DOI 10.1109/CVPR.2010.5539965
   Chawla NV, 2002, J ARTIF INTELL RES, V16, P321, DOI 10.1613/jair.953
   Fanelli G, 2011, PROC CVPR IEEE, P617, DOI 10.1109/CVPR.2011.5995458
   Gross R., 2001, CMURITR0118
   Hamm J., 2008, P 25 INT C MACH LEAR, P376
   Harandi M. T., 2012, 2012 IEEE Workshop on Applications of Computer Vision (WACV), P433, DOI 10.1109/WACV.2012.6163005
   Harandi M. T., 2010, P IEEE C COMP VIS PA, P2705
   Hayat M, 2014, PROC CVPR IEEE, P1915, DOI 10.1109/CVPR.2014.246
   Hinton G., 2010, MOMENTUM, V9, P926
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton G, 2006, COGNITIVE SCI, V30, P725, DOI 10.1207/s15516709cog0000_76
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hu YQ, 2012, IEEE T PATTERN ANAL, V34, P1992, DOI 10.1109/TPAMI.2011.283
   Kim TK, 2007, IEEE T PATTERN ANAL, V29, P1005, DOI 10.1109/TPAMI.2007.1037
   Kumar N, 2009, IEEE I CONF COMP VIS, P365, DOI 10.1109/ICCV.2009.5459250
   Lee H., 2009, P 26 ANN INT C MACH, P609, DOI DOI 10.1145/1553374.1553453
   Lee KC, 2003, PROC CVPR IEEE, P313
   Leibe B, 2003, PROC CVPR IEEE, P409
   Li BYL, 2013, IEEE WORK APP COMP, P186, DOI 10.1109/WACV.2013.6475017
   Mian AS, 2007, IEEE T PATTERN ANAL, V29, P1927, DOI 10.1109/TPAMI.2007.1105
   Mian AS, 2006, IEEE T PATTERN ANAL, V28, P1584, DOI 10.1109/TPAMI.2006.213
   Minyoung K., 2008, IEEE C COMP VIS PATT, V1, P1, DOI DOI 10.1109/CVPR.2008.4587572
   Naseem I, 2010, IEEE T PATTERN ANAL, V32, P2106, DOI 10.1109/TPAMI.2010.128
   Nishiyama M., 2007, P IEEE C COMP VIS PA, P1
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Ortiz EG, 2013, PROC CVPR IEEE, P3531, DOI 10.1109/CVPR.2013.453
   P Smolensky, 1986, PARALLEL DISTRIBUTED, VI
   Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7
   Taylor GW, 2010, LECT NOTES COMPUT SC, V6316, P140, DOI 10.1007/978-3-642-15567-3_11
   Turaga P, 2011, IEEE T PATTERN ANAL, V33, P2273, DOI 10.1109/TPAMI.2011.52
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Wang R, 2008, IEEE C COMP VIS PATT, P1, DOI DOI 10.1109/CVPR.2008.4587719
   Wang RP, 2012, PROC CVPR IEEE, P2496, DOI 10.1109/CVPR.2012.6247965
   Wang RP, 2009, PROC CVPR IEEE, P429, DOI 10.1109/CVPRW.2009.5206850
   Wolf L, 2011, PROC CVPR IEEE, P529, DOI 10.1109/CVPR.2011.5995566
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Yamaguchi O, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P318, DOI 10.1109/AFGR.1998.670968
   Yang D, 2013, IEEE INT WORKSH COMP, P13, DOI 10.1109/CAMAD.2013.6708080
   Zhao W, 2003, ACM COMPUT SURV, V35, P399, DOI 10.1145/954339.954342
   Zhiwu Huang, 2013, Computer Vision - ACCV 2012. 11th Asian Conference on Computer Vision. Revised Selected Papers, P589, DOI 10.1007/978-3-642-37444-9_46
   Zhu PF, 2013, IEEE I CONF COMP VIS, P2664, DOI 10.1109/ICCV.2013.331
NR 45
TC 74
Z9 80
U1 0
U2 70
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 0162-8828
EI 1939-3539
J9 IEEE T PATTERN ANAL
JI IEEE Trans. Pattern Anal. Mach. Intell.
PD APR
PY 2015
VL 37
IS 4
BP 713
EP 727
DI 10.1109/TPAMI.2014.2353635
PG 15
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
SC Computer Science; Engineering
GA CD6QF
UT WOS:000351213400002
PM 26353289
DA 2020-02-19
ER

PT J
AU Sainath, TN
   Kingsbury, B
   Saon, G
   Soltau, H
   Mohamed, AR
   Dahl, G
   Ramabhadran, B
AF Sainath, Tara N.
   Kingsbury, Brian
   Saon, George
   Soltau, Hagen
   Mohamed, Abdel-rahman
   Dahl, George
   Ramabhadran, Bhuvana
TI Deep Convolutional Neural Networks for Large-scale Speech Tasks
SO NEURAL NETWORKS
LA English
DT Article
DE Deep learning; Neural networks; Speech recognition
ID RECOGNITION
AB Convolutional Neural Networks (CNNs) are an alternative type of neural network that can be used to reduce spectral variations and model spectral correlations which exist in signals. Since speech signals exhibit both of these properties, we hypothesize that CNNs are a more effective model for speech compared to Deep Neural Networks (DNNs). In this paper, we explore applying CNNs to large vocabulary continuous speech recognition (LVCSR) tasks. First, we determine the appropriate architecture to make CNNs effective compared to DNNs for LVCSR tasks. Specifically, we focus on how many convolutional layers are needed, what is an appropriate number of hidden units, what is the best pooling strategy. Second, investigate how to incorporate speaker-adapted features, which cannot directly be modeled by CNNs as they do not obey locality in frequency, into the CNN framework. Third, given the importance of sequence training for speech tasks, we introduce a strategy to use ReLU+dropout during Hessian-free sequence training of CNNs. Experiments on 3 LVCSR tasks indicate that a CNN with the proposed speaker-adapted and ReLU+dropout ideas allow for a 12%-14% relative improvement in WER over a strong DNN system, achieving state-of-the art results in these 3 tasks. (C) 2014 Elsevier Ltd. All rights reserved.
C1 [Sainath, Tara N.; Kingsbury, Brian; Saon, George; Soltau, Hagen; Ramabhadran, Bhuvana] IBM TJ Watson Res Ctr, Yorktown Hts, NY 10598 USA.
   [Mohamed, Abdel-rahman; Dahl, George] Univ Toronto, Dept Comp Sci, Toronto, ON M5S 1A1, Canada.
RP Sainath, TN (reprint author), IBM TJ Watson Res Ctr, Yorktown Hts, NY 10598 USA.
EM tsainath@google.com; bedk@us.ibm.com; gsaon@us.ibm.com;
   soltau@google.com; asamir@cs.toronto.edu; gdahl@cs.toronto.edu;
   bhuvana@us.ibm.com
CR Abdel-Hamid O., 2012, PROC ICASSP
   Bourlard H.A., 1993, CONNECTIONIST SPEECH
   Dahl G., 2013, PROC ICASSP
   Dahl GE, 2012, IEEE T AUDIO SPEECH, V20, P30, DOI 10.1109/TASL.2011.2134090
   Deng L., 2013, PROC ICASSP
   Gales MJF, 1998, COMPUT SPEECH LANG, V12, P75, DOI 10.1006/csla.1998.0043
   Gales MJF, 1999, IEEE T SPEECH AUDI P, V7, P272, DOI 10.1109/89.759034
   Glorot X., PROC AI STATS
   Hinton G., 2012, THE COMPUTING RESEAR, V1207
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Jaitly N., 2012, PROC INTERSPEECH
   Kingsbury B., 2009, PROC ICASSP
   Kingsbury B., 2012, PROC INTERSPEECH
   Krizhevsky A., 2012, ADVANCES IN NEURAL I
   Lawrence S, 1997, IEEE T NEURAL NETWOR, V8, P98, DOI 10.1109/72.554195
   Lecun Y., 1998, PROCEEDINGS OF THE I
   LeCun Y., 1995, THE HANDBOOK OF BRAI
   LeCun Y., 2004, PROC CVPR
   Lee L., 1996, PROC ICASSP
   Martens J., 2010, PROC INTL CONF ON MA
   Mohamed A, 2012, ICASSP
   Povey D, 2008, INT CONF ACOUST SPEE, P4057, DOI 10.1109/ICASSP.2008.4518545
   Sainath T., 2013, PROC ICASSP
   Sainath T. N., 2013, PROC ASRU
   Sainath T. N., 2011, EXEMPLAR BASED SPARS
   Sainath T. N., 2011, PROC ASRU
   Saon G., 2013, PROC ASRU IN PREPERA
   Seide F., 2011, PROC INTERSPEECH
   Sermanet P., 2012, PATTERN RECOGNITION
   Soltau H., 2010, PROC SLT
   WAIBEL A, 1989, IEEE T ACOUST SPEECH, V37, P328, DOI 10.1109/29.21701
   Young S. J., 2006, THE HTK BOOK FOR HTK
   Young SJ, 1994, P WORKSH HUM LANG TE, P307, DOI DOI 10.3115/1075812.1075885
   Zeiler M., 2013, PROC OF THE INTERNAT
NR 34
TC 180
Z9 195
U1 18
U2 210
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0893-6080
EI 1879-2782
J9 NEURAL NETWORKS
JI Neural Netw.
PD APR
PY 2015
VL 64
SI SI
BP 39
EP 48
DI 10.1016/j.neunet.2014.08.005
PG 10
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA CD2RL
UT WOS:000350926600006
PM 25439765
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Li, SZ
   Yu, B
   Wu, W
   Su, SZ
   Ji, RR
AF Li, Shao-Zi
   Yu, Bin
   Wu, Wei
   Su, Song-Zhi
   Ji, Rong-Rong
TI Feature learning based on SAE-PCA network for human gesture recognition
   in RGBD images
SO NEUROCOMPUTING
LA English
DT Article
DE Deep learning; Auto-encoder; Convolutional neural networks; American
   sign language recognition
ID 3-D OBJECT RETRIEVAL
AB Coming with the emerging of depth sensors link Microsoft Kinect, human hand gesture recognition has received ever increasing research interests recently. A successful gesture recognition system has usually heavily relied on having a good feature representation of data, which is expected to be task-dependent as well as coping with the challenges and opportunities induced by depth sensor. In this paper, a feature learning approach based on sparse auto-encoder (SAE) and principle component analysis is proposed for recognizing human actions, i.e. finger-spelling or sign language, for RGB-D inputs. The proposed model of feature learning is consisted of two components: First, features are learned respectively from the RGB and depth channels, using sparse auto-encoder with convolutional neural networks. Second, the learned features from both channels is concatenated and fed into a multiple layer PCA to get the final feature. Experimental results on American sign language (ASL) dataset demonstrate that the proposed feature learning model is significantly effective, which improves the recognition rate from 75% to 99.05% and outperforms the state-of-the-art. (C) 2014 Elsevier B.V. All rights reserved.
C1 [Li, Shao-Zi; Yu, Bin; Wu, Wei; Su, Song-Zhi; Ji, Rong-Rong] Xiamen Univ, Sch Informat Sci & Technol, Xiamen 361005, Peoples R China.
   [Li, Shao-Zi; Yu, Bin; Wu, Wei; Su, Song-Zhi; Ji, Rong-Rong] Xiamen Univ, Fujian Key Lab Brain Like Intelligent Syst, Xiamen 361005, Peoples R China.
   [Yu, Bin] GuiZhou Normal Univ, Inst Math & Comp Sci, Beijing 550001, Guizhou, Peoples R China.
RP Ji, RR (reprint author), Xiamen Univ, Sch Informat Sci & Technol, Xiamen 361005, Peoples R China.
EM rrji@xmu.edu.cn
FU Nature Science Foundation of ChinaNational Natural Science Foundation of
   China [61422210, 61373076, 61202143]; Fundamental Research Funds for the
   Central UniversitiesFundamental Research Funds for the Central
   Universities [2013121026, 2011121052]; 985 Project of Xiamen
   UniversityXiamen University; Natural Science Foundation of Fujian
   ProvinceNatural Science Foundation of Fujian Province [2013J05100,
   2010J01345, 2011J01367]; Key Projects Fund of Science and Technology in
   Xiamen [3502Z20123017]; Research Fund for the Doctoral Program of Higher
   Education of ChinaResearch Fund for the Doctoral Program of Higher
   Education of China (RFDP)Specialized Research Fund for the Doctoral
   Program of Higher Education (SRFDP) [201101211120024]; Special Fund for
   Developing Shenzhen's Strategic Emerging Industries
   [JCYJ20120614164600201]; Hunan Provincial Natural Science
   FoundationNatural Science Foundation of Hunan Province [12JJ2040]; Hunan
   Province Research Foundation of Education Committee [09A046]
FX This work is supported by the Nature Science Foundation of China (Nos.
   61422210, 61373076, 61202143), the Fundamental Research Funds for the
   Central Universities (Nos. 2013121026, 2011121052), the 985 Project of
   Xiamen University, the Natural Science Foundation of Fujian Province
   (Nos. 2013J05100, 2010J01345 and 2011J01367), the Key Projects Fund of
   Science and Technology in Xiamen (no. 3502Z20123017), the Research Fund
   for the Doctoral Program of Higher Education of China (no.
   201101211120024), the Special Fund for Developing Shenzhen's Strategic
   Emerging Industries (no. JCYJ20120614164600201), the Hunan Provincial
   Natural Science Foundation (12JJ2040), and the Hunan Province Research
   Foundation of Education Committee (09A046).
CR Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1016/j.cviu.2007.09.014
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bo LF, 2011, IEEE INT C INT ROBOT, P821, DOI 10.1109/IROS.2011.6048717
   Dalal N, 2005, PROC CVPR IEEE, P886
   Estrela B.N., 2013, WVC 9 WORKSH VIS COM
   Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231
   Gao Y, 2014, IEEE T IND ELECTRON, V61, P2088, DOI 10.1109/TIE.2013.2262760
   Gao Y, 2012, IEEE T IMAGE PROCESS, V21, P4290, DOI 10.1109/TIP.2012.2199502
   Gao Y, 2012, IEEE T IMAGE PROCESS, V21, P2269, DOI 10.1109/TIP.2011.2170081
   Gao Y, 2011, IEEE T MULTIMEDIA, V13, P1007, DOI 10.1109/TMM.2011.2160619
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Isaacs J, 2004, SOUTHEAST SYMP SYSTE, P132
   Ji R., 2009, P 17 ACM INT C MULT, P105
   Ji RR, 2013, IEEE T MULTIMEDIA, V15, P153, DOI 10.1109/TMM.2012.2225035
   Ji RR, 2012, IEEE T IMAGE PROCESS, V21, P2282, DOI 10.1109/TIP.2011.2176950
   Ji RR, 2012, INT J COMPUT VISION, V96, P290, DOI 10.1007/s11263-011-0472-9
   Ji RR, 2010, PROC CVPR IEEE, P918, DOI 10.1109/CVPR.2010.5540118
   Keskin C., 2013, CONSUMER DEPTH CAMER, P119, DOI DOI 10.1007/978-1-4471-4640-7_7
   Keskin C., 2012, IEEE COMP SOC C COMP, P31
   Kim Y, 2013, INT CONF ACOUST SPEE, P3687, DOI 10.1109/ICASSP.2013.6638346
   Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI DOI 10.1145/3065386
   Le Q. V., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3361, DOI 10.1109/CVPR.2011.5995496
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Masci J, 2011, LECT NOTES COMPUT SC, V6791, P52, DOI 10.1007/978-3-642-21735-7_7
   Mitchell B, 2012, 2012 11TH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA 2012), VOL 1, P162, DOI 10.1109/ICMLA.2012.34
   Ngiam J, 2011, IEEE INT C MACH LEAR, V28, P689, DOI DOI 10.1145/2647868
   Otiniano Rodriguez K., 2013, 2013 XXVI Conference on Graphics, Patterns and Images (SIBGRAPI 2013), P1, DOI 10.1109/SIBGRAPI.2013.10
   Pugeault N., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P1114, DOI 10.1109/ICCVW.2011.6130290
   Sermanet P, 2013, PROC CVPR IEEE, P3626, DOI 10.1109/CVPR.2013.465
   Shotton J, 2013, COMMUN ACM, V56, P116, DOI 10.1145/2398356.2398381
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470
   Socher R., 2012, ADV NEURAL INFORM PR, V3, P665, DOI DOI 10.1002/2014GB005021
   Srivastava N., 2012, ADV NEURAL INFORM PR, V25, P2231
   Tang B, 2013, IEEE SYS MAN CYBERN, P1, DOI 10.1109/SMC.2013.8
   Van den Bergh M., 2011, P IEEE WORKSH APPL C, P66, DOI DOI 10.1109/WACV.2011.5711485
   Yuan Z., 2013, P 21 ACM INT C MULT, P253
NR 37
TC 42
Z9 47
U1 2
U2 116
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0925-2312
EI 1872-8286
J9 NEUROCOMPUTING
JI Neurocomputing
PD MAR 5
PY 2015
VL 151
BP 565
EP 573
DI 10.1016/j.neucom.2014.06.086
PN 2
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA AY7QF
UT WOS:000347753500005
DA 2020-02-19
ER

PT J
AU Menchon-Lara, RM
   Sancho-Gomez, JL
AF Menchon-Lara, Rosa-Maria
   Sancho-Gomez, Jose-Luis
TI Fully automatic segmentation of ultrasound common carotid artery images
   based on machine learning
SO NEUROCOMPUTING
LA English
DT Article
DE Pattern recognition; Auto-encoders; Deep learning; Atherosclerosis;
   Ultrasound imaging; Intima-media thickness
ID INTIMA-MEDIA THICKNESS; B-MODE ULTRASOUND; MEASUREMENT SYSTEM; HOUGH
   TRANSFORM; WALL; ALGORITHM; SNAKES
AB Atherosclerosis is responsible for a large proportion of cardiovascular diseases (CVD), which are the leading cause of death in the world. The atherosclerotic process is a complex degenerative condition mainly affecting the medium- and large-size arteries, which begins in childhood and may remain unnoticed during decades. It causes thickening and the reduction of elasticity in the blood vessels. An early diagnosis of this condition is crucial to prevent patients from suffering more serious pathologies (heart attacks and strokes). The evaluation of the Intima-Media Thickness (IMT) of the Common Carotid Artery (CCA) in B-mode ultrasound images is considered the most useful tool for the investigation of preclinical atherosclerosis. Usually, it is manually measured by the radiologists. This paper proposes a fully automatic segmentation technique based on Machine Learning and Statistical Pattern Recognition to measure IMT from ultrasound CCA images. The pixels are classified by means of artificial neural networks to identify the IMT boundaries. Moreover, the concepts of Auto-Encoders (AE) and Deep Learning have been included in the classification strategy. The suggested approach is tested on a set of 55 longitudinal ultrasound images of the CCA by comparing the automatic segmentation with four manual tracings. (C) 2014 Elsevier B.V. All rights reserved.
C1 [Menchon-Lara, Rosa-Maria; Sancho-Gomez, Jose-Luis] Univ Politecn Cartagena, Dept Tecnol Informac & Comunicac, Murcia 30202, Spain.
RP Menchon-Lara, RM (reprint author), Univ Politecn Cartagena, Dept Tecnol Informac & Comunicac, Plaza Hosp 1, Murcia 30202, Spain.
EM rmml@alu.upct.es
RI Sancho-Gomez, Jose Luis/A-2293-2015
OI Sancho-Gomez, Jose Luis/0000-0001-8009-1616; Menchon-Lara, Rosa
   Maria/0000-0002-1543-6670
CR Bots ML, 2003, STROKE, V34, P2985, DOI 10.1161/01.STR.0000102044.27905.B5
   CECCARELLI M, 2006, P IEEE INT C AC SPEE, V2, P709
   Chan RC, 2000, COMPUT CARDIOL, V27, P37, DOI 10.1109/CIC.2000.898449
   Cheng DC, 2008, IEEE T INF TECHNOL B, V12, P792, DOI 10.1109/TITB.2008.926413
   Cheng DC, 2002, COMPUT METH PROG BIO, V67, P27, DOI 10.1016/S0169-2607(00)00149-8
   Bastida-Jumilla MC, 2013, J DIGIT IMAGING, V26, P129, DOI 10.1007/s10278-012-9481-7
   Delsanto S, 2007, IEEE T INSTRUM MEAS, V56, P1265, DOI 10.1109/TIM.2007.900433
   Destrempes F, 2009, IEEE T MED IMAGING, V28, P215, DOI 10.1109/TMI.2008.929098
   Faita F, 2008, J ULTRAS MED, V27, P1353, DOI 10.7863/jum.2008.27.9.1353
   Golemati S, 2007, ULTRASOUND MED BIOL, V33, P1918, DOI 10.1016/j.ultrasmedbio.2007.05.021
   Gonzalez J., RADIOLOGY, V247
   Gonzalez R C, 2004, DIGITAL IMAGE PROCES
   GUSTAVSSON T, 1994, P IEEE COMP CARD, P297
   Gutierrez MA, 2002, COMPUT CARDIOL, V29, P359, DOI 10.1109/CIC.2002.1166783
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Ilea DE, 2009, IEEE ENG MED BIO, P515, DOI 10.1109/IEMBS.2009.5333773
   Kasun LLC, 2013, IEEE INTELL SYST, V28, P31
   Lee YB, 2010, COMPUT BIOL MED, V40, P687, DOI 10.1016/j.compbiomed.2010.03.010
   Liang Q, 2000, IEEE T MED IMAGING, V19, P127, DOI 10.1109/42.836372
   Liguori C, 2001, IEEE T INSTRUM MEAS, V50, P1684, DOI 10.1109/19.982968
   Liu G., 2008, P 4 INT C WIR COMM N, P1
   Loizou CP, 2007, MED BIOL ENG COMPUT, V45, P35, DOI 10.1007/s11517-006-0140-3
   Menchon-Lara RM, 2014, MED BIOL ENG COMPUT, V52, P169, DOI 10.1007/s11517-013-1128-4
   Molinari F, 2012, ULTRASONICS, V52, P949, DOI 10.1016/j.ultras.2012.03.005
   Molinari F, 2010, COMPUT METH PROG BIO, V100, P201, DOI 10.1016/j.cmpb.2010.04.007
   Molinari F, 2010, IEEE T ULTRASON FERR, V57, P1112, DOI 10.1109/TUFFC.2010.1522
   MOLLER MF, 1993, NEURAL NETWORKS, V6, P525, DOI 10.1016/S0893-6080(05)80056-5
   Petroudi S, 2012, IEEE T BIO-MED ENG, V59, P3060, DOI 10.1109/TBME.2012.2214387
   Pham DL, 2000, ANNU REV BIOMED ENG, V2, P315, DOI 10.1146/annurev.bioeng.2.1.315
   Rocha R, 2010, IMAGE VISION COMPUT, V28, P614, DOI 10.1016/j.imavis.2009.09.017
   Santhiyakumari N, 2008, SIGNAL IMAGE VIDEO P, V2, P183, DOI 10.1007/s11760-007-0048-x
   Selzer RH, 2001, ATHEROSCLEROSIS, V154, P185, DOI 10.1016/S0021-9150(00)00461-5
   Stein JH, 2005, J AM SOC ECHOCARDIOG, V18, P244, DOI 10.1016/j.echo.2004.12.002
   Touboul PJ, 2012, CEREBROVASC DIS, V34, P290, DOI 10.1159/000343145
   W.H. Organization, GLOB ATL CARD DIS PR
   Wendelhag I, 1997, STROKE, V28, P2195, DOI 10.1161/01.STR.28.11.2195
   Xu XY, 2012, COMPUT MED IMAG GRAP, V36, P248, DOI 10.1016/j.compmedimag.2011.06.007
NR 37
TC 20
Z9 22
U1 6
U2 78
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0925-2312
EI 1872-8286
J9 NEUROCOMPUTING
JI Neurocomputing
PD MAR 3
PY 2015
VL 151
BP 161
EP 167
DI 10.1016/j.neucom.2014.09.066
PN 1
PG 7
WC Computer Science, Artificial Intelligence
SC Computer Science
GA AY7QE
UT WOS:000347753400019
DA 2020-02-19
ER

PT J
AU Liu, H
   Ma, BP
   Qin, L
   Pang, JB
   Zhang, CJ
   Huang, QM
AF Liu, Hao
   Ma, Bingpeng
   Qin, Lei
   Pang, Junbiao
   Zhang, Chunjie
   Huang, Qingming
TI Set-label modeling and deep metric learning on person re-identification
SO NEUROCOMPUTING
LA English
DT Article
DE Person re-identification; Mutual-information; Metric learning; Deep
   learning; Neighborhood component analysis
AB Person re-identification aims at matching individuals across multiple non-overlapping adjacent cameras. By condensing multiple gallery images of a person as a whole, we propose a novel method named Set-Label Model (SLM) to improve the performance of person re-identification under the multi-shot setting. Moreover, we utilize mutual-information to measure the relevance between query image and gallery sets. To decrease the computational complexity, we apply a Naive-Bayes Nearest-Neighbor algorithm to approximate the mutual-information value. To overcome the limitations of traditional linear metric learning, we further develop a deep non-linear metric learning (DeepML) approach based on Neighborhood Component Analysis and Deep Belief Network To evaluate the effectiveness of our proposed approaches, SLM and DeepML, we have carried out extensive experiments on two challenging datasets LIDS and ETHZ. The experimental results demonstrate that the proposed methods can obtain better performances compared with the state-of-the-art methods. (C) 2014 Elsevier B.V. All rights reserved.
C1 [Liu, Hao; Ma, Bingpeng; Zhang, Chunjie; Huang, Qingming] Univ Chinese Acad Sci, Sch Comp & Control Engn, Beijing 100190, Peoples R China.
   [Qin, Lei; Huang, Qingming] Chinese Acad Sci, Key Lab Intelligent Informat Proc, Inst Comp Technol, Beijing 100190, Peoples R China.
   [Pang, Junbiao] Beijing Univ Technol, Coll Metropolitan Transportat, Beijing Key Lab Multimedia & Intelligent Software, Beijing 100124, Peoples R China.
RP Qin, L (reprint author), Chinese Acad Sci, Key Lab Intelligent Informat Proc, Inst Comp Technol, Beijing 100190, Peoples R China.
EM h-liu14@mails.tsinghua.edu.cn; bpma@ucas.ac.cn; qinlei@ict.ac.cn;
   jbpang@jdl.ac.cn; zhangcj@ucas.ac.cn; qmhuang@ict.ac.cn
RI Liu, Hao/AAE-2455-2020; zhang, chunjie/Z-3035-2019
OI Liu, Hao/0000-0003-0954-5405; zhang, chunjie/0000-0002-1161-8995
FU National Basic Research Program of China (973 Program)National Basic
   Research Program of China [2012CB316400]; National Natural Science
   Foundation of ChinaNational Natural Science Foundation of China
   [61025011, 61133003, 61332016, 61390510, 61303154]
FX This work was supported in part by National Basic Research Program of
   China (973 Program): 2012CB316400, in part by National Natural Science
   Foundation of China: 61025011, 61133003, 61332016, 61390510, 61303154.
CR Avraham T, 2012, LECT NOTES COMPUT SC, V7583, P381, DOI 10.1007/978-3-642-33863-2_38
   Bazzani L., 2010, INT C PATT REC
   Boiman O., 2008, IEEE C COMP VIS PATT
   Davis J., 2007, INT C MACH LEARN
   Donahue J., 2014, INT C MACH LEARN
   Farenzena M, 2010, IEEE C COMP VIS PATT
   Girshick R., 2014, IEEE C COMP VIS PATT
   Globerson A, 2005, ADV NEURAL INFORM PR
   Gray D., 2008, EUR C COMP VIS
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hirzer M., 2012, EUR C COMP VIS
   Khan S.H., 2014, IEEE C COMP VIS PATT
   Kostinger M., 2012, IEEE C COMP VIS PATT
   Layne R, 2012, BRIT MACH VIS C
   Liu Y., 2014, IEEE INT C IM PROC
   Ma BP, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.57
   Ma BP, 2012, LECT NOTES COMPUT SC, V7583, P413, DOI 10.1007/978-3-642-33863-2_41
   Oquab M., 2014, IEEE C COMP VIS PATT
   Prosser B. J., 2010, BRIT MACH VIS C
   Qin L., 2005, IEEE INT C IM PROC, V3, P77
   Razavian A. Sharif, 2014, IEEE C COMP VIS PATT
   Salakhutdinov R, 2009, INT C ART INT STAT
   Salakhutdinov R., 2010, INT C MACH LEARN
   Salakhutdinov R., 2004, INT C ART INT STAT
   Schwartz W., 2009, COMPUTER GRAPHICS IM
   Sermanet P, 2014, INT C LEARN REPR
   Weinberger Kilian Q, 2006, ADV NEURAL INFORM PR, P1473, DOI DOI 10.1007/978-3-319-13168-9_
   Xing E., 2002, ADV NEURAL INFORM PR
   Yuan J., 2009, IEEE C COMP VIS PATT
   Zheng W., 2012, IEEE C COMP VIS PATT
   Zheng W. S., 2011, IEEE C COMP VIS PATT
   Zheng WS, 2013, IEEE T PATTERN ANAL, V35, P653, DOI 10.1109/TPAMI.2012.138
NR 33
TC 18
Z9 19
U1 1
U2 37
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0925-2312
EI 1872-8286
J9 NEUROCOMPUTING
JI Neurocomputing
PD MAR 3
PY 2015
VL 151
BP 1283
EP 1292
DI 10.1016/j.neucom.2014.11.002
PN 3
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA AY7QG
UT WOS:000347753600034
DA 2020-02-19
ER

PT J
AU Gunawan, AAS
   Jatmiko, W
AF Gunawan, Alexander A. S.
   Jatmiko, Wisnu
TI GEOMETRIC DEEP PARTICLE FILTER FOR MOTORCYCLE TRACKING: DEVELOPMENT OF
   INTELLIGENT TRAFFIC SYSTEM IN JAKARTA
SO INTERNATIONAL JOURNAL ON SMART SENSING AND INTELLIGENT SYSTEMS
LA English
DT Article
DE visual tracking; motorcycle; nonretinotopic; particle filter; deep
   learning; geometric computing; affine transformation
ID EXTREME LEARNING-MACHINE; OBJECT
AB Intelligent Transportation Systems (ITS) is the combination of transportation systems with Information and Communication Technology (ICT). In Jakarta traffic, there is unique issue that does not arise in developed countries: very large number of motorcycles. Nevertheless, the enabling technologies for the detection, measurement, recording, and information distribution of motorcycle have not been fully developed in the existing researches. With the above considerations, we establish research which aimed to develop enabling technology especially in here for tracking motorcycle using camera.
   This paper is presented our proposed tracker which called as Geometric Deep Particle Filter (GDPF) for tracking motorcycle using camera. The tracker is inspired by human visual perception which has nonretinotopic nature. Based on particle filter approach, our goal is to improve the transition model in order to overcome motorcycle maneuver. We will exploit this curved nature of the state space using geometric computing theory, such as Lie groups, and Lie algebras. A number of experiments have been conducted for this research, and it has been found that GDPF has achieved certain degree of success in object tracking.
C1 [Gunawan, Alexander A. S.] Bina Nusantara Univ, Sch Comp Sci, Dept Math, Jakarta, Indonesia.
   [Jatmiko, Wisnu] Univ Indonesia, Fac Comp Sci, Depok, Indonesia.
RP Gunawan, AAS (reprint author), Bina Nusantara Univ, Sch Comp Sci, Dept Math, Jakarta, Indonesia.
EM aagung@binus.edu; wisnuj@cs.ui.ac.id
RI Gunawan, Alexander A S/AAC-2557-2019
OI Gunawan, Alexander A S/0000-0002-1097-5173; Jatmiko,
   Wisnu/0000-0002-0530-7955
CR Ambardekar Amol, 2013, J ELECTRON IMAGING, V22, P1
   Askenasy Jean, 2013, FRONT PSYCHOL, V4, P1
   Bar-Shalom Y., 2001, ESTIMATION APPL TRAC
   Challa S., 2011, FUNDAMENTALS OBJECT
   Chen K., 1999, ITS HDB 2000
   Cheng Y, 2007, IEEE T AERO ELEC SYS, V43, P1454, DOI 10.1109/TAES.2007.4441751
   Chiu Chung-Cheng, 2007, WIAMIS 07, P32
   Chiverton J, 2012, IET INTELL TRANSP SY, V6, P259, DOI 10.1049/iet-its.2011.0138
   Dorst Leo, 2007, GEOMETRIC ALGEBRA CO
   Duan Bobo, 2009, IND TECHN 2009 ICIT, P1
   Engelbrecht AP., 2007, COMPUTATIONAL INTELL
   Febriani, 2010, TEMPO INTERACTIVE
   GILMORE R, 2008, [No title captured]
   Gunawan Alexander A. S., 2014, Journal of Theoretical and Applied Information Technology, V63, P104
   Gunawan Fergyanto E., 2013, INT C ADV SCI CONT E, V68
   Hardjono B, 2013, INT J SMART SENS INT, V6, P1830
   Haug A. J., 2012, BAYESIAN ESTIMATION
   Herzog MH, 2012, FRONT PSYCHOL, V3, DOI 10.3389/fpsyg.2012.00119
   Huang GB, 2006, NEUROCOMPUTING, V70, P489, DOI 10.1016/j.neucom.2005.12.126
   Huang GB, 2012, IEEE T SYST MAN CY B, V42, P513, DOI 10.1109/TSMCB.2011.2168604
   Huval Brody, 2013, CORR
   Hyunggi Cho, 2011, IEEE International Conference on Robotics and Automation, P4391
   Isard M, 1998, INT J COMPUT VISION, V29, P5, DOI 10.1023/A:1008078328650
   Kanhere NK, 2010, TRANSPORT RES REC, P69, DOI 10.3141/2160-08
   Kim YS, 2004, INT J CONTROL AUTOM, V2, P310
   Klein D. A., 2010, BOBOT BONN BENCHMARK
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kwon J, 2007, ROBOTICA, V25, P725, DOI 10.1017/S0263574707003529
   Kwon J, 2014, IEEE T PATTERN ANAL, V36, P625, DOI 10.1109/TPAMI.2013.170
   Kwon J, 2009, PROC CVPR IEEE, P991, DOI 10.1109/CVPRW.2009.5206501
   Li XR, 2005, IEEE T AERO ELEC SYS, V41, P1255, DOI 10.1109/TAES.2005.1561886
   Liu J, 2001, STAT ENG IN, P197
   Marcenaro L., 2013, EFFECTIVE SURVEILLAN, P489
   Min-Yu Ku, 2008, WSEAS Transactions on Electronics, V5, P121
   Morichi S., 2005, J E ASIA SOC TRANSPO, V6, P1
   Mukhtar A, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON CONTROL SYSTEM, COMPUTING AND ENGINEERING (ICCSCE 2013), P63, DOI 10.1109/ICCSCE.2013.6719933
   Nguyen PV, 2008, LECT NOTES ARTIF INT, V5351, P819, DOI 10.1007/978-3-540-89197-0_76
   Nurhadiyatna A, 2012, 2012 INTERNATIONAL CONFERENCE ON ADVANCED COMPUTER SCIENCE AND INFORMATION SYSTEMS, P179
   Ogmen H, 2010, P IEEE, V98, P479, DOI 10.1109/JPROC.2009.2039028
   Palm Rasmus Berg, 2012, INFORM MATH MODELLIN
   Plomp G, 2009, NEUROIMAGE, V48, P405, DOI 10.1016/j.neuroimage.2009.06.031
   Ristic B., 2004, KALMAN FILTER PARTIC
   Rosenhahn B, 2000, LECT NOTES COMPUT SC, V1888, P284
   Rosenhahn B., 2001, Robot Vision. International Workshop RobVis 2001. Proceedings (Lecture Notes in Computer Science Vol.1998), P9
   ROSENHAHN B, 2003, THESIS CHRISTIAN ALB
   Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7
   Silva Romuere, 2013, CLEI ELECT J, V16, P1
   Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128
   WANG N B, 2013, P 22 INT C COMP COMM, P1
   Wang XF, 2011, PROG ELECTROMAGN RES, V118, P1, DOI 10.2528/PIER11051907
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Yilmaz A, 2006, ACM COMPUT SURV, V38, DOI 10.1145/1177352.1177355
NR 52
TC 5
Z9 5
U1 0
U2 2
PU INT JOURNAL SMART SENSING & INTELLIGENT SYSTEMS
PI PALMERSTON
PA INT JOURNAL SMART SENSING & INTELLIGENT SYSTEMS, PALMERSTON, 00000, NEW
   ZEALAND
SN 1178-5608
J9 INT J SMART SENS INT
JI Int. J. Smart Sens. Intell. Syst.
PD MAR
PY 2015
VL 8
IS 1
BP 429
EP 463
PG 35
WC Engineering, Electrical & Electronic
SC Engineering
GA CR3FT
UT WOS:000361217000021
OA DOAJ Gold
DA 2020-02-19
ER

PT J
AU Rashwan, MAA
   Al Sallab, AA
   Raafat, HM
   Rafea, A
AF Rashwan, Mohsen A. A.
   Al Sallab, Ahmad A.
   Raafat, Hazem M.
   Rafea, Ahmed
TI Deep Learning Framework with Confused Sub-Set Resolution Architecture
   for Automatic Arabic Diacritization
SO IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING
LA English
DT Article
DE Arabic diacritization; classifier design; deep networks; part-of-speech
   (PoS) tagging
AB The Arabic language belongs to a group of languages that require diacritization over their characters. Modern Standard Arabic (MSA) transcripts omit the diacritics, which are essential for many machine learning tasks like Text-To-Speech (TTS) systems. In this work Arabic diacritics restoration is tackled under a deep learning framework that includes the Confused Sub-set Resolution (CSR) method to improve the classification accuracy, in addition to an Arabic Part-of-Speech (PoS) tagging framework using deep neural nets. Special focus is given to syntactic diacritization, which still suffers low accuracy as indicated in prior works. Evaluation is done versus state-of-the-art systems reported in literature, with quite challenging datasets collected from different domains. Standard datasets like the LDC Arabic Tree Bank are used in addition to custom ones we have made available online to allow other researchers to replicate these results. Results show significant improvement of the proposed techniques over other approaches, reducing the syntactic classification error to 9.9% and morphological classification error to 3% compared to 12.7% and 3.8% of the best reported results in literature, improving the error by 22% over the best reported systems.
C1 [Rashwan, Mohsen A. A.] Engn Co Dev Comp Syst RDI, Giza 12613, Egypt.
   [Rashwan, Mohsen A. A.] Cairo Univ, Fac Engn, Dept Elect & Elect Commun, Giza 00202, Egypt.
   [Al Sallab, Ahmad A.] Valeo Interbranch Automot Software, Giza, Egypt.
   [Al Sallab, Ahmad A.] Cairo Univ, Fac Engn, Dept Elect & Elect Commun, Giza 12613, Egypt.
   [Raafat, Hazem M.] Kuwait Univ, Dept Comp Sci, Safat 13060, Kuwait.
   [Rafea, Ahmed] Amer Univ Cairo, Dept Comp Sci, Cairo 11835, Egypt.
RP Rashwan, MAA (reprint author), Engn Co Dev Comp Syst RDI, Giza 12613, Egypt.
EM mohsen_rashwan@rdi-eg.com; ahmad.el-sallab@valeo.com;
   hazem@cs.ku.edu.kw; rafea@aucegypt.edu
RI Rafea, Ahmed/AAE-3967-2020; rashwan, mohsen/AAA-5747-2020; Raafat,
   Hazem/J-9205-2017
OI Rafea, Ahmed/0000-0001-8109-1845; Rashwan, mohsen/0000-0003-3712-5408;
   Raafat, Hazem/0000-0001-7356-5078
CR Habash N., 2007, P 8 M N AM CHAP ASS
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Le HS, 2011, INT CONF ACOUST SPEE, P5524
   Minh-Thang L., 2013, P C COMP NAT LANG LE
   Raafat H., 1993, Proceedings of the Second International Conference on Document Analysis and Recognition (Cat. No.93TH0578-5), P939, DOI 10.1109/ICDAR.1993.395582
   Rashwan M. A. A., 2009, P INT C NAT LANG PRO, P1
   Rashwan MAA, 2011, IEEE T AUDIO SPEECH, V19, P166, DOI 10.1109/TASL.2010.2045240
   Salakhutdinov R., 2009, THESIS U TORONTO TOR
   Zitouni I., 2006, P 21 INT C COMP LING
NR 9
TC 10
Z9 10
U1 1
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 2329-9290
J9 IEEE-ACM T AUDIO SPE
JI IEEE-ACM Trans. Audio Speech Lang.
PD MAR
PY 2015
VL 23
IS 3
BP 505
EP 516
DI 10.1109/TASLP.2015.2395255
PG 12
WC Acoustics; Engineering, Electrical & Electronic
SC Acoustics; Engineering
GA CD2AO
UT WOS:000350876100009
DA 2020-02-19
ER

PT J
AU Nakashika, T
   Takiguchi, T
   Ariki, Y
AF Nakashika, Toru
   Takiguchi, Tetsuya
   Ariki, Yasuo
TI Voice Conversion Using RNN Pre-Trained by Recurrent Temporal Restricted
   Boltzmann Machines
SO IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING
LA English
DT Article
DE Deep Learning; recurrent neural network; recurrent temporal restricted
   Boltzmann machine (RTRBM); speaker specific features; voice conversion
ID SPEECH RECOGNITION
AB This paper presents a voice conversion (VC) method that utilizes the recently proposed probabilistic models called recurrent temporal restricted Boltzmann machines (RTRBMs). One RTRBM is used for each speaker, with the goal of capturing high-order temporal dependencies in an acoustic sequence. Our algorithm starts from the separate training of one RTRBM for a source speaker and another for a target speaker using speaker-dependent training data. Because each RTRBM attempts to discover abstractions to maximally express the training data at each time step, as well as the temporal dependencies in the training data, we expect that the models represent the linguistic-related latent features in high-order spaces. In our approach, we convert (match) features of emphasis for the source speaker to those of the target speaker using a neural network (NN), so that the entire network (consisting of the two RTRBMs and the NN) acts as a deep recurrent NN and can be fine-tuned. Using VC experiments, we confirm the high performance of our method, especially in terms of objective criteria, relative to conventional VC methods such as approaches based on Gaussian mixture models and on NNs.
C1 [Nakashika, Toru] Kobe Univ, Grad Sch Syst Informat, Kobe, Hyogo 6578501, Japan.
   [Takiguchi, Tetsuya; Ariki, Yasuo] Kobe Univ, Org Adv Sci & Technol, Kobe, Hyogo 6578501, Japan.
RP Nakashika, T (reprint author), Kobe Univ, Grad Sch Syst Informat, Kobe, Hyogo 6578501, Japan.
EM nakashika@me.cs.scitec.kobe-u.ac.jp
CR Abe M., 1988, ICASSP 88: 1988 International Conference on Acoustics, Speech, and Signal Processing (Cat. No.88CH2561-9), P655, DOI 10.1109/ICASSP.1988.196671
   Boulanger-Lewandowski N., 2012, P INT C MACH LEARN
   Cho K, 2011, LECT NOTES COMPUT SC, V6791, P10, DOI 10.1007/978-3-642-21735-7_2
   Deng L, 2001, INT CONF ACOUST SPEE, P301, DOI 10.1109/ICASSP.2001.940827
   Desai S, 2009, INT CONF ACOUST SPEE, P3893, DOI 10.1109/ICASSP.2009.4960478
   Deselaers T., 2009, P 4 WORKSH STAT MACH, P233
   Freund Yoav, 1994, UNSUPERVISED LEARNIN
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Jian ZH, 2007, SNPD 2007: EIGHTH ACIS INTERNATIONAL CONFERENCE ON SOFTWARE ENGINEERING, ARTIFICIAL INTELLIGENCE, NETWORKING, AND PARALLEL/DISTRIBUTED COMPUTING, VOL 1, PROCEEDINGS, P210
   Kain A, 1998, INT CONF ACOUST SPEE, P285, DOI 10.1109/ICASSP.1998.674423
   Kawahara H, 2008, INT CONF ACOUST SPEE, P3933, DOI 10.1109/ICASSP.2008.4518514
   Krizhevsky A, 2009, LEARNING MULTIPLE LA
   Kunikoshi A., 2009, P INTERSPEECH, P308
   KUREMATSU A, 1990, SPEECH COMMUN, V9, P357, DOI 10.1016/0167-6393(90)90011-W
   Lee CH, 2006, INTERSPEECH 2006 AND 9TH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE PROCESSING, VOLS 1-5, P2254
   Ling Z.-H., 2006, P BLIZZ CHALL WORKSH
   Ling ZH, 2012, IEEE T AUDIO SPEECH, V20, P1492, DOI 10.1109/TASL.2011.2182511
   Ling ZH, 2013, IEEE T AUDIO SPEECH, V21, P2129, DOI 10.1109/TASL.2013.2269291
   LingHui  C., 2013, P INTERSPEECH, P3052
   McDermott E, 2007, IEEE T AUDIO SPEECH, V15, P203, DOI 10.1109/TASL.2006.876778
   Milner Ben, 2002, P INTERSPEECH, P2421
   Mohamed AR, 2012, IEEE T AUDIO SPEECH, V20, P14, DOI 10.1109/TASL.2011.2109382
   Nair V, 2009, ADV NEURAL INF PROCE, V22, P1339
   Nakamura K, 2012, SPEECH COMMUN, V54, P134, DOI 10.1016/j.specom.2011.07.007
   Nakashika T., 2014, P IEEE INT C AC SPEE, P7939
   Nakashika T, 2013, INTERSPEECH, P369
   Pascanu R., 2012, ARXIV12115063
   Saito D., 2011, P INTERSPEECH, P653
   Saito D, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 3 AND 4, P1728
   Scholkopf B., 2006, ADV NEURAL INFORM PR, P1345
   Smolensky P., 1986, PARALLEL DISTRIB PRO, V1
   Stylianou Y, 1998, IEEE T SPEECH AUDI P, V6, P131, DOI 10.1109/89.661472
   SUTSKEVER I, 2008, NIPS, V21, P2008
   Takashima R, 2012, IEEE W SP LANG TECH, P313, DOI 10.1109/SLT.2012.6424242
   Toda T, 2007, IEEE T AUDIO SPEECH, V15, P2222, DOI 10.1109/TASL.2007.907344
   Toda T, 2007, IEICE T INF SYST, VE90D, P816, DOI 10.1093/ietisy/e90-d.5.816
   Toda T, 2006, INTERSPEECH 2006 AND 9TH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE PROCESSING, VOLS 1-5, P2446
   VALBRET H, 1992, SPEECH COMMUN, V11, P175, DOI 10.1016/0167-6393(92)90012-V
   Veaux C., 2011, P INTERSPEECH, P2765
   Wu YJ, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL I, PROCEEDINGS, P629
   Wu Z., 2013, P 8 ISCA SPEECH SYNT
   Zhizheng Wu, 2013, 2013 IEEE China Summit and International Conference on Signal and Information Processing (ChinaSIP), P104, DOI 10.1109/ChinaSIP.2013.6625307
NR 42
TC 32
Z9 33
U1 0
U2 23
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 2329-9290
J9 IEEE-ACM T AUDIO SPE
JI IEEE-ACM Trans. Audio Speech Lang.
PD MAR
PY 2015
VL 23
IS 3
BP 580
EP 587
DI 10.1109/TASLP.2014.2379589
PG 8
WC Acoustics; Engineering, Electrical & Electronic
SC Acoustics; Engineering
GA CD2AO
UT WOS:000350876100015
OA Bronze
DA 2020-02-19
ER

PT J
AU Kim, IJ
   Xie, XH
AF Kim, In-Jung
   Xie, Xiaohui
TI Handwritten Hangul recognition using deep convolutional neural networks
SO INTERNATIONAL JOURNAL ON DOCUMENT ANALYSIS AND RECOGNITION
LA English
DT Article
DE Handwritten Hangul recognition; Character recognition; Deep
   convolutional neural network; Deep learning; Gradient-based learning
ID ALGORITHM
AB In spite of the advances in recognition technology, handwritten Hangul recognition (HHR) remains largely unsolved due to the presence of many confusing characters and excessive cursiveness in Hangul handwritings. Even the best existing recognizers do not lead to satisfactory performance for practical applications and have much lower performance than those developed for Chinese or alphanumeric characters. To improve the performance of HHR, here we developed a new type of recognizers based on deep neural networks (DNNs). DNN has recently shown excellent performance in many pattern recognition and machine learning problems, but have not been attempted for HHR. We built our Hangul recognizers based on deep convolutional neural networks and proposed several novel techniques to improve the performance and training speed of the networks. We systematically evaluated the performance of our recognizers on two public Hangul image databases, SERI95a and PE92. Using our framework, we achieved a recognition rate of 95.96 % on SERI95a and 92.92 % on PE92. Compared with the previous best records of 93.71 % on SERI95a and 87.70 % on PE92, our results yielded improvements of 2.25 and 5.22 %, respectively. These improvements lead to error reduction rates of 35.71 % on SERI95a and 42.44 % on PE92, relative to the previous lowest error rates. Such improvement fills a significant portion of the large gap between practical requirement and the actual performance of Hangul recognizers.
C1 [Kim, In-Jung] Handong Global Univ, Sch CSEE, Pohang 791708, Gyeongbuk, South Korea.
   [Xie, Xiaohui] Univ Calif Irvine, Sch Informat & Comp Sci, Dept Comp Sci, Irvine, CA 92697 USA.
RP Kim, IJ (reprint author), Handong Global Univ, Sch CSEE, Pohang 791708, Gyeongbuk, South Korea.
EM ijkim@handong.edu; xhx@ics.uci.edu
CR Bae H.J., 1990, P KOR INF SCI AUT C, V17, P251
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Boureau Y.-L., 2010, P INT C MACH LEARN I
   Ciresan Dan Claudiu, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P581, DOI 10.1007/978-3-642-35289-8_31
   Ciresan D. C., 2012, IEEE C COMP VIS PATT
   Ciresan Dan, 2013, IDSIA0513
   Everitt B.S., 1992, ANAL CONTINGENCY TAB
   FUKUSHIMA K, 1980, BIOL CYBERN, V36, P193, DOI 10.1007/BF00344251
   Glorot X, 2011, P 14 INT C ART INT S, V15
   GOODHILL GJ, 1994, NEURAL COMPUT, V6, P255, DOI 10.1162/neco.1994.6.2.255
   HINTON GE, 1995, SCIENCE, V268, P1158, DOI 10.1126/science.7761831
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   JANG SY, 2002, [No title captured]
   Jeong S.H., 2002, HANDWRITTEN HANGUL R
   Kang KW, 2004, IEEE T PATTERN ANAL, V26, P1185, DOI 10.1109/TPAMI.2004.74
   Kim D. H., 1993, Proceedings of the Second International Conference on Document Analysis and Recognition (Cat. No.93TH0578-5), P470, DOI 10.1109/ICDAR.1993.395693
   KIM DI, 1998, P 6 INT WORKSH FRONT, P455
   Kim HY, 2001, PATTERN RECOGN, V34, P187, DOI 10.1016/S0031-3203(99)00222-8
   Kim M.W., 1992, TECHNICAL REPORT
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Liu C.-L., 2011, ICDAR 2011 CHINESE H
   Liu H., 2005, P 8 INT C DOC AN REC
   Nair V, 2010, P 27 INT C MACH LEAR
   Ozkan C, 2003, PHOTOGRAMM ENG REM S, V69, P1225
   Park GR, 2013, INT J DOC ANAL RECOG, V16, P273, DOI 10.1007/s10032-012-0191-y
   Simard D., 2003, P INT C DOC AN REC I, P958
   Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128
   Vincent P., 2008, P 25 INT C MACH LEAR, P1096, DOI DOI 10.1145/1390156.1390294
NR 28
TC 14
Z9 14
U1 0
U2 43
PU SPRINGER HEIDELBERG
PI HEIDELBERG
PA TIERGARTENSTRASSE 17, D-69121 HEIDELBERG, GERMANY
SN 1433-2833
EI 1433-2825
J9 INT J DOC ANAL RECOG
JI Int. J. Doc. Anal. Recognit.
PD MAR
PY 2015
VL 18
IS 1
BP 1
EP 13
DI 10.1007/s10032-014-0229-4
PG 13
WC Computer Science, Artificial Intelligence
SC Computer Science
GA CC2XL
UT WOS:000350208300001
DA 2020-02-19
ER

PT J
AU Khamis, MA
   Gomaa, W
   Ahmed, WF
AF Khamis, Mohamed A.
   Gomaa, Walid
   Ahmed, Walaa F.
TI Machine learning in computational docking
SO ARTIFICIAL INTELLIGENCE IN MEDICINE
LA English
DT Article
DE Machine learning; Random forest; Support vector machine; Drug discovery;
   Computational docking; Scoring function; Virtual screening; Complex
   binding affinity; Ligands ranking accuracy; Force field interaction;
   Pharmacophore fingerprint
ID PROTEIN-LIGAND BINDING; EMPIRICAL SCORING FUNCTIONS; AFFINITY
   PREDICTION; GENETIC ALGORITHM; RANDOM FOREST; COMPLEXES; VALIDATION;
   DERIVATIVES; ACCURACY; SURFACE
AB Objective: The objective of this paper is to highlight the state-of-the-art machine learning (ML) techniques in computational docking. The use of smart computational methods in the life cycle of drug design is relatively a recent development that has gained much popularity and interest over the last few years. Central to this methodology is the notion of computational docking which is the process of predicting the best pose (orientation + conformation) of a small molecule (drug candidate) when bound to a target larger receptor molecule (protein) in order to form a stable complex molecule. In computational docking, a large number of binding poses are evaluated and ranked using a scoring function. The scoring function is a mathematical predictive model that produces a score that represents the binding free energy, and hence the stability, of the resulting complex molecule. Generally, such a function should produce a set of plausible ligands ranked according to their binding stability along with their binding poses. In more practical terms, an effective scoring function should produce promising drug candidates which can then be synthesized and physically screened using high throughput screening process. Therefore, the key to computer-aided drug design is the design of an efficient highly accurate scoring function (using ML techniques).
   Methods: The methods presented in this paper are specifically based on ML techniques. Despite many traditional techniques have been proposed, the performance was generally poor. Only in the last few years started the application of the ML technology in the design of scoring functions; and the results have been very promising. Material: The ML-based techniques are based on various molecular features extracted from the abundance of protein-ligand information in the public molecular databases, e.g., protein data bank bind (PDBbind).
   Results: In this paper, we present this paradigm shift elaborating on the main constituent elements of the ML approach to molecular docking along with the state-of-the-art research in this area. For instance, the best random forest (RF)-based scoring function 1351 on PDBbind v2007 achieves a Pearson correlation coefficient between the predicted and experimentally determined binding affinities of 0.803 while the best conventional scoring function achieves 0.644 [34]. The best RF-based ranking power [6] ranks the ligands correctly based on their experimentally determined binding affinities with accuracy 62.5% and identifies the top binding ligand with accuracy 78.1%.
   Conclusions: We conclude with open questions and potential future research directions that can be pursued in smart computational docking; using molecular features of different nature (geometrical, energy terms, pharmacophore), advanced ML techniques (e.g., deep learning), combining more than one ML models. (C) 2015 Elsevier B.V. All rights reserved.
C1 [Khamis, Mohamed A.; Gomaa, Walid; Ahmed, Walaa F.] E JUST, Cyber Phys Syst Lab, Alexandria 21934, Egypt.
   [Gomaa, Walid] Univ Alexandria, Fac Engn, Alexandria 21544, Egypt.
RP Khamis, MA (reprint author), E JUST, Cyber Phys Syst Lab, POB 179, Alexandria 21934, Egypt.
EM mohamed.khamis@ejust.edu.eg; walid.gomaa@ejust.edu.eg;
   wella_dba@yahoo.com
RI Gomaa, Walid/A-4094-2015
OI Gomaa, Walid/0000-0002-8518-8908
FU Information Technology Industry Development Agency (ITIDA) under the
   ITAC Program grant [58]; E-JUST Research Fellowship
FX This work is mainly supported by the Information Technology Industry
   Development Agency (ITIDA) under the ITAC Program grant no. CFP#58 and
   in part by E-JUST Research Fellowship.
CR ALBUS JS, 1975, ASME, V97, P220, DOI DOI 10.1115/1.3426922
   Amini A, 2007, PROTEINS, V69, P823, DOI 10.1002/prot.21782
   [Anonymous], 2000, MACCS DRUG DAT REP
   [Anonymous], 2013, MOL OP ENV MOE
   Ashtawy HM, 2012, IEEE ACM T COMPUT BI, V9, P1301, DOI 10.1109/TCBB.2012.36
   Ballester PJ, 2014, J CHEM INF MODEL, V54, P944, DOI 10.1021/ci500091r
   Ballester PJ, 2012, J R SOC INTERFACE, V9, P3196, DOI 10.1098/rsif.2012.0569
   Ballester PJ, 2010, BIOINFORMATICS, V26, P1169, DOI 10.1093/bioinformatics/btq112
   Ballester PJ, 2013, MACHINE LEARNING APP
   Berman HM, 2000, NUCLEIC ACIDS RES, V28, P235, DOI 10.1093/nar/28.1.235
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Brooks BR, 2009, J COMPUT CHEM, V30, P1545, DOI 10.1002/jcc.21287
   Cambridge Soft Corporation, 2004, CHEM CS SOFTW PACK V
   Cao Y, 2014, BIOINFORMATICS, V30, P1674, DOI 10.1093/bioinformatics/btu104
   Case DA, 2005, J COMPUT CHEM, V26, P1668, DOI 10.1002/jcc.20290
   ChemicalFileFormatWikipedia, 2014, CHEM FIL FORM
   Cheng TJ, 2009, J CHEM INF MODEL, V49, P1079, DOI 10.1021/ci9000053
   CHENG Y, 1973, BIOCHEM PHARMACOL, V22, P3099
   Cosconati S, 2010, EXPERT OPIN DRUG DIS, V5, P597, DOI 10.1517/17460441.2010.484460
   Denny WA, 1998, DESIGN DEV ANTICANCE
   Dimitriadou E, 2010, E1071 MISCELLANEOUS
   Durrant JD, 2011, J CHEM INF MODEL, V51, P2897, DOI 10.1021/ci2003889
   Eldridge MD, 1997, J COMPUT AID MOL DES, V11, P425, DOI 10.1023/A:1007996124545
   F_ScoreWikipedia, 2014, F SCOR
   Fogel G., 2010, P IEEE 2010 S COMP I, P1
   Friesner RA, 2004, J MED CHEM, V47, P1739, DOI 10.1021/jm0306430
   Gabel J, 2014, J CHEM INF MODEL, V54, P2807, DOI 10.1021/ci500406k
   Gaulton A, 2012, NUCLEIC ACIDS RES, V40, pD1100, DOI 10.1093/nar/gkr777
   Gorodetsky V, 2010, JMLR WORKSH CONF PRO, V10, P55
   Hanwell MD, 2012, J CHEMINFORMATICS, V4, DOI 10.1186/1758-2946-4-17
   Hildebrandt A, 2010, BMC BIOINFORMATICS, V11, DOI 10.1186/1471-2105-11-531
   Hongjian Li, 2012, 2012 IEEE Symposium on Computational Intelligence in Bioinformatics and Computational Biology (CIBCB), P77, DOI 10.1109/CIBCB.2012.6217214
   Hu LG, 2005, PROTEINS, V60, P333, DOI 10.1002/prot.20512
   Huang N, 2006, J MED CHEM, V49, P6789, DOI 10.1021/jm0608356
   Irwin JJ, 2012, J CHEM INF MODEL, V52, P1757, DOI 10.1021/ci3001277
   JONES G, 1995, J MOL BIOL, V245, P43, DOI 10.1016/S0022-2836(95)80037-9
   Jones G, 1997, J MOL BIOL, V267, P727, DOI 10.1006/jmbi.1996.0897
   Khamis MA, 2015, DEEP LEARNING UNPUB
   Khamis MA, 2015, COMP ASSESSMEN UNPUB
   Kinnings SL, 2011, J CHEM INF MODEL, V51, P408, DOI 10.1021/ci100369f
   Klein Christian, 2007, Pharm Unserer Zeit, V36, P450, DOI 10.1002/pauz.200700242
   Korb O, 2009, J CHEM INF MODEL, V49, P84, DOI 10.1021/ci800298z
   Kramer B, 1999, PROTEINS, V37, P228, DOI 10.1002/(SICI)1097-0134(19991101)37:2<228::AID-PROT8>3.0.CO;2-8
   Kumar A, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003318
   Li GB, 2013, J CHEM INF MODEL, V53, P592, DOI 10.1021/ci300493w
   Li HJ, 2014, BMC BIOINFORMATICS, V15, DOI 10.1186/1471-2105-15-291
   Li L, 2011, J CHEM INF MODEL, V51, P2132, DOI 10.1021/ci200078f
   Li Y, 2014, J CHEM INF MODEL, V54, P1717, DOI 10.1021/ci500081m
   Liu Q, 2013, J CHEM INF MODEL, V53, P3076, DOI 10.1021/ci400450h
   Liu TQ, 2007, NUCLEIC ACIDS RES, V35, pD198, DOI 10.1093/nar/gkl999
   lnpharmatica Ltd, 2007, STARL
   Lu SJ, 2012, INT J MOL SCI, V13, P4496, DOI 10.3390/ijms13044496
   MARSHALL GR, 1987, ANNU REV PHARMACOL, V27, P193
   Milborrow S., 2010, EARTH MULTIVARIATE A
   Mooij WTM, 2005, PROTEINS, V61, P272, DOI 10.1002/prot.20588
   Morris GM, 2009, J COMPUT CHEM, V30, P2785, DOI 10.1002/jcc.21256
   Morris GM, 1998, J COMPUT CHEM, V19, P1639, DOI 10.1002/(SICI)1096-987X(19981115)19:14<1639::AID-JCC10>3.0.CO;2-B
   Moustakas DT, 2006, J COMPUT AID MOL DES, V20, P601, DOI 10.1007/s10822-006-9060-4
   Muegge I, 2006, J MED CHEM, V49, P5895, DOI 10.1021/jm050038s
   Mysinger MM, 2012, J MED CHEM, V55, P6582, DOI 10.1021/jm300687e
   Naim M, 2007, J CHEM INF MODEL, V47, P122, DOI 10.1021/ci600406v
   Ouyang XC, 2011, J BIOINFORM COMPUT B, V9, P1, DOI 10.1142/S021972001100577X
   Peng L, 2013, J PARALLEL DISTR COM, V73, P1469, DOI 10.1016/j.jpdc.2012.07.007
   PrecisionRecallWikipedia, 2014, PREC AND REC
   Purohit R, 2014, J BIOMOL STRUCT DYN, V32, P1033, DOI 10.1080/07391102.2013.803264
   Purohit R, 2011, J BIOMOL STRUCT DYN, V29, P137, DOI 10.1080/07391102.2011.10507379
   Purohit R, 2011, J MOL MODEL, V17, P869, DOI 10.1007/s00894-010-0785-6
   Purohit R, 2009, INTERDISCIP SCI, V1, P320, DOI 10.1007/s12539-009-0043-8
   Rajendran V, 2014, J BIOMOL STRUCT DYN, V32, P209, DOI 10.1080/07391102.2012.759885
   Rajendran V, 2012, AMINO ACIDS, V43, P603, DOI 10.1007/s00726-011-1108-7
   ReceiverOperatingCharacteristicWikipedia, 2014, REC OP CHAR
   RICHARDSON JL, 1991, COMPUT PHYS COMMUN, V63, P84, DOI 10.1016/0010-4655(91)90240-L
   Ridgeway G, 2010, GBM GEN BOOSTED REGR
   Rupp B, 2007, J VIROL, V81, P5508, DOI 10.1128/JVI.02796-06
   Sanner MF, 1999, J MOL GRAPH MODEL, V17, P57
   Sato T, 2010, J CHEM INF MODEL, V50, P170, DOI 10.1021/ci900382e
   Sayes C, 2010, RISK ANAL, V30, P1723, DOI 10.1111/j.1539-6924.2010.01438.x
   Schliep K, 2010, KKNN WEIGHTED K NEAR
   Schnecke V, 2000, PERSPECT DRUG DISCOV, V20, P171, DOI 10.1023/A:1008737207775
   Schrodinger L., 2005, SCHRODINGER SOFTWARE
   Shattuck TW, 2008, COLBY COLL MOL MECH
   Thomsen R, 2006, J MED CHEM, V49, P3315, DOI 10.1021/jm051197e
   Trott O, 2010, J COMPUT CHEM, V31, P455, DOI 10.1002/jcc.21334
   Vassiliev V, 2011, COMMUNICATION
   Wang JM, 2001, J AM CHEM SOC, V123, P5221, DOI 10.1021/ja003834q
   Wang RX, 2004, J MED CHEM, V47, P2977, DOI 10.1021/jm030580l
   Wang RX, 2002, J COMPUT AID MOL DES, V16, P11, DOI 10.1023/A:1016357811882
   Wang Y, 2009, NUCL ACIDS RES, V4
   Wang Y. B., 2014, COMMUNICATION
   Zavodszky MI, 2005, PROTEIN SCI, V14, P1104, DOI 10.1110/ps.041153605
   Zavodszky MI, 2002, J COMPUT AID MOL DES, V16, P883, DOI 10.1023/A:1023866311551
   Zilian D, 2013, J CHEM INF MODEL, V53, P1923, DOI 10.1021/ci400120b
   Zsoldos Z, 2007, J MOL GRAPH MODEL, V26, P198, DOI 10.1016/j.jmgm.2006.06.002
NR 93
TC 25
Z9 27
U1 2
U2 65
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0933-3657
EI 1873-2860
J9 ARTIF INTELL MED
JI Artif. Intell. Med.
PD MAR
PY 2015
VL 63
IS 3
BP 135
EP 152
DI 10.1016/j.artmed.2015.02.002
PG 18
WC Computer Science, Artificial Intelligence; Engineering, Biomedical;
   Medical Informatics
SC Computer Science; Engineering; Medical Informatics
GA CI8NQ
UT WOS:000355029400001
PM 25724101
DA 2020-02-19
ER

PT J
AU Nakashika, T
   Takiguchi, T
   Ariki, Y
AF Nakashika, Toru
   Takiguchi, Tetsuya
   Ariki, Yasuo
TI Voice conversion using speaker-dependent conditional restricted
   Boltzmann machine
SO EURASIP JOURNAL ON AUDIO SPEECH AND MUSIC PROCESSING
LA English
DT Article
DE Voice conversion; Conditional restricted Boltzmann machine; Deep
   learning; Recurrent neural network; Speaker-specific features
ID SPEECH RECOGNITION; LEARNING ALGORITHM; TRANSFORMATION
AB This paper presents a voice conversion (VC) method that utilizes conditional restricted Boltzmann machines (CRBMs) for each speaker to obtain high-order speaker-independent spaces where voice features are converted more easily than those in an original acoustic feature space. The CRBM is expected to automatically discover common features lurking in time-series data. When we train two CRBMs for a source and target speaker independently using only speaker-dependent training data, it can be considered that each CRBM tries to construct subspaces where there are fewer phonemes and relatively more speaker individuality than the original acoustic space because the training data include various phonemes while keeping the speaker individuality unchanged. Each obtained high-order feature is then concatenated using a neural network (NN) from the source to the target. The entire network (the two CRBMs and the NN) can be also fine-tuned as a recurrent neural network (RNN) using the acoustic parallel data since both the CRBMs and the concatenating NN have network-based representation with time dependencies. Through voice-conversion experiments, we confirmed the high performance of our method especially in terms of objective evaluation, comparing it with conventional GMM, NN, RNN, and our previous work, speaker-dependent DBN approaches.
C1 [Nakashika, Toru] Kobe Univ, Grad Sch Syst Informat, Nada Ku, Kobe, Hyogo 6578501, Japan.
   [Takiguchi, Tetsuya; Ariki, Yasuo] Kobe Univ, Org Adv Sci & Technol, Nada Ku, Kobe, Hyogo 6578501, Japan.
RP Nakashika, T (reprint author), Kobe Univ, Grad Sch Syst Informat, Nada Ku, 1-1 Rokkodai, Kobe, Hyogo 6578501, Japan.
EM nakashika@me.cs.scitec.kobe-u.ac.jp
CR ACKLEY DH, 1985, COGNITIVE SCI, V9, P147
   Cho K, 2011, LECT NOTES COMPUT SC, V6791, P10, DOI 10.1007/978-3-642-21735-7_2
   Deng L, 2001, INT CONF ACOUST SPEE, P301, DOI 10.1109/ICASSP.2001.940827
   Desai S, 2009, INT CONF ACOUST SPEE, P3893, DOI 10.1109/ICASSP.2009.4960478
   Deselaers T., 2009, P 4 WORKSH STAT MACH, P233
   Freund Y., 1991, ADV NEURAL INFORM PR, P912
   Gray R. M., 1984, IEEE ASSP Magazine, V1, P4, DOI 10.1109/MASSP.1984.1162229
   Helander E, 2010, IEEE T AUDIO SPEECH, V18, P912, DOI 10.1109/TASL.2010.2041699
   Hinton G, 2010, TECH REP DEP COMPUTE
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Jian ZH, 2007, SNPD 2007: EIGHTH ACIS INTERNATIONAL CONFERENCE ON SOFTWARE ENGINEERING, ARTIFICIAL INTELLIGENCE, NETWORKING, AND PARALLEL/DISTRIBUTED COMPUTING, VOL 1, PROCEEDINGS, P210
   Kain A, 1998, INT CONF ACOUST SPEE, P285, DOI 10.1109/ICASSP.1998.674423
   Kawahara H, 2008, INT CONF ACOUST SPEE, P3933, DOI 10.1109/ICASSP.2008.4518514
   Krizhevsky A, 2009, LEARNING MULTIPLE LA
   Kunikoshi A., 2009, P INTERSPEECH, P308
   KUREMATSU A, 1990, SPEECH COMMUN, V9, P357, DOI 10.1016/0167-6393(90)90011-W
   Lee CH, 2006, INTERSPEECH 2006 AND 9TH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE PROCESSING, VOLS 1-5, P2254
   Ling Z-H, 2006, BLIZZ CHALL WORKSH U
   Ling ZH, 2012, IEEE T AUDIO SPEECH, V20, P1492, DOI 10.1109/TASL.2011.2182511
   Ling ZH, 2013, IEEE T AUDIO SPEECH, V21, P2129, DOI 10.1109/TASL.2013.2269291
   LingHui  C., 2013, P INTERSPEECH, P3052
   McDermott E, 2007, IEEE T AUDIO SPEECH, V15, P203, DOI 10.1109/TASL.2006.876778
   Milner Ben, 2002, P INTERSPEECH, P2421
   Mohamed AR, 2012, IEEE T AUDIO SPEECH, V20, P14, DOI 10.1109/TASL.2011.2109382
   Nair V, 2009, ADV NEURAL INF PROCE, V22, P1339
   Nakamura K, 2012, SPEECH COMMUN, V54, P134, DOI 10.1016/j.specom.2011.07.007
   Nakashika T, 2013, INTERSPEECH, P369
   NARENDRANATH M, 1995, SPEECH COMMUN, V16, P207, DOI 10.1016/0167-6393(94)00058-I
   Pascanu R., 2012, DIFFICULTY TRAINING
   SAITO D, 2010, [No title captured], P1728
   Saito D., 2011, P INTERSPEECH, P653
   Scholkopf B., 2006, ADV NEURAL INFORM PR, P1345
   Smolensky P, 1986, PARALLEL DISTRIBUTED, P194
   Stylianou Y, 1998, IEEE T SPEECH AUDI P, V6, P131, DOI 10.1109/89.661472
   Takashima R, 2012, IEEE W SP LANG TECH, P313, DOI 10.1109/SLT.2012.6424242
   TODA T, 2006, [No title captured], P2446
   Toda T, 2007, IEEE T AUDIO SPEECH, V15, P2222, DOI 10.1109/TASL.2007.907344
   Toda T, 2007, IEICE T INF SYST, VE90D, P816, DOI 10.1093/ietisy/e90-d.5.816
   VALBRET H, 1992, SPEECH COMMUN, V11, P175, DOI 10.1016/0167-6393(92)90012-V
   Veaux C., 2011, P INTERSPEECH, P2765
   Wu YJ, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL I, PROCEEDINGS, P629
   Wu Z, 2013, P 8 ISCA SPEECH SYNT, P221
   Wu Z., 2013, P IEEE CHIN SUMM INT
NR 44
TC 10
Z9 10
U1 0
U2 24
PU SPRINGEROPEN
PI LONDON
PA CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
SN 1687-4722
J9 EURASIP J AUDIO SPEE
JI EURASIP J. Audio Speech Music Process.
PD FEB 25
PY 2015
AR 8
DI 10.1186/s13636-014-0044-3
PG 12
WC Acoustics; Engineering, Electrical & Electronic
SC Acoustics; Engineering
GA CC9GR
UT WOS:000350676900001
OA DOAJ Gold, Green Published
DA 2020-02-19
ER

PT J
AU Yu, WC
   Zhuang, FZ
   He, Q
   Shi, ZZ
AF Yu, Wenchao
   Zhuang, Fuzhen
   He, Qing
   Shi, Zhongzhi
TI Learning deep representations via extreme learning machines
SO NEUROCOMPUTING
LA English
DT Article
DE Extreme learning machine; Deep learning; Representation learning;
   Stacked ELMs; Stacked generalization; DrELM
AB Extreme learning machine (ELM) as an emerging technology has achieved exceptional performance in large-scale settings, and is well suited to binary and multi-class classification, as well as regression tasks. However, existing ELM and its variants predominantly employ single hidden layer feedforward networks, leaving the popular and potentially powerful stacked generalization principle unexploited for seeking predictive deep representations of input data. Deep architectures can find higher-level representations, thus can potentially capture relevant higher-level abstractions. But most of current deep learning methods require solving a difficult and non-convex optimization problem. In this paper, we propose a stacked model, DrELM, to learn deep representations via extreme learning machine according to stacked generalization philosophy. The proposed model utilizes ELM as a base building block and incorporates random shift and kernelization as stacking elements. Specifically, in each layer. DrELM integrates a random projection of the predictions obtained by ELM into the original feature, and then applies kernel functions to generate the resultant feature. To verify the classification and regression performance of DrELM, we conduct the experiments on both synthetic and real-world data sets. The experimental results show that DrELM outperforms ELM and kernel ELMs, which appear to demonstrate that DrELM could yield predictive features that are suitable for prediction tasks. The performances of the deep models (i.e. Stacked Auto-encoder) are comparable. However, due to the utilization of ELM, DrELM is easier to learn and faster in testing. (C) 2014 Elsevier B.V. All rights reserved.
C1 [Yu, Wenchao; Zhuang, Fuzhen; He, Qing; Shi, Zhongzhi] Chinese Acad Sci, Key Lab Intelligent Informat Proc, Inst Comp Technol, Beijing 100190, Peoples R China.
   [Yu, Wenchao; Zhuang, Fuzhen] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
RP Yu, WC (reprint author), Chinese Acad Sci, Key Lab Intelligent Informat Proc, Inst Comp Technol, Beijing 100190, Peoples R China.
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [6117505261203297, 60933004, 61035003]; National
   High-tech R&D Program of China (863 Program) [2013AA01A606,
   2012AA011003]; National Program on Key Basic Research Project (973
   Program)National Basic Research Program of China [2013CB329502]
FX This work is supported by the National Natural Science Foundation of
   China (No. 6117505261203297, 60933004, 61035003), National High-tech R&D
   Program of China (863 Program) (No. 2013AA01A606, 2012AA011003),
   National Program on Key Basic Research Project (973 Program) (No.
   2013CB329502).
CR Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153
   Bengio Y., 2013, P 30 INT C MACH LEAR
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Blake C.L., 1998, UCI REPOSITORY MACHI
   Breiman L, 1996, MACH LEARN, V24, P49
   Frenay B., 2010, ESANN
   Frenay B, 2011, NEUROCOMPUTING, V74, P2526, DOI 10.1016/j.neucom.2010.11.037
   Hinton G., 2012, NEURAL NETWORKS TRIC, V9, P599, DOI DOI 10.1007/978-3-642-35289-8_32
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Huang G.-B., 2010, IEEE T CYBERNETICS, P1
   Huang G-B, 2005, IASTED INT C COMP IN
   Huang GB, 2004, IEEE IJCNN, P985
   Huang GB, 2006, NEUROCOMPUTING, V70, P489, DOI 10.1016/j.neucom.2005.12.126
   Huang GB, 2006, IEEE T NEURAL NETWOR, V17, P879, DOI 10.1109/TNN.2006.875977
   Huang GB, 2011, INT J MACH LEARN CYB, V2, P107, DOI 10.1007/s13042-011-0019-y
   Jarrett K, 2009, IEEE I CONF COMP VIS, P2146, DOI 10.1109/ICCV.2009.5459469
   Krizhevsky A., 2009, THESIS U TORONTO
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Liu Q, 2008, LECT NOTES ARTIF INT, V5012, P222
   Miche Y, 2010, IEEE T NEURAL NETWOR, V21, P158, DOI 10.1109/TNN.2009.2036259
   Mike M, 1989, STAT DATASETS
   Minsky M., 1969, PERCEPTION INTRO COM, V19, P88
   Ng A., 2010, ECONOMETRIC ANAL
   Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0
   Poon H., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P689, DOI 10.1109/ICCVW.2011.6130310
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Socher R., 2011, P C EMP METH NAT LAN, P151
   Vinyals O, 2012, ADV NEURAL INFORM PR, V15, P2834
   Wieland A., TWIN SPIRAL DATASET
   WOLPERT DH, 1992, NEURAL NETWORKS, V5, P241, DOI 10.1016/S0893-6080(05)80023-1
   Yu K, 2011, PROC CVPR IEEE, P1713, DOI 10.1109/CVPR.2011.5995732
NR 32
TC 48
Z9 53
U1 1
U2 47
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0925-2312
EI 1872-8286
J9 NEUROCOMPUTING
JI Neurocomputing
PD FEB 3
PY 2015
VL 149
BP 308
EP 315
DI 10.1016/j.neucom.2014.03.077
PN A
PG 8
WC Computer Science, Artificial Intelligence
SC Computer Science
GA CP6WL
UT WOS:000360028800037
DA 2020-02-19
ER

PT J
AU Yu, YT
   Li, J
   Guan, HY
   Jia, F
   Wang, C
AF Yu, Yongtao
   Li, Jonathan
   Guan, Haiyan
   Jia, Fukai
   Wang, Cheng
TI Learning Hierarchical Features for Automated Extraction of Road Markings
   From 3-D Mobile LiDAR Point Clouds
SO IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE
   SENSING
LA English
DT Article
DE Deep learning; mobile light detection and ranging (LiDAR); point cloud;
   road marking; three dimensional (3-D) extraction
ID TRACKING
AB This paper presents a novel method for automated extraction of road markings directly from three dimensional (3-D) point clouds acquired by a mobile light detection and ranging (LiDAR) system. First, road surface points are segmented from a raw point cloud using a curb-based approach. Then, road markings are directly extracted from road surface points through multisegment thresholding and spatial density filtering. Finally, seven specific types of road markings are further accurately delineated through a combination of Euclidean distance clustering, voxel-based normalized cut segmentation, large-size marking classification based on trajectory and curb-lines, and small-size marking classification based on deep learning, and principal component analysis (PCA). Quantitative evaluations indicate that the proposed method achieves an average completeness, correctness, and F-measure of 0.93, 0.92, and 0.93, respectively. Comparative studies also demonstrate that the proposed method achieves better performance and accuracy than those of the two existing methods.
C1 [Yu, Yongtao; Jia, Fukai; Wang, Cheng] Xiamen Univ, Sch Informat Sci & Engn, Fujian Key Lab Sensing & Comp Smart Cities SCSC, Xiamen 361005, Peoples R China.
   [Li, Jonathan] Xiamen Univ, Key Lab Underwater Acoust Commun & Marin Informat, Xiamen 361005, Peoples R China.
   [Li, Jonathan; Guan, Haiyan] Univ Waterloo, Dept Geog & Environm Management, Waterloo, ON N2L 3G1, Canada.
RP Li, J (reprint author), Xiamen Univ, Key Lab Underwater Acoust Commun & Marin Informat, Xiamen 361005, Peoples R China.
EM junli@xmu.edu.cn
RI Wang, Cheng/A-9472-2012
OI Wang, Cheng/0000-0001-6075-796X
FU Natural Sciences and Engineering Research Council of Canada
   (NSERC)Natural Sciences and Engineering Research Council of Canada
   [111368]; Natural Science Foundation of China (NSFC)National Natural
   Science Foundation of China [41471379]
FX This work was supported in part by the discovery grant from the Natural
   Sciences and Engineering Research Council of Canada (NSERC) under Grant
   111368 and in part by the general grant from the Natural Science
   Foundation of China (NSFC) under Grant 41471379. (Corresponding author:
   Jonathan Li.)
CR Chen X., 2009, P 17 ACM SIGSPATIAL, P488, DOI DOI 10.1145/1653771.1653851
   Gopalan R, 2012, IEEE T INTELL TRANSP, V13, P1088, DOI 10.1109/TITS.2012.2184756
   Guan HY, 2014, ISPRS J PHOTOGRAMM, V87, P93, DOI 10.1016/j.isprsjprs.2013.11.005
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   KAPUR JN, 1985, COMPUT VISION GRAPH, V29, P273, DOI 10.1016/0734-189X(85)90125-2
   Larochelle H., 2008, P 25 INT C MACH LEAR, P536, DOI [10.1145/1390156.1390224, DOI 10.1145/1390156.1390224]
   Lindberg P., 2009, P 12 INT IEEE C INT, P1
   Mancini A., 2012, 2012 IEEE/ASME 8th International Conference on Mechatronic and Embedded Systems and Applications (MESA), P281, DOI 10.1109/MESA.2012.6275575
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Salakhutdinov R, 2009, ARTIF INTELL, P448, DOI DOI 10.1109/CVPRW.2009.5206577
   Salakhutdinov R, 2013, IEEE T PATTERN ANAL, V35, P1958, DOI 10.1109/TPAMI.2012.269
   Salakhutdinov R, 2012, NEURAL COMPUT, V24, P1967, DOI 10.1162/NECO_a_00311
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Thuy M, 2010, METROL MEAS SYST, V17, P311, DOI 10.2478/v10178-010-0027-3
   Tieleman T., 2008, P 25 INT C MACH LEAR, P1064, DOI DOI 10.1145/1390156.1390290
   Tournaire O, 2009, ISPRS J PHOTOGRAMM, V64, P621, DOI 10.1016/j.isprsjprs.2009.05.005
   Yang BS, 2012, PHOTOGRAMM ENG REM S, V78, P331, DOI 10.14358/PERS.78.4.331
NR 18
TC 54
Z9 60
U1 9
U2 65
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1939-1404
EI 2151-1535
J9 IEEE J-STARS
JI IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens.
PD FEB
PY 2015
VL 8
IS 2
BP 709
EP 726
DI 10.1109/JSTARS.2014.2347276
PG 18
WC Engineering, Electrical & Electronic; Geography, Physical; Remote
   Sensing; Imaging Science & Photographic Technology
SC Engineering; Physical Geography; Remote Sensing; Imaging Science &
   Photographic Technology
GA CF1AY
UT WOS:000352277100023
DA 2020-02-19
ER

PT J
AU Zhang, YF
   Shang, CJ
AF Zhang, Yongfeng
   Shang, Changjing
TI Combining Newton interpolation and deep learning for image
   classification
SO ELECTRONICS LETTERS
LA English
DT Article
DE video coding; high definition video; computational complexity; logic
   partitioning; high-performance 4 x 4 intra prediction hardware;
   high-efficiency video coding; HEVC; block partition; computational
   complexity; transform logic; HD video
AB A novel approach for image classification, by integrating deep learning and feature interpolation, supported with advanced learning classification techniques, is presented. The recently introduced deep spatiotemporal inference network (DeSTIN) is employed to carry out limited original feature extraction. Newton interpolation is then used to artificially increase the dimensionality of the extracted feature sets for accurate classification, without incurring heavy computational cost. Support vector machines are utilised for image classification. The proposed approach is tested against the popular MNIST dataset of handwritten digits, demonstrating the potential of the approach.
C1 [Zhang, Yongfeng; Shang, Changjing] Aberystwyth Univ, Inst Math Phys & Comp Sci, Dept Comp Sci, Aberystwyth, Dyfed, Wales.
RP Zhang, YF (reprint author), Aberystwyth Univ, Inst Math Phys & Comp Sci, Dept Comp Sci, Aberystwyth, Dyfed, Wales.
EM yoz1@aber.ac.uk
CR Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Zhang H, 2014, IEEE T CIRC SYST VID, V24, P660, DOI 10.1109/TCSVT.2013.2290578
NR 2
TC 2
Z9 3
U1 0
U2 46
PU INST ENGINEERING TECHNOLOGY-IET
PI HERTFORD
PA MICHAEL FARADAY HOUSE SIX HILLS WAY STEVENAGE, HERTFORD SG1 2AY, ENGLAND
SN 0013-5194
EI 1350-911X
J9 ELECTRON LETT
JI Electron. Lett.
PD JAN 8
PY 2015
VL 51
IS 1
BP 40
EP U67
DI 10.1049/el.2014.3223
PG 2
WC Engineering, Electrical & Electronic
SC Engineering
GA AX2HT
UT WOS:000346764900020
DA 2020-02-19
ER

PT J
AU Matsuzaka, K
   Tanaka, H
   Ohkubo, S
   Morie, T
AF Matsuzaka, Kenji
   Tanaka, Hideki
   Ohkubo, Satoru
   Morie, Takashi
TI VLSI implementation of coupled MRF model using pulse-coupled phase
   oscillators
SO ELECTRONICS LETTERS
LA English
DT Article
DE interpolation; Newton method; image classification; feature extraction;
   spatiotemporal phenomena; support vector machines; learning (artificial
   intelligence); MNIST dataset; SVM; support vector machine; feature
   extraction; DeSTIN; deep spatiotemporal inference network; advanced
   learning classification technique; feature interpolation; image
   classification; deep learning; Newton interpolation
AB Efficient pixel-parallel image processing using a pulse-coupled phase oscillator model and its very large scale integration (VLSI) implementation is proposed. A processing unit that corresponds to a pixel of an image transmits spike pulses to other units, and updates its own analogue state value at timing when spikes come from other units. From a VLSI implementation point of view, this mechanism is suitable for very low-power operation because analogue buffers are unnecessary for data transmission. On the basis of this model, a VLSI image processor chip that performs a coupled Markov random field model for image region segmentation is designed and fabricated. A very low-power VLSI design has been achieved by the combination of an analogue oscillator and digital coupling function generator circuits with time-domain computation. The processing performance of the fabricated oscillator-based image processor chip using a 0.25 mu m CMOS process has achieved 43.2 GOPS or 656 GOPS/W. Experiments using the fabricated chip have shown successful image region segmentation in one-and two-dimensional images.
C1 [Matsuzaka, Kenji; Tanaka, Hideki; Ohkubo, Satoru; Morie, Takashi] Kyushu Inst Technol, Kitakyushu, Japan.
RP Matsuzaka, K (reprint author), Kyushu Inst Technol, Kitakyushu, Japan.
EM morie@brain.kyutech.ac.jp
FU JSPS KAKENHIMinistry of Education, Culture, Sports, Science and
   Technology, Japan (MEXT)Japan Society for the Promotion of
   ScienceGrants-in-Aid for Scientific Research (KAKENHI) [23650118,
   22300081, 22240022]; Aihara ProjectMinistry of Education, Culture,
   Sports, Science and Technology, Japan (MEXT)Japan Society for the
   Promotion of Science; FIRST programme from JSPS
FX This work was partly supported by JSPS KAKENHI, grant numbers 23650118,
   22300081 and 22240022, and by the Aihara Project, the FIRST programme
   from JSPS, initiated by CSTP. The LSI chip design was supported by VDEC,
   the University of Tokyo in collaboration with Cadence Design Systems,
   Inc.
CR Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Mohri M., 2012, FDN MACHINE LEARNING
   Shang CJ, 2013, COMPUT VIS IMAGE UND, V117, P202, DOI 10.1016/j.cviu.2012.12.002
NR 3
TC 5
Z9 5
U1 1
U2 8
PU INST ENGINEERING TECHNOLOGY-IET
PI HERTFORD
PA MICHAEL FARADAY HOUSE SIX HILLS WAY STEVENAGE, HERTFORD SG1 2AY, ENGLAND
SN 0013-5194
EI 1350-911X
J9 ELECTRON LETT
JI Electron. Lett.
PD JAN 8
PY 2015
VL 51
IS 1
BP 46
EP U73
DI 10.1049/el.2014.2105
PG 2
WC Engineering, Electrical & Electronic
SC Engineering
GA AX2HT
UT WOS:000346764900023
DA 2020-02-19
ER

PT S
AU Schuller, B
AF Schuller, Bjorn
BE Bassis, S
   Esposito, A
   Morabito, FC
TI Deep Learning Our Everyday Emotions A Short Overview
SO ADVANCES IN NEURAL NETWORKS: COMPUTATIONAL AND THEORETICAL ISSUES
SE Smart Innovation Systems and Technologies
LA English
DT Article; Book Chapter
DE Deep Learning; Neural Networks; Emotion Recognition; Affective Computing
AB Emotion is omnipresent in our daily lives and has a significant influence on our functional activities. Thus, computer-based recognising and monitoring of affective cues can be of interest such as when interacting with intelligent systems, or for health-care and security reasons. In this light, this short overview focuses on audio/visual and textual cues as input feature modality for automatic emotion recognition. In particular, it shows how these can best be modelled in a Neural Network context. This includes deep learning, and sparse auto-encoders for transfer learning of a compact task and population representation. It further shows avenues towards massively autonomous rich multitask-learning and required confidence estimation as is needed to prepare such technology for real-life application.
C1 [Schuller, Bjorn] Imperial Coll London, Dept Comp, London SW7 2AZ, England.
RP Schuller, B (reprint author), Imperial Coll London, Dept Comp, London SW7 2AZ, England.
EM bjoern.schuller@imperial.ac.uk
CR Amer M. R., 2013, P IEEE INT C AC SPEE
   Bennett I., 2012, US Patent, Patent No. [8,214,214, 8214214]
   Bruckner R., 2012, P INTERSPEECH 2012 1
   Bruckner R., 2013, P 13 BIANN IEEE AUT
   Bruckner R., 2014, CONFLICT NEGOTIATION
   Brueckner  R., 2014, P ICASSP, P4856
   Cibau N. E., 2013, P 15 REUN TRAB PROC
   Coutinho E., 2014, P 2014 INT JOINT C N, P6
   DENG J, 2013, P 5 BIANN HUM ASS C, P511, DOI DOI 10.1109/ACII.2013.90
   Deng J., 2014, P ICASSP, P4851
   Deng J., 2012, P INTERSPEECH 2012
   Deng J., 2012, P SPEECH COMM 10 ITG, P1
   DENG J, 2014, P ICPR 2014 STOCKH S, P761
   Erhan D, 2010, J MACH LEARN RES, V11, P625
   Esparza Jose, 2012, Partially Supervised Learning: First IAPR TC3 Workshop (PSL 2011). Revised Selected Papers, P19, DOI 10.1007/978-3-642-28258-4_3
   Eyben F, 2012, ACM T INTERACT INTEL, V2, DOI 10.1145/2133366.2133372
   Han W., 2013, P INTERSPEECH, P2856
   Han W., 2012, P 5 INT S COMM CONTR, P1
   Hinton G.E., 2012, CORRABS12070580
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Huang CC, 2014, MATH PROBL ENG, DOI 10.1155/2014/749604
   Jirayucharoensak S, 2014, SCI WORLD J, DOI 10.1155/2014/627892
   KAHOU SE, 2013, P 15 ACM INT C MULT, P543, DOI DOI 10.1145/2522848.2531745
   Kim Y., 2013, P IEEE INT C AC SPEE
   Le D, 2013, 2013 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING (ASRU), P216, DOI 10.1109/ASRU.2013.6707732
   Li L, 2013, PROCEEDINGS OF THE AMERICAN SOCIETY FOR COMPOSITES
   Maas A.L., 2013, P ICML WORKSH DEEP L
   Popovic B., 2013, 12 INT SCI PROF S IN, V12, P939
   Sanchez-Gutierrez ME, 2014, LECT NOTES COMPUT SC, V8495, P311, DOI 10.1007/978-3-319-07491-7_32
   Schmidhuber J., 2014, IDSIA0314
   Schmidt EM, 2011, 2011 IEEE WORKSHOP ON APPLICATIONS OF SIGNAL PROCESSING TO AUDIO AND ACOUSTICS (WASPAA), P65, DOI 10.1109/ASPAA.2011.6082328
   Stuhlsatz A, 2011, INT CONF ACOUST SPEE, P5688
   Vincent P, 2008, P 25 INT C MACH LEAR, P1096, DOI DOI 10.1145/1390156.1390294
   Weninger F, 2013, FRONT PSYCHOL, V4, DOI 10.3389/fpsyg.2013.00292
   Zeiler MD, 2013, INT CONF ACOUST SPEE, P3517, DOI 10.1109/ICASSP.2013.6638312
   Zhang Z., 2012, P ANN C INT SPEECH C, P4
   Zhang Z, 2013, P INTERSPEECH, P2841
   Zixing Zhang, 2011, 2011 IEEE Workshop on Automatic Speech Recognition & Understanding (ASRU), P523, DOI 10.1109/ASRU.2011.6163986
NR 38
TC 6
Z9 6
U1 0
U2 1
PU SPRINGER-VERLAG BERLIN
PI BERLIN
PA HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY
SN 2190-3018
BN 978-3-319-18164-6; 978-3-319-18163-9
J9 SMART INNOV SYST TEC
PY 2015
VL 37
BP 339
EP 346
DI 10.1007/978-3-319-18164-6_33
D2 10.1007/978-3-319-18164-6
PG 8
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BF7DU
UT WOS:000383980800034
DA 2020-02-19
ER

PT J
AU Zorzi, M
   Zanella, A
   Testolin, A
   De Grazia, MD
   Zorzi, M
AF Zorzi, Michele
   Zanella, Andrea
   Testolin, Alberto
   De Grazia, Michele De Filippo
   Zorzi, Marco
TI Cognition-Based Networks: A New Perspective on Network Optimization
   Using Learning and Distributed Intelligence
SO IEEE ACCESS
LA English
DT Article
DE Cognitive networks; deep learning; hierarchical generative models;
   optimization
ID CHALLENGES; RADIO; MULTIPLE; LANGUAGE; 5G
AB In response to the new challenges in the design and operation of communication networks, and taking inspiration from how living beings deal with complexity and scalability, in this paper we introduce an innovative system concept called COgnition-BAsed NETworkS (COBANETS). The proposed approach develops around the systematic application of advanced machine learning techniques and, in particular, unsupervised deep learning and probabilistic generative models for system-wide learning, modeling, optimization, and data representation. Moreover, in COBANETS, we propose to combine this learning architecture with the emerging network virtualization paradigms, which make it possible to actuate automatic optimization and reconfiguration strategies at the system level, thus fully unleashing the potential of the learning approach. Compared with the past and current research efforts in this area, the technical approach outlined in this paper is deeply interdisciplinary and more comprehensive, calling for the synergic combination of expertise of computer scientists, communications and networking engineers, and cognitive scientists, with the ultimate aim of breaking new ground through a profound rethinking of how the modern understanding of cognition can be used in the management and optimization of telecommunication networks.
C1 [Zorzi, Michele; Zanella, Andrea] Univ Padua, Dept Informat Engn, I-35131 Padua, Italy.
   [Testolin, Alberto; De Grazia, Michele De Filippo; Zorzi, Marco] Univ Padua, Dept Gen Psychol, I-35131 Padua, Italy.
   [Zorzi, Marco] IRCCS San Camillo Fdn, I-30126 Venice, Italy.
RP Zorzi, M (reprint author), Univ Padua, Dept Informat Engn, I-35131 Padua, Italy.
EM zorzi@dei.unipd.it
RI Testolin, Alberto/Q-9649-2018; Zorzi, Marco/B-7863-2008
OI Testolin, Alberto/0000-0001-7062-4861; Zorzi, Marco/0000-0002-4651-6390;
   De Filippo De Grazia, Michele/0000-0003-3682-8582
FU Fondazione CaRiPaRo
FX This work was supported by Fondazione CaRiPaRo through the Project A
   Novel Approach to Wireless Networking based on Cognitive Science and
   Distributed Intelligence within the framework "Progetti di Eccellenza
   2012."
CR ACKLEY DH, 1985, COGNITIVE SCI, V9, P147
   Acs G, 2013, INT CON DISTR COMP S, P41, DOI 10.1109/ICDCS.2013.12
   Agoulmine N, 2011, AUTONOMIC NETWORK MANAGEMENT PRINCIPLES: FORM CONCEPTS TO APPLICATIONS, P1
   Akyildiz IF, 2006, COMPUT NETW, V50, P2127, DOI 10.1016/j.comnet.2006.05.001
   Akyildiz IF, 2009, AD HOC NETW, V7, P811
   Aliu OG, 2013, IEEE COMMUN SURV TUT, V15, P336, DOI 10.1109/SURV.2012.021312.00116
   [Anonymous], 2008, 36902 3GPP TR
   [Anonymous], 2008, 36300 3GPP TR
   Antani S, 2002, PATTERN RECOGN, V35, P945, DOI 10.1016/S0031-3203(01)00086-3
   Ba J., 2014, ADV NEURAL INFORM PR, P2654
   Badia L., 2014, P IEEE 15 INT S WORL, P1
   Balachandran A, 2013, ACM SIGCOMM COMP COM, V43, P339, DOI 10.1145/2534169.2486025
   Bastos AM, 2012, NEURON, V76, P695, DOI 10.1016/j.neuron.2012.10.038
   Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153
   Bengio Y., 2011, P ICML WORKSH UNS TR, V7, P1
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Bengio Y, 2009, NEURAL COMPUT, V21, P1601, DOI 10.1162/neco.2008.11-07-647
   Biral A, 2015, DIGIT COMMUN NETW, V1, P1, DOI 10.1016/j.dcan.2015.02.001
   Bkassiny M, 2013, IEEE COMMUN SURV TUT, V15, P1136, DOI 10.1109/SURV.2012.100412.00017
   Boccardi F, 2014, IEEE COMMUN MAG, V52, P74, DOI 10.1109/MCOM.2014.6736746
   Changuel N, 2014, IEEE J SEL AREA COMM, V32, P746, DOI 10.1109/JSAC.2014.140407
   Chen M, 2004, INTERNATIONAL CONFERENCE ON AUTONOMIC COMPUTING, PROCEEDINGS, P36, DOI 10.1109/ICAC.2004.1301345
   Chin WH, 2014, IEEE WIREL COMMUN, V21, P106, DOI 10.1109/MWC.2014.6812298
   Clancy C, 2007, IEEE WIREL COMMUN, V14, P47, DOI 10.1109/MWC.2007.4300983
   Clark DD, 2003, ACM SIGCOMM COMP COM, V33, P3
   Collobert R, 2011, J MACH LEARN RES, V12, P2493
   Conti M., 2015, P 5 ACM C DAT APPL S, P297
   Dal Col L, 2014, IEEE DECIS CONTR P, P5296, DOI 10.1109/CDC.2014.7040217
   Danieletto M, 2014, IEEE COMMUN MAG, V52, P98, DOI 10.1109/MCOM.2014.6894459
   De Filippo De Grazia M., 2012, P 2012 EUR S ART NN, P621
   Dean J., 2012, ADV NEURAL INFORM PR, P1232
   Demestichas P, 2013, IEEE VEH TECHNOL MAG, V8, P47, DOI 10.1109/MVT.2013.2269187
   Devroye N, 2008, IEEE SIGNAL PROC MAG, V25, P12, DOI 10.1109/MSP.2008.929286
   Dirani Mariana, 2010, 2010 8th International Symposium on Modeling and Optimization in Mobile, Ad Hoc and Wireless Networks (WiOpt), P170
   Erman J., 2006, ACM SIGCOMM, V2006, P281, DOI DOI 10.1145/1162678.1162679
   Felleman DJ, 1991, CEREB CORTEX, V1, P1, DOI 10.1093/cercor/1.1.1
   FITZEK FHP, 2007, COGNITIVE WIRELESS N
   Forster A, 2007, PROCEEDINGS OF THE 2007 INTERNATIONAL CONFERENCE ON INTELLIGENT SENSORS, SENSOR NETWORKS AND INFORMATION PROCESSING, P365, DOI 10.1109/ISSNIP.2007.4496871
   Fortuna C, 2009, COMPUT NETW, V53, P1354, DOI 10.1016/j.comnet.2009.01.002
   Friston KJ, 2012, FRONT PSYCHOL, V3, DOI 10.3389/fpsyg.2012.00151
   Fu B, 2014, IEEE COMMUN SURV TUT, V16, P110, DOI 10.1109/SURV.2013.081313.00231
   Giupponi L, 2010, IEEE WIREL COMMUN, V17, P47, DOI 10.1109/MWC.2010.5547921
   Guidolin Francesco, 2014, 2014 13th Annual Mediterranean Ad Hoc Networking Workshop (MED-HOC-NET), P134, DOI 10.1109/MedHocNet.2014.6849115
   Hamalainen Seppo, 2012, LTE SELF ORG NETW SO
   Haykin S, 2005, IEEE J SEL AREA COMM, V23, P201, DOI 10.1109/JSAC.2004.839380
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 1997, PHILOS T ROY SOC B, V352, P1177, DOI 10.1098/rstb.1997.0101
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hinton GE, 2007, TRENDS COGN SCI, V11, P428, DOI 10.1016/j.tics.2007.09.004
   Kawadia V, 2005, IEEE WIREL COMMUN, V12, P3, DOI 10.1109/MWC.2005.1404568
   Khan A, 2012, IEEE T MULTIMEDIA, V14, P431, DOI 10.1109/TMM.2011.2176324
   Koller D., 2009, PROBABILISTIC GRAPHI
   Kreutz D, 2015, P IEEE, V103, P14, DOI 10.1109/JPROC.2014.2371999
   Krizhevsky A., 2012, P ADV NEUR INF PROC, V24, P609
   Le Q. V., 2012, P 29 INT C MACH LEAR, P1
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Levorato M, 2012, IEEE T WIREL COMMUN, V11, P3101, DOI 10.1109/TWC.2012.062012.111342
   Li Y, 2009, IEEE T MULTIMEDIA, V11, P1182, DOI 10.1109/TMM.2009.2026102
   Lien SY, 2011, IEEE COMMUN LETT, V15, P311, DOI 10.1109/LCOMM.2011.011811.101798
   Lopez-Perez D, 2012, IEEE COMMUN MAG, V50, P70, DOI 10.1109/MCOM.2012.6384454
   Ma JS, 2015, J CHEM INF MODEL, V55, P263, DOI 10.1021/ci500747n
   Mahonen P., 2006, P 25 C COMP COMM, P23
   Mesnier M, 2004, INTERNATIONAL CONFERENCE ON AUTONOMIC COMPUTING, PROCEEDINGS, P44, DOI 10.1109/ICAC.2004.1301346
   Miller B., 2014, P 2014 WORKSH ART IN, P3
   Mirza M, 2010, IEEE ACM T NETWORK, V18, P1026, DOI 10.1109/TNET.2009.2037812
   Mitola J., 2000, THESIS ROYAL I TECHN
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Mohamed AR, 2012, IEEE T AUDIO SPEECH, V20, P14, DOI 10.1109/TASL.2011.2109382
   Moore A. W., 2005, ACM SIGMETRICS PERFO, V33, P50, DOI DOI 10.1145/1064212.1064220
   Munaretto D, 2015, 2015 INTERNATIONAL CONFERENCE ON COMPUTING, NETWORKING AND COMMUNICATIONS (ICNC), P653, DOI 10.1109/ICCNC.2015.7069422
   Naseer ul Islam Muhammad, 2012, 2012 IEEE Wireless Communications and Networking Conference (WCNC), P2818, DOI 10.1109/WCNC.2012.6214281
   Nguyen TTT, 2008, IEEE COMMUN SURV TUT, V10, P56, DOI 10.1109/SURV.2008.080406
   Nickolls John, 2008, ACM Queue, V6, DOI 10.1145/1365490.1365500
   Osyczka A., 1985, DESIGN OPTIMIZATION, P193, DOI DOI 10.1016/B978-0-12-280910-1.50012-X
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Petrova M., 2007, COGNITIVE WIRELESS N, P397
   Raina R, 2009, P 26 ANN INT C MACH, V382, P873, DOI DOI 10.1145/1553374.1553486
   Razavi R, 2010, 2010 IEEE 21ST INTERNATIONAL SYMPOSIUM ON PERSONAL INDOOR AND MOBILE RADIO COMMUNICATIONS (PIMRC), P1865, DOI 10.1109/PIMRC.2010.5671622
   Razavi R, 2010, BELL LABS TECH J, V15, P153, DOI 10.1002/bltj.20463
   Srivastava N., 2012, ADV NEURAL INFORM PR, P2222, DOI DOI 10.1109/CVPR.2013.49
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Srivastava V, 2005, IEEE COMMUN MAG, V43, P112, DOI 10.1109/MCOM.2005.1561928
   Stoianov I, 2012, NAT NEUROSCI, V15, P194, DOI 10.1038/nn.2996
   SUTSKEVER I, 2008, NIPS, V21, P2008
   Sutton R. S, 1998, REINFORCEMENT LEARNI
   Taylor G.W., 2007, ADV NEURAL INFORM PR, V19, P1345
   Taylor GW, 2009, P 26 ANN INT C MACH, P1025, DOI DOI 10.1145/1553374.1553505
   TERRENCE J, 1999, UNSUPERVISED LEARNIN
   Testolin Alberto, 2014, 2014 13th Annual Mediterranean Ad Hoc Networking Workshop (MED-HOC-NET), P31, DOI 10.1109/MedHocNet.2014.6849102
   Testolin A., 2013, FRONTIERS PSYCH, V4
   Testolin A, 2016, COGNITIVE SCI, V40, P579, DOI 10.1111/cogs.12258
   Thakolsri S., 2011, IEEE INT C COMM ICC, P1, DOI DOI 10.1109/ICC.2011.5963296
   Thomas RW, 2005, 2005 1st IEEE International Symposium on New Frontiers in Dynamic Spectrum Access Networks, Conference Record, P352
   Thomas RW, 2007, SIGNALS COMMUN TECHN, P17, DOI 10.1007/978-1-4020-5542-3_2
   Thomas RW, 2006, IEEE COMMUN MAG, V44, P51, DOI 10.1109/MCOM.2006.273099
   Tootoonchian A., 2010, P 2010 INT NETW MAN, P3
   Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Whiteson S, 2004, ENG APPL ARTIF INTEL, V17, P855, DOI 10.1016/j.engappai.2004.08.027
   Williams N, 2006, ACM SIGCOMM COMP COM, V36, P7
   Winstein K, 2013, ACM SIGCOMM COMP COM, V43, P123, DOI 10.1145/2534169.2486020
   Xiong HY, 2015, SCIENCE, V347, DOI 10.1126/science.1254806
   Zanforlin M, 2014, 2014 12TH INTERNATIONAL SYMPOSIUM ON MODELING AND OPTIMIZATION IN MOBILE, AD HOC, AND WIRELESS NETWORKS (WIOPT), P656, DOI 10.1109/WIOPT.2014.6850361
   Zinner Thomas, 2010, Proceedings of the 2010 Second International Workshop on Quality of Multimedia Experience (QoMEX 2010), P29, DOI 10.1109/QOMEX.2010.5518277
   Zorzi M, 2013, FRONT PSYCHOL, V4, DOI 10.3389/fpsyg.2013.00515
NR 106
TC 34
Z9 35
U1 2
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 2169-3536
J9 IEEE ACCESS
JI IEEE Access
PY 2015
VL 3
BP 1512
EP 1530
DI 10.1109/ACCESS.2015.2471178
PG 19
WC Computer Science, Information Systems; Engineering, Electrical &
   Electronic; Telecommunications
SC Computer Science; Engineering; Telecommunications
GA DF5JP
UT WOS:000371388200115
OA DOAJ Gold
DA 2020-02-19
ER

PT J
AU Polyak, A
   Wolf, L
AF Polyak, Adam
   Wolf, Lior
TI Channel-Level Acceleration of Deep Face Representations
SO IEEE ACCESS
LA English
DT Article
DE Face recognition; neural network compression
AB A major challenge in biometrics is performing the test at the client side, where hardware resources are often limited. Deep learning approaches pose a unique challenge: while such architectures dominate the field of face recognition with regard to accuracy, they require elaborate, multi-stage computations. Recently, there has been some work on compressing networks for the purpose of reducing run time and network size. However, it is not clear that these compression methods would work in deep face nets, which are, generally speaking, less redundant than the object recognition networks, i.e., they are already relatively lean. We propose two novel methods for compression: one based on eliminating lowly active channels and the other on coupling pruning with repeated use of already computed elements. Pruning of entire channels is an appealing idea, since it leads to direct saving in run time in almost every reasonable architecture.
C1 [Polyak, Adam; Wolf, Lior] Tel Aviv Univ, Blavatnik Sch Comp Sci, IL-69978 Tel Aviv, Israel.
RP Wolf, L (reprint author), Tel Aviv Univ, Blavatnik Sch Comp Sci, IL-69978 Tel Aviv, Israel.
EM wolf@cs.tau.ac.il
FU Broadcom Foundation; Tel Aviv University Authentication Initiative
FX This project was partly funded by the Broadcom Foundation and Tel Aviv
   University Authentication Initiative.
CR Ba J., 2014, ADV NEURAL INFORM PR, P2654
   Chen D, 2012, LECT NOTES COMPUT SC, V7574, P566, DOI 10.1007/978-3-642-33712-3_41
   Collobert R., 2011, P NIPS WORKSH BIGLEA
   Courbariaux M., 2014, CORR
   Denton E. L., 2014, ADV NEURAL INFORM PR, P1269
   Everingham M., 2006, P BRIT MACH VIS C, P1
   Figurnov M., 2015, PERFORATEDCNNS ACCEL
   Han Song, 2015, CORR
   Hassibi B., 1992, ADV NEURAL INFORM PR, V5
   He K, 2015, DELVING DEEP RECTIFI
   Hinton G.E., 2015, CORR
   Hinton G. E., 2012, IMPROVING NEURAL NET
   Huang G. B., 2014, UMCS2014003
   Jaderberg M., 2014, P BRIT MACH VIS C
   Lebedev V., 2015, CORR
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   LeCun Y., 1990, ADV NEURAL INFORMATI, P598
   Lin M., 2013, P INT C LEARN REPR I
   Liu J., 2015, CORR
   Mathieu M., 2014, P INT C LEARN REPR I
   Romero A., 2015, P INT C LEARN REPR I
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Simonyan K., 2015, P INT C LEARN REPR I
   Strom N., 1997, P EUR, P1
   Sun Y., 2015, CORR
   Sun Y, 2014, ADV NEURAL INFORM PR, V60, P1988
   Sun Y, 2015, PROC CVPR IEEE, P2892, DOI 10.1109/CVPR.2015.7298907
   Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Thimm G., 1995, P INT S ART NEUR NET, V2, P1
   Tompson J, 2015, PROC CVPR IEEE, P648, DOI 10.1109/CVPR.2015.7298664
   Vanhoucke V., 2011, P DEEP LEARN UNS FEA, P1
   Vasilache N, 2015, FAST CONVOLUTIONAL N
   Wan L., 2013, P 30 INT C MACH LEAR, P1058
   Yi D., 2014, CORR
   Zhang X., 2015, CORR
NR 37
TC 26
Z9 27
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 2169-3536
J9 IEEE ACCESS
JI IEEE Access
PY 2015
VL 3
BP 2163
EP 2175
DI 10.1109/ACCESS.2015.2494536
PG 13
WC Computer Science, Information Systems; Engineering, Electrical &
   Electronic; Telecommunications
SC Computer Science; Engineering; Telecommunications
GA DF5JP
UT WOS:000371388200166
OA DOAJ Gold
DA 2020-02-19
ER

PT J
AU Zhang, WS
   Duan, PC
   Li, ZW
   Lu, QH
   Gong, WJ
   Yang, S
AF Zhang, Weishan
   Duan, Pengcheng
   Li, Zhongwei
   Lu, Qinghua
   Gong, Wenjuan
   Yang, Su
TI A Deep Awareness Framework for Pervasive Video Cloud
SO IEEE ACCESS
LA English
DT Article
DE Pervasive video cloud; deep learning; framework; context awareness;
   cloud computing
ID TIME-SERIES
AB Context-awareness for big data applications is different from that of traditional applications in that it is getting challenging to obtain the contexts from big data due to the complexity, velocity, variety, and other aspects of big data, especially big video data. The awareness of contexts in big data is more difficult, and should be more in-depth than that of classical applications. Therefore, in this paper, we propose an in-depth context-awareness framework for a pervasive video cloud in order to obtain underlying contexts in big video data. In this framework, we propose an approach that combines the historical view with the current view to obtain meaningful in-depth contexts, where deep learning techniques are used to obtain raw context data. We have conducted initial evaluations to show the effectiveness of the proposed approach in terms of performance and also the accuracy of obtaining the contexts. The evaluation results show that the proposed approach is effective for real-time context-awareness in a pervasive video cloud.
C1 [Zhang, Weishan; Duan, Pengcheng; Li, Zhongwei; Lu, Qinghua; Gong, Wenjuan] China Univ Petr, Dept Software Engn, Qingdao 266580, Peoples R China.
   [Yang, Su] Fudan Univ, Coll Comp Sci & Technol, Shanghai 200433, Peoples R China.
RP Zhang, WS (reprint author), China Univ Petr, Dept Software Engn, Qingdao 266580, Peoples R China.
EM zhangws@upc.edu.cn
OI Zhang, Weishan/0000-0001-9800-1068; Gong, Wenjuan/0000-0001-7805-3629
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61402533]; Natural Science Foundation of Shandong
   ProvinceNatural Science Foundation of Shandong Province [ZR2014FM038,
   ZR2015FL015]; Key Technologies Development Plan of Qingdao Technical
   Economic Development Area; Start-Up Funds for Academic Top-Notch
   Professors through the China University of Petroleum
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61402533, in part by the Natural Science
   Foundation of Shandong Province under Grant ZR2014FM038 and Grant
   ZR2015FL015, and in part by the Key Technologies Development Plan of
   Qingdao Technical Economic Development Area. The work of W. Zhang was
   supported by the Start-Up Funds for Academic Top-Notch Professors
   through the China University of Petroleum.
CR Bishop CM, 2006, PATTERN RECOGNITION
   Creswell J.W., 2010, DESIGNING CONDUCTING
   Di  S., 2012, P INT C HIGH PERF CO, P1
   Dinda P. A., 2000, Cluster Computing, V3, P265, DOI 10.1023/A:1019048724544
   GARDNER ES, 1989, MANAGE SCI, V35, P372, DOI 10.1287/mnsc.35.3.372
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Hamilton J. D., 1994, TIME SERIES ANAL, V2
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hu RD, 2014, SCI WORLD J, DOI 10.1155/2014/321231
   Huval B., 2015, EMPIRICAL EVALUATION
   Hyndman RJ, 2008, J STAT SOFTW, V27, P1
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kuremoto T, 2014, NEUROCOMPUTING, V137, P47, DOI 10.1016/j.neucom.2013.03.047
   MATYAS TA, 1990, J APPL BEHAV ANAL, V23, P341, DOI 10.1901/jaba.1990.23-341
   Nakamiti G., 2012, PRACTICAL APPL INTEL, P543
   Reiss Charles, 2011, GOOGLE CLUSTER USAGE
   Roy N, 2011, INT CONF PERVAS COMP, P63, DOI 10.1109/PERCOM.2011.5767596
   Sermanet P, 2013, OVERFEAT INTEGRATED
   Simonyan K., 2014, VERY DEEP CONVOLUTIO
   Weishan Zhang, 2013, Service-Oriented Computing - ICSOC 2012 Workshops. ICSOC 2012 International Workshops: ASC, DISA, PAASC, SCEB, SeMaPS, WESOA and Satellite Events. Revised Selected Papers, P275
   Zeng SR, 2014, COMM COM INF SC, V484, P228
   Zhang GP, 2001, COMPUT OPER RES, V28, P381, DOI 10.1016/S0305-0548(99)00123-9
   Zhang WS, 2010, PERVASIVE MOB COMPUT, V6, P198, DOI 10.1016/j.pmcj.2009.07.002
   Zhao JD, 2012, STUD COMPUT INTELL, V399, P167
   Zurovac J., 2012, ORTHOGONAL DESIGN PO, P12
NR 27
TC 6
Z9 6
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 2169-3536
J9 IEEE ACCESS
JI IEEE Access
PY 2015
VL 3
BP 2227
EP 2237
DI 10.1109/ACCESS.2015.2497278
PG 11
WC Computer Science, Information Systems; Engineering, Electrical &
   Electronic; Telecommunications
SC Computer Science; Engineering; Telecommunications
GA DF5JP
UT WOS:000371388200169
OA DOAJ Gold
DA 2020-02-19
ER

PT J
AU Nagpal, S
   Singh, M
   Singh, R
   Vatsa, M
AF Nagpal, Shruti
   Singh, Maneet
   Singh, Richa
   Vatsa, Mayank
TI Regularized Deep Learning for Face Recognition With Weight Variations
SO IEEE ACCESS
LA English
DT Article
DE Face recognition; biometrics; body-weight variations; facial aging
AB Body weight variations are an integral part of a person's aging process. However, the lack of association between the age and the weight of an individual makes it challenging to model these variations for automatic face recognition. In this paper, we propose a regularizer-based approach to learn weight invariant facial representations using two different deep learning architectures, namely, sparse-stacked denoising autoencoders and deep Boltzmann machines. We incorporate a body-weight aware regularization parameter in the loss function of these architectures to help learn weight-aware features. The experiments performed on the extended WIT database show that the introduction of weight aware regularization improves the identification accuracy of the architectures both with and without dropout.
C1 [Nagpal, Shruti; Singh, Maneet; Singh, Richa; Vatsa, Mayank] Indraprastha Inst Informat Technol Delhi, New Delhi 110020, India.
RP Vatsa, M (reprint author), Indraprastha Inst Informat Technol Delhi, New Delhi 110020, India.
EM mayank@iiitd.ac.in
RI Vatsa, Mayank/I-5050-2013; Singh, Richa/M-9961-2017
OI Vatsa, Mayank/0000-0001-5952-2274; Singh, Richa/0000-0003-4060-4573
FU Department of Electronics and Information Technology, Government of
   India
FX This work was supported in part by the Department of Electronics and
   Information Technology, Government of India.
CR Baldi P., 2012, J MACH LEARN RES P T, V27, P37
   Bengio Y., 2012, J MACHINE LEARNING R, P17
   Bengio Yoshua, 2007, ADV NEURAL INFORM PR
   Bhatt H. S., 2015, IIITDTR2015002
   Ding CX, 2015, IEEE T MULTIMEDIA, V17, P2049, DOI 10.1109/TMM.2015.2477042
   Erhan D, 2010, J MACH LEARN RES, V11, P625
   Goswami G., INF FUSION IN PRESS
   Goswami G., 2014, P IEEE INT JOINT C B, P1
   Hinton G. E., 2012, ADV NEURAL INFORM PR, P2447
   Ho T. K., 1995, P 3 INT C DOC AN REC, V1, P278
   Iaigman Y., 2014, P IEEE C COMP VIS PA, P1701
   Nagpal S., 2015, P BIOM THEOR APPL SY, P1
   Salakhutdinov R, 2009, ARTIF INTELL, P448, DOI DOI 10.1109/CVPRW.2009.5206577
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Singh M, 2014, IEEE ACCESS, V2, P822, DOI 10.1109/ACCESS.2014.2344667
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Sun Y, 2014, ADV NEURAL INFORM PR, V60, P1988
   Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Zhao W, 2003, ACM COMPUT SURV, V35, P399, DOI 10.1145/954339.954342
NR 21
TC 12
Z9 12
U1 1
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 2169-3536
J9 IEEE ACCESS
JI IEEE Access
PY 2015
VL 3
BP 3010
EP 3018
DI 10.1109/ACCESS.2015.2510865
PG 9
WC Computer Science, Information Systems; Engineering, Electrical &
   Electronic; Telecommunications
SC Computer Science; Engineering; Telecommunications
GA DF5JP
UT WOS:000371388200229
OA DOAJ Gold
DA 2020-02-19
ER

PT S
AU Furui, S
AF Furui, Sadaoki
BA Yu, D
   Deng, L
BF Yu, D
   Deng, L
TI Automatic Speech Recognition A Deep Learning Approach Introduction
SO AUTOMATIC SPEECH RECOGNITION: A DEEP LEARNING APPROACH
SE Signals and Communication Technology
LA English
DT Editorial Material; Book Chapter
ID REPRESENTATIONS; ERROR
AB Automatic speech recognition (ASR) is an important technology to enable and improve the human-human and human-computer interactions. In this chapter, we introduce the main application areas of ASR systems, describe their basic architecture, and then introduce the organization of the book.
C1 [Furui, Sadaoki] Toyota Technol Inst, Chicago, IL 60637 USA.
   [Furui, Sadaoki] Tokyo Inst Technol, Chicago, IL USA.
RP Furui, S (reprint author), Toyota Technol Inst, Chicago, IL 60637 USA.
CR Bengio Y, 2006, ADV NEURAL INFORM PR, P153
   Clayton S., 2012, MICROSOFT RES SHOWS
   Dahl GE, 2012, IEEE T AUDIO SPEECH, V20, P30, DOI 10.1109/TASL.2011.2134090
   DAVIS SB, 1980, IEEE T ACOUST SPEECH, V28, P357, DOI 10.1109/TASSP.1980.1163420
   Deng L., 2003, SPEECH PROCESSING DY
   Deng L., 2014, DEEP LEARNING METHOD
   HERMANSKY H, 1990, J ACOUST SOC AM, V87, P1738, DOI 10.1121/1.399423
   Hinton G., 2010, 2010003 UTML U TOR
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Huang X., 2001, SPOKEN LANGUAGE PROC
   Huang XD, 2010, CH CRC MACH LEARN PA, P339
   Huang Xuedong, 2001, SPOKEN LANGUAGE PROC, V18
   Juang BH, 1997, IEEE T SPEECH AUDI P, V5, P257, DOI 10.1109/89.568732
   LeCun Y, 1998, LECT NOTES COMPUT SC, V1524, P9
   Moon TK, 1996, IEEE SIGNAL PROC MAG, V13, P47, DOI 10.1109/79.543975
   Povey D, 2002, INT CONF ACOUST SPEE, P105
   Rabiner L. R., 1986, IEEE ASSP Magazine, V3, P4, DOI 10.1109/MASSP.1986.1165342
   Rabiner L. R., 1993, FUNDAMENTALS SPEECH
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Seide F., 2011, 2011 IEEE Workshop on Automatic Speech Recognition & Understanding (ASRU), P24, DOI 10.1109/ASRU.2011.6163899
   Seide F., 2011, P INTERSPEECH, P437
   Seltzer ML, 2011, IEEE SIGNAL PROC MAG, V28, P50, DOI 10.1109/MSP.2011.941065
   Wang YY, 2008, IEEE SIGNAL PROC MAG, V25, P29, DOI 10.1109/MSP.2008.918411
   YU D, 2007, P INTERSPEECH, P2709
   Zweig G., 2011, P INTERSPEECH, P609
NR 26
TC 0
Z9 0
U1 0
U2 1
PU SPRINGER-VERLAG BERLIN
PI BERLIN
PA HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY
SN 1860-4862
BN 978-1-4471-5779-3; 978-1-4471-5778-6
J9 SIGNALS COMMUN TECHN
PY 2015
BP 1
EP 9
DI 10.1007/978-1-4471-5779-3_1
D2 10.1007/978-1-4471-5779-3
PG 9
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
SC Computer Science; Engineering
GA BD0XH
UT WOS:000357814700003
DA 2020-02-19
ER

PT S
AU Furui, S
AF Furui, Sadaoki
BA Yu, D
   Deng, L
BF Yu, D
   Deng, L
TI Automatic Speech Recognition A Deep Learning Approach Foreword
SO AUTOMATIC SPEECH RECOGNITION: A DEEP LEARNING APPROACH
SE Signals and Communication Technology
LA English
DT Editorial Material; Book Chapter
C1 [Furui, Sadaoki] Toyota Technol Inst, Chicago, IL 60637 USA.
   [Furui, Sadaoki] Tokyo Inst Technol, Chicago, IL USA.
RP Furui, S (reprint author), Toyota Technol Inst, Chicago, IL 60637 USA.
NR 0
TC 0
Z9 0
U1 0
U2 1
PU SPRINGER-VERLAG BERLIN
PI BERLIN
PA HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY
SN 1860-4862
BN 978-1-4471-5779-3; 978-1-4471-5778-6
J9 SIGNALS COMMUN TECHN
PY 2015
BP VII
EP VIII
D2 10.1007/978-1-4471-5779-3
PG 2
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
SC Computer Science; Engineering
GA BD0XH
UT WOS:000357814700001
DA 2020-02-19
ER

PT S
AU Furui, S
AF Furui, Sadaoki
BA Yu, D
   Deng, L
BF Yu, D
   Deng, L
TI Automatic Speech Recognition A Deep Learning Approach Preface
SO AUTOMATIC SPEECH RECOGNITION: A DEEP LEARNING APPROACH
SE Signals and Communication Technology
LA English
DT Editorial Material; Book Chapter
C1 [Furui, Sadaoki] Toyota Technol Inst, Chicago, IL 60637 USA.
   [Furui, Sadaoki] Tokyo Inst Technol, Chicago, IL USA.
RP Furui, S (reprint author), Toyota Technol Inst, Chicago, IL 60637 USA.
NR 0
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER-VERLAG BERLIN
PI BERLIN
PA HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY
SN 1860-4862
BN 978-1-4471-5779-3; 978-1-4471-5778-6
J9 SIGNALS COMMUN TECHN
PY 2015
BP IX
EP XI
D2 10.1007/978-1-4471-5779-3
PG 3
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
SC Computer Science; Engineering
GA BD0XH
UT WOS:000357814700002
DA 2020-02-19
ER

PT S
AU Furui, S
AF Furui, Sadaoki
BA Yu, D
   Deng, L
BF Yu, D
   Deng, L
TI Advanced Model Initialization Techniques
SO AUTOMATIC SPEECH RECOGNITION: A DEEP LEARNING APPROACH
SE Signals and Communication Technology
LA English
DT Article; Book Chapter
ID ALGORITHM
AB In this chapter, we introduce several advanced deep neural network (DNN) model initialization or pretraining techniques. These techniques have played important roles in the early days of deep learning research and continue to be useful under many conditions. We focus our presentation of pretraining DNNs on the following topics: the restricted Boltzmann machine (RBM), which by itself is an interesting generative model, the deep belief network (DBN), the denoising autoencoder, and the discriminative pretraining.
C1 [Furui, Sadaoki] Toyota Technol Inst, Chicago, IL 60637 USA.
   [Furui, Sadaoki] Tokyo Inst Technol, Chicago, IL USA.
RP Furui, S (reprint author), Toyota Technol Inst, Chicago, IL 60637 USA.
CR Bengio Y, 2006, ADV NEURAL INFORM PR, P153
   Bottou Lon, 1998, ONLINE LEARNING NEUR, V17, P9
   Coates A., 2011, INT C ART INT STAT, V15, P215, DOI 10.1177/1753193410390845
   Dahl GE, 2012, IEEE T AUDIO SPEECH, V20, P30, DOI 10.1109/TASL.2011.2134090
   Erhan D., 2009, J MACHINE LEARNING R, V5, P153
   Erhan D, 2010, J MACH LEARN RES, V11, P625
   Hinton G. E, 2012, ARXIV12070580
   Hinton G. E., 2010, 2010003 UTML TR
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   HINTON GE, 1995, SCIENCE, V268, P1158, DOI 10.1126/science.7761831
   Hinton GE, 2009, ADV NEURAL INFORM PR, P1607
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Larochelle H., 2008, P 25 INT C MACH LEAR, P536, DOI [10.1145/1390156.1390224, DOI 10.1145/1390156.1390224]
   Ling ZH, 2013, IEEE T AUDIO SPEECH, V21, P2129, DOI 10.1109/TASL.2013.2269291
   Sainath TN, 2012, P NEUR INF PROC SYST
   Salakhutdinov R., 2007, P INT C MACH LEARN, V24, P791, DOI DOI 10.1145/1273496.1273596
   Saul LK, 1996, J ARTIF INTELL RES, V4, P61, DOI 10.1613/jair.251
   Seide F., 2011, 2011 IEEE Workshop on Automatic Speech Recognition & Understanding (ASRU), P24, DOI 10.1109/ASRU.2011.6163899
   Seide F., 2011, P INTERSPEECH, P437
   Smolensky Paul, 1986, INFORM PROCESSING DY
   Vincent P, 2008, P 25 INT C MACH LEAR, P1096, DOI DOI 10.1145/1390156.1390294
   Yu D., 2010, P NEUR INF PROC SYST
   Zhang S, 2014, P INT C AC SPEECH SI, P6899
NR 23
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER-VERLAG BERLIN
PI BERLIN
PA HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY
SN 1860-4862
BN 978-1-4471-5779-3; 978-1-4471-5778-6
J9 SIGNALS COMMUN TECHN
PY 2015
BP 79
EP 95
DI 10.1007/978-1-4471-5779-3_5
D2 10.1007/978-1-4471-5779-3
PG 17
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
SC Computer Science; Engineering
GA BD0XH
UT WOS:000357814700007
DA 2020-02-19
ER

PT S
AU Furui, S
AF Furui, Sadaoki
BA Yu, D
   Deng, L
BF Yu, D
   Deng, L
TI Computational Network
SO AUTOMATIC SPEECH RECOGNITION: A DEEP LEARNING APPROACH
SE Signals and Communication Technology
LA English
DT Article; Book Chapter
ID NEURAL-NETWORKS
AB In the previous chapters, we have discussed various deep learning models for automatic speech recognition (ASR). In this chapter, we introduce computational network (CN), a unified framework for describing arbitrary learning machines, such as deep neural networks (DNNs), computational neural networks (CNNs), recurrent neural networks (RNNs), long short term memory (LSTM), logistic regression, and matrixum entropy model, that can be illustrated as a series of computational steps. A CN is a directed graph in which each leaf node represents an input value or a parameter and each nonleaf node represents a matrix operation upon its children. We describe algorithms to carry out forward computation and gradient calculation in CN and introduce most popular computation node types used in a typical CN.
C1 [Furui, Sadaoki] Toyota Technol Inst, Chicago, IL 60637 USA.
   [Furui, Sadaoki] Tokyo Inst Technol, Chicago, IL USA.
RP Furui, S (reprint author), Toyota Technol Inst, Chicago, IL 60637 USA.
CR Abdel-Hamid O., 2013, EXPLORING CONVOLUTIO, P3366
   Abdel-Hamid O, 2012, INT CONF ACOUST SPEE, P4277, DOI 10.1109/ICASSP.2012.6288864
   Bergstra J, 2010, P PYTH SCI COMP C SC, V4
   Bischof C., 1997, URBANA, V51, P61802
   Chellapilla Kumar, 2006, 10 INT WORKSH FRONT
   Ciresan D. C., 2012, 2012 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2012.6252544
   Dahl GE, 2012, IEEE T AUDIO SPEECH, V20, P30, DOI 10.1109/TASL.2011.2134090
   Deng L, 2013, INT CONF ACOUST SPEE, P6669, DOI 10.1109/ICASSP.2013.6638952
   Graves A., 2013, ARXIV13080850
   Griewank Andreas, 2008, EVALUATING DERIVATIV
   Guenter B., 2013, OPT2013 NIPS 2013 WO
   Guenter B, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239559
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
   Hopcroft JE, 1983, DATA STRUCTURES ALGO
   Jaitly N., 2012, P ANN C INT SPEECH C
   Kavukcuoglu K., 2010, NIPS, V1, P5
   Krizhevsky A., 2012, ADV NEURAL INFORM PR, V1, P4
   LeCun Y., 1995, HDB BRAIN THEORY NEU, P3361
   Mikolov T, 2012, IEEE W SP LANG TECH, P234, DOI 10.1109/SLT.2012.6424228
   Sainath TN, 2013, 2013 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING (ASRU), P315, DOI 10.1109/ASRU.2013.6707749
   Sainath TN, 2013, INT CONF ACOUST SPEE, P8614, DOI 10.1109/ICASSP.2013.6639347
   Seide F., 2011, 2011 IEEE Workshop on Automatic Speech Recognition & Understanding (ASRU), P24, DOI 10.1109/ASRU.2011.6163899
   Seide F., 2011, P INTERSPEECH, P437
   Shi Y., 2012, P ANN C INT SPEECH C
   Socher R., 2011, P 28 INT C MACH LEAR, P129, DOI DOI 10.1007/978-3-540-87479-9
   Sutskever I., 2011, P 28 INT C MACH LEAR, P1017
   Tarjan R., 1972, SIAM Journal on Computing, V1, P146, DOI 10.1137/0201010
   Wiesler S., 2014, P INT C AC SPEECH SI, P3305
   Yu D., 2014, MSRTR2014112
   Yu D, 2011, 12TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2011 (INTERSPEECH 2011), VOLS 1-5, P244
NR 31
TC 1
Z9 1
U1 0
U2 0
PU SPRINGER-VERLAG BERLIN
PI BERLIN
PA HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY
SN 1860-4862
BN 978-1-4471-5779-3; 978-1-4471-5778-6
J9 SIGNALS COMMUN TECHN
PY 2015
BP 267
EP 298
DI 10.1007/978-1-4471-5779-3_14
D2 10.1007/978-1-4471-5779-3
PG 32
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
SC Computer Science; Engineering
GA BD0XH
UT WOS:000357814700016
DA 2020-02-19
ER

PT S
AU Furui, S
AF Furui, Sadaoki
BA Yu, D
   Deng, L
BF Yu, D
   Deng, L
TI Summary and Future Directions
SO AUTOMATIC SPEECH RECOGNITION: A DEEP LEARNING APPROACH
SE Signals and Communication Technology
LA English
DT Article; Book Chapter
ID NEURAL-NETWORKS; SPEECH; RECOGNITION; MODEL; FEATURES
AB In this chapter, we summarize the book by first listing and analyzing what we view as major milestone studies in the recent history of developing the deep learning-based ASR techniques and systems. We describe the motivations of these studies, the innovations they have engendered, the improvements they have provided, and the impacts they have generated. In this road map, we will first cover the historical context in which the DNN technology made inroad into ASR around 2009 resulting from academic and industry collaborations. Then we select seven main themes in which innovations flourished across-the-board in ASR industry and academic research after the early debut of DNNs. Finally, our belief is provided on the current state-of-the-art of speech recognition systems, and we also discuss our thoughts and analysis on the future research directions.
C1 [Furui, Sadaoki] Toyota Technol Inst, Chicago, IL 60637 USA.
   [Furui, Sadaoki] Tokyo Inst Technol, Chicago, IL USA.
RP Furui, S (reprint author), Toyota Technol Inst, Chicago, IL 60637 USA.
CR Abdel-Hamid O., 2013, EXPLORING CONVOLUTIO, P3366
   Abdel-Hamid O, 2013, INT CONF ACOUST SPEE, P7942, DOI 10.1109/ICASSP.2013.6639211
   Abdel-Hamid O, 2012, INT CONF ACOUST SPEE, P4277, DOI 10.1109/ICASSP.2012.6288864
   Andrew G., 2013, NEURAL INFORM PROCES
   Bengio S, 2014, INTERSPEECH, P1053
   Bengio Y., 2013, CORR
   Bromberg I., 2007, INTERSPEECH, P1829
   Chen D., 2014, P INT C AC SPEECH SI
   Chen J., 2014, P ICLR
   Cooke M, 2010, COMPUT SPEECH LANG, V24, P1, DOI 10.1016/j.csl.2009.02.006
   Dahl GE, 2012, IEEE T AUDIO SPEECH, V20, P30, DOI 10.1109/TASL.2011.2134090
   Dahl GE, 2011, INT CONF ACOUST SPEE, P4688
   DENG L, 1994, J ACOUST SOC AM, V95, P2702, DOI 10.1121/1.409839
   DENG L, 1994, NEURAL NETWORKS, V7, P331, DOI 10.1016/0893-6080(94)90027-2
   Deng L., 2010, P ANN C INT SPEECH C
   Deng L., 2014, P INT C AC SPEECH SI
   Deng L., 2014, P ANN C INT SPEECH C
   Deng L., 2011, P ANN C INT SPEECH C
   Deng L., 2013, P INT C AC SPEECH SI
   Deng L., 2009, NIPS WORKSH WHISTL C
   Deng L., 1999, COMPUTATIONAL MODELS, P214
   Deng L., 2014, DEEP LEARNING METHOD
   Deng L, 2007, INT CONF ACOUST SPEE, P445
   Deng L, 2006, IEEE T AUDIO SPEECH, V14, P1492, DOI 10.1109/TASL.2006.878265
   Deng L, 2013, INT CONF ACOUST SPEE, P6669, DOI 10.1109/ICASSP.2013.6638952
   Deng L, 2012, IEEE W SP LANG TECH, P210, DOI 10.1109/SLT.2012.6424224
   Ephraim Y, 2005, IEEE SIGNAL PROC LET, V12, P166, DOI 10.1109/LSP.2004.840914
   Gales MJF, 1998, COMPUT SPEECH LANG, V12, P75, DOI 10.1006/csla.1998.0043
   Ghoshal A., 2013, P INT C AC SPEECH SI
   Godfrey J. J., 1992, ICASSP-92: 1992 IEEE International Conference on Acoustics, Speech and Signal Processing (Cat. No.92CH3103-9), P517, DOI 10.1109/ICASSP.1992.225858
   He XD, 2008, IEEE SIGNAL PROC MAG, V25, P14, DOI 10.1109/MSP.2008.926652
   Heigold G., 2013, P INT C AC SPEECH SI
   Hinton G, 2013, P INT C AC SPEECH SI
   Hoffman MD, 2013, J MACH LEARN RES, V14, P1303
   Huang J.-T., 2013, P INT C AC SPEECH SI
   Huang J, 2013, INT CONF ACOUST SPEE, P7596, DOI 10.1109/ICASSP.2013.6639140
   Huang LF, 2012, PROC INT CONF ANTI
   Huang Po-Sen, 2013, ACM INT C INF KNOWL
   Hutchinson B., 2012, P INT C AC SPEECH SI
   Hutchinson B., 2013, IEEE T PATTERN ANAL
   Kingsbury B., 2012, P ANN C INT SPEECH C
   Kingsbury B, 2009, INT CONF ACOUST SPEE, P3761, DOI 10.1109/ICASSP.2009.4960445
   Kristjansson T. T., 2006, P ANN C INT SPEECH C
   Kumar N, 1998, SPEECH COMMUN, V26, P283, DOI 10.1016/S0167-6393(98)00061-2
   LANG KJ, 1990, NEURAL NETWORKS, V3, P23, DOI 10.1016/0893-6080(90)90044-L
   Le Q.V, 2011, ARXIV11126209
   Lee C.H., 2004, P INTERSPEECH, P109
   Li JY, 2012, IEEE W SP LANG TECH, P131, DOI 10.1109/SLT.2012.6424210
   Lin H, 2009, INT CONF ACOUST SPEE, P4333, DOI 10.1109/ICASSP.2009.4960588
   Lu Y., 2004, P AUSTR INT C SPEECH
   Martens J., 2010, P 27 INT C MACH LEAR, V951, P2010
   Mesgarani N, 2014, SCIENCE, V343, P1006, DOI 10.1126/science.1245994
   Mesgarani N, 2012, NATURE, V485, P233, DOI 10.1038/nature11020
   Mikolov T., 2012, THESIS BRNO U TECHNO
   Mnih A., 2014, P INT C MACH LEARN I
   Mohamed A, 2009, NIPS WORKSH DEEP LEA
   Mohamed A.R., 2010, P INTERSPEECH, P2846
   Mohamed AR, 2012, INT CONF ACOUST SPEE, P4273, DOI 10.1109/ICASSP.2012.6288863
   Moore R., 2014, 2 INT C STAT LANG SP
   MORGAN N, 1995, P IEEE, V83, P742, DOI 10.1109/5.381844
   Niu F., 2011, ARXIV11065730
   Rezende Danilo Jimenez, 2014, P INT C MACH LEARN I
   ROBINSON AJ, 1994, IEEE T NEURAL NETWOR, V5, P298, DOI 10.1109/72.279192
   Sainath T., 2013, P IEEE WORKSH AUT SP
   Sainath TN, 2013, 2013 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING (ASRU), P315, DOI 10.1109/ASRU.2013.6707749
   Sainath TN, 2013, INT CONF ACOUST SPEE, P8614, DOI 10.1109/ICASSP.2013.6639347
   Sainath TN, 2013, INT CONF ACOUST SPEE, P6655, DOI 10.1109/ICASSP.2013.6638949
   Sainath TN, 2009, 2009 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION & UNDERSTANDING (ASRU 2009), P359, DOI 10.1109/ASRU.2009.5373263
   Sak H., 2014, P ANN C INT SPEECH C
   Saon G, 2013, 2013 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING (ASRU), P55, DOI 10.1109/ASRU.2013.6707705
   Schultz Tanja, 1998, P DARPA WORKSH BROAD, P259
   Seide F., 2011, 2011 IEEE Workshop on Automatic Speech Recognition & Understanding (ASRU), P24, DOI 10.1109/ASRU.2011.6163899
   Seide F., 2011, P INTERSPEECH, P437
   Seltzer M. L., 2013, P INT C AC SPEECH SI
   Seltzer ML, 2013, INT CONF ACOUST SPEE, P6965, DOI 10.1109/ICASSP.2013.6639012
   Sheikhzadeh H, 1994, IEEE T SPEECH AUDI P, V2, P80, DOI 10.1109/89.260337
   Shen Y., 2014, ACM INT C INF KNOWL
   Socher R., 2011, P 28 INT C MACH LEAR, P129, DOI DOI 10.1007/978-3-540-87479-9
   Socher R, 2012, P JOINT C EMP METH N
   Su Hang, 2013, P INT C AC SPEECH SI
   Sun JP, 2002, J ACOUST SOC AM, V111, P1086, DOI 10.1121/1.1420380
   Thomas S, 2012, INT CONF ACOUST SPEE, P4269, DOI 10.1109/ICASSP.2012.6288862
   Tuske Z., 2014, P ANN C INT SPEECH C
   Vanhoucke V., 2011, P NIPS WORKSH DEEP L
   Vesely  K., 2013, P ANN C INT SPEECH C
   Vinyals O., 2012, P NEUR INF PROC SYST, V15
   WAIBEL A, 1989, IEEE T ACOUST SPEECH, V37, P328, DOI 10.1109/29.21701
   Weng C., 2014, P INT C AC SPEECH SI, P5669
   Weninger F, 2014, COMPUT SPEECH LANG, V28, P888, DOI 10.1016/j.csl.2014.01.001
   Xue J., 2013, P ANN C INT SPEECH C
   Xue S., 2014, P INT C AC SPEECH SI, P6389
   Yu D., 2013, FEATURE LEARNING DEE
   Yu D., 2010, P NEUR INF PROC SYST
   Yu D., 2012, P ANN C INT SPEECH C
   Yu D, 2007, INT CONF ACOUST SPEE, P1137
   Yu D, 2013, INT CONF ACOUST SPEE, P7893, DOI 10.1109/ICASSP.2013.6639201
   Yu D, 2013, IEEE T AUDIO SPEECH, V21, P388, DOI 10.1109/TASL.2012.2227738
   Yu D, 2008, COMPUT SPEECH LANG, V22, P415, DOI 10.1016/j.csl.2008.03.002
NR 98
TC 0
Z9 0
U1 0
U2 0
PU SPRINGER-VERLAG BERLIN
PI BERLIN
PA HEIDELBERGER PLATZ 3, D-14197 BERLIN, GERMANY
SN 1860-4862
BN 978-1-4471-5779-3; 978-1-4471-5778-6
J9 SIGNALS COMMUN TECHN
PY 2015
BP 299
EP 315
DI 10.1007/978-1-4471-5779-3_15
D2 10.1007/978-1-4471-5779-3
PG 17
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
SC Computer Science; Engineering
GA BD0XH
UT WOS:000357814700017
DA 2020-02-19
ER

PT J
AU Lv, Q
   Dou, Y
   Niu, X
   Xu, JQ
   Xu, JB
   Xia, F
AF Lv, Qi
   Dou, Yong
   Niu, Xin
   Xu, Jiaqing
   Xu, Jinbo
   Xia, Fei
TI Urban Land Use and Land Cover Classification Using Remotely Sensed SAR
   Data through Deep Belief Networks
SO JOURNAL OF SENSORS
LA English
DT Article
ID IMAGES; SEGMENTATION; AREAS; MODEL; ALGORITHM; MACHINES; TEXTURE; FUSION
AB Land use and land cover (LULC) mapping in urban areas is one of the core applications in remote sensing, and it plays an important role in modern urban planning and management. Deep learning is springing up in the field of machine learning recently. By mimicking the hierarchical structure of the human brain, deep learning can gradually extract features from lower level to higher level. The Deep Belief Networks (DBN) model is a widely investigated and deployed deep learning architecture. It combines the advantages of unsupervised and supervised learning and can archive good classification performance. This study proposes a classification approach based on the DBN model for detailed urban mapping using polarimetric synthetic aperture radar (PolSAR) data. Through the DBN model, effective contextual mapping features can be automatically extracted from the PolSAR data to improve the classification performance. Two-date high-resolution RADARSAT-2 PolSAR data over the Great Toronto Area were used for evaluation. Comparisons with the support vector machine (SVM), conventional neural networks (NN), and stochastic Expectation-Maximization (SEM) were conducted to assess the potential of the DBN-based classification approach. Experimental results show that the DBN-based method outperforms three other approaches and produces homogenous mapping results with preserved shape details.
C1 [Lv, Qi; Dou, Yong; Niu, Xin] Natl Univ Def Technol, Sci & Technol Parallel & Distributed Proc Lab, Changsha 410073, Hunan, Peoples R China.
   [Lv, Qi; Dou, Yong; Niu, Xin; Xu, Jiaqing; Xu, Jinbo] Natl Univ Def Technol, Sch Comp, Changsha 410073, Hunan, Peoples R China.
   [Xia, Fei] Naval Univ Engn, Elect Engn Coll, Wuhan 430033, Peoples R China.
RP Lv, Q (reprint author), Natl Univ Def Technol, Sci & Technol Parallel & Distributed Proc Lab, Changsha 410073, Hunan, Peoples R China.
EM lvqi@nudt.edu.cn
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [U1435219, 61125201, 61202126, 61202127, 61402507]
FX This work is mainly supported by the National Natural Science Foundation
   of China under Grants U1435219, 61125201, 61202126, 61202127, and
   61402507.
CR Akbari V, 2013, IEEE T GEOSCI REMOTE, V51, P2442, DOI 10.1109/TGRS.2012.2211367
   Arel I, 2010, IEEE COMPUT INTELL M, V5, P13, DOI 10.1109/MCI.2010.938364
   Ban YF, 2013, IEEE T GEOSCI REMOTE, V51, P1998, DOI 10.1109/TGRS.2012.2236560
   Bengio Yoshua, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P437, DOI 10.1007/978-3-642-35289-8_26
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Benz UC, 2004, ISPRS J PHOTOGRAMM, V58, P239, DOI 10.1016/j.isprsjprs.2003.10.002
   Blaschke T, 2010, ISPRS J PHOTOGRAMM, V65, P2, DOI 10.1016/j.isprsjprs.2009.06.004
   Bombrun L, 2011, IEEE T GEOSCI REMOTE, V49, P726, DOI 10.1109/TGRS.2010.2060730
   Chamundeeswari VV, 2009, IEEE GEOSCI REMOTE S, V6, P214, DOI 10.1109/LGRS.2008.2009954
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Dekker RJ, 2003, IEEE T GEOSCI REMOTE, V41, P1950, DOI 10.1109/TGRS.2003.814628
   Ersahin K, 2010, IEEE T GEOSCI REMOTE, V48, P164, DOI 10.1109/TGRS.2009.2024303
   Gleich D, 2012, IEEE J-STARS, V5, P952, DOI 10.1109/JSTARS.2011.2179524
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hinton GE., 2010, PRACTICAL GUIDE TRAI
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hoekman DH, 2011, IEEE J-STARS, V4, P402, DOI 10.1109/JSTARS.2010.2042280
   Jones N, 2014, NATURE, V505, P146, DOI 10.1038/505146a
   Larochelle H, 2009, J MACH LEARN RES, V10, P1
   Le Roux N, 2008, NEURAL COMPUT, V20, P1631, DOI 10.1162/neco.2008.04-07-510
   Liu B, 2013, IEEE T GEOSCI REMOTE, V51, P907, DOI 10.1109/TGRS.2012.2203358
   Lombardo P, 2003, IEEE T GEOSCI REMOTE, V41, P1959, DOI 10.1109/TGRS.2003.814632
   Lopes N, 2014, PATTERN RECOGN, V47, P114, DOI 10.1016/j.patcog.2013.06.029
   Moreira A, 2013, IEEE GEOSC REM SEN M, V1, P6, DOI 10.1109/MGRS.2013.2248301
   Myint SW, 2011, REMOTE SENS ENVIRON, V115, P1145, DOI 10.1016/j.rse.2010.12.017
   Niu X, 2013, CAN J REMOTE SENS, V39, P138, DOI 10.5589/m13-019
   Niu X, 2013, INT J REMOTE SENS, V34, P1, DOI 10.1080/01431161.2012.700133
   Niu X, 2012, IEEE J-STARS, V5, P1129, DOI 10.1109/JSTARS.2012.2201448
   Pellizzeri TM, 2003, ISPRS J PHOTOGRAMM, V58, P55, DOI 10.1016/S0924-2716(03)00017-0
   THOMPSON WD, 1988, J CLIN EPIDEMIOL, V41, P949, DOI 10.1016/0895-4356(88)90031-5
   Voisin A, 2013, IEEE GEOSCI REMOTE S, V10, P96, DOI 10.1109/LGRS.2012.2193869
   Waske B, 2007, IEEE T GEOSCI REMOTE, V45, P3858, DOI 10.1109/TGRS.2007.898446
   Wu YH, 2008, IEEE GEOSCI REMOTE S, V5, P668, DOI 10.1109/LGRS.2008.2002263
   Yu D, 2011, IEEE SIGNAL PROC MAG, V28, P145, DOI 10.1109/MSP.2010.939038
   Yu P, 2012, IEEE T GEOSCI REMOTE, V50, P1302, DOI 10.1109/TGRS.2011.2164085
NR 37
TC 32
Z9 32
U1 5
U2 80
PU HINDAWI LTD
PI LONDON
PA ADAM HOUSE, 3RD FLR, 1 FITZROY SQ, LONDON, W1T 5HF, ENGLAND
SN 1687-725X
EI 1687-7268
J9 J SENSORS
JI J. Sens.
PY 2015
AR 538063
DI 10.1155/2015/538063
PG 10
WC Engineering, Electrical & Electronic; Instruments & Instrumentation
SC Engineering; Instruments & Instrumentation
GA CO6CW
UT WOS:000359245800001
OA DOAJ Gold
DA 2020-02-19
ER

PT J
AU Liu, YZ
   Lasang, P
   Siegel, M
   Sun, QS
AF Liu, Yazhou
   Lasang, Pongsak
   Siegel, Mel
   Sun, Quansen
TI Geodesic Invariant Feature: A Local Descriptor in Depth
SO IEEE TRANSACTIONS ON IMAGE PROCESSING
LA English
DT Article
DE Body parts recognition; pose recognition; depth image; deep learning;
   superpixel
ID BINARY PATTERNS; REPRESENTATION; POSE; SCALE; SIFT; PCA
AB Different from the photometric images, depth images resolve the distance ambiguity of the scene, while the properties, such as weak texture, high noise, and low resolution, may limit the representation ability of the well-developed descriptors, which are elaborately designed for the photometric images. In this paper, a novel depth descriptor, geodesic invariant feature (GIF), is presented for representing the parts of the articulate objects in depth images. GIF is a multilevel feature representation framework, which is proposed based on the nature of depth images. Low-level, geodesic gradient is introduced to obtain the invariance to the articulate motion, such as scale and rotation variation. Midlevel, superpixel clustering is applied to reduce depth image redundancy, resulting in faster processing speed and better robustness to noise. High-level, deep network is used to exploit the nonlinearity of the data, which further improves the classification accuracy. The proposed descriptor is capable of encoding the local structures in the depth data effectively and efficiently. Comparisons with the state-of-the-art methods reveal the superiority of the proposed method.
C1 [Liu, Yazhou; Sun, Quansen] Nanjing Inst Sci & Technol, Dept Comp Sci & Engn, Nanjing 210048, Jiangsu, Peoples R China.
   [Lasang, Pongsak] Panason Res & Dev Ctr Singapore, Singapore 639798, Singapore.
   [Siegel, Mel] Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA.
RP Liu, YZ (reprint author), Nanjing Inst Sci & Technol, Dept Comp Sci & Engn, Nanjing 210048, Jiangsu, Peoples R China.
EM yazhouliu@njust.edu.cn; pongsak.lasang@sg.panasoni.com; mws@andew.edu;
   sunquansen@nust.edu.cn
RI Siegel, Mel/J-2424-2013
FU Program of Introducing Talents of Discipline to Universities [B13022];
   Ministry of Education, ChinaMinistry of Education, China
   [20133219120033]; Open Project Program through the Jiangsu Key
   Laboratory of Image and Video Understanding for Social Safety
   [JSKL201306]; National Natural Science Foundation of ChinaNational
   Natural Science Foundation of China [61273251, 61300161, 61371168]
FX This work was supported in part by the Program of Introducing Talents of
   Discipline to Universities under Grant B13022, in part by the Doctoral
   Fund through the Ministry of Education, China, under Grant
   20133219120033, in part by the Open Project Program through the Jiangsu
   Key Laboratory of Image and Video Understanding for Social Safety under
   Grant JSKL201306, and in part by the National Natural Science Foundation
   of China under Grant 61273251, Grant 61300161, and Grant 61371168. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Dimitrios Tzovaras.
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   Arel I, 2010, IEEE COMPUT INTELL M, V5, P13, DOI 10.1109/MCI.2010.938364
   Baak A, 2011, IEEE I CONF COMP VIS, P1092, DOI 10.1109/ICCV.2011.6126356
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Bayramoglu Neslihan, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P352, DOI 10.1109/ICPR.2010.95
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Bengio Yoshua, 2007, ADV NEURAL INFORM PR
   Bergstra J., 2010, P 9 PYTH SCI C, P1
   Bordes A., 2012, INT C ART INT STAT, P127
   Calonder Michael, 2010, Computer Vision - ECCV 2010. Proceedings 11th European Conference on Computer Vision, P778, DOI 10.1007/978-3-642-15561-1_56
   Calonder M, 2012, IEEE T PATTERN ANAL, V34, P1281, DOI 10.1109/TPAMI.2011.222
   Chen J, 2013, IEEE T IMAGE PROCESS, V22, P326, DOI 10.1109/TIP.2012.2210234
   Chen J, 2010, IEEE T PATTERN ANAL, V32, P1705, DOI 10.1109/TPAMI.2009.155
   Dalal N, 2005, PROC CVPR IEEE, P886
   Demirdjian D, 2005, IEEE I CONF COMP VIS, P357
   Duy-Nguyen Ta, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2937, DOI 10.1109/CVPRW.2009.5206831
   Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Ganapathi V, 2012, LECT NOTES COMPUT SC, V7577, P738, DOI 10.1007/978-3-642-33783-3_53
   Ganapathi V, 2010, PROC CVPR IEEE, P755, DOI 10.1109/CVPR.2010.5540141
   Girshick R, 2011, IEEE I CONF COMP VIS, P415, DOI 10.1109/ICCV.2011.6126270
   Glorot X., 2010, JLMR P TRACK, p[249, 2010], DOI DOI 10.1177/1753193409103364.
   GOODALE MA, 1992, TRENDS NEUROSCI, V15, P20, DOI 10.1016/0166-2236(92)90344-8
   Helten T, 2013, 2013 INTERNATIONAL CONFERENCE ON 3D VISION (3DV 2013), P279, DOI 10.1109/3DV.2013.44
   Hoffmann H, 2007, PATTERN RECOGN, V40, P863, DOI 10.1016/j.patcog.2006.07.009
   Hong XP, 2014, IEEE T IMAGE PROCESS, V23, P2557, DOI 10.1109/TIP.2014.2316640
   Huynh T, 2012, P AS C COMP VIS, P133, DOI DOI 10.1007/978-3-642-37410-412
   Izadi S, 2011, P UIST, P559, DOI DOI 10.1145/2047196.2047270
   Jiu MY, 2014, PATTERN RECOGN LETT, V50, P122, DOI 10.1016/j.patrec.2013.09.021
   Kavukcuoglu K., 2010, ADV NEURAL INFORM PR
   Ke Y, 2004, PROC CVPR IEEE, P506
   Krizhevsky A., 2012, ADV NEURAL INFORM PR
   Lallemand J, 2013, 2013 INTERNATIONAL CONFERENCE ON 3D VISION (3DV 2013), P271, DOI 10.1109/3DV.2013.43
   Lepetit V, 2006, IEEE T PATTERN ANAL, V28, P1465, DOI 10.1109/TPAMI.2006.188
   Li YM, 2003, IMAGE VISION COMPUT, V21, P1077, DOI 10.1016/j.imavis.2003.08.010
   Lo TWR, 2009, COMPUT VIS IMAGE UND, V113, P1235, DOI 10.1016/j.cviu.2009.06.005
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mika S., 1999, Neural Networks for Signal Processing IX: Proceedings of the 1999 IEEE Signal Processing Society Workshop (Cat. No.98TH8468), P41, DOI 10.1109/NNSP.1999.788121
   Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188
   Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378
   Ozuysal M, 2010, IEEE T PATTERN ANAL, V32, P448, DOI 10.1109/TPAMI.2009.23
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Plagemann C, 2010, IEEE INT CONF ROBOT, P3108, DOI 10.1109/ROBOT.2010.5509559
   Rahmani R, 2008, IEEE T PATTERN ANAL, V30, P1902, DOI 10.1109/TPAMI.2008.112
   Rosten E, 2006, LECT NOTES COMPUT SC, V3951, P430
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467
   Shen XH, 2013, PROC CVPR IEEE, P3460, DOI 10.1109/CVPR.2013.444
   Shotton J, 2013, IEEE T PATTERN ANAL, V35, P2821, DOI 10.1109/TPAMI.2012.241
   Shotton J, 2011, PROC CVPR IEEE, P1297, DOI 10.1109/CVPR.2011.5995316
   Socher R., 2011, P C EMP METH NAT LAN, P151
   Socher Richard, 2011, ADV NEURAL INFORM PR
   Subrahmanyam M, 2012, SIGNAL PROCESS, V92, P1467, DOI 10.1016/j.sigpro.2011.12.005
   Tang B, 2013, IEEE SYS MAN CYBERN, P1, DOI 10.1109/SMC.2013.8
   Torralba A, 2004, PROC CVPR IEEE, P762
   Valle E., 2008, THESIS U CERGY PONTO
   Vincent P, 2008, P 25 INT C MACH LEAR, P1096, DOI DOI 10.1145/1390156.1390294
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Wang XY, 2009, IEEE I CONF COMP VIS, P32, DOI 10.1109/ICCV.2009.5459207
   Yang X, 2012, P 20 ACM INT C MULT, P1057, DOI DOI 10.1145/2393347.2396382
   Ye M, 2014, PROC CVPR IEEE, P2353, DOI 10.1109/CVPR.2014.301
   Zhang WC, 2005, IEEE I CONF COMP VIS, P786
   Zhao GY, 2007, IEEE T PATTERN ANAL, V29, P915, DOI 10.1109/TPAMI.2007.1110
NR 65
TC 5
Z9 6
U1 0
U2 24
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1057-7149
EI 1941-0042
J9 IEEE T IMAGE PROCESS
JI IEEE Trans. Image Process.
PD JAN
PY 2015
VL 24
IS 1
BP 236
EP 248
DI 10.1109/TIP.2014.2378019
PG 13
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
SC Computer Science; Engineering
GA AX2RX
UT WOS:000346792900001
PM 25494504
DA 2020-02-19
ER

PT J
AU Brosch, T
   Tam, R
AF Brosch, Tom
   Tam, Roger
TI Efficient Training of Convolutional Deep Belief Networks in the
   Frequency Domain for Application to High-Resolution 2D and 3D Images
SO NEURAL COMPUTATION
LA English
DT Article
AB Deep learning has traditionally been computationally expensive, and advances in training methods have been the prerequisite for improving its efficiency in order to expand its application to a variety of image classification problems. In this letter, we address the problem of efficient training of convolutional deep belief networks by learning the weights in the frequency domain, which eliminates the time-consuming calculation of convolutions. An essential consideration in the design of the algorithm is to minimize the number of transformations to and from frequency space. We have evaluated the running time improvements using two standard benchmark data sets, showing a speed-up of up to 8times on 2D images and up to 200times on 3D volumes. Our training algorithm makes training of convolutional deep belief networks on 3D medical images with a resolution of up to 128 x 128 x 128 voxels practical, which opens new directions for using deep learning for medical image analysis.
C1 [Brosch, Tom; Tam, Roger] MS MRI Res Grp, Vancouver, BC V6T 2B5, Canada.
   [Brosch, Tom] Univ British Columbia, Dept Elect & Comp Engn, Vancouver, BC V6T 1Z4, Canada.
   [Tam, Roger] Univ British Columbia, Dept Radiol, Vancouver, BC V5Z 1M9, Canada.
RP Brosch, T (reprint author), MS MRI Res Grp, Vancouver, BC V6T 2B5, Canada.
EM brosch.tom@gmail.com; roger.tam@ubc.ca
FU NCRR NIH HHSUnited States Department of Health & Human ServicesNational
   Institutes of Health (NIH) - USANIH National Center for Research
   Resources (NCRR) [U24 RR021382]; NIA NIH HHSUnited States Department of
   Health & Human ServicesNational Institutes of Health (NIH) - USANIH
   National Institute on Aging (NIA) [P01 AG03991, R01 AG021910, P50
   AG05681]; NIMH NIH HHSUnited States Department of Health & Human
   ServicesNational Institutes of Health (NIH) - USANIH National Institute
   of Mental Health (NIMH) [P20 MH071616]
CR Cruz-Roa AA, 2013, LECT NOTES COMPUT SC, V8150, P403, DOI 10.1007/978-3-642-40763-5_50
   Brosch T, 2013, LECT NOTES COMPUT SC, V8150, P633, DOI 10.1007/978-3-642-40763-5_78
   Ciresan D. C, 2012, ADV NEURAL INFORM PR, P1
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Hinton G. E, 2012, ARXIV12070580
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Krizhevsky A., 2012, HIGH PERFORMANCE C C
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lee H., 2009, P 26 ANN INT C MACH, P609, DOI DOI 10.1145/1553374.1553453
   Lee H, 2011, COMMUN ACM, V54, P95, DOI 10.1145/2001269.2001295
   Lee N, 2011, I S BIOMED IMAGING, P321, DOI 10.1109/ISBI.2011.5872414
   Liao S, 2013, LECT NOTES COMPUT SC, V8150, P254, DOI 10.1007/978-3-642-40763-5_32
   Marcus DS, 2007, J COGNITIVE NEUROSCI, V19, P1498, DOI 10.1162/jocn.2007.19.9.1498
   Mathieu M., 2014, P INT C LEARN REPR, P1
   Nair V, 2010, ICML, V27, P807, DOI DOI 10.0RG/PAPERS/432.PDF
   Raina R, 2009, P 26 INT C MACH LEAR
   Scherer D, 2010, LECT NOTES COMPUT SC, V6354, P92, DOI 10.1007/978-3-642-15825-4_10
   Wu GR, 2013, LECT NOTES COMPUT SC, V8150, P649, DOI 10.1007/978-3-642-40763-5_80
   Zeiler M., 2013, INT C LEARN REPR ICL, P1
NR 20
TC 28
Z9 28
U1 1
U2 54
PU MIT PRESS
PI CAMBRIDGE
PA ONE ROGERS ST, CAMBRIDGE, MA 02142-1209 USA
SN 0899-7667
EI 1530-888X
J9 NEURAL COMPUT
JI Neural Comput.
PD JAN
PY 2015
VL 27
IS 1
BP 211
EP 227
DI 10.1162/NECO_a_00682
PG 17
WC Computer Science, Artificial Intelligence; Neurosciences
SC Computer Science; Neurosciences & Neurology
GA AX2GY
UT WOS:000346762700009
PM 25380341
DA 2020-02-19
ER

PT J
AU Hu, W
   Huang, YY
   Wei, L
   Zhang, F
   Li, HC
AF Hu, Wei
   Huang, Yangyu
   Wei, Li
   Zhang, Fan
   Li, Hengchao
TI Deep Convolutional Neural Networks for Hyperspectral Image
   Classification
SO JOURNAL OF SENSORS
LA English
DT Article
AB Recently, convolutional neural networks have demonstrated excellent performance on various visual tasks, including the classification of common two-dimensional images. In this paper, deep convolutional neural networks are employed to classify hyperspectral images directly in spectral domain. More specifically, the architecture of the proposed classifier contains five layers with weights which are the input layer, the convolutional layer, the max pooling layer, the full connection layer, and the output layer. These five layers are implemented on each spectral signature to discriminate against others. Experimental results based on several hyperspectral image data sets demonstrate that the proposed method can achieve better classification performance than some traditional methods, such as support vector machines and the conventional deep learning-based methods.
C1 [Hu, Wei; Huang, Yangyu; Wei, Li; Zhang, Fan] Beijing Univ Chem Technol, Coll Informat Sci & Technol, Beijing 10029, Peoples R China.
   [Li, Hengchao] Southwest Jiaotong Univ, Sichuan Prov Key Lab Informat Coding & Transmiss, Chengdu 610031, Peoples R China.
   [Li, Hengchao] Univ Colorado, Dept Aerosp Engn Sci, Boulder, CO 80309 USA.
RP Hu, W (reprint author), Beijing Univ Chem Technol, Coll Informat Sci & Technol, Beijing 10029, Peoples R China.
EM thu.wei.hu@qq.com
RI Zhang, Fan/W-3340-2019
OI Zhang, Fan/0000-0002-2058-2373
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61371165, 61302164]; 973 Program of ChinaNational
   Basic Research Program of China [2011CB706900]; Program for New Century
   Excellent Talents in UniversityProgram for New Century Excellent Talents
   in University (NCET) [NCET-11-0711]; Interdisciplinary Research Project
   in Beijing University of Chemical Technology; Beijing Higher Education
   Young Elite Teacher Project [YETP0501, YETP0500]
FX This work was supported jointly by the National Natural Science
   Foundation of China (nos. 61371165 and 61302164), the 973 Program of
   China (no. 2011CB706900), the Program for New Century Excellent Talents
   in University under Grant no. NCET-11-0711, and the Interdisciplinary
   Research Project in Beijing University of Chemical Technology. Wei Hu
   and Fan Zhang are also supported by the Beijing Higher Education Young
   Elite Teacher Project under Grant nos. YETP0501 and YETP0500,
   respectively.
CR Abdel-Hamid O, 2012, INT CONF ACOUST SPEE, P4277, DOI 10.1109/ICASSP.2012.6288864
   Atkinson PM, 1997, INT J REMOTE SENS, V18, P699, DOI 10.1080/014311697218700
   Bergstra J., 2011, P NIPS 2011 BIG LEAR, P712
   Bruzzone L, 1999, IEEE T GEOSCI REMOTE, V37, P1179, DOI 10.1109/36.752239
   Chen YS, 2014, IEEE J-STARS, V7, P2094, DOI 10.1109/JSTARS.2014.2329330
   CHOPRA S, 2005, PROC CVPR IEEE, P539, DOI DOI 10.1109/CVPR.2005.202
   Cires D. C., 2011, P 22 INT JOINT C ART, V22, P1237, DOI DOI 10.5591/978-1-57735-516-8/IJCAI11-210
   Ciresan D, 2012, PROC CVPR IEEE, P3642, DOI 10.1109/CVPR.2012.6248110
   Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231
   Foody GM, 2004, IEEE T GEOSCI REMOTE, V42, P1335, DOI 10.1109/TGRS.2004.827257
   FUKUSHIMA K, 1988, NEURAL NETWORKS, V1, P119, DOI 10.1016/0893-6080(88)90014-7
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Gualtieri JA, 2000, INT GEOSCI REMOTE SE, P813, DOI 10.1109/IGARSS.2000.861712
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   HUBEL DH, 1968, J PHYSIOL-LONDON, V195, P215, DOI 10.1113/jphysiol.1968.sp008455
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Landgrebe D, 2002, IEEE SIGNAL PROC MAG, V19, P17, DOI 10.1109/79.974718
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y, 2012, LECT NOTES COMPUT SC, V7700, P9, DOI [DOI 10.1007/978-3-642-35289-8, DOI 10.1007/3-540-49430-8_]
   LeCun Y., 1998, MNIST DATABASE HANDW
   Li J, 2013, IEEE T GEOSCI REMOTE, V51, P844, DOI 10.1109/TGRS.2012.2205263
   Li W, 2012, IEEE T GEOSCI REMOTE, V50, P1185, DOI 10.1109/TGRS.2011.2165957
   Melgani F, 2004, IEEE T GEOSCI REMOTE, V42, P1778, DOI 10.1109/TGRS.2004.831865
   Mountrakis G, 2011, ISPRS J PHOTOGRAMM, V66, P247, DOI 10.1016/j.isprsjprs.2010.11.001
   Ratle F, 2010, IEEE T GEOSCI REMOTE, V48, P2271, DOI 10.1109/TGRS.2009.2037898
   Sainath TN, 2013, INT CONF ACOUST SPEE, P8614, DOI 10.1109/ICASSP.2013.6639347
   Sermanet P, 2012, INT C PATT RECOG, P3288
   Sermanet P, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P2809, DOI 10.1109/IJCNN.2011.6033589
   Simard PY, 2003, PROC INT CONF DOC, P958
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Sutskever I, 2008, NEURAL COMPUT, V20, P2629, DOI 10.1162/neco.2008.12-07-661
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Tarabalka Y, 2009, IEEE T GEOSCI REMOTE, V47, P2973, DOI 10.1109/TGRS.2009.2016214
NR 34
TC 292
Z9 294
U1 13
U2 166
PU HINDAWI LTD
PI LONDON
PA ADAM HOUSE, 3RD FLR, 1 FITZROY SQ, LONDON, W1T 5HF, ENGLAND
SN 1687-725X
EI 1687-7268
J9 J SENSORS
JI J. Sens.
PY 2015
AR 258619
DI 10.1155/2015/258619
PG 12
WC Engineering, Electrical & Electronic; Instruments & Instrumentation
SC Engineering; Instruments & Instrumentation
GA CO6AQ
UT WOS:000359240000001
OA DOAJ Gold
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Chiachia, G
   Falcao, AX
   Pinto, N
   Rocha, A
   Cox, D
AF Chiachia, Giovani
   Falcao, Alexandre X.
   Pinto, Nicolas
   Rocha, Anderson
   Cox, David
TI Learning Person-Specific Representations From Faces in the Wild
SO IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY
LA English
DT Article
DE Face recognition; face information modeling; representation learning;
   deep learning; biologically-inspired computer vision; partial least
   squares; support vector machines
ID RECOGNITION; MODELS; PLS
AB Humans are natural face recognition experts, far out-performing current automated face recognition algorithms, especially in naturalistic, "in the wild" settings. However, a striking feature of human face recognition is that we are dramatically better at recognizing highly familiar faces, presumably because we can leverage large amounts of past experience with the appearance of an individual to aid future recognition. Meanwhile, the analogous situation in automated face recognition, where a large number of training examples of an individual are available, has been largely underexplored, in spite of the increasing relevance of this setting in the age of social media. Inspired by these observations, we propose to explicitly learn enhanced face representations on a per-individual basis, and we present two methods enabling this approach. By learning and operating within person-specific representations, we are able to significantly outperform the previous state-of-the-art on PubFig83, a challenging benchmark for familiar face recognition in the wild, using a novel method for learning representations in deep visual hierarchies. We suggest that such person-specific representations aid recognition by introducing an intermediate form of regularization to the problem.
C1 [Chiachia, Giovani; Falcao, Alexandre X.; Rocha, Anderson] Univ Estadual Campinas, Inst Comp, BR-13083970 Campinas, SP, Brazil.
   [Pinto, Nicolas; Cox, David] Harvard Univ, Cambridge, MA 02138 USA.
RP Chiachia, G (reprint author), Univ Estadual Campinas, Inst Comp, BR-13083970 Campinas, SP, Brazil.
EM chiachia@ic.unicamp.br; afalcao@ic.unicamp.br; pinto@alum.mit.edu;
   anderson.rocha@ic.unicamp.br; davidcox@fas.harvard.edu
RI Falcao, Alexandre/F-8361-2012; Cox, David/C-4888-2008
OI Cox, David/0000-0002-2189-9743
FU Fundacao de Amparo a Pesquisa do Estado de Sao PauloFundacao de Amparo a
   Pesquisa do Estado de Sao Paulo (FAPESP) [2010/00994-8, 2010/05647-4,
   2013/11359-0]; National Council for Scientific and Technological
   Development, Brasilia, BrazilNational Council for Scientific and
   Technological Development (CNPq) [303673/2010-9, 304352/2012-8,
   477662/2013-7, 479070/2013-0]; Microsoft Research, Redmond, WA, USA;
   Rowland Institute at Harvard, Cambridge, MA, USA
FX This work was supported in part by the Fundacao de Amparo a Pesquisa do
   Estado de Sao Paulo under Grant 2010/00994-8, Grant 2010/05647-4, and
   Grant 2013/11359-0, in part by the National Council for Scientific and
   Technological Development, Brasilia, Brazil, under Grant 303673/2010-9,
   Grant 304352/2012-8, Grant 477662/2013-7, and Grant 479070/2013-0, in
   part by Microsoft Research, Redmond, WA, USA, and in part by the Rowland
   Institute at Harvard, Cambridge, MA, USA. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Aly A. Farag.
CR Abdi H, 2010, WIRES COMPUT STAT, V2, P97, DOI 10.1002/wics.51
   Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228
   Bengio Yoshua, 2007, ADV NEURAL INFORM PR
   Bergstra J, 2013, P 30 INT C MACH LEAR, V28, pI
   Bingham E., 2001, KDD-2001. Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P245
   BRUCE V, 1986, BRIT J PSYCHOL, V77, P305, DOI 10.1111/j.2044-8295.1986.tb02199.x
   Burton AM, 2011, BRIT J PSYCHOL, V102, P943, DOI 10.1111/j.2044-8295.2011.02039.x
   Burton AM, 1999, PSYCHOL SCI, V10, P243, DOI 10.1111/1467-9280.00144
   Burton AM, 2005, COGNITIVE PSYCHOL, V51, P256, DOI 10.1016/j.cogpsych.2005.06.003
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chellappa R, 2010, COMPUTER, V43, P46, DOI 10.1109/MC.2010.37
   Chen D, 2013, PROC CVPR IEEE, P3025, DOI 10.1109/CVPR.2013.389
   Chiachia G, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.101
   Coates A., 2011, P 28 INT C MACH LEAR, V28, P921
   Costa FD, 2014, PATTERN RECOGN LETT, V39, P92, DOI 10.1016/j.patrec.2013.09.006
   Cox D, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P8, DOI 10.1109/FG.2011.5771385
   Cui Z, 2013, PROC CVPR IEEE, P3554, DOI 10.1109/CVPR.2013.456
   de Carlos Paulo G., 2013, P 10 IEEE INT C AUT, P1
   FUKUSHIMA K, 1980, BIOL CYBERN, V36, P193, DOI 10.1007/BF00344251
   Guo HD, 2011, INT J DIGIT EARTH, V4, P1, DOI 10.1080/17538947.2011.544077
   Heisele B, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P688, DOI 10.1109/ICCV.2001.937693
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Huang G. B., 2007, 0749 U MASS DEP COMP
   Jarrett K, 2009, IEEE I CONF COMP VIS, P2146, DOI 10.1109/ICCV.2009.5459469
   Kamarainen Joni-Kristian, 2011, HDB FACE RECOGNITION, P79
   Kembhavi A, 2011, IEEE T PATTERN ANAL, V33, P1250, DOI 10.1109/TPAMI.2010.182
   Krishna S., 2005, Advances in Biometrics. International Conference, ICB 2006. Proceedings (Lecture Notes in Computer Science Vol.3832), P182
   Krizhevsky A., 2012, ADV NEURAL INFORM PR
   Kumar N, 2009, IEEE I CONF COMP VIS, P365, DOI 10.1109/ICCV.2009.5459250
   Le Q. V., 2010, ADV NEURAL INFORM PR
   Le Q. V., 2012, P 29 INT C MACH LEAR, P81, DOI DOI 10.1109/MSP.2011.940881
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   LINDGREN F, 1995, J CHEMOMETR, V9, P331, DOI 10.1002/cem.1180090502
   Lowe D., 1999, P INT C COMP VIS, V2, P1150, DOI [10.1109/ICCV.1999.790410, DOI 10.1109/ICCV.1999.790410]
   Nguyen DV, 2002, BIOINFORMATICS, V18, P39, DOI 10.1093/bioinformatics/18.1.39
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Pinto N., 2011, 2011 IEEE COMP SOC C, P35, DOI DOI 10.1109/CVPRW.2011.5981788
   Poggio T., 2011, NATURE P
   Quiroga RQ, 2005, NATURE, V435, P1102, DOI 10.1038/nature03687
   Ranzato MA, 2007, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2007.383157
   Riesenhuber M, 1999, NAT NEUROSCI, V2, P1019
   Rosipal R, 2006, LECT NOTES COMPUT SC, V3940, P34
   Saxe A., 2011, P 28 INT C MACH LEAR, P1089
   Scheirer WJ, 2013, IEEE T PATTERN ANAL, V35, P1757, DOI 10.1109/TPAMI.2012.256
   Schwartz WR, 2012, IEEE T IMAGE PROCESS, V21, P2245, DOI 10.1109/TIP.2011.2176951
   Schwartz WR, 2009, IEEE I CONF COMP VIS, P24, DOI 10.1109/ICCV.2009.5459205
   Sinha P, 2006, P IEEE, V94, P1948, DOI 10.1109/JPROC.2006.884093
   Sivic J, 2009, PROC CVPR IEEE, P1145
   Turk M. A., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P586, DOI 10.1109/CVPR.1991.139758
   Vasilescu MAO, 2002, INT C PATT RECOG, P511, DOI 10.1109/ICPR.2002.1048350
   Wiskott L, 1997, IEEE T PATTERN ANAL, V19, P775, DOI 10.1109/34.598235
   Wold H., 1985, ENCY STAT SCI, V6, P581, DOI DOI 10.1002/0471667196
   Wolf L, 2009, IEEE I CONF COMP VIS, P897, DOI 10.1109/ICCV.2009.5459323
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Zafeiriou S, 2007, IEEE T INF FOREN SEC, V2, P55, DOI 10.1109/TIFS.2006.890308
NR 56
TC 20
Z9 21
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1556-6013
EI 1556-6021
J9 IEEE T INF FOREN SEC
JI IEEE Trans. Inf. Forensic Secur.
PD DEC
PY 2014
VL 9
IS 12
BP 2089
EP 2099
DI 10.1109/TIFS.2014.2359543
PG 11
WC Computer Science, Theory & Methods; Engineering, Electrical & Electronic
SC Computer Science; Engineering
GA CG4WX
UT WOS:000353289900007
DA 2020-02-19
ER

PT J
AU Goh, H
   Thome, N
   Cord, M
   Lim, JH
AF Goh, Hanlin
   Thome, Nicolas
   Cord, Matthieu
   Lim, Joo-Hwee
TI Learning Deep Hierarchical Visual Feature Coding
SO IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
LA English
DT Article
DE Bag-of-words (BoW) framework; computer vision; deep learning; dictionary
   learning; hierarchical visual architecture; image categorization;
   restricted Boltzmann machine (RBM); sparse feature coding; transfer
   learning
ID SPARSE; REPRESENTATIONS; EFFICIENT; RECOGNITION; CODE
AB In this paper, we propose a hybrid architecture that combines the image modeling strengths of the bag of words framework with the representational power and adaptability of learning deep architectures. Local gradient-based descriptors, such as SIFT, are encoded via a hierarchical coding scheme composed of spatial aggregating restricted Boltzmann machines (RBM). For each coding layer, we regularize the RBM by encouraging representations to fit both sparse and selective distributions. Supervised fine-tuning is used to enhance the quality of the visual representation for the categorization task. We performed a thorough experimental evaluation using three image categorization data sets. The hierarchical coding scheme achieved competitive categorization accuracies of 79.7% and 86.4% on the Caltech-101 and 15-Scenes data sets, respectively. The visual representations learned are compact and the model's inference is fast, as compared with sparse coding methods. The low-level representations of descriptors that were learned using this method result in generic features that we empirically found to be transferrable between different image data sets. Further analysis reveal the significance of supervised fine-tuning when the architecture has two layers of representations as opposed to a single layer.
C1 [Goh, Hanlin; Lim, Joo-Hwee] Agcy Sci Technol & Res, Inst Infocomm Res, Singapore 119613, Singapore.
   [Goh, Hanlin; Lim, Joo-Hwee] Image & Pervas Access Lab, Singapore 138632, Singapore.
   [Thome, Nicolas; Cord, Matthieu] Sorbonne Univ, Univ Paris 06, Lab Informat Paris 6, F-75005 Paris, France.
RP Goh, H (reprint author), Agcy Sci Technol & Res, Inst Infocomm Res, Singapore 119613, Singapore.
EM hlgoh@i2r.a-star.edu.sg; nicolas.thome@lip6.fr; matthieu.cord@lip6.fr;
   joohwee@i2r.a-star.edu.sg
FU French Embassy in Singapore
FX This work was supported in part by the French Embassy in Singapore
   through the Merlion Project and Ph.D. Program from 2010 to 2012.
CR Avila S, 2013, COMPUT VIS IMAGE UND, V117, P453, DOI 10.1016/j.cviu.2012.09.007
   Barlow HB, 1989, NEURAL COMPUT, V1, P295, DOI 10.1162/neco.1989.1.3.295
   Bengio Y, 2006, ADV NEURAL INFORM PR, P153
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   BLACKWELL D, 1947, ANN MATH STAT, V18, P105, DOI 10.1214/aoms/1177730497
   Bo LF, 2013, PROC CVPR IEEE, P660, DOI 10.1109/CVPR.2013.91
   Boiman O, 2008, PROC CVPR IEEE, P1992, DOI 10.1109/CVPR.2008.4587598
   Boureau Y., 2011, P ICCV, P1
   Boureau Y., 2010, P 27 INT C MACH LEAR, P111, DOI DOI 10.1016/J.NEUNET.2012.02.023
   Boureau YL, 2010, PROC CVPR IEEE, P2559, DOI 10.1109/CVPR.2010.5539963
   Dalal N, 2005, PROC CVPR IEEE, P886
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Duchenne O, 2011, IEEE I CONF COMP VIS, P1792, DOI 10.1109/ICCV.2011.6126445
   Fei-Fei L., 2004, P CVPR WORKSH
   Feng J., 2011, P IEEE C COMP VIS PA, P2609
   Foldiak P, 2009, CURR BIOL, V19, pR904, DOI 10.1016/j.cub.2009.08.020
   Gog H. G., 2010, P 7 IEEE CONS COMM N, P1
   Goh H., 2013, P NIPS
   Goh H, 2012, LECT NOTES COMPUT SC, V7576, P298, DOI 10.1007/978-3-642-33715-4_22
   Goh H, 2011, IEEE IMAGE PROC, P1241, DOI 10.1109/ICIP.2011.6115657
   Griffin G., 2007, 7694 CALTECH
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton G. E., 2010, TR2010003 UTML DEP C
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hinton GE, 2007, PROG BRAIN RES, V165, P535, DOI 10.1016/S0079-6123(06)65034-6
   HORNIK K, 1991, NEURAL NETWORKS, V4, P251, DOI 10.1016/0893-6080(91)90009-T
   Hyvarinen A, 2001, VISION RES, V41, P2413, DOI 10.1016/S0042-6989(01)00114-6
   Jegou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039
   Jiang ZL, 2011, PROC CVPR IEEE, P1697, DOI 10.1109/CVPR.2011.5995354
   Kavukcuoglu K., 2010, P NIPS, V1-9
   Kavukcuoglu K, 2009, PROC CVPR IEEE, P1605, DOI 10.1109/CVPRW.2009.5206545
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Larochelle H., 2008, P 25 INT C MACH LEAR, P536, DOI [10.1145/1390156.1390224, DOI 10.1145/1390156.1390224]
   Lazebnik S., 2006, P IEEE INT C COMP VI, V2, P2169, DOI DOI 10.1109/CVPR.2006.68
   Lazebnik S, 2009, IEEE T PATTERN ANAL, V31, P1294, DOI 10.1109/TPAMI.2008.138
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y., 1985, P COGNITIVA, V85, P599
   Lee H., 2009, P 26 ANN INT C MACH, P609, DOI DOI 10.1145/1553374.1553453
   Lee HY, 2008, INT EL DEVICES MEET, P297
   Liu LQ, 2011, IEEE I CONF COMP VIS, P2486, DOI 10.1109/ICCV.2011.6126534
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mairal J., 2008, P ADV NEUR INF PROC, P1033
   McCann S., 2012, P ACCV, P204
   Mitchell T. M., 1980, CBMTR117 RUTG U DEP
   Mutch J, 2008, INT J COMPUT VISION, V80, P45, DOI 10.1007/s11263-007-0118-0
   Nair V, 2009, ADV NEURAL INF PROCE, V22, P1339
   Ngiam J., 2011, P NIPS, P17
   Oliveira GL, 2012, IEEE INT CONF ROBOT, P2592, DOI 10.1109/ICRA.2012.6224785
   Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0
   Perronnin F., 2007, P IEEE COMP SOC C CO, P1, DOI DOI 10.1109/CVPR.2007.383266
   Raina R., 2007, LEARNING, P759, DOI DOI 10.1145/1273496.1273592
   Ranzato MA, 2007, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2007.383157
   Riesenhuber M, 1999, NAT NEUROSCI, V2, P1019
   Rolls ET, 1990, NETWORK-COMP NEURAL, V1, P407, DOI 10.1088/0954-898X/1/4/002
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Serre T, 2007, IEEE T PATTERN ANAL, V29, P411, DOI 10.1109/TPAMI.2007.56
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470
   Smolensky P, 1986, PARALLEL DISTRIBUTED, P194
   Sohn K, 2011, IEEE I CONF COMP VIS, P2643, DOI 10.1109/ICCV.2011.6126554
   Swersky K., 2010, INF THEOR APPL WORKS, P1, DOI DOI 10.1109/ITA.2010.5454138
   Theriault C, 2013, IEEE T IMAGE PROCESS, V22, P764, DOI 10.1109/TIP.2012.2222900
   Tuytelaars T, 2011, IEEE I CONF COMP VIS, P1824, DOI 10.1109/ICCV.2011.6126449
   van Gemert JC, 2010, IEEE T PATTERN ANAL, V32, P1271, DOI 10.1109/TPAMI.2009.132
   Vapnik VN, 1995, NATURE STAT LEARNING
   Vincent P, 2008, P 25 INT C MACH LEAR, P1096, DOI DOI 10.1145/1390156.1390294
   Wang JJ, 2010, PROC CVPR IEEE, P3360, DOI 10.1109/CVPR.2010.5540018
   Welling M., 2003, P NIPS, P1
   Willmore B, 2001, NETWORK-COMP NEURAL, V12, P255, DOI 10.1088/0954-898X/12/3/302
   Yang JC, 2010, LECT NOTES COMPUT SC, V6315, P113, DOI 10.1007/978-3-642-15555-0_9
   Yang JC, 2010, PROC CVPR IEEE, P3517, DOI 10.1109/CVPR.2010.5539958
   Yang JC, 2009, PROC CVPR IEEE, P1794, DOI 10.1109/CVPRW.2009.5206757
   Yang L., 2008, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/DYSPAN.2008.47
   Yu K, 2011, PROC CVPR IEEE, P1713, DOI 10.1109/CVPR.2011.5995732
   Zeiler MD, 2010, PROC CVPR IEEE, P2528, DOI 10.1109/CVPR.2010.5539957
   Zhou X, 2010, LECT NOTES COMPUT SC, V6315, P141, DOI 10.1007/978-3-642-15555-0_11
NR 77
TC 49
Z9 52
U1 2
U2 41
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 2162-237X
EI 2162-2388
J9 IEEE T NEUR NET LEAR
JI IEEE Trans. Neural Netw. Learn. Syst.
PD DEC
PY 2014
VL 25
IS 12
BP 2212
EP 2225
DI 10.1109/TNNLS.2014.2307532
PG 14
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Computer Science, Theory & Methods; Engineering,
   Electrical & Electronic
SC Computer Science; Engineering
GA AU3MZ
UT WOS:000345518900008
PM 25420244
DA 2020-02-19
ER

PT J
AU Jiu, MY
   Wolf, C
   Taylor, G
   Baskurt, A
AF Jiu, Mingyuan
   Wolf, Christian
   Taylor, Graham
   Baskurt, Atilla
TI Human body part estimation from depth images via spatially-constrained
   deep learning
SO PATTERN RECOGNITION LETTERS
LA English
DT Article
DE Segmentation; Spatial layout; Deep learning; Convolutional networks;
   Depth images
AB Object recognition, human pose estimation and scene recognition are applications which are frequently solved through a decomposition into a collection of parts. The resulting local representation has significant advantages, especially in the case of occlusions and when the subject is non-rigid. Detection and recognition require modelling the appearance of the different object parts as well as their spatial layout. This representation has been particularly successful in body part estimation from depth images.
   Integrating the spatial layout of parts may require the minimization of complex energy functions. This is prohibitive in most real world applications and therefore often omitted. However, ignoring the spatial layout puts all the burden on the classifier, whose only available information is local appearance. We propose a new method to integrate spatial layout into parts classification without costly pairwise terms during testing. Spatial relationships are exploited in the training algorithm, but not during testing. As with competing methods, the proposed method classifies pixels independently, which makes real-time processing possible. We show that training a classifier with spatial relationships increases generalization performance when compared to classical training minimizing classification error on the training set. We present an application to human body part estimation from depth images. (C) 2013 Elsevier B.V. All rights reserved.
C1 [Jiu, Mingyuan; Wolf, Christian; Baskurt, Atilla] Univ Lyon, CNRS, Lyon, France.
   [Jiu, Mingyuan; Wolf, Christian; Baskurt, Atilla] INSA Lyon, LIRIS, UMR5205, Villeurbanne, France.
   [Taylor, Graham] Univ Guelph, Sch Engn, Guelph, ON N1G 2W1, Canada.
RP Jiu, MY (reprint author), INSA Lyon, LIRIS, UMR5205, Villeurbanne, France.
EM mingyuan.jiu@liris.cnrs.fr; christian.wolf@liris.cnrs.fr;
   gwtaylor@uoguelph.ca; atilla.baskurt@liris.cnrs.fr
CR Burges C. J. C., 2005, ICML
   Cox T., 1994, MULTIDIMENSIONAL SCA
   Dalai N., 2005, CVPR
   Dekel O., 2004, NIPS
   Farabet C., 2012, ICML
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Felzenszwalb PF, 2005, INT J COMPUT VISION, V61, P55, DOI 10.1023/B:VISI.0000042934.15159.49
   Freund Y., 2003, J MACHINE LEARNING R, V4, P933, DOI DOI 10.1162/JMLR.2003.4.6.933
   Goldberger J., 2004, NIPS
   Grangier G., 2009, ICML DEEP LEARN WORK
   Hadsell R., 2006, IEEE C COMP VIS PATT, V2, P1735, DOI DOI 10.1109/CVPR.2006.100
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Holt B., 2011, ICCV WORKSH CONS DEP
   Jarrett K., 2009, ICCV
   Jiu M., 2013, P C COMP VIS THEOR A
   JOLLIFFE T, 1986, PRINCIPAL COMPONENT
   Kolmogorov V, 2004, IEEE T PATTERN ANAL, V26, P147, DOI 10.1109/TPAMI.2004.1262177
   Krizhevsky A., 2012, NIPS
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Salakhutdinov R., 2007, AISTATS, V11
   Shotton J., 2011, CVPR
   Taylor G. W., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2729, DOI 10.1109/CVPR.2011.5995538
   Turaga SC, 2010, NEURAL COMPUT, V22, P511, DOI 10.1162/neco.2009.10-08-881
   Winn J., 2006, P IEEE C COMP VIS PA, V1, P37
NR 25
TC 15
Z9 15
U1 0
U2 37
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0167-8655
EI 1872-7344
J9 PATTERN RECOGN LETT
JI Pattern Recognit. Lett.
PD DEC 1
PY 2014
VL 50
SI SI
BP 122
EP 129
DI 10.1016/j.patrec.2013.09.021
PG 8
WC Computer Science, Artificial Intelligence
SC Computer Science
GA AS7HT
UT WOS:000344428300014
DA 2020-02-19
ER

PT J
AU Xue, SF
   Abdel-Hamid, O
   Jiang, H
   Dai, LR
   Liu, QF
AF Xue, Shaofei
   Abdel-Hamid, Ossama
   Jiang, Hui
   Dai, Lirong
   Liu, Qingfeng
TI Fast Adaptation of Deep Neural Network Based on Discriminant Codes for
   Speech Recognition
SO IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING
LA English
DT Article
DE Condition code; cross entropy (CE); deep neural network (DNN); fast
   adaptation; maximum mutual information (MMI); speaker code
ID SPEAKER ADAPTATION; FEATURES
AB Fast adaptation of deep neural networks (DNN) is an important research topic in deep learning. In this paper, we have proposed a general adaptation scheme for DNN based on discriminant condition codes, which are directly fed to various layers of a pre-trained DNN through a new set of connection weights. Moreover, we present several training methods to learn connection weights from training data as well as the corresponding adaptation methods to learn new condition code from adaptation data for each new test condition. In this work, the fast adaptation scheme is applied to supervised speaker adaptation in speech recognition based on either frame-level cross-entropy or sequence-level maximum mutual information training criterion. We have proposed three different ways to apply this adaptation scheme based on the so-called speaker codes: i) Nonlinear feature normalization in feature space; ii) Direct model adaptation of DNN based on speaker codes; iii) Joint speaker adaptive training with speaker codes. We have evaluated the proposed adaptation methods in two standard speech recognition tasks, namely TIMIT phone recognition and large vocabulary speech recognition in the Switchboard task. Experimental results have shown that all three methods are quite effective to adapt large DNN models using only a small amount of adaptation data. For example, the Switchboard results have shown that the proposed speaker-code-based adaptation methods may achieve up to 8-10% relative error reduction using only a few dozens of adaptation utterances per speaker. Finally, we have achieved very good performance in Switchboard (12.1% in WER) after speaker adaptation using sequence training criterion, which is very close to the best performance reported in this task ("Deep convolutional neural networks for LVCSR," T. N. Sainath et al., Proc. IEEE Acoust., Speech, Signal Process., 2013).
C1 [Xue, Shaofei; Dai, Lirong; Liu, Qingfeng] Univ Sci & Technol China, Natl Engn Lab Speech & Language Informat Proc, Hefei 230026, Peoples R China.
   [Abdel-Hamid, Ossama; Jiang, Hui] York Univ, Dept Elect Engn & Comp Sci, Toronto, ON M3J 1P3, Canada.
RP Xue, SF (reprint author), Univ Sci & Technol China, Natl Engn Lab Speech & Language Informat Proc, Hefei 230026, Peoples R China.
EM xuesf@mail.ustc.edu.cn; ossama@cse.yorku.ca; hj@cse.yorku.ca;
   lrdai@ustc.edu.cn; qliu@iflytek.com
FU National Nature Science Foundation of ChinaNational Natural Science
   Foundation of China [61273264]; National 973 program of ChinaNational
   Basic Research Program of China [2012CB326405]; Natural Sciences and
   Engineering Research Council of Canada (NSERC)Natural Sciences and
   Engineering Research Council of Canada
FX This work was supported in part by the National Nature Science
   Foundation of China under Grant No. 61273264 and the National 973
   program of China under Grant 2012CB326405, as well as a discovery grant
   from Natural Sciences and Engineering Research Council of Canada
   (NSERC). The associate editor coordinating the review of this manuscript
   and approving it for publication was Dr. Dong Yu.
CR Abdel- Hamid O., 2013, P INTERSPEECH
   Abdel-Hamid O, 2013, INT CONF ACOUST SPEE, P7942, DOI 10.1109/ICASSP.2013.6639211
   Anastasakos T, 1996, ICSLP 96 - FOURTH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE PROCESSING, PROCEEDINGS, VOLS 1-4, P1137, DOI 10.1109/ICSLP.1996.607807
   Anastasakos T, 1997, INT CONF ACOUST SPEE, P1043, DOI 10.1109/ICASSP.1997.596119
   Bao YB, 2012, INT CONF SIGN PROCES, P562, DOI 10.1109/ICoSP.2012.6491550
   Bao YB, 2013, INT CONF ACOUST SPEE, P6980, DOI 10.1109/ICASSP.2013.6639015
   Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153
   Bridle John S, 1991, ADV NEURAL INFORMATI, P234
   Dahl GE, 2012, IEEE T AUDIO SPEECH, V20, P30, DOI 10.1109/TASL.2011.2134090
   Dehak N, 2011, IEEE T AUDIO SPEECH, V19, P788, DOI 10.1109/TASL.2010.2064307
   DIGALAKIS VV, 1995, IEEE T SPEECH AUDI P, V3, P357, DOI 10.1109/89.466659
   Gauvain JL, 1994, IEEE T SPEECH AUDI P, V2, P291, DOI 10.1109/89.279278
   Gemello R, 2007, SPEECH COMMUN, V49, P827, DOI 10.1016/j.specom.2006.11.005
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Jain AK, 1996, COMPUTER, V29, P31, DOI 10.1109/2.485891
   Jiang H, 2002, IEEE T SPEECH AUDI P, V10, P9, DOI 10.1109/89.979381
   Jiang H., 2001, P ISCA INT WORKSH HA
   LEE KF, 1989, IEEE T ACOUST SPEECH, V37, P1641, DOI 10.1109/29.46546
   Lee L, 1996, INT CONF ACOUST SPEE, P353, DOI 10.1109/ICASSP.1996.541105
   LEGGETTER CJ, 1995, COMPUT SPEECH LANG, V9, P171, DOI 10.1006/csla.1995.0010
   Li B., 2010, P INTERSPEECH
   Liao H, 2013, INT CONF ACOUST SPEE, P7947, DOI 10.1109/ICASSP.2013.6639212
   Mohamed AR, 2012, IEEE T AUDIO SPEECH, V20, P14, DOI 10.1109/TASL.2011.2109382
   Neto J., 1995, P EUR
   Pan J., 2012, P INT S CHIN SPOK LA
   Pan Zhou, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P5627, DOI 10.1109/ICASSP.2014.6854680
   Sainath T. N., 2011, 2011 IEEE Workshop on Automatic Speech Recognition & Understanding (ASRU), P30, DOI 10.1109/ASRU.2011.6163900
   Sainath TN, 2013, INT CONF ACOUST SPEE, P8614, DOI 10.1109/ICASSP.2013.6639347
   Saon G, 2013, 2013 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING (ASRU), P55, DOI 10.1109/ASRU.2013.6707705
   Seide F., 2011, 2011 IEEE Workshop on Automatic Speech Recognition & Understanding (ASRU), P24, DOI 10.1109/ASRU.2011.6163899
   Seide F., 2011, P INTERSPEECH, P437
   Shiliang Zhang, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P6849, DOI 10.1109/ICASSP.2014.6854927
   Siniscalchi SM, 2013, IEEE T AUDIO SPEECH, V21, P2152, DOI 10.1109/TASL.2013.2270370
   Stadermann J, 2005, INT CONF ACOUST SPEE, P977
   Su H, 2013, INT CONF ACOUST SPEE, P6664, DOI 10.1109/ICASSP.2013.6638951
   Tuske Z, 2013, INT CONF ACOUST SPEE, P6970, DOI 10.1109/ICASSP.2013.6639013
   Xue S., 2014, P IEEE INT C AC SPEE, P6369
   Yao KS, 2012, IEEE W SP LANG TECH, P366, DOI 10.1109/SLT.2012.6424251
   Yu D, 2013, INT CONF ACOUST SPEE, P7893, DOI 10.1109/ICASSP.2013.6639201
   Zhou P., 2013, IEEE T ACOUST UNPUB
NR 40
TC 78
Z9 83
U1 3
U2 89
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 2329-9290
J9 IEEE-ACM T AUDIO SPE
JI IEEE-ACM Trans. Audio Speech Lang.
PD DEC
PY 2014
VL 22
IS 12
BP 1713
EP 1725
DI 10.1109/TASLP.2014.2346313
PG 13
WC Acoustics; Engineering, Electrical & Electronic
SC Acoustics; Engineering
GA AO8TJ
UT WOS:000341627500003
DA 2020-02-19
ER

PT J
AU Gan, JY
   Li, LC
   Zhai, YK
   Liu, YH
AF Gan, Junying
   Li, Lichen
   Zhai, Yikui
   Liu, Yinhua
TI Deep self-taught learning for facial beauty prediction
SO NEUROCOMPUTING
LA English
DT Article
DE Deep self-taught learning; Regression methods; Local binary pattern;
   Facial beauty prediction
ID MACHINE; ATTRACTIVENESS; ALGORITHM; MODELS
AB Most modern research of facial beauty prediction focuses on geometric features by traditional machine learning methods. Geometric features may easily lose much feature information characterizing facial beauty, rely heavily on accurate manual landmark localization of facial features and impose strict restrictions on training samples. Deep architectures have been recently demonstrated to be a promising area of research in statistical machine learning. In this paper, deep self-taught learning is utilized to obtain hierarchical representations, learn the concept of facial beauty and produce human-like predictor. Deep learning is helpful to recognize a broad range of visual concept effectively characterizing facial beauty. Through deep learning, reasonable apparent features of face images are extracted without depending completely on artificial feature selection. Self-taught learning, which has the ability of automatically improving network systems to understand the characteristics of data distribution and making recognition significantly easier and cheaper, is used to relax strict restrictions of training samples. Moreover, in order to choose a more appropriate method for mapping high-level representations into beauty ratings efficiently, we compare the performance of five regression methods and prove that support vector machine (SVM) regression is better. In addition, novel applications of deep self-taught learning on local binary pattern (LBP) and Gabor filters are presented, and the improvements on facial beauty prediction are shown by deep self-taught learning combined with LBP. Finally, human-like performance is obtained with learning features in full-sized and high-resolution images. (C) 2014 Elsevier B.V. All rights reserved.
C1 [Gan, Junying; Li, Lichen; Zhai, Yikui; Liu, Yinhua] Wuyi Univ, Sch Informat Engn, Jiangmen 529020, Guangdong, Peoples R China.
RP Zhai, YK (reprint author), Wuyi Univ, Sch Informat Engn, Jiangmen 529020, Guangdong, Peoples R China.
EM junyinggan@163.com; lilichen0906@163.com; yikuizhai@163.com;
   yinhualiu2109@163.com
OI Zhai, Yikui/0000-0003-0154-9743
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61072127, 61372193, 61070167]; NSF of Guangdong
   Province, PRC [S2013010013311, 10152902001000002, S2011010001085,
   S2011040004211]; High Level Personal Project of Guandong Colleges
   [[2010] 79]; Foundation for Distinguished Young Talents in Higher
   Education of Guangdong, China [2012LYM_0127]; Science Foundation of
   Young Teachers of Wuyi University [2013zk07]; Zhejiang Key Laboratory
   for Signal Processing [ZJKL_4_SP-OP2014-05]
FX This work is supported by the National Natural Science Foundation of
   China (No. 61072127, 61372193, and 61070167), the NSF of Guangdong
   Province, PRC (No. S2013010013311, 10152902001000002, S2011010001085,
   and S2011040004211), the High Level Personal Project of Guandong
   Colleges (No. [2010] 79), the Foundation for Distinguished Young Talents
   in Higher Education of Guangdong, China under Grant No. 2012LYM_0127,
   and Science Foundation of Young Teachers of Wuyi University(No.
   2013zk07), and the Opening Project of Zhejiang Key Laboratory for Signal
   Processing (No. ZJKL_4_SP-OP2014-05).
CR Altwaijry H, 2013, IEEE WORK APP COMP, P117, DOI 10.1109/WACV.2013.6475008
   Arel I, 2010, IEEE COMPUT INTELL M, V5, P13, DOI 10.1109/MCI.2010.938364
   Bastien F., 2010, 1353 DEP IRO U MONTR
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Brereton RG, 2010, ANALYST, V135, P230, DOI 10.1039/b918972f
   Chen F., LECT NOTES COMPUTER, V6165
   Chen H, 2003, IEE P-VIS IMAGE SIGN, V150, P153, DOI 10.1049/ip-vis:20030362
   COOTES TF, 1995, COMPUT VIS IMAGE UND, V61, P38, DOI 10.1006/cviu.1995.1004
   Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467
   Doi E, 2003, NEURAL COMPUT, V15, P397, DOI 10.1162/089976603762552960
   Eisenthal Y, 2006, NEURAL COMPUT, V18, P119, DOI 10.1162/089976606774841602
   Fuller W.A., STATISTICS C, V37
   Gray D, 2010, LECT NOTES COMPUT SC, V6316, P434, DOI 10.1007/978-3-642-15567-3_32
   Hastie T, 2001, ELEMENTS STAT LEARNI
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Huang G. B., 2012, P INT C COMP VIS PAT, P223
   Kagian A., 2007, ADV NEURAL INFORM PR
   Kagian A, 2008, VISION RES, V48, P235, DOI 10.1016/j.visres.2007.11.007
   Kay S., 1993, FUNDAMENTALS STAT SI, P1
   LEE H, 2007, [No title captured], P873
   Lee H., 2010, THESIS STANFORD U
   Lee H., 2009, P INT C MACH LEARN I, P21
   Lee H., COMMUN ACM, V54
   Lee H., 2007, ADV NEURAL INF PROCE, P801
   Mohammad N., 2009, P IEEE C COMP VIS PA, P2735
   Mu YD, 2013, NEUROCOMPUTING, V99, P59, DOI 10.1016/j.neucom.2012.06.020
   Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4
   Raina R, 2007, P 24 INT C MACH LEAR, P759, DOI DOI 10.1145/1273496.1273592
   Rhodes G, 2006, ANNU REV PSYCHOL, V57, P199, DOI 10.1146/annurev.psych.57.102904.190208
   Whitehill J., 2008, P FG 08, P1
   Zhang D, 2011, PATTERN RECOGN, V44, P940, DOI 10.1016/j.patcog.2010.10.013
NR 31
TC 32
Z9 34
U1 2
U2 89
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0925-2312
EI 1872-8286
J9 NEUROCOMPUTING
JI Neurocomputing
PD NOV 20
PY 2014
VL 144
BP 295
EP 303
DI 10.1016/j.neucom.2014.05.028
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA AO9LS
UT WOS:000341677800027
DA 2020-02-19
ER

PT J
AU Yuan, ML
   Tang, HJ
   Li, HZ
AF Yuan, Miaolong
   Tang, Huajin
   Li, Haizhou
TI Real-Time Keypoint Recognition Using Restricted Boltzmann Machine
SO IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
LA English
DT Article
DE Classification; deep learning; feature matching; keypoint recognition;
   real-time tracking; restricted Boltzmann machine (RBM)
AB Feature point recognition is a key component in many vision-based applications, such as vision-based robot navigation, object recognition and classification, image-based modeling, and augmented reality. Real-time performance and high recognition rates are of crucial importance to these applications. In this brief, we propose a novel method for real-time keypoint recognition using restricted Boltzmann machine (RBM). RBMs are generative models that can learn probability distributions of many different types of data including labeled and unlabeled data sets. Due to the inherent noise of the training data sets, we use an RBM to model statistical distributions of the training data. Furthermore, the learned RBM can be used as a competitive classifier to recognize the keypoints in real-time during the tracking stage, thus making it advantageous to be employed in applications that require real-time performance. Experiments have been conducted under a variety of conditions to demonstrate the effectiveness and generalization of the proposed approach.
C1 [Yuan, Miaolong; Tang, Huajin; Li, Haizhou] Agcy Sci Technol & Res, Inst Infocomm Res, Singapore 138632, Singapore.
   [Tang, Huajin] Sichuan Univ, Coll Comp Sci, Chengdu 610065, Peoples R China.
   [Li, Haizhou] Univ New S Wales, Sch Elect Engn & Telecommun, Sydney, NSW 2052, Australia.
RP Tang, HJ (reprint author), Agcy Sci Technol & Res, Inst Infocomm Res, Singapore 138632, Singapore.
EM myuan@i2r.a-star.edu.sg; htang@i2r.a-star.edu.sg; hli@i2r.a-star.edu.sg
RI Li, Haizhou/Q-6438-2019
OI Li, Haizhou/0000-0001-9158-9401
CR Baumberg A, 2000, PROC CVPR IEEE, P774, DOI 10.1109/CVPR.2000.855899
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1016/j.cviu.2007.09.014
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Brown M, 2011, IEEE T PATTERN ANAL, V33, P43, DOI 10.1109/TPAMI.2010.54
   Ferrari V, 2001, IEEE AND ACM INTERNATIONAL SYMPOSIUM ON AUGMENTED REALITY, PROCEEDINGS, P87, DOI 10.1109/ISAR.2001.970518
   Gehler Peter V, 2006, P 23 INT C MACH LEAR, P337, DOI DOI 10.1145/1143844.1143887
   Goedeme T, 2004, PROC CVPR IEEE, P24
   Grabner Helmut, 2006, P BMVC, V1, P6, DOI DOI 10.5244/C.20.6
   Hess R., 2012, SCALE INVARIANT FEAT
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Huang GB, 2012, PROC CVPR IEEE, P2518, DOI 10.1109/CVPR.2012.6247968
   Ke Y, 2004, PROC CVPR IEEE, P506
   Larochelle H., 2008, P 25 INT C MACH LEAR, P536, DOI [10.1145/1390156.1390224, DOI 10.1145/1390156.1390224]
   Le Ly D, 2010, IEEE T NEURAL NETWOR, V21, P1780, DOI 10.1109/TNN.2010.2073481
   LeCun Y., 1998, MNIST DATABASE HANDW
   Lepetit V, 2004, PROC CVPR IEEE, P244
   Lepetit V, 2006, IEEE T PATTERN ANAL, V28, P1465, DOI 10.1109/TPAMI.2006.188
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mikolajczyk K, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P525, DOI 10.1109/ICCV.2001.937561
   Mindru F., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P368, DOI 10.1109/CVPR.1999.786965
   Ng AY, 2002, ADV NEUR IN, V14, P841
   Ozuysal M, 2006, LECT NOTES COMPUT SC, V3953, P592
   Ozuysal M, 2010, IEEE T PATTERN ANAL, V32, P448, DOI 10.1109/TPAMI.2009.23
   Pritchett P., 1998, P ICCV, P863
   Schmid C, 1997, IEEE T PATTERN ANAL, V19, P530, DOI 10.1109/34.589215
   Scholkopf B., 2006, ADV NEURAL INFORM PR, P1345
   Shotton J., 2008, IEEE C COMP VIS PATT, P1, DOI DOI 10.1109/CVPR.2008.4587503
   Skrypnyk I, 2004, ISMAR 2004: THIRD IEEE AND ACM INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY, P110, DOI 10.1109/ISMAR.2004.53
   Smolensky P, 1986, PARALLEL DISTRIBUTED, P194
   Stuhlsatz A, 2012, IEEE T NEUR NET LEAR, V23, P596, DOI 10.1109/TNNLS.2012.2183645
   Tang H. J., 2010, IEEE C EV COMP, P3958
   Tell D., 2002, THESIS ROYAL I TECHN
   Tuytelaars T, 1999, ICRA '99: IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOLS 1-4, PROCEEDINGS, P1601, DOI 10.1109/ROBOT.1999.772588
   Welling M., 2005, ADV NEURAL INFORM PR, P1481
   Williams BT, 2007, ROUTL STUD LITERACY, V3, P1
   ZHANG ZY, 1995, ARTIF INTELL, V78, P87, DOI 10.1016/0004-3702(95)00022-4
NR 36
TC 5
Z9 6
U1 0
U2 30
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 2162-237X
EI 2162-2388
J9 IEEE T NEUR NET LEAR
JI IEEE Trans. Neural Netw. Learn. Syst.
PD NOV
PY 2014
VL 25
IS 11
BP 2119
EP 2126
DI 10.1109/TNNLS.2014.2303478
PG 8
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Computer Science, Theory & Methods; Engineering,
   Electrical & Electronic
SC Computer Science; Engineering
GA AS2DJ
UT WOS:000344089300015
PM 25330434
DA 2020-02-19
ER

PT J
AU Charalampous, K
   Gasteratos, A
AF Charalampous, Konstantinos
   Gasteratos, Antonios
TI A tensor-based deep learning framework
SO IMAGE AND VISION COMPUTING
LA English
DT Article
DE Deep learning; Hierarchical Temporal Memory (HTM); Tensor algebra;
   L-1-norm; Support Vector Clustering; Spatio-temporal features
ID SUPPORT; RECOGNITION; PROJECTIONS; ALGORITHM; CORTEX; ROBUST
AB This paper presents an unsupervised deep leaming framework that derives spatio-temporal features for human-robot interaction. The respective models extract high-level features from low-level ones through a hierarchical network, viz. the Hierarchical Temporal Memory (HTM), providing at the same time a solution to the curse of dimensionality in shallow techniques. The presented work incorporates the tensor-based framework within the operation of the nodes and, thus, enhances the feature derivation procedure. This is due to the fact that tensors allow the preservation of the initial data format and their respective correlation and, moreover, attain more compact representations. The computational nodes form spatial and temporal groups by exploiting the multilinear algebra and subsequently express the samples according to those groups in terms of proximity. This generic framework may be applied in a diverse of visual data, while it has been examined on sequences of color and depth images, exhibiting remarkable performance. (C) 2014 Elsevier B.V. All rights reserved.
C1 [Charalampous, Konstantinos; Gasteratos, Antonios] Democritus Univ Thrace, Dept Prod & Management Engn, GR-67100 Xanthi, Greece.
RP Charalampous, K (reprint author), Democritus Univ Thrace, Dept Prod & Management Engn, Vas Sofias 12,Bldg 1,Off 205, GR-67100 Xanthi, Greece.
EM kchara@pme.duth.gr; agaster@pme.duth.gr
RI Gasteratos, Antonios/B-7796-2012
OI Gasteratos, Antonios/0000-0002-5421-0332
CR Bazzani L., 2011, P 28 INT C MACH LEAR, P937
   Ben-Hur A, 2002, J MACH LEARN RES, V2, P125, DOI 10.1162/15324430260185565
   Ben-Hur A, 2001, ADV NEUR IN, V13, P367
   Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153
   Cecotti H, 2011, IEEE T PATTERN ANAL, V33, P433, DOI 10.1109/TPAMI.2010.125
   Charalampous K, 2012, ELECTRON LETT, V48, P1259, DOI 10.1049/el.2012.1033
   Chen SSB, 2001, SIAM REV, V43, P129, DOI 10.1137/S003614450037906X
   Chiang JH, 2003, IEEE T FUZZY SYST, V11, P518, DOI 10.1109/TFUZZ.2003.814839
   Tao DC, 2007, KNOWL INF SYST, V13, P1, DOI 10.1007/s10115-006-0050-6
   Dean J., 2012, ADV NEURAL INFORM PR
   Dollar P., 2005, Proceedings. 2nd Joint IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance (VS-PETS) (IEEE Cat. No. 05EX1178), P65
   Donoho DL, 2006, COMMUN PUR APPL MATH, V59, P797, DOI 10.1002/cpa.20132
   Efros AA, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P726
   Fan JL, 2010, IEEE T NEURAL NETWOR, V21, P1610, DOI 10.1109/TNN.2010.2066286
   Fletcher R, 1987, PRACTICAL METHODS OP
   George  D., 2008, THESIS
   George D, 2009, PLOS COMPUT BIOL, V5, DOI 10.1371/journal.pcbi.1000532
   Guo WW, 2012, IEEE T IMAGE PROCESS, V21, P816, DOI 10.1109/TIP.2011.2165291
   He X., 2005, ADV NEURAL INFORM PR
   Hinton G. E., 2012, CORR
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Huang F. J., 2006, C COMP VIS PATT REC, P284, DOI [10.1109/CVPR.2006.164, DOI 10.1109/CVPR.2006.164]
   Ikizler N, 2009, IMAGE VISION COMPUT, V27, P1515, DOI 10.1016/j.imavis.2009.02.002
   Jhuang H, 2007, IEEE I CONF COMP VIS, P1253
   JI S, 2013, TPAMI, V35, P221, DOI DOI 10.1109/TPAMI.2012.59
   Klaser A., 2008, BMVC 2008 19 BRIT MA, P275
   Ko B, 2013, IMAGE VISION COMPUT, V31, P786, DOI 10.1016/j.imavis.2013.08.001
   Kostavelis I, 2012, PATTERN RECOGN LETT, V33, P670, DOI 10.1016/j.patrec.2011.11.017
   Kotsia I, 2012, PATTERN RECOGN, V45, P4192, DOI 10.1016/j.patcog.2012.04.033
   Kotsia I, 2011, PROC CVPR IEEE, P633, DOI 10.1109/CVPR.2011.5995663
   Kurakin A, 2012, EUR SIGNAL PR CONF, P1975
   Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7
   Laptev I, 2008, PROC CVPR IEEE, P3222
   Le Q. V., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3361, DOI 10.1109/CVPR.2011.5995496
   Lee H., 2009, P 26 ANN INT C MACH, P609, DOI DOI 10.1145/1553374.1553453
   Lee J, 2005, IEEE T PATTERN ANAL, V27, P461, DOI 10.1109/TPAMI.2005.47
   Lee J, 2006, IEEE T PATTERN ANAL, V28, P1869, DOI 10.1109/TPAMI.2006.225
   Lee TS, 1998, VISION RES, V38, P2429, DOI 10.1016/S0042-6989(97)00464-1
   Mountcastle V.B., 1979, ORG PRINCIPLE CEREBR
   Niebles JC, 2008, INT J COMPUT VISION, V79, P299, DOI 10.1007/s11263-007-0122-4
   Norouzi Mohammad, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2735, DOI 10.1109/CVPRW.2009.5206577
   Qiao LS, 2010, PATTERN RECOGN, V43, P331, DOI 10.1016/j.patcog.2009.05.005
   Ranzato M., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2857, DOI 10.1109/CVPR.2011.5995710
   Ranzato MA, 2007, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2007.383157
   Rodriguez MD, 2008, PROC CVPR IEEE, P3001
   Schuldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462
   Serre T, 2007, IEEE T PATTERN ANAL, V29, P411, DOI 10.1109/TPAMI.2007.56
   Tang Y., 2010, P 27 INT C MACH LEAR, P1055
   TAO D, 2006, IEEE COMPUT VIS PATT, V2, P1670
   Tax DMJ, 1999, PATTERN RECOGN LETT, V20, P1191, DOI 10.1016/S0167-8655(99)00087-2
   Taylor GW, 2010, LECT NOTES COMPUT SC, V6316, P140, DOI 10.1007/978-3-642-15567-3_11
   Turaga SC, 2010, NEURAL COMPUT, V22, P511, DOI 10.1162/neco.2009.10-08-881
   VASILESCU MAO, 2003, PROC CVPR IEEE, P93, DOI DOI 10.1109/CVPR2003.1211457
   von Melchner L, 2000, NATURE, V404, P871, DOI 10.1038/35009102
   Wang H., 2009, BMVC
   Wang J, 2012, LECT NOTES COMPUT SC, V7573, P872, DOI 10.1007/978-3-642-33709-3_62
   Welling Max, 2004, ADV NEURAL INFORM PR, P1481
   Willems G, 2008, LECT NOTES COMPUT SC, V5303, P650, DOI 10.1007/978-3-540-88688-4_48
   Xiaofei He, 2005, 13th Annual ACM International Conference on Multimedia, P132
   Yan SC, 2007, IEEE T IMAGE PROCESS, V16, P212, DOI 10.1109/TIP.2006.884929
   Yang JH, 2002, ICONIP'02: PROCEEDINGS OF THE 9TH INTERNATIONAL CONFERENCE ON NEURAL INFORMATION PROCESSING, P898
   Zafeiriou S, 2009, IEEE T NEURAL NETWOR, V20, P217, DOI 10.1109/TNN.2008.2005293
   Zeiler MD, 2010, PROC CVPR IEEE, P2528, DOI 10.1109/CVPR.2010.5539957
NR 63
TC 5
Z9 5
U1 0
U2 41
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0262-8856
EI 1872-8138
J9 IMAGE VISION COMPUT
JI Image Vis. Comput.
PD NOV
PY 2014
VL 32
IS 11
BP 916
EP 929
DI 10.1016/j.imavis.2014.08.003
PG 14
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods; Engineering, Electrical
   & Electronic; Optics
SC Computer Science; Engineering; Optics
GA AS7FP
UT WOS:000344422900009
DA 2020-02-19
ER

PT J
AU Zou, Q
   Cao, Y
   Li, QQ
   Huang, CH
   Wang, S
AF Zou, Qin
   Cao, Yu
   Li, Qingquan
   Huang, Chuanhe
   Wang, Song
TI Chronological classification of ancient paintings using appearance and
   shape features
SO PATTERN RECOGNITION LETTERS
LA English
DT Article
DE Painting classification; Painting style analysis; Deep learning; Image
   classification
ID VISION; SYSTEM
AB Ancient paintings are valuable for historians and archeologists to study the humanities, customs and economy of the corresponding eras. For this purpose, it is important to first determine the era in which a painting was drawn. This problem can be very challenging when the paintings from different eras present a same topic and only show subtle difference in terms of the painting styles. In this paper, we propose a novel computational approach to address this problem by using the appearance and shape features extracted from the paintings. In this approach, we first extract the appearance and shape features using the SIFT and kAS descriptors, respectively. We then encode these features with deep learning in an unsupervised way. Finally, we combine all the features in the form of bag-of-visual-words and train a classifier in a supervised fashion. In the experiments, we collect 660 Flying-Apsaras paintings from Mogao Grottoes in Dunhuang, China and classify them into three different eras, with very promising results. (C) 2014 Elsevier B.V. All rights reserved.
C1 [Zou, Qin; Huang, Chuanhe] Wuhan Univ, Sch Comp Sci, Wuhan 430072, Peoples R China.
   [Cao, Yu; Wang, Song] Univ S Carolina, Dept Comp Sci & Engn, Columbia, SC 29208 USA.
   [Li, Qingquan] Shenzhen Univ, Shenzhen Key Lab Spatial Informat Smart Sensing &, Shenzhen 518060, Shandong, Peoples R China.
RP Zou, Q (reprint author), Wuhan Univ, Sch Comp Sci, Wuhan 430072, Peoples R China.
EM qinnzou@gmail.com; cao@cec.sc.edu; qqIi@whu.edu.cn; huangch@whu.edu.cn;
   songwang@sc.edu
RI Cao, Yu/G-9532-2015
FU National Basic Research Program of ChinaNational Basic Research Program
   of China [2012CB725303]; China Postdoctoral Science FoundationChina
   Postdoctoral Science Foundation [2012M521472]; National Natural Science
   Foundation of ChinaNational Natural Science Foundation of China
   [61301277, 41371431]; Hubei Provincial Natural Science Foundation
   [2013CFB299]; AFOSRUnited States Department of DefenseAir Force Office
   of Scientific Research (AFOSR) [FA9550-1-1-0327]; NSFNational Science
   Foundation (NSF) [IIS-1017199]
FX This research was supported by the National Basic Research Program of
   China under Grant No. 2012CB725303, the China Postdoctoral Science
   Foundation funded project (2012M521472), National Natural Science
   Foundation of China (61301277 and 41371431), Hubei Provincial Natural
   Science Foundation (2013CFB299), the fund of AFOSR FA9550-1-1-0327, and
   NSF IIS-1017199.
CR Arora RS, 2012, INT C PATT RECOG, P3541
   Bengio Y, 2006, ADV NEURAL INFORM PR, P153
   Bressan M, 2008, IEEE IMAGE PROC, P113, DOI 10.1109/ICIP.2008.4711704
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Condorovici RG, 2013, LECT NOTES COMPUT SC, V7944, P687
   Fawcett T, 2006, PATTERN RECOGN LETT, V27, P861, DOI 10.1016/j.patrec.2005.10.010
   Ferrari V, 2008, IEEE T PATTERN ANAL, V30, P36, DOI 10.1109/TPAMI.2007.1144
   Graham DJ, 2012, WIRES COMPUT STAT, V4, P115, DOI 10.1002/wics.197
   Gunsel B., 2005, P ICIP
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Icoglu Oguz, 2004, 2004 12th European Signal Processing Conference (EUSIPCO), P749
   Ivanova Krassimira, 2012, Data Engineering and Management. Second International Conference, ICDEM 2010. Revised Selected Papers, P146, DOI 10.1007/978-3-642-27872-3_22
   Ivanoya K., 2008, SERDICA J COMPUT, V2, P111
   Jacobsen CR, 2013, SIGNAL PROCESS, V93, P579, DOI 10.1016/j.sigpro.2012.09.019
   Khan Fahad Shahbaz, 2010, 2010 CREATE C, P329
   Lewis PH, 2004, IEEE T IMAGE PROCESS, V13, P302, DOI 10.1109/TIP.2003.821346
   Li J, 2004, IEEE T IMAGE PROCESS, V13, P338, DOI 10.1109/TIP.2003.821349
   Lombardi T., 2005, THESIS PACE U
   Lombardi T., 2004, ACM SIGMM INT WORKSH, P107
   Lowe D., 1999, P INT C COMP VIS, V2, P1150, DOI [10.1109/ICCV.1999.790410, DOI 10.1109/ICCV.1999.790410]
   Martin DR, 2004, IEEE T PATTERN ANAL, V26, P530, DOI 10.1109/TPAMI.2004.1273918
   Perronnin F., 2007, P IEEE COMP SOC C CO, P1, DOI DOI 10.1109/CVPR.2007.383266
   Ranzato M., 2006, ADV NEURAL INFORM PR, V19, P1137
   Rumelhart D.E., 1986, PARALLEL DISTRIBUTED, V1
   Sablatnig R, 1998, INT C PATT RECOG, P172, DOI 10.1109/ICPR.1998.711107
   Shamir L, 2010, ACM T APPL PERCEPT, V7, DOI 10.1145/1670671.1670672
   Stork DG, 2009, LECT NOTES COMPUT SC, V5702, P9
   Temel B, 2009, ISTANB UNIV-J ELECTR, V9, P791
   Van De Weijer J., 2007, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2007.383218
   Vedaldi A., 2008, VLFEAT OPEN PORTABLE
   Yu X., 2007, BMVC, P1
   Zhong S., 2011, P 19 ACM INT C MULT, P343
   Zujovic J, 2009, IEEE INT WORKSH MULT, P501
NR 34
TC 12
Z9 15
U1 0
U2 13
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0167-8655
EI 1872-7344
J9 PATTERN RECOGN LETT
JI Pattern Recognit. Lett.
PD NOV 1
PY 2014
VL 49
BP 146
EP 154
DI 10.1016/j.patrec.2014.07.002
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA AR8WO
UT WOS:000343852400020
DA 2020-02-19
ER

PT J
AU Srivastava, N
   Salakhutdinov, R
AF Srivastava, Nitish
   Salakhutdinov, Ruslan
TI Multimodal Learning with Deep Boltzmann Machines
SO JOURNAL OF MACHINE LEARNING RESEARCH
LA English
DT Article
DE Boltzmann machines; unsupervised learning; multimodal learning; neural
   networks; deep learning
ID EXTRACTION
AB Data often consists of multiple diverse modalities. For example, images are tagged with textual information and videos are accompanied by audio. Each modality is characterized by having distinct statistical properties. We propose a Deep Boltzmann Machine for learning a generative model of such multimodal data. We show that the model can be used to create fused representations by combining features across modalities. These learned representations are useful for classification and information retrieval. By sampling from the conditional distributions over each data modality, it is possible to create these representations even when some data modalities are missing. We conduct experiments on bi-modal image-text and audio-video data. The fused representation achieves good classification results on the MIR-Flickr data set matching or outperforming other deep models as well as SVM based models that use Multiple Kernel Learning. We further demonstrate that this multimodal model helps classification and retrieval even when only unimodal data is available at test time.
C1 [Srivastava, Nitish] Univ Toronto, Dept Comp Sci, Toronto, ON M5S 3G4, Canada.
   [Salakhutdinov, Ruslan] Univ Toronto, Dept Comp Sci & Stat, Toronto, ON M5S 3G4, Canada.
RP Srivastava, N (reprint author), Univ Toronto, Dept Comp Sci, 10 Kings Coll Rd,Rm 3302, Toronto, ON M5S 3G4, Canada.
EM NITISH@CS.TORONTO.EDU; RSALAKHU@CS.TORONTO.EDU
FU Google; Samsung; ONROffice of Naval Research [N00014-14-1-0232]
FX This research was supported by Google, Samsung, and ONR Grant
   N00014-14-1-0232.
CR Bastan M, 2010, IEEE MULTIMEDIA, V17, P62, DOI 10.1109/MMUL.2010.5692184
   Bosch A, 2007, IEEE I CONF COMP VIS, P1863
   Cox S., 2008, INT C AUD VIS SPEECH, P179
   Dalal N, 2005, PROC CVPR IEEE, P886
   Fisher W. M., 1986, P DARPA WORKSH SPEEC, P93
   Freund Yoav, 1994, TECHNICAL REPORT
   Guillaumin M, 2010, PROC CVPR IEEE, P902, DOI 10.1109/CVPR.2010.5540120
   Gurban M, 2009, IEEE T SIGNAL PROCES, V57, P4765, DOI 10.1109/TSP.2009.2026513
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton G. E., 2012, CORR
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hinton GE, 2009, ADV NEURAL INFORM PR, P1607
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Huiskes M. J., 2010, P INT C MULT INF RET, P527, DOI DOI 10.1145/1743384.1743475
   Huiskes M. J., 2008, ACM INT C MULTIMEDIA
   Lee H., 2009, P 26 ANN INT C MACH, P609, DOI DOI 10.1145/1553374.1553453
   Lucey P, 2006, P HCSNET WORKSH US V, P79
   Manjunath BS, 2001, IEEE T CIRC SYST VID, V11, P703, DOI 10.1109/76.927424
   Matthews I, 2002, IEEE T PATTERN ANAL, V24, P198, DOI 10.1109/34.982900
   Mohamed A., 2011, IEEE T AUDIO SPEECH
   Ngiam J, 2011, P 28 INT C MACH LEAR
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0
   Papandreou G, 2007, 2007 IEEE NINTH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P264, DOI 10.1109/MMSP.2007.4412868
   Papandreou G, 2009, IEEE T AUDIO SPEECH, V17, P423, DOI 10.1109/TASL.2008.2011515
   Patterson EK, 2002, INT CONF ACOUST SPEE, P2017
   ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586
   Salakhutdinov R. R., 2009, P INT C ARTIFICIAL I, V12
   Smolensky P, 1986, PARALLEL DISTRIBUTED, P194
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Taylor G. W., 2010, EURPEAN C COMPUTER V
   Tieleman T., 2008, P 25 INT C MACH LEAR, P1064, DOI DOI 10.1145/1390156.1390290
   Vedaldi A., 2008, VLFEAT OPEN PORTABLE
   Verbeek Jakob, 2010, P INT C MULT INF RET, P537, DOI DOI 10.1145/1743384.1743476
   Vincent P, 2008, P 25 INT C MACH LEAR, P1096, DOI DOI 10.1145/1390156.1390294
   Welling M., 2005, ADV NEURAL INFORM PR, P1481
   Xing E.P., 2005, P 21 C UNC ART INT, P633
   YOUNES L, 1989, PROBAB THEORY REL, V82, P625, DOI 10.1007/BF00341287
   Younes L, 1998, STOCHASTICS STOCHAST, P177
   Yuille A. L., 2004, ADV NEURAL INFORM PR, V17
   Zhao GY, 2009, IEEE T MULTIMEDIA, V11, P1254, DOI 10.1109/TMM.2009.2030637
NR 41
TC 123
Z9 128
U1 5
U2 74
PU MICROTOME PUBL
PI BROOKLINE
PA 31 GIBBS ST, BROOKLINE, MA 02446 USA
SN 1532-4435
J9 J MACH LEARN RES
JI J. Mach. Learn. Res.
PD OCT
PY 2014
VL 15
BP 2949
EP 2980
PG 32
WC Automation & Control Systems; Computer Science, Artificial Intelligence
SC Automation & Control Systems; Computer Science
GA AT0PY
UT WOS:000344638800004
DA 2020-02-19
ER

PT J
AU Couprie, C
   Farabet, C
   Najman, L
   LeCun, Y
AF Couprie, Camille
   Farabet, Clement
   Najman, Laurent
   LeCun, Yann
TI Convolutional Nets and Watershed Cuts for Real-Time Semantic Labeling of
   RGBD Videos
SO JOURNAL OF MACHINE LEARNING RESEARCH
LA English
DT Article
DE deep learning; optimization; convolutional networks; superpixels; depth
   information
ID MEAN-SHIFT; RECOGNITION; FEATURES
AB This work addresses multi-class segmentation of indoor scenes with RGB-D inputs. While this area of research has gained much attention recently, most works still rely on handcrafted features. In contrast, we apply a multiscale convolutional network to learn features directly from the images and the depth information. Using a frame by frame labeling, we obtain nearly state-of-the-art performance on the NYU-v2 depth data set with an accuracy of 64.5%. We then show that the labeling can be further improved by exploiting the temporal consistency in the video sequence of the scene. To that goal, we present a method producing temporally consistent superpixels from a streaming video. Among the different methods producing superpixel segmentations of an image, the graph-based approach of Felzenszwalb and Huttenlocher is broadly employed. One of its interesting properties is that the regions are computed in a greedy manner in quasi-linear time by using a minimum spanning tree. In a framework exploiting minimum spanning trees all along, we propose an efficient video segmentation approach that computes temporally consistent pixels in a causal manner, filling the need for causal and real-time applications. We illustrate the labeling of indoor scenes in video sequences that could be processed in real-time using appropriate hardware such as an FPGA.
C1 [Couprie, Camille] IFP Energies Nouvelles, Technol Comp Sci & Appl Math Div, Rueil Malmaison, France.
   [Farabet, Clement] Twitter Inc, San Francisco, CA 94103 USA.
   [Najman, Laurent] Univ Paris Est, Lab Informat Gaspard Monge, Equipe ESIEE Paris A3SI, Paris, France.
   [LeCun, Yann] NYU & Facebook AI Res, Courant Inst Math Sci, New York, NY 10003 USA.
RP Couprie, C (reprint author), IFP Energies Nouvelles, Technol Comp Sci & Appl Math Div, Rueil Malmaison, France.
EM camille.couprie@ifpen.fr; cfarabet@twitter.com; l.najman@esiee.fr;
   yann@cs.nyu.edu
RI Najman, Laurent/AAB-4212-2020; Math et Info, Direction Math/C-1462-2013
OI Najman, Laurent/0000-0002-6190-0235; 
CR Allene C, 2010, IMAGE VISION COMPUT, V28, P1460, DOI 10.1016/j.imavis.2009.06.017
   Cadena C., 2013, 3 WORKSH SEM PERC MA
   Cires D. C., 2011, P 22 INT JOINT C ART, V22, P1237, DOI DOI 10.5591/978-1-57735-516-8/IJCAI11-210
   Ciresan D. C., 2013, MICCAI
   Ciresan Dan C., 2012, NIPS, P2852
   Collobert R., 2011, NIPS BIG LEARN WORKS
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Couprie C, 2011, IEEE T PATTERN ANAL, V33, P1384, DOI 10.1109/TPAMI.2010.200
   Couprie Camille, 2012, 20 EUR SIGN PROC C B
   Couprie Camille, 2013, P IEEE INT C IM PROC
   Couprie Camille, 2013, SOURCE CODE CAUSAL G
   Couprie Camille, 2013, INT C LEARN REPR
   Cousty J, 2009, IEEE T PATTERN ANAL, V31, P1362, DOI 10.1109/TPAMI.2008.173
   Cruz Leandro, 2012, SIBGRAPI TUT
   Farabet C., 2012, P INT C MACH LEARN E
   Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231
   Farabet Clement, 2014, THESIS U PARIS EST
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   FUKUSHIMA K, 1980, BIOL CYBERN, V36, P193, DOI 10.1007/BF00344251
   Glasner D., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2385, DOI 10.1109/CVPR.2011.5995436
   Gomila C., 2003, P IEEE INT C IM PROC
   Grundmann Matthias, 2010, P IEEE COMP VIS PATT
   Hinton Geoffrey E., 2012, CORRABS12070580
   Hoiem D, 2005, IEEE I CONF COMP VIS, P654
   Jaitly N, 2012, P INT
   Janoch A., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P1168, DOI 10.1109/ICCVW.2011.6130382
   Joulin A, 2012, PROC CVPR IEEE, P542, DOI 10.1109/CVPR.2012.6247719
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Yann, 2004, P IEEE COMP VIS PATT
   Lee J, 2005, 2005 IEEE International Conference on Multimedia and Expo (ICME), Vols 1 and 2, P394
   Lenz I., 2013, ROBOTICS SCI SYSTEMS
   Levin A, 2004, ACM T GRAPHIC, V23, P689, DOI 10.1145/1015706.1015780
   Meyer F, 1994, COMP IMAG VIS, V2, P77
   Miksik Ondrej, 2013, P IEEE INT C ROB AUT
   MORRIS OJ, 1986, IEE PROC-F, V133, P146, DOI 10.1049/ip-f-1.1986.0025
   Muller AC, 2014, IEEE INT CONF ROBOT, P6232, DOI 10.1109/ICRA.2014.6907778
   Paris S, 2008, LECT NOTES COMPUT SC, V5303, P460, DOI 10.1007/978-3-540-88688-4_34
   Ren XF, 2012, PROC CVPR IEEE, P2759, DOI 10.1109/CVPR.2012.6247999
   Riesenhuber M, 1999, NAT NEUROSCI, V2, P1019
   Schulz H., 2012, 11 EUR S ART NEUR NE
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Silberman  Nathan, 2012, P IEEE EUR C COMP VI, P3
   Silberman Nathan, 2011, 3DRR WORKSH IEEE INT
   Sinop A., 2007, P INT C COMP VIS
   Socher R., 2012, ADV NEURAL INFORM PR, V25
   Stuckler J., 2013, J REAL-TIME IMAGE PR, P1
   Veksler O, 2010, LECT NOTES COMPUT SC, V6315, P211, DOI 10.1007/978-3-642-15555-0_16
   Xu CL, 2012, LECT NOTES COMPUT SC, V7577, P626, DOI 10.1007/978-3-642-33783-3_45
NR 48
TC 12
Z9 12
U1 0
U2 19
PU MICROTOME PUBL
PI BROOKLINE
PA 31 GIBBS ST, BROOKLINE, MA 02446 USA
SN 1532-4435
J9 J MACH LEARN RES
JI J. Mach. Learn. Res.
PD OCT
PY 2014
VL 15
BP 3489
EP 3511
PG 23
WC Automation & Control Systems; Computer Science, Artificial Intelligence
SC Automation & Control Systems; Computer Science
GA AT0PY
UT WOS:000344638800022
DA 2020-02-19
ER

PT J
AU Huang, WH
   Song, GJ
   Hong, HK
   Xie, KQ
AF Huang, Wenhao
   Song, Guojie
   Hong, Haikun
   Xie, Kunqing
TI Deep Architecture for Traffic Flow Prediction: Deep Belief Networks With
   Multitask Learning
SO IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS
LA English
DT Article
DE Deep learning; multitask learning (MTL); task grouping; traffic flow
   prediction
ID INTELLIGENT TRANSPORTATION SYSTEMS
AB Traffic flow prediction is a fundamental problem in transportation modeling and management. Many existing approaches fail to provide favorable results due to being: 1) shallow in architecture; 2) hand engineered in features; and 3) separate in learning. In this paper we propose a deep architecture that consists of two parts, i.e., a deep belief network (DBN) at the bottom and a multitask regression layer at the top. A DBN is employed here for unsupervised feature learning. It can learn effective features for traffic flow prediction in an unsupervised fashion, which has been examined and found to be effective for many areas such as image and audio classification. To the best of our knowledge, this is the first paper that applies the deep learning approach to transportation research. To incorporate multitask learning (MTL) in our deep architecture, a multitask regression layer is used above the DBN for supervised prediction. We further investigate homogeneous MTL and heterogeneous MTL for traffic flow prediction. To take full advantage of weight sharing in our deep architecture, we propose a grouping method based on the weights in the top layer to make MTL more effective. Experiments on transportation data sets show good performance of our deep architecture. Abundant experiments show that our approach achieved close to 5% improvements over the state of the art. It is also presented that MTL can improve the generalization performance of shared tasks. These positive results demonstrate that deep learning and MTL are promising in transportation research.
C1 [Huang, Wenhao; Song, Guojie; Hong, Haikun; Xie, Kunqing] Peking Univ, Sch Elect Engn & Comp Sci, Beijing 100871, Peoples R China.
   [Song, Guojie] Peking Univ, Res Ctr Intelligent Informat Proc, Beijing 100871, Peoples R China.
   [Xie, Kunqing] Peking Univ, Res Ctr Intelligent Traff Syst, Beijing 100871, Peoples R China.
RP Song, GJ (reprint author), Peking Univ, Sch Elect Engn & Comp Sci, Beijing 100871, Peoples R China.
EM gjsong@pku.edu.cn
FU National High Technology Research and Development Program of
   ChinaNational High Technology Research and Development Program of China
   [2014AA015103]; National Science and Technology Support Plan
   [2014BAG01B02]
FX This work was supported in part by the National High Technology Research
   and Development Program of China under Grant 2014AA015103 and in part by
   the National Science and Technology Support Plan under Grant
   2014BAG01B02.
CR Caruana R., 1998, MULTITASK LEARNING
   Castro-Neto M, 2009, EXPERT SYST APPL, V36, P6164, DOI 10.1016/j.eswa.2008.07.069
   Chan KY, 2012, IEEE T INTELL TRANSP, V13, P644, DOI 10.1109/TITS.2011.2174051
   Clark S, 2003, J TRANSP ENG, V129, P161, DOI 10.1061/(ASCE)0733-947X(2003)129:2(161)
   Collobert R., 2008, P 25 ICML, V25, P160, DOI [10.1145/1390156.1390177, DOI 10.1145/1390156.1390177]
   Dean J., 2012, ADV NEURAL INFORM PR, P1232
   Deng L, 2012, INT CONF ACOUST SPEE, P2133, DOI 10.1109/ICASSP.2012.6288333
   Ghosh B, 2007, J TRANSP ENG-ASCE, V133, P180, DOI 10.1061/(ASCE)0733-947X(2007)133:3(180)
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Jin F, 2008, IEEE IJCNN, P1897, DOI 10.1109/IJCNN.2008.4634057
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Larochelle H, 2009, J MACH LEARN RES, V10, P1
   Lee H., 2008, ADV NEURAL INFORM PR, V20, P873
   Mohamed Abdel-rahman, 2009, SCIENCE, P1, DOI DOI 10.4249/SCHOLARPEDIA.5947
   Moorthy C. K., 1988, TRANSPORT PLAN TECHN, V12, P45, DOI DOI 10.1080/03081068808717359
   Morris BT, 2012, IEEE T INTELL TRANSP, V13, P1667, DOI 10.1109/TITS.2012.2208222
   Ngiam J, 2011, IEEE INT C MACH LEAR, V28, P689, DOI DOI 10.1145/2647868
   Pan TL, 2013, IEEE T INTELL TRANSP, V14, P1242, DOI 10.1109/TITS.2013.2258916
   Salakhutdinov R., 2008, ADV NEURAL INFORM PR, V20, P1249
   Shuai M., 2008, P 16 ACM SIGSPATIAL, P45
   Smith B.L., 1994, TRANSPORT RES REC, V1453, P98
   Sun SL, 2007, IEEE T INTELL TRANSP, V8, P367, DOI 10.1109/TITS.2006.888603
   Sun SL, 2011, IEEE T INTELL TRANSP, V12, P466, DOI 10.1109/TITS.2010.2093575
   Sun SL, 2009, WORLD SUMMIT ON GENETIC AND EVOLUTIONARY COMPUTATION (GEC 09), P961
   Sun SL, 2006, IEEE T INTELL TRANSP, V7, P124, DOI 10.1109/TITS.2006.869623
   Tan MC, 2009, IEEE T INTELL TRANSP, V10, P60, DOI 10.1109/TITS.2008.2011693
   Teh YW, 2001, ADV NEUR IN, V13, P908
   Thomas T, 2010, IEEE T INTELL TRANSP, V11, P71, DOI 10.1109/TITS.2009.2028149
   Transportation Research Board, 2000, HIGHW CAP MAN
   vanderVoort M, 1996, TRANSPORT RES C-EMER, V4, P307, DOI 10.1016/S0968-090X(97)82903-8
   Wang FY, 2010, IEEE T INTELL TRANSP, V11, P630, DOI 10.1109/TITS.2010.2060218
   Ye Q, 2012, IEEE T INTELL TRANSP, V13, P1727, DOI 10.1109/TITS.2012.2203122
   Yu GQ, 2003, IEEE IV2003: INTELLIGENT VEHICLES SYMPOSIUM, PROCEEDINGS, P208, DOI 10.1109/IVS.2003.1212910
   Zhang JP, 2011, IEEE T INTELL TRANSP, V12, P1624, DOI 10.1109/TITS.2011.2158001
   Zheng WZ, 2006, J TRANSP ENG, V132, P114, DOI 10.1061/(ASCE)0733-947X(2006)132:2(114)
NR 37
TC 255
Z9 272
U1 28
U2 206
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1524-9050
EI 1558-0016
J9 IEEE T INTELL TRANSP
JI IEEE Trans. Intell. Transp. Syst.
PD OCT
PY 2014
VL 15
IS 5
BP 2191
EP 2201
DI 10.1109/TITS.2014.2311123
PG 11
WC Engineering, Civil; Engineering, Electrical & Electronic; Transportation
   Science & Technology
SC Engineering; Transportation
GA AQ7MR
UT WOS:000343002400029
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Xu, JG
   Li, H
   Zhou, SL
AF Xu, Jungang
   Li, Hui
   Zhou, Shilong
TI Improving mixing rate with tempered transition for learning restricted
   Boltzmann machines
SO NEUROCOMPUTING
LA English
DT Article
DE Restricted Boltzmann machines; Tempered transition; Mixing rate; Deep
   learning
ID DEEP; PRODUCTS
AB Recently, as the building block of deep generative models such as Deep Belief Networks (DBNs), Restricted Boltzmann Machines (RBMs) have attracted much attention. RBM is a Markov Random Field (MRF) associated with a bipartite undirected graph which is famous for powerful expression and tractable inference. While training an RBM, we need to sample from the model. The larger the mixing rate is, the smaller the bias of the samples is. However, neither Gibbs sampling based training methods such as Contrastive Divergence (CD) nor Parallel Tempering based training methods can achieve satisfying mixing rate, which causes poor rendering of the diversity of the modes captured by these trained models. This property may hinder the existing methods to approximate the likelihood gradient. In order to alleviate this problem, we attempt to introduce Tempered Transition, an advanced tempered Markov Chain Monte Carlo method, into training RBMs to replace Gibbs sampling or Parallel Tempering for sampling from RBMs. Experimental results show that our proposed method outperforms the existing methods to achieve better mixing rate and to help approximate the likelihood gradient. (C) 2014 Elsevier B.V. All rights reserved.
C1 [Xu, Jungang; Li, Hui; Zhou, Shilong] Univ Chinese Acad Sci, Sch Comp & Control Engn, Beijing 101408, Peoples R China.
RP Xu, JG (reprint author), Univ Chinese Acad Sci, Sch Comp & Control Engn, Beijing 101408, Peoples R China.
EM xujg@ucas.ac.cn; lihui211@mails.ucas.ac.cn;
   zhoushilong12@mails.ucas.ac.cn
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61372171]; National Key Technology R&D Program of
   ChinaNational Key Technology R&D Program [2012 BAH23B03]
FX Our work is supported in part by the National Natural Science Foundation
   of China under Grant no. 61372171 and the National Key Technology R&D
   Program of China under Contract no. 2012 BAH23B03.
CR Bengio Y., 2012, ARXIV12074404
   Bengio Y, 2011, LECT NOTES ARTIF INT, V6925, P18, DOI 10.1007/978-3-642-24412-4_3
   Bengio Y, 2009, NEURAL COMPUT, V21, P1601, DOI 10.1162/neco.2008.11-07-647
   Cho K.H., 2010, P 2010 INT JOINT C N, P1
   Desjardins G., 2010, P 13 INT C ART INT S, V9, P145
   Earl DJ, 2005, PHYS CHEM CHEM PHYS, V7, P3910, DOI 10.1039/b509983h
   Erhan D, 2010, J MACH LEARN RES, V11, P625
   Fischer Asja, 2012, Progress in Pattern Recognition, Image Analysis, ComputerVision, and Applications. Proceedings 17th Iberoamerican Congress, CIARP 2012, P14, DOI 10.1007/978-3-642-33275-3_2
   Fischer A, 2010, LECT NOTES COMPUT SC, V6354, P208, DOI 10.1007/978-3-642-15825-4_26
   Freund Yoav, 1994, UNSUPERVISED LEARNIN
   Hinton G., 2012, LNCS, V7700, P599, DOI DOI 10.1007/978-3-642-35289-8_32
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hinton GE, 1999, IEE CONF PUBL, P1, DOI 10.1049/cp:19991075
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Hinton GE, 2007, TRENDS COGN SCI, V11, P428, DOI 10.1016/j.tics.2007.09.004
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Iba Y, 2001, INT J MOD PHYS C, V12, P623, DOI 10.1142/S0129183101001912
   Krizhevsky A., 2009, THESIS U TORONTO
   Krizhevsky A., 2011, P 19 EUR S ART NEUR
   Le Roux N, 2008, NEURAL COMPUT, V20, P1631, DOI 10.1162/neco.2008.04-07-510
   LeCun Y., MNIST DATABASE HANDW
   Mohamed AR, 2012, IEEE T AUDIO SPEECH, V20, P14, DOI 10.1109/TASL.2011.2109382
   Neal RM, 1996, STAT COMPUT, V6, P353, DOI 10.1007/BF00143556
   Rumelhart D.E., 1986, PARALLEL DISTRIBUTED, V1, P318, DOI DOI 10.1016/B978-1-4832-1446-7.50035-2
   Salakhutdinov R., 2009, ADV NEURAL INFORM PR, P1598
   Salakhutdinov R, 2009, ARTIF INTELL, P448, DOI DOI 10.1109/CVPRW.2009.5206577
   Salakhutdinov R, 2009, INT J APPROX REASON, V50, P969, DOI 10.1016/j.ijar.2008.11.006
   Smolensky P, 1986, PARALLEL DISTRIBUTED, P194
   Tieleman T., 2008, P 25 INT C MACH LEAR, P1064, DOI DOI 10.1145/1390156.1390290
   Tieleman T., 2009, P 26 ANN INT C MACH, P1033, DOI DOI 10.1145/1553374.1553506
   Welling M., 2005, ADV NEURAL INFORM PR, P1481
NR 32
TC 7
Z9 7
U1 0
U2 24
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0925-2312
EI 1872-8286
J9 NEUROCOMPUTING
JI Neurocomputing
PD SEP 2
PY 2014
VL 139
SI SI
BP 328
EP 335
DI 10.1016/j.neucom.2014.02.024
PG 8
WC Computer Science, Artificial Intelligence
SC Computer Science
GA AJ4QV
UT WOS:000337661800031
DA 2020-02-19
ER

PT J
AU Shen, CY
   Zhao, Q
AF Shen, Chengyao
   Zhao, Qi
TI Learning to predict eye fixations for semantic contents using
   multi-layer sparse network
SO NEUROCOMPUTING
LA English
DT Article
DE Semantic saliency; Gaze prediction; Sparse coding; Deep learning
ID OBJECT RECOGNITION; VISUAL-ATTENTION; SALIENCY; MECHANISM; SHIFTS
AB In this paper, we present a novel model for saliency prediction under a unified framework of feature integration. The model distinguishes itself by directly learning from natural images and automatically incorporating higher-level semantic information in a scalable manner for gaze prediction. Unlike most existing saliency models that rely on specific features or object detectors, our model learns multiple stages of features that mimic the hierarchical organization of the ventral stream in the visual cortex and integrate them by adapting their weights based on the ground-truth fixation data. To accomplish this, we utilize a multi-layer sparse network to learn low-, mid- and high-level features from natural images and train a linear support vector machine (SVM) for weight adaption and feature integration. Experimental results show that our model could learn high-level semantic features like faces and texts and can perform competitively among existing approaches in predicting eye fixations. (C) 2014 Elsevier B.V. All rights reserved.
C1 [Shen, Chengyao] Natl Univ Singapore, NUS Grad Sch Integrat Sci & Engn NGS, Singapore 117456, Singapore.
   [Zhao, Qi] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 117576, Singapore.
RP Zhao, Q (reprint author), Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 117576, Singapore.
EM eleqiz@nus.edu.sg
FU Singapore Ministry of Education Academic Research Fund Tier 1Ministry of
   Education, Singapore [R-263-000-648-133]
FX The work is supported by the Singapore Ministry of Education Academic
   Research Fund Tier 1 (No.R-263-000-648-133).
CR Barlow H. B., 1961, SENS COMMUN, P217, DOI DOI 10.7551/MITPRESS/9780262518420.003.0013
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Boureau Y., 2010, P 27 INT C MACH LEAR, P111, DOI DOI 10.1016/J.NEUNET.2012.02.023
   Cerf M., 2008, ADV NEURAL INF PROCE, V20
   Cerf M, 2009, J VISION, V9, DOI 10.1167/9.12.10
   Dan Y, 1996, J NEUROSCI, V16, P3351
   EINHAUSER W, 2008, J VIS, V8
   FUKUSHIMA K, 1980, BIOL CYBERN, V36, P193, DOI 10.1007/BF00344251
   Harel J, 2007, ADV NEURAL INF PROCE, P545, DOI DOI 10.7551/mitpress/7503.003.0073
   Hyvarinen A, 2009, COMPUT IMAGING VIS, V39, P1
   Itti L, 2000, VISION RES, V40, P1489, DOI 10.1016/S0042-6989(99)00163-7
   Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462
   KOCH C, 1985, HUM NEUROBIOL, V4, P219
   Le Q., ARXIV11126209 ARXIV
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Lee H., 2009, P 26 ANN INT C MACH, P609, DOI DOI 10.1145/1553374.1553453
   Lee H., 2007, ADV NEURAL INF PROCE, P801
   Li FF, 2007, COMPUT VIS IMAGE UND, V106, P59, DOI 10.1016/j.cviu.2005.09.012
   Lu Y, 2012, PROC CVPR IEEE, P1067, DOI 10.1109/CVPR.2012.6247785
   Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7
   Riesenhuber M, 1999, NAT NEUROSCI, V2, P1019
   Serre T, 2005, TECHNICAL REPORT
   Serre T, 2007, IEEE T PATTERN ANAL, V29, P411, DOI 10.1109/TPAMI.2007.56
   Shen C, 2012, DEEP LEARN UNS FEAT
   TREISMAN AM, 1980, COGNITIVE PSYCHOL, V12, P97, DOI 10.1016/0010-0285(80)90005-5
   VanRullen R, 2003, J PHYSIOL-PARIS, V97, P365, DOI 10.1016/j.jphysparis.2003.09.010
   Yang JM, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.19
   Zeiler MD, 2011, IEEE I CONF COMP VIS, P2018, DOI 10.1109/ICCV.2011.6126474
   Zhao Q., 2011, 2011 45 ANN C INF SC, P1
   Zhao Q, 2013, SIGNAL PROCESS, V93, P1401, DOI 10.1016/j.sigpro.2012.06.014
   Zhao Q, 2011, J VISION, V11, DOI 10.1167/11.3.9
NR 31
TC 36
Z9 37
U1 0
U2 30
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0925-2312
EI 1872-8286
J9 NEUROCOMPUTING
JI Neurocomputing
PD AUG 22
PY 2014
VL 138
BP 61
EP 68
DI 10.1016/j.neucom.2013.09.053
PG 8
WC Computer Science, Artificial Intelligence
SC Computer Science
GA AI9OK
UT WOS:000337261700006
DA 2020-02-19
ER

PT J
AU Hu, XL
   Zhang, JW
   Qi, P
   Zhang, B
AF Hu, Xiaolin
   Zhang, Jianwei
   Qi, Peng
   Zhang, Bo
TI Modeling response properties of V2 neurons using a hierarchical K-means
   model
SO NEUROCOMPUTING
LA English
DT Article
DE Neural network; Deep learning; K-means; V1; V2
ID QUADRATIC-PROGRAMMING PROBLEMS; NATURAL SCENES; NEURAL-NETWORK; AREA V2;
   EMERGENCE
AB Many computational models have been proposed for interpreting the properties of neurons in the primary visual cortex (V1). But relatively fewer models have been proposed for interpreting the properties of neurons beyond VI. Recently, it was found that the sparse deep belief network (DBN) could reproduce some properties of the secondary visual cortex (V2) neurons when trained on natural images. In this paper, by investigating the key factors that contribute to the success of the sparse DBN, we propose a hierarchical model based on a simple algorithm, K-means, which can be realized by competitive Hebbian learning. The resulting model exhibits some response properties of V2 neurons, and it is more biologically feasible and computationally efficient than the sparse DBN. (C) 2014 Elsevier B.V. All rights reserved.
C1 [Hu, Xiaolin; Qi, Peng; Zhang, Bo] Tsinghua Univ, State Key Lab Intelligent Technol & Syst, Tsinghua Natl Lab Informat Sci & Technol TNList, Beijing 100084, Peoples R China.
   [Hu, Xiaolin; Qi, Peng; Zhang, Bo] Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
   [Zhang, Jianwei] Univ Hamburg, Dept Informat, D-22527 Hamburg, Germany.
RP Hu, XL (reprint author), Tsinghua Univ, State Key Lab Intelligent Technol & Syst, Tsinghua Natl Lab Informat Sci & Technol TNList, Beijing 100084, Peoples R China.
EM xlhu@tsinghua.edu.cn
RI Hu, Xiaolin/K-2443-2013
FU National Basic Research Program (973 Program) of ChinaNational Basic
   Research Program of China [2013CB329403, 2012CB316301]; National Natural
   Science Foundation of ChinaNational Natural Science Foundation of China
   [61273023, 91120011]; Tsinghua University Initiative Scientific Research
   Program [20121088071]; Tsinghua National Laboratory for Information
   Science and Technology (TNList) Cross-discipline Foundation; DFG-MOE
   International Research Training Group IGK 1247 CINACS
FX This work was supported by the National Basic Research Program (973
   Program) of China (Grant Nos. 2013CB329403 and 2012CB316301), National
   Natural Science Foundation of China (Grant Nos. 61273023 and 91120011),
   Tsinghua University Initiative Scientific Research Program No.
   20121088071, Tsinghua National Laboratory for Information Science and
   Technology (TNList) Cross-discipline Foundation and the DFG-MOE
   International Research Training Group IGK 1247 CINACS.
CR Anzai A, 2007, NAT NEUROSCI, V10, P1313, DOI 10.1038/nn1975
   Bell AJ, 1997, VISION RES, V37, P3327, DOI 10.1016/S0042-6989(97)00121-1
   Bishop CM, 2006, PATTERN RECOGNITION
   Cadieu C, 2007, J NEUROPHYSIOL, V98, P1733, DOI 10.1152/jn.01265.2006
   Coates A., 2011, P 14 INT C ART INT S
   COULTRIP R, 1992, NEURAL NETWORKS, V5, P47, DOI 10.1016/S0893-6080(05)80006-1
   Dayan P, 2001, THEORETICAL NEUROSCI
   Ekanadham C, 2007, THESIS STANFORD U
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hinton GE., 2010, PRACTICAL GUIDE TRAI
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hu XL, 2008, IEEE T NEURAL NETWOR, V19, P2022, DOI 10.1109/TNN.2008.2003287
   Hu XL, 2009, IEEE T NEURAL NETWOR, V20, P654, DOI 10.1109/TNN.2008.2011266
   HUBEL DH, 1965, J NEUROPHYSIOL, V28, P229
   Ito M, 2004, J NEUROSCI, V24, P3313, DOI 10.1523/JNEUROSCI.4364-03.2004
   Karklin Y, 2009, NATURE, V457, P83, DOI 10.1038/nature07481
   Le Q. V., 2012, P 29 INT C MACH LEAR, P81, DOI DOI 10.1109/MSP.2011.940881
   Lee H., 2007, ADV NEURAL INFORM PR
   LEE H, 2009, P 26 ANN INT C MACH, P609, DOI DOI 10.1145/1553374.1553453
   Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7
   Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0
   P Qi, NEURAL COMP IN PRESS
   Ranzato M., 2007, ADV NEURAL INFORM PR, V20
   Riesenhuber M, 1999, NAT NEUROSCI, V2, P1019
   Saxe A.M., 2011, ADV NEURAL INFORM PR, V24, P1971
   Sohn K., 2011, P INT C COMP VIS
   Yan KL, 2005, NEURAL COMPUT, V17, P397, DOI 10.1162/0899766053011474
NR 27
TC 4
Z9 4
U1 0
U2 10
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0925-2312
EI 1872-8286
J9 NEUROCOMPUTING
JI Neurocomputing
PD JUN 25
PY 2014
VL 134
SI SI
BP 198
EP 205
DI 10.1016/j.neucom.2013.07.052
PG 8
WC Computer Science, Artificial Intelligence
SC Computer Science
GA AG5VE
UT WOS:000335486000026
DA 2020-02-19
ER

PT J
AU Chen, YS
   Lin, ZH
   Zhao, X
   Wang, G
   Gu, YF
AF Chen, Yushi
   Lin, Zhouhan
   Zhao, Xing
   Wang, Gang
   Gu, Yanfeng
TI Deep Learning-Based Classification of Hyperspectral Data
SO IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE
   SENSING
LA English
DT Article
DE Autoencoder (AE); deep learning; feature extraction; hyperspectral data
   classification; logistic regression; stacked autoencoder (SAE); support
   vector machine (SVM)
ID DIMENSIONALITY REDUCTION; SPATIAL CLASSIFICATION; IMAGE CLASSIFICATION;
   FEATURE-SELECTION; BELIEF NETWORKS; REPRESENTATION; ALGORITHM
AB Classification is one of the most popular topics in hyperspectral remote sensing. In the last two decades, a huge number of methods were proposed to deal with the hyperspectral data classification problem. However, most of them do not hierarchically extract deep features. In this paper, the concept of deep learning is introduced into hyperspectral data classification for the first time. First, we verify the eligibility of stacked autoencoders by following classical spectral information-based classification. Second, a new way of classifying with spatial-dominated information is proposed. We then propose a novel deep learning framework to merge the two features, from which we can get the highest classification accuracy. The framework is a hybrid of principle component analysis (PCA), deep learning architecture, and logistic regression. Specifically, as a deep learning architecture, stacked autoencoders are aimed to get useful high-level features. Experimental results with widely-used hyperspectral data indicate that classifiers built in this deep learning-based framework provide competitive performance. In addition, the proposed joint spectral-spatial deep neural network opens a new window for future research, showcasing the deep learning-based methods' huge potential for accurate hyperspectral data classification.
C1 [Chen, Yushi; Lin, Zhouhan; Zhao, Xing; Gu, Yanfeng] Harbin Inst Technol, Inst Image & Informat Technol, Harbin 150001, Peoples R China.
   [Wang, Gang] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
RP Chen, YS (reprint author), Harbin Inst Technol, Inst Image & Informat Technol, Harbin 150001, Peoples R China.
EM chenyushi@hit.edu.cn; lin.zhouhan@gmail.com; zhaoxing@hit.edu.cn;
   wanggang@ntu.edu.sg; yfgu@hit.edu.cn
RI Gu, Yanfeng/F-7781-2015; Wang, Gang/B-7027-2013
OI Gu, Yanfeng/0000-0003-1625-7989; 
FU Fundamental Research Funds for the Central UniversitiesFundamental
   Research Funds for the Central Universities [HIT.NSRIF.2013028];
   National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61301206, 61371180]
FX This work was supported in part by the Fundamental Research Funds for
   the Central Universities under Grant HIT.NSRIF.2013028 and in part by
   National Natural Science Foundation of China under Grant 61301206 and
   Grant 61371180.
CR Ambikapathi A., 2013, P IEEE WHISP GAIN FL
   Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bioucas-Dias JM, 2013, IEEE GEOSC REM SEN M, V1, P6, DOI 10.1109/MGRS.2013.2244672
   Bruce LM, 2002, IEEE T GEOSCI REMOTE, V40, P2331, DOI 10.1109/TGRS.2002.804721
   Camps-Valls G, 2005, IEEE T GEOSCI REMOTE, V43, P1351, DOI 10.1109/TGRS.2005.846154
   Chang CI, 1999, IEEE T GEOSCI REMOTE, V37, P2631, DOI 10.1109/36.803411
   Egerton R, 1996, ELECT ENERGY LOSS SP
   Fauvel M, 2008, IEEE T GEOSCI REMOTE, V46, P3804, DOI 10.1109/TGRS.2008.922034
   Foody GM, 2004, IEEE T GEOSCI REMOTE, V42, P1335, DOI 10.1109/TGRS.2004.827257
   FUKUSHIMA K, 1980, BIOL CYBERN, V36, P193, DOI 10.1007/BF00344251
   Gowen AA, 2007, TRENDS FOOD SCI TECH, V18, P590, DOI 10.1016/j.tifs.2007.06.001
   Gualtieri JA, 2000, INT GEOSCI REMOTE SE, P813, DOI 10.1109/IGARSS.2000.861712
   HARSANYI JC, 1994, IEEE T GEOSCI REMOTE, V32, P779, DOI 10.1109/36.298007
   Hecht-Nielsen R, 1988, IEEE T NEURAL NETWOR, DOI [DOI 10.1109/IJCNN.1989.118638, 10.1016/0893-6080(88)90469-8]
   Hege K, 2003, P SOC PHOTO-OPT INS, V5159, P380, DOI 10.1117/12.506426
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton G. E., 2010, TR2010003 UTML DEP C
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Jimenez LO, 1999, IEEE T GEOSCI REMOTE, V37, P2653, DOI 10.1109/36.803413
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kruger N, 2013, IEEE T PATTERN ANAL, V35, P1847, DOI 10.1109/TPAMI.2012.272
   Lacar FM, 2001, INT GEOSCI REMOTE SE, P2875, DOI 10.1109/IGARSS.2001.978191
   Landgrebe D, 2002, IEEE SIGNAL PROC MAG, V19, P17, DOI 10.1109/79.974718
   Le Roux N, 2010, NEURAL COMPUT, V22, P2192, DOI 10.1162/neco.2010.08-09-1081
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Li J, 2013, IEEE T GEOSCI REMOTE, V51, P844, DOI 10.1109/TGRS.2012.2205263
   Liu JJ, 2013, IEEE J-STARS, V6, P2462, DOI 10.1109/JSTARS.2013.2252150
   Malthus TJ, 2003, INT J REMOTE SENS, V24, P2805, DOI 10.1080/0143116031000066954
   Melgani F, 2004, IEEE T GEOSCI REMOTE, V42, P1778, DOI 10.1109/TGRS.2004.831865
   Mohamed AR, 2011, INT CONF ACOUST SPEE, P5060
   Plaza A., 2009, MACHINE LEARNING SIG, P1
   Rajan S, 2008, IEEE T GEOSCI REMOTE, V46, P1231, DOI 10.1109/TGRS.2007.910220
   Richards J. A., 2013, REMOTE SENSING DIGIT
   Salakhutdinov R, 2009, ARTIF INTELL, P448, DOI DOI 10.1109/CVPRW.2009.5206577
   Samadzadegan F, 2012, CAN J REMOTE SENS, V38, P139, DOI 10.5589/m12-022
   Serpico SB, 2001, IEEE T GEOSCI REMOTE, V39, P1360, DOI 10.1109/36.934069
   Sutskever I, 2008, NEURAL COMPUT, V20, P2629, DOI 10.1162/neco.2008.12-07-661
   Tarabalka Y, 2009, IEEE T GEOSCI REMOTE, V47, P2973, DOI 10.1109/TGRS.2009.2016214
   THOMPSON WD, 1988, J CLIN EPIDEMIOL, V41, P949, DOI 10.1016/0895-4356(88)90031-5
   Van der Meer F., 2004, INT J APPL EARTH OBS, V5, P55, DOI DOI 10.1016/J.JAG.2003.09.001
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Yu D., 2009, P NIPS WORKSH, P1
   Yuen PWT, 2010, IMAGING SCI J, V58, P241, DOI 10.1179/174313110X12771950995716
   Zhu Z, 2012, REMOTE SENS ENVIRON, V117, P72, DOI 10.1016/j.rse.2011.07.020
   Zhuo L., 2008, P GEOINF JOINT C GIS
   Zuo Z, 2014, IEEE SIGNAL PROC LET, V21, P1159, DOI 10.1109/LSP.2014.2298888
NR 47
TC 622
Z9 682
U1 114
U2 651
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1939-1404
EI 2151-1535
J9 IEEE J-STARS
JI IEEE J. Sel. Top. Appl. Earth Observ. Remote Sens.
PD JUN
PY 2014
VL 7
IS 6
SI SI
BP 2094
EP 2107
DI 10.1109/JSTARS.2014.2329330
PG 14
WC Engineering, Electrical & Electronic; Geography, Physical; Remote
   Sensing; Imaging Science & Photographic Technology
SC Engineering; Physical Geography; Remote Sensing; Imaging Science &
   Photographic Technology
GA AN5HK
UT WOS:000340621200022
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Srivastava, N
   Hinton, G
   Krizhevsky, A
   Sutskever, I
   Salakhutdinov, R
AF Srivastava, Nitish
   Hinton, Geoffrey
   Krizhevsky, Alex
   Sutskever, Ilya
   Salakhutdinov, Ruslan
TI Dropout: A Simple Way to Prevent Neural Networks from Overfitting
SO JOURNAL OF MACHINE LEARNING RESEARCH
LA English
DT Article
DE neural networks; regularization; model combination; deep learning
AB Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different "thinned" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.
C1 [Srivastava, Nitish; Hinton, Geoffrey; Krizhevsky, Alex; Sutskever, Ilya; Salakhutdinov, Ruslan] Univ Toronto, Dept Comp Sci, Toronto, ON M5G 3G4, Canada.
RP Srivastava, N (reprint author), Univ Toronto, Dept Comp Sci, 10 Kings Coll Rd,Rm 3302, Toronto, ON M5G 3G4, Canada.
EM nitish@cs.toronto.edu; hinton@cs.toronto.edu; kriz@cs.toronto.edu;
   ilya@cs.toronto.edu; rsalakhu@cs.toronto.edu
FU OGSOntario Graduate Scholarship; NSERCNatural Sciences and Engineering
   Research Council of Canada; Early Researcher Award
FX This research was supported by OGS, NSERC and an Early Researcher Award.
CR Chen M., 2012, P 29 INT C MACH LEAR, P767, DOI DOI 10.1007/S11222-007-9033-Z
   Dahl G., 2010, ADV NEURAL INFORM PR, P469
   Dekel O, 2010, MACH LEARN, V81, P149, DOI 10.1007/s10994-009-5124-8
   Fergus Rob, 2013, CORR
   Globerson A., 2006, P 23 INT C MACH LEAR, P353
   Goodfellow I. J., 2013, JMLR W CP, P1319
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Jarrett K., 2009, P INT C COMP VIS ICC
   Krizhevsky A., 2009, TECHNICAL REPORT
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Lin Y., 2010, LARGE SCALE VISUAL R
   Livnat A, 2010, P NATL ACAD SCI USA, V107, P1452, DOI 10.1073/pnas.0910734106
   Maaten Laurens, 2013, P 30 INT C MACH LEAR, V28, P410
   Mnih V., 2009, 2009004 UTML U TOR D
   Mohamed A., 2010, IEEE T AUDIO SPEECH
   Neal Radford M., 1996, BAYESIAN LEARNING NE
   Netzer Y, 2011, NIPS WORKSH DEEP LEA
   Nowlan S. J., 1992, NEURAL COMPUTATION, V4
   Povey Daniel, 2011, IEEE 2011 WORKSH AUT
   Salakhutdinov R., 2008, P 25 INT C MACH LEAR
   Salakhutdinov R, 2009, ARTIF INTELL, P448, DOI DOI 10.1109/CVPRW.2009.5206577
   Sanchez J, 2011, PROC CVPR IEEE, P1665, DOI 10.1109/CVPR.2011.5995504
   Sermanet P, 2012, INT C PATT REC ICPR
   Simard PY, 2003, PROC INT CONF DOC, P958
   Snoek J., 2012, ADV NEURAL INFORM PR, P2960
   Srebro N, 2005, LECT NOTES COMPUT SC, V3559, P545, DOI 10.1007/11503415_37
   Srivastava N., 2013, THESIS U TORONTO
   Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267
   Tikhonov N., 1943, DOKL AKAD NAUK SSSR, V39, P195
   Vincent P, 2008, P 25 INT C MACH LEAR, P1096, DOI DOI 10.1145/1390156.1390294
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Wager S., 2013, ADV NEURAL INFORM PR, V26, P351
   Wang Sida, 2013, P 30 INT C MACH LEAR, P118
   Xiong HY, 2011, BIOINFORMATICS, V27, P2554, DOI 10.1093/bioinformatics/btr444
NR 36
TC 6339
Z9 6633
U1 169
U2 832
PU MICROTOME PUBL
PI BROOKLINE
PA 31 GIBBS ST, BROOKLINE, MA 02446 USA
SN 1532-4435
J9 J MACH LEARN RES
JI J. Mach. Learn. Res.
PD JUN
PY 2014
VL 15
BP 1929
EP 1958
PG 30
WC Automation & Control Systems; Computer Science, Artificial Intelligence
SC Automation & Control Systems; Computer Science
GA AT0PS
UT WOS:000344638300002
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Noda, K
   Arie, H
   Suga, Y
   Ogata, T
AF Noda, Kuniaki
   Arie, Hiroaki
   Suga, Yuki
   Ogata, Tetsuya
TI Multimodal integration learning of robot behavior using deep neural
   networks
SO ROBOTICS AND AUTONOMOUS SYSTEMS
LA English
DT Article
DE Object manipulation; Multimodal integration; Cross-modal memory
   retrieval; Deep learning
ID MEMORY-IMPAIRED INDIVIDUALS
AB For humans to accurately understand the world around them, multimodal integration is essential because it enhances perceptual precision and reduces ambiguity. Computational models replicating such human ability may contribute to the practical use of robots in daily human living environments; however, primarily because of scalability problems that conventional machine learning algorithms suffer from, sensory-motor information processing in robotic applications has typically been achieved via modal-dependent processes. In this paper, we propose a novel computational framework enabling the integration of sensory-motor time-series data and the self-organization of multimodal fused representations based on a deep learning approach. To evaluate our proposed model, we conducted two behavior-learning experiments utilizing a humanoid robot; the experiments consisted of object manipulation and bell-ringing tasks. From our experimental results, we show that large amounts of sensory-motor information, including raw RGB images, sound spectrums, and joint angles, are directly fused to generate higher-level multimodal representations. Further, we demonstrated that our proposed framework realizes the following three functions: (1) cross-modal memory retrieval utilizing the information complementation capability of the deep autoencoder; (2) noise-robust behavior recognition utilizing the generalization capability of multimodal features; and (3) multimodal causality acquisition and sensory-motor prediction based on the acquired causality. (C) 2014 The Authors. Published by Elsevier B.V.
C1 [Noda, Kuniaki; Arie, Hiroaki; Suga, Yuki; Ogata, Tetsuya] Waseda Univ, Grad Sch Fundamental Sci & Engn, Dept Intermedia Art & Sci, Shinjuku Ku, Tokyo 1698555, Japan.
RP Noda, K (reprint author), Waseda Univ, Grad Sch Fundamental Sci & Engn, Dept Intermedia Art & Sci, Shinjuku Ku, 3-4-1 Okubo, Tokyo 1698555, Japan.
EM kuniaki.noda@akane.waseda.jp; arie@aoni.waseda.jp; ysuga@ysuga.net;
   ogata@waseda.jp
OI Ogata, Tetsuya/0000-0001-7015-0379
FU JST PRESTO "Information Environment and Humans"; MEXTMinistry of
   Education, Culture, Sports, Science and Technology, Japan (MEXT)
   [24119003]
FX This work has been supported by JST PRESTO "Information Environment and
   Humans" and MEXT Grant-in-Aid for Scientific Research on Innovative
   Areas "Constructive Developmental Science" (24119003).
CR Aldebaran Robotics, 2012, NAO HUM
   Bekkerman Ron, 2011, SCALING MACHINE LEAR
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Brooks RA, 1998, FIFTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-98) AND TENTH CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICAL INTELLIGENCE (IAAI-98) - PROCEEDINGS, P961
   Chuck Rosenberg, 2013, IMPROVING PHOTO SEAR
   Coen M.H., 2001, P INT JOINT C ART IN, V17, P1417
   Deneve S, 2004, J PHYSIOLOGY-PARIS, V98, P249, DOI 10.1016/j.jphysparis.2004.03.011
   Dewey J., 1896, PSYCHOL REV, V3, P357, DOI DOI 10.1037/H0070405
   Ernst MO, 2004, TRENDS COGN SCI, V8, P162, DOI 10.1016/j.tics.2004.02.002
   Franc V., STAT PATTERN RECOGNI
   Gallagher S, 2000, TRENDS COGN SCI, V4, P14, DOI 10.1016/S1364-6613(99)01417-5
   Grilli MD, 2011, J INT NEUROPSYCH SOC, V17, P929, DOI 10.1017/S1355617711000737
   Grilli MD, 2010, NEUROPSYCHOLOGY, V24, P698, DOI 10.1037/a0020318
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Jauffret Adrien, 2012, From Animals to Animats 12. Proceedings of the 12th International Conference on Simulation of Adaptive Behavior, SAB 2012, P136, DOI 10.1007/978-3-642-33093-3_14
   Kaneko K, 2004, IEEE INT CONF ROBOT, P1083, DOI 10.1109/ROBOT.2004.1307969
   Kawabe T, 2013, P ROY SOC B-BIOL SCI, V280, DOI 10.1098/rspb.2013.0991
   Krizhevsky A., 2011, P 19 EUR S ART NEUR
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kuriyama T., 2010, P 10 INT C EP ROB OR, P57
   LANG KJ, 1990, NEURAL NETWORKS, V3, P23, DOI 10.1016/0893-6080(90)90044-L
   Le Q. V., 2012, P 29 INT C MACH LEAR, P81, DOI DOI 10.1109/MSP.2011.940881
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Martens J., 2011, P 28 INT C MACH LEAR, P1033
   Martens J., 2010, P 27 INT C MACH LEAR, P735, DOI DOI 10.1155/2011/176802
   Murphy R. R., 2000, INTRO AI ROBOTICS
   Ngiam J, 2011, IEEE INT C MACH LEAR, V28, P689, DOI DOI 10.1145/2647868
   Ogino M, 2006, ROBOT AUTON SYST, V54, P414, DOI 10.1016/j.robot.2006.01.005
   PEARLMUTTER BA, 1994, NEURAL COMPUT, V6, P147, DOI 10.1162/neco.1994.6.1.147
   Pitto A., 2012, P IEEE INT C DEV LEA, P1
   Pouget A, 2002, NAT REV NEUROSCI, V3, P741, DOI 10.1038/nrn914
   Robert Hof, 2013, MEET GUY WHO HELPED
   Sakagami Y, 2002, 2002 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-3, PROCEEDINGS, P2478, DOI 10.1109/IRDS.2002.1041641
   Sauser EL, 2006, 2006 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-12, P5619, DOI 10.1109/IROS.2006.282283
   Schraudolph NN, 2002, NEURAL COMPUT, V14, P1723, DOI 10.1162/08997660260028683
   Srivastava N., 2012, ADV NEURAL INFORM PR, V25, P2231
   Stein B. E., 1993, MERGING SENSES
   Sutskever I., 2011, P 28 INT C MACH LEAR, P1017
   Willow Grarage, PERSONAL ROBOT 2 PR2
NR 40
TC 56
Z9 58
U1 6
U2 76
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0921-8890
EI 1872-793X
J9 ROBOT AUTON SYST
JI Robot. Auton. Syst.
PD JUN
PY 2014
VL 62
IS 6
BP 721
EP 736
DI 10.1016/j.robot.2014.03.003
PG 16
WC Automation & Control Systems; Computer Science, Artificial Intelligence;
   Robotics
SC Automation & Control Systems; Computer Science; Robotics
GA AH9NF
UT WOS:000336468400002
OA Other Gold
DA 2020-02-19
ER

PT J
AU Zhou, SS
   Chen, QC
   Wang, XL
AF Zhou, Shusen
   Chen, Qingcai
   Wang, Xiaolong
TI Fuzzy deep belief networks for semi-supervised sentiment classification
SO NEUROCOMPUTING
LA English
DT Article
DE Supervised learning; Deep learning; Fuzzy sets; Sentiment classification
ID NEURAL-NETWORKS
AB By embedding prior knowledge into the learning structure, this paper presents a two-step semi-supervised learning method called fuzzy deep belief networks (FDBN) for sentiment classification. First, we train the general deep belief networks (DBN) by the semi-supervised learning taken on training dataset. Then, we design a fuzzy membership function for each class of reviews based on the learned deep architecture. Since the training of DBN maps each review into the DBN output space, the distribution of all training samples in the space is treated as prior knowledge and is encoded by a series of fuzzy membership functions. Second, based on the fuzzy membership functions and the DBN obtained in the first step, a novel FDBN architecture is constructed and the supervised learning stage is applied to improve the classification performance of the FDBN. FDBN not only inherits the powerful abstraction ability of DBN, but also demonstrates the attractive fuzzy classification ability for handling sentiment data. To inherit the advantages of both active learning and FDBN, we also propose an active FDBN (AFD) semi-supervised learning method. The empirical validation on five sentiment classification datasets demonstrates the effectiveness of FDBN and AFD methods. Crown Copyright (C) 2013 Published by Elsevier B.V. All rights reserved.
C1 [Zhou, Shusen] Ludong Univ, Sch Informat & Elect Engn, Yantai, Peoples R China.
   [Chen, Qingcai; Wang, Xiaolong] Harbin Inst Technol, Shenzhen Grad Sch, Shenzhen, Peoples R China.
RP Zhou, SS (reprint author), Ludong Univ, Sch Informat & Elect Engn, Yantai, Peoples R China.
EM zhoushusen@hotmail.com; qingcai.chen@hitsz.edu.cn;
   wangxl@insun.hit.edu.cn
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61300155, 61173075]; Ludong University [LY2013004]
FX This work is supported in part by National Natural Science Foundation of
   China (No. 61300155 and No. 61173075) and Scientific Research Fund of
   Ludong University (LY2013004).
CR Aue A., 2005, INT C REC ADV NAT LA
   Bengio Y., 2007, LARGE SCALE KERNEL M
   Blitzer J., 2007, ACL, V45, P440, DOI DOI 10.1109/IRPS.2011.5784441
   Chapelle O., 2006, SEMISUPERVISED LEARN
   Collobert R, 2006, J MACH LEARN RES, V7, P1687
   Dasgupta S., 2009, P JOINT C 47 ANN M A, P701
   Dave K., 2003, P 12 INT C WORLD WID, P519, DOI DOI 10.1145/775152.775226
   Fu G., 2010, INT C COMP LING ASS, P312
   Goldberg A.B., 2006, P 1 WORKSH GRAPH BAS, P45, DOI DOI 10.3115/1654758.1654769
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Kamvar S.D., 2003, P 18 INT JOINT C ART, P561
   LEUBA G, 1994, ANAT EMBRYOL, V190, P351
   Li SS, 2010, ACL 2010: 48TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, P414
   Li Shoushan, 2010, P 23 INT C COMP LING, P635
   Li Shoushan, 2008, P 46 ANN M ASS COMP, P257
   Liu Y, 2010, SIGIR 2010: PROCEEDINGS OF THE 33RD ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH DEVELOPMENT IN INFORMATION RETRIEVAL, P873
   McDonald R., 2007, ACL 07, V45, P432
   Mullen T., 2004, P 2004 C EMP METH NA, V4, P412, DOI DOI 10.3115/1219044.1219069
   Ng V., 2006, P COLING ACL 2006 MA, P611, DOI DOI 10.3115/1273073.1273152
   Pan SJ, 2010, P 19 INT C WORLD WID, P751, DOI DOI 10.1145/1772690.1772767
   Pang B, 2002, PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING, P79
   Pang B., 2004, ANN M ASS COMP LING, V42, P271, DOI DOI 10.3115/1218955.1218990
   Purushothaman G, 1997, IEEE T NEURAL NETWOR, V8, P679, DOI 10.1109/72.572106
   Rutkowska D., 2002, NEUROFUZZY ARCHITECT
   SIMPSON PK, 1992, IEEE T NEURAL NETWOR, V3, P776, DOI 10.1109/72.159066
   Sindhwani V, 2008, IEEE DATA MINING, P1025, DOI 10.1109/ICDM.2008.113
   Smolensky P, 1986, PARALLEL DISTRIBUTED, P194
   Songbo T, 2007, P 16 ACM C C INF KNO, P979, DOI DOI 10.1145/1321440.1321590
   Tong S, 2002, J MACH LEARN RES, V2, P45
   TREISMAN AM, 1980, COGNITIVE PSYCHOL, V12, P97, DOI 10.1016/0010-0285(80)90005-5
   Turney PD, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P417
   Wan X., 2009, P JOINT C 47 ANN M A, V1, P235, DOI DOI 10.3115/1687878.1687913
   Wang M, 2012, ACM COMPUT SURV, V44, DOI 10.1145/2333112.2333120
   Wang M, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1899412.1899414
   Wang M, 2009, IEEE T CIRC SYST VID, V19, P733, DOI 10.1109/TCSVT.2009.2017400
   Wang XM, 2010, NEUROCOMPUTING, V73, P2186, DOI 10.1016/j.neucom.2010.01.021
   Wei W, 2010, ACL 2010: 48TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, P404
   Xia Y, 2008, ANN M ASS COMP LING, P133
   Yu J, 2012, IEEE T IMAGE PROCESS, V21, P4636, DOI 10.1109/TIP.2012.2207395
   Yu J, 2012, IEEE T IMAGE PROCESS, V21, P3262, DOI 10.1109/TIP.2012.2190083
   Yu J, 2012, NEUROCOMPUTING, V79, P105, DOI 10.1016/j.neucom.2011.10.003
   Yu J, 2011, IEEE T IMAGE PROCESS, V20, P3257, DOI 10.1109/TIP.2011.2158225
   ZADEH LA, 1965, INFORM CONTROL, V8, P338, DOI 10.1016/S0019-9958(65)90241-X
   Zagibalov T., 2008, P 22 INT C COMP LING, P1073, DOI DOI 10.3115/1599081.1599216
   Zeki S., 2008, SPLENDORS MISERIES B
   Zeki S., 1993, VISION BRAIN
   Zhou S., 2010, P 23 INT C COMP LING, P1515
   Zhu X., 2007, THESIS
NR 50
TC 36
Z9 39
U1 1
U2 88
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0925-2312
EI 1872-8286
J9 NEUROCOMPUTING
JI Neurocomputing
PD MAY 5
PY 2014
VL 131
BP 312
EP 322
DI 10.1016/j.neucom.2013.10.011
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA AC8SS
UT WOS:000332805700032
DA 2020-02-19
ER

PT J
AU Young, S
   Lu, JJ
   Holleman, J
   Arel, I
AF Young, Steven
   Lu, Junjie
   Holleman, Jeremy
   Arel, Itamar
TI On the Impact of Approximate Computation in an Analog DeSTIN
   Architecture
SO IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
LA English
DT Article
DE Analog circuits; analog computation; deep machine learning; feature
   extraction; floating gates
AB Deep machine learning (DML) holds the potential to revolutionize machine learning by automating rich feature extraction, which has become the primary bottleneck of human engineering in pattern recognition systems. However, the heavy computational burden renders DML systems implemented on conventional digital processors impractical for large-scale problems. The highly parallel computations required to implement large-scale deep learning systems are well suited to custom hardware. Analog computation has demonstrated power efficiency advantages of multiple orders of magnitude relative to digital systems while performing nonideal computations. In this paper, we investigate typical error sources introduced by analog computational elements and their impact on system-level performance in DeSTIN-a compositional deep learning architecture. These inaccuracies are evaluated on a pattern classification benchmark, clearly demonstrating the robustness of the underlying algorithm to the errors introduced by analog computational elements. A clear understanding of the impacts of nonideal computations is necessary to fully exploit the efficiency of analog circuits.
C1 [Young, Steven; Lu, Junjie; Holleman, Jeremy; Arel, Itamar] Univ Tennessee, Dept Elect Engn & Comp, Knoxville, TN 37996 USA.
RP Young, S (reprint author), Univ Tennessee, Dept Elect Engn & Comp, Knoxville, TN 37996 USA.
EM syoung22@eecs.utk.edu; jlu9@eecs.utk.edu; jhollema@eecs.utk.edu;
   itamar@eecs.utk.edu
OI Young, Steven/0000-0003-0591-4330
FU Intelligence Advanced Research Projects Activity via Army Research
   Office [W911NF 12-1-0017]; NSFNational Science Foundation (NSF)
   [CCF-1218492]
FX This work was supported in part by the Intelligence Advanced Research
   Projects Activity via Army Research Office under Grant W911NF 12-1-0017
   and in part by NSF under Grant CCF-1218492.
CR Adil F, 2002, 2002 45TH MIDWEST SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL I, CONFERENCE PROCEEDINGS, P251
   Arel I., 2009, P NIPS WORKSH DEEP L, P1
   Arel I, 2009, P AAAI WORKSH BIOL I, P1150
   Bengio Y., 2012, UNSUPERVISED FEATURE
   Bergstra J., 2011, J MACH LEARN RES, P1
   Bhargava R., 2003, P INT C MACH LEARN W, V3, P1
   BIALEK W, 1991, SCIENCE, V252, P1854, DOI 10.1126/science.2063199
   Chawla R, 2004, PROCEEDINGS OF THE IEEE 2004 CUSTOM INTEGRATED CIRCUITS CONFERENCE, P651, DOI 10.1109/CICC.2004.1358910
   Delbruck T., 1991, IJCNN-91-Seattle: International Joint Conference on Neural Networks (Cat. No.91CH3049-4), P475, DOI 10.1109/IJCNN.1991.155225
   Erhan D., 2009, J MACHINE LEARNING R, V5, P153
   Figueroa M, 2004, IEEE J SOLID-ST CIRC, V39, P1196, DOI 10.1109/JSSC.2004.829933
   FIGUEROA M, 2005, ADV NEURAL INFORM PR, V17, P441
   Gray P. R., 2001, ANAL DESIGN ANALOG I
   Hall TS, 2005, IEEE T CIRCUITS-I, V52, P2298, DOI 10.1109/TCSI.2005.853401
   Hamal P., 2010, ISMIR, P339
   Hasler P, 2005, IEEE T CIRCUITS-I, V52, P834, DOI 10.1109/TCSI.2005.846663
   Hasler P., 1995, Advances in Neural Information Processing Systems 7, P817
   Holleman J., 2009, THESIS U WASHINGTON
   Holleman J, 2008, IEEE J SOLID-ST CIRC, V43, P1324, DOI 10.1109/JSSC.2008.920327
   Holleman J, 2008, IEEE CUST INTEGR CIR, P333, DOI 10.1109/CICC.2008.4672089
   Hsu D, 2002, IEEE T NEURAL NETWOR, V13, P732, DOI 10.1109/TNN.2002.1000139
   Hsu D., 2003, ADV NEURAL INF PROCE, V15, P1107
   Larochelle H., 2007, P 24 INT C MACH LEAR, P473, DOI DOI 10.1145/1273496.1273556
   LeCun Y., 2010, MNIST HANDWRITTEN DI
   Liu S.-C., 2002, ANALOG VLSI CIRCUITS
   Ngiam J, 2011, IEEE INT C MACH LEAR, V28, P689, DOI DOI 10.1145/2647868
   PELGROM MJM, 1989, IEEE J SOLID-ST CIRC, V24, P1433, DOI 10.1109/JSSC.1989.572629
   Sarpeshkar R, 1998, NEURAL COMPUT, V10, P1601, DOI 10.1162/089976698300017052
   Simard PY, 2003, PROC INT CONF DOC, P958
   Tsividis Y., 2011, OPERATION MODELING M
   Twigg C, 2009, DIGIT SIGNAL PROCESS, V19, P904, DOI 10.1016/j.dsp.2007.09.013
   van Rossum MCW, 2003, J NEUROPHYSIOL, V89, P2406, DOI 10.1152/jn.01106.2002
   vanSteveninck RRD, 1997, SCIENCE, V275, P1805, DOI 10.1126/science.275.5307.1805
   Young Steven, 2010, Proceedings of the Seventh International Conference on Information Technology: New Generations (ITNG 2010), P204, DOI 10.1109/ITNG.2010.148
NR 34
TC 6
Z9 6
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 2162-237X
EI 2162-2388
J9 IEEE T NEUR NET LEAR
JI IEEE Trans. Neural Netw. Learn. Syst.
PD MAY
PY 2014
VL 25
IS 5
BP 934
EP 946
DI 10.1109/TNNLS.2013.2283730
PG 13
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Computer Science, Theory & Methods; Engineering,
   Electrical & Electronic
SC Computer Science; Engineering
GA AF5FC
UT WOS:000334738400007
PM 24808039
DA 2020-02-19
ER

PT J
AU Hu, H
   Pang, L
   Tian, DP
   Shi, ZZ
AF Hu, Hong
   Pang, Liang
   Tian, Dongping
   Shi, Zhongzhi
TI Perception granular computing in visual haze-free task
SO EXPERT SYSTEMS WITH APPLICATIONS
LA English
DT Article
DE Granular computing; Leveled granular system; Fuzzy logic; Machine
   learning; Haze free; Brain-like computer
ID INFORMATION GRANULATION; FUZZY
AB In the past decade, granular computing (GrC) has been an active topic of research in machine learning and computer vision. However, the granularity division is itself an open and complex problem. Deep learning, at the same time, has been proposed by Geoffrey Hinton, which simulates the hierarchical structure of human brain, processes data from lower level to higher level and gradually composes more and more semantic concepts. The information similarity, proximity and functionality constitute the key points in the original insight of granular computing proposed by Zadeh. Many GrC researches are based on the equivalence relation or the more general tolerance relation, either of which can be described by some distance functions. The information similarity and proximity depended on the samples distribution can be easily described by the fuzzy logic. From this point of view, GrC can be considered as a set of fuzzy logical formulas, which is geometrically defined as a layered framework in a multi-scale granular system. The necessity of such kind multi-scale layered granular system can be supported by the columnar organization of the neocortex. So the granular system proposed in this paper can be viewed as a new explanation of deep learning that simulates the hierarchical structure of human brain. In view of this, a novel learning approach, which combines fuzzy logical designing with machine learning, is proposed in this paper to construct a GrC system to explore a novel direction for deep learning. Unlike those previous works on the theoretical framework of GrC, our granular system is abstracted from brain science and information science, so it can be used to guide the research of image processing and pattern recognition. Finally, we take the task of haze-free as an example to demonstrate that our multi-scale GrC has high ability to increase the texture information entropy and improve the effect of haze-removing. Crown Copyright (C) 2013 Published by Elsevier Ltd. All rights reserved.
C1 [Hu, Hong; Pang, Liang; Tian, Dongping; Shi, Zhongzhi] Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100080, Peoples R China.
RP Pang, L (reprint author), Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100080, Peoples R China.
EM huhong@ict.ac.cn; pangl@ics.ict.ac.cn; tiandp@ics.ict.ac.cn;
   shizz@ics.ict.ac.cn
FU National Program on Key Basic Research Project (973 Program)National
   Basic Research Program of China [2013CB329502]; National Natural Science
   Foundation of ChinaNational Natural Science Foundation of China
   [61072085, 61035003, 61202212, 60933004]; National High-tech R and D
   Program of China (863 Program)National High Technology Research and
   Development Program of China [2012AA011003]; National Science and
   Technology Support Program [2012BA107B02]; China Information Technology
   Security Evaluation Center [CNITSEC-KY-2012-006/1]
FX This work is partially supported by the National Program on Key Basic
   Research Project (973 Program) (No. 2013CB329502), the National Natural
   Science Foundation of China (Nos. 61072085, 61035003, 61202212,
   60933004), the National High-tech R and D Program of China (863 Program)
   (No. 2012AA011003), the National Science and Technology Support Program
   (2012BA107B02) and the China Information Technology Security Evaluation
   Center (CNITSEC-KY-2012-006/1).
CR Bargiela A, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON GRANULAR COMPUTING, P806, DOI 10.1109/GRC.2006.1635922
   CASTRO JL, 1995, IEEE T SYST MAN CYB, V25, P629, DOI 10.1109/21.370193
   Fung G., 2001, KDD-2001. Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P77
   HAYKIN S, 2008, NEURAL NETWORKS COMP
   Haykin S, 1994, NEURAL NETWORKS COMP
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   Levin A, 2008, IEEE T PATTERN ANAL, V30, P228, DOI 10.1109/TPAMI.2007.1177
   Li HX, 2000, IEEE T NEURAL NETWOR, V11, P356, DOI 10.1109/72.839006
   Lin T. Y., 1999, COMPUTING WORDS INFO, P183
   Lin T.Y., 2007, NEIGHBORHOOD SYSTEMS
   Lin Tsau Young, 2012, COMPUT COMPLEX, P1404
   LIN TY, 1998, ROUGH SETS KNOWLEDGE, P107
   Liu H., 2012, MATH COMPUTER MODELL
   Mountcastle VB, 1997, BRAIN, V120, P701, DOI 10.1093/brain/120.4.701
   Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4
   Pedrycz A, 2012, FUZZY SET SYST, V203, P17, DOI 10.1016/j.fss.2012.03.009
   Yao Y., 2006, DEF SEC S, P624105
   Yao YY, 2001, INT J INTELL SYST, V16, P87, DOI 10.1002/1098-111X(200101)16:1<87::AID-INT7>3.0.CO;2-S
   Yao YY, 1998, INFORM SCIENCES, V111, P239, DOI 10.1016/S0020-0255(98)10006-3
   Yao YY, 2000, PROCEEDINGS OF THE FIFTH JOINT CONFERENCE ON INFORMATION SCIENCES, VOLS 1 AND 2, P186
   Yao YY, 2001, P INT COMP SOFTW APP, P638, DOI 10.1109/CMPSAC.2001.960680
   YAO YY, 2013, [No title captured], V13, P307
   Yao YY, 1999, ADV SOFT COMPUTING E, P539, DOI 10.1007/978-1-4471-0819-1_40
   Zadeh LA, 1997, FUZZY SET SYST, V90, P111, DOI 10.1016/S0165-0114(97)00077-8
   ZADEH LA, 1965, INFORM CONTROL, V8, P338, DOI 10.1016/S0019-9958(65)90241-X
   Zhang L, 2004, FUND INFORM, V59, P287
   Zhang L, 2005, ALGORITHMICA, V43, P1, DOI 10.1007/s00453-005-1154-1
   ZHANG L, 2003, CHINESE J SOFTW, V14, P770
NR 28
TC 7
Z9 8
U1 2
U2 55
PU PERGAMON-ELSEVIER SCIENCE LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
SN 0957-4174
EI 1873-6793
J9 EXPERT SYST APPL
JI Expert Syst. Appl.
PD MAY
PY 2014
VL 41
IS 6
BP 2729
EP 2741
DI 10.1016/j.eswa.2013.11.006
PG 13
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic; Operations Research & Management Science
SC Computer Science; Engineering; Operations Research & Management Science
GA AA3TX
UT WOS:000331018000011
DA 2020-02-19
ER

PT J
AU Sarikaya, R
   Hinton, GE
   Deoras, A
AF Sarikaya, Ruhi
   Hinton, Geoffrey E.
   Deoras, Anoop
TI Application of Deep Belief Networks for Natural Language Understanding
SO IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING
LA English
DT Article
DE Call-Routing; DBN; Deep Learning; Deep Neural Nets; Natural language
   Understanding; RBM
AB Applications of Deep Belief Nets (DBN) to various problems have been the subject of a number of recent studies ranging from image classification and speech recognition to audio classification. In this study we apply DBNs to a natural language understanding problem. The recent surge of activity in this area was largely spurred by the development of a greedy layer-wise pretraining method that uses an efficient learning algorithm called Contrastive Divergence (CD). CD allows DBNs to learn a multi-layer generative model from unlabeled data and the features discovered by this model are then used to initialize a feed-forward neural network which is fine-tuned with backpropagation. We compare a DBN-initialized neural network to three widely used text classification algorithms: Support Vector Machines (SVM), boosting and Maximum Entropy (MaxEnt). The plain DBN-based model gives a call-routing classification accuracy that is equal to the best of the other models. However, using additional unlabeled data for DBN pre-training and combining DBN-based learned features with the original features provides significant gains over SVMs, which, in turn, performed better than both MaxEnt and Boosting.
C1 [Sarikaya, Ruhi; Deoras, Anoop] Microsoft Corp, Redmond, WA 98052 USA.
   [Hinton, Geoffrey E.] Univ Toronto, Dept Comp Sci, Toronto, ON M5S 3G4, Canada.
RP Sarikaya, R (reprint author), Microsoft Corp, Redmond, WA 98052 USA.
EM ruhi.sarikaya@microsoft.com; hinton@cs.toronto.edu;
   anoop.deoras@microsoft.com
CR Chen SF, 2000, IEEE T SPEECH AUDI P, V8, P37, DOI 10.1109/89.817452
   Dahl G., 2010, ADV NEURAL INFORM PR
   DellaPietra S, 1997, IEEE T PATTERN ANAL, V19, P380, DOI 10.1109/34.588021
   Erhan D, 2010, J MACH LEARN RES, V11, P625
   Gorin AL, 1997, SPEECH COMMUN, V23, P113, DOI 10.1016/S0167-6393(97)00040-X
   Haffner P, 2003, 2003 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL I, PROCEEDINGS, P632
   Hinton G., 2010003 UTML TR
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hinton GE, 2007, TRENDS COGN SCI, V11, P428, DOI 10.1016/j.tics.2007.09.004
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Lafferty J., 2001, P INT C MACH LEARN
   Ng A. Y., 2002, ADV NEURAL INFORM PR, V11
   Price P. J., 1990, P DARPA WORKSH SPEEC
   Raymond C., 2007, P INTERSPEECH
   Sarikaya R., 2005, P INTERSPEECH
   Sarikaya R, 2011, INT CONF ACOUST SPEE, P5680
   Schapire RE, 2000, MACH LEARN, V39, P135, DOI 10.1023/A:1007649029923
   Tur G., 2011, SPOKEN LANGUAGE UNDE
   Vapnik VN, 1995, NATURE STAT LEARNING
   Wang Y.-Y., 2006, P ICSLP
   Welling M., 2005, ADV NEURAL INFORM PR, P1481
NR 21
TC 161
Z9 186
U1 11
U2 181
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 2329-9290
J9 IEEE-ACM T AUDIO SPE
JI IEEE-ACM Trans. Audio Speech Lang.
PD APR
PY 2014
VL 22
IS 4
BP 778
EP 784
DI 10.1109/TASLP.2014.2303296
PG 7
WC Acoustics; Engineering, Electrical & Electronic
SC Acoustics; Engineering
GA AD5YZ
UT WOS:000333330900002
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Qin, FW
   Li, LY
   Gao, SM
   Yang, XL
   Chen, X
AF Qin, Fei-wei
   Li, Lu-ye
   Gao, Shu-ming
   Yang, Xiao-ling
   Chen, Xiang
TI A deep learning approach to the classification of 3D CAD models
SO JOURNAL OF ZHEJIANG UNIVERSITY-SCIENCE C-COMPUTERS & ELECTRONICS
LA English
DT Article
DE CAD model classification; Design reuse; Machine learning; Neural network
ID NEURAL-NETWORK; DESIGN
AB Model classification is essential to the management and reuse of 3D CAD models. Manual model classification is laborious and error prone. At the same time, the automatic classification methods are scarce due to the intrinsic complexity of 3D CAD models. In this paper, we propose an automatic 3D CAD model classification approach based on deep neural networks. According to prior knowledge of the CAD domain, features are selected and extracted from 3D CAD models first, and then preprocessed as high dimensional input vectors for category recognition. By analogy with the thinking process of engineers, a deep neural network classifier for 3D CAD models is constructed with the aid of deep learning techniques. To obtain an optimal solution, multiple strategies are appropriately chosen and applied in the training phase, which makes our classifier achieve better performance. We demonstrate the efficiency and effectiveness of our approach through experiments on 3D CAD model datasets.
C1 [Qin, Fei-wei; Li, Lu-ye; Gao, Shu-ming; Yang, Xiao-ling; Chen, Xiang] Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310058, Zhejiang, Peoples R China.
RP Gao, SM (reprint author), Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310058, Zhejiang, Peoples R China.
EM qinfeiwei@zjucadcg.cn; liluye@cad.zju.edu.cn; smgao@cad.zju.edu.cn;
   sunny_aday@163.com; xchen@cad.zju.edu.cn
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61163016, 61173125]
FX Project supported by the National Natural Science Foundation of China
   (Nos. 61163016 and 61173125)
CR Bai J, 2010, COMPUT AIDED DESIGN, V42, P1069, DOI 10.1016/j.cad.2010.07.002
   Barutcuoglu Z, 2006, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2006, PROCEEDINGS, P289
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Bergstra J., 2010, P 9 PYTH SCI C, P1
   Bishop CM, 1995, NEURAL NETWORKS PATT
   Bordes A, 2014, MACH LEARN, V94, P233, DOI 10.1007/s10994-013-5363-6
   Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669
   CHEN DY, 2003, [No title captured]
   Cheuk Yiu Ip, 2005, Computer-Aided Design and Applications, V2, P609
   Del Bimbo A, 2006, ACM T MULTIM COMPUT, V2, P20
   Dengsheng Zhang, 2002, Proceedings of the Fifth Asian Conference on Computer Vision, P652
   Dreiseitl S, 2002, J BIOMED INFORM, V35, P352, DOI 10.1016/S1532-0464(03)00034-0
   Duda R.O., 2001, PATTERN CLASSIFICATI
   Fang X, 2005, COMPUT STRUCT, V83, P2150, DOI 10.1016/j.compstruc.2005.02.029
   Glorot X., 2010, JLMR P TRACK, p[249, 2010], DOI DOI 10.1177/1753193409103364.
   GUNN TG, 1982, SCI AM, V247, P114, DOI 10.1038/scientificamerican0982-114
   Haykin S., 2008, NEURAL NETWORKS LEAR
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Huang F. J., 2006, C COMP VIS PATT REC, P284, DOI [10.1109/CVPR.2006.164, DOI 10.1109/CVPR.2006.164]
   Ip CY, 2005, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDINGS, P361, DOI 10.1109/SMI.2005.27
   Ip CY, 2003, P 8 ACM S SOL MOD AP, P322, DOI DOI 10.1145/781606.781659]
   Iyer N, 2005, COMPUT AIDED DESIGN, V37, P509, DOI 10.1016/j.cad.2004.07.002
   Kavukcuoglu K., 2010, ADV NEURAL INFORM PR, V23, P1090
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Larochelle H, 2009, J MACH LEARN RES, V10, P1
   Mnih Volodymyr, 2010, ADV NEURAL INFORM PR, P2002
   Ngiam J, 2011, P 28 INT C MACH LEAR, P1105
   Prechelt L, 1998, NEURAL NETWORKS, V11, P761, DOI 10.1016/S0893-6080(98)00010-0
   REED R, 1993, IEEE T NEURAL NETWOR, V4, P740, DOI 10.1109/72.248452
   Shilane P, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P167, DOI 10.1109/SMI.2004.1314504
   Suyu Hou, 2005, Computer-Aided Design and Applications, V2, P155
   WANG Weiming, 2013, COMPUT AIDED DRAFT D, V23, P60
   Wei Wei, 2008, 2008 International Conference on Computer Science and Software Engineering (CSSE 2008), P883, DOI 10.1109/CSSE.2008.1177
   Wu MC, 1996, INT J ADV MANUF TECH, V11, P325, DOI 10.1007/BF01845691
   Yao Y, 2007, CONSTR APPROX, V26, P289, DOI 10.1007/s00365-006-0663-2
NR 36
TC 20
Z9 28
U1 6
U2 61
PU ZHEJIANG UNIV
PI HANGZHOU
PA EDITORIAL BOARD, 20 YUGU RD, HANGZHOU, 310027, PEOPLES R CHINA
SN 1869-1951
EI 1869-196X
J9 J ZHEJIANG U-SCI C
JI J. Zhejiang Univ.-SCI. C.
PD FEB
PY 2014
VL 15
IS 2
BP 91
EP 106
DI 10.1631/jzus.C1300185
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Engineering, Electrical & Electronic
SC Computer Science; Engineering
GA AA3FU
UT WOS:000330979100002
DA 2020-02-19
ER

PT J
AU Young, SR
   Davis, A
   Mishtal, A
   Arel, I
AF Young, S. R.
   Davis, A.
   Mishtal, A.
   Arel, I.
TI Hierarchical spatiotemporal feature extraction using recurrent online
   clustering
SO PATTERN RECOGNITION LETTERS
LA English
DT Article
DE Deep machine learning; Online clustering; Recurrent clustering;
   Unsupervised feature extraction; Spatiotemporal signals; Pattern
   recognition
ID INFERENCE; FACE
AB Deep machine learning offers a comprehensive framework for extracting meaningful features from complex observations in an unsupervised manner. The majority of deep learning architectures described in the literature primarily focus on extracting spatial features. However, in real-world settings, capturing temporal dependencies in observations is critical for accurate inference. This paper introduces an enhancement to DeSTIN - a compositional deep learning architecture in which each layer consists of multiple instantiations of a common node - that learns to represent spatiotemporal patterns in data based on a novel recurrent clustering algorithm. Contrary to mainstream deep architectures, such as deep belief networks where layer-by-layer training is assumed, each of the nodes in the proposed architecture is trained independently and in parallel. Moreover, top-down and bottom-up information flows facilitate rich feature formation. A semi-supervised setting is demonstrated achieving state-of-the-art results on the MNIST classification benchmarks. A GPU implementation is discussed further accentuating the scalability properties of the proposed framework. (C) 2013 Elsevier B.V. All rights reserved.
C1 [Young, S. R.; Davis, A.; Mishtal, A.; Arel, I.] Univ Tennessee, Dept Elect Engn & Comp Sci, Machine Intelligence Lab, Knoxville, TN 37996 USA.
RP Young, SR (reprint author), Univ Tennessee, Dept Elect Engn & Comp Sci, Machine Intelligence Lab, Knoxville, TN 37996 USA.
EM syoung22@utk.edu; adavis72@utk.edu; amishtal@utk.edu; itamar@ieee.org
OI Young, Steven/0000-0003-0591-4330
FU Intelligence Advanced Research Projects Activity (IARPA) via Army
   Research Office (ARO) [W911NF-12-1-0017]
FX This work was supported by the Intelligence Advanced Research Projects
   Activity (IARPA) via Army Research Office (ARO) agreement No.
   W911NF-12-1-0017. The U.S. Government is authorized to reproduce and
   distribute reprints for Governmental purposes notwithstanding any
   copyright annotation thereon.
CR Arel I, 2009, P AAAI WORKSH BIOL I, P1150
   Arel I, 2010, IEEE COMPUT INTELL M, V5, P13, DOI 10.1109/MCI.2010.938364
   Chen YN, 2006, INT C PATT RECOG, P552
   Cormen T. H., 2009, INTRO ALGORITHMS
   Cuturi M., 2011, AUTOREGRESSIVE KERNE
   Cuturi M., 2011, P 28 INT C MACH LEAR, P929
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Karnowski Thomas P, 2010, 2010 Ninth International Conference on Machine Learning and Applications (ICMLA 2010), P883, DOI 10.1109/ICMLA.2010.138
   Karnowski T.P., 2012, THESIS U TENNESSEE K
   Karnowski TP, 2011, FRONT ARTIF INTEL AP, V233, P174, DOI 10.3233/978-1-60750-959-2-174
   Kegl B., 2009, INT C MACH LEARN MON, V26, P497, DOI DOI 10.1145/1553374.1553439
   Krizhevsky A., 2009, THESIS U TORONTO TOR
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y., 1998, MNIST DATABASE HANDW
   Lee H., 2009, P 26 ANN INT C MACH, P609, DOI DOI 10.1145/1553374.1553453
   Lee H, 2011, COMMUN ACM, V54, P95, DOI 10.1145/2001269.2001295
   Lee TS, 2003, J OPT SOC AM A, V20, P1434, DOI 10.1364/JOSAA.20.001434
   Lockett A.J., 2009, AI0904 U TEX AUST DE
   Mobahi H., 2009, P 26 ANN INT C MACH, P737
   NVIDIA, 2012, NVIDIA CUDA PROGR GU
   Owens JD, 2008, P IEEE, V96, P879, DOI 10.1109/JPROC.2008.917757
   Salakhutdinov R., 2007, J MACHINE LEARNING R, P412
   Simard PY, 2003, PROC INT CONF DOC, P958
   Sutskever I., 2007, LEARNING MULTILEVEL
   Wallis G, 1997, PROG NEUROBIOL, V51, P167, DOI 10.1016/S0301-0082(96)00054-8
   Wallis G, 1999, TRENDS COGN SCI, V3, P22, DOI 10.1016/S1364-6613(98)01261-3
   Young Steven, 2010, Proceedings of the Seventh International Conference on Information Technology: New Generations (ITNG 2010), P204, DOI 10.1109/ITNG.2010.148
NR 27
TC 5
Z9 5
U1 0
U2 28
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0167-8655
EI 1872-7344
J9 PATTERN RECOGN LETT
JI Pattern Recognit. Lett.
PD FEB 1
PY 2014
VL 37
SI SI
BP 115
EP 123
DI 10.1016/j.patrec.2013.07.013
PG 9
WC Computer Science, Artificial Intelligence
SC Computer Science
GA AA5RJ
UT WOS:000331157000013
DA 2020-02-19
ER

PT J
AU Chen, J
   Zhao, S
   Zhang, YP
AF Chen, Jie
   Zhao, Shu
   Zhang, Yanping
TI Hierarchical Covering Algorithm
SO TSINGHUA SCIENCE AND TECHNOLOGY
LA English
DT Article
DE deep architecture; hierarchy; fuzzy equivalence relation; covering tree;
   MNIST dataset
AB The concept of deep learning has been applied to many domains, but the definition of a suitable problem depth has not been sufficiently explored. In this study, we propose a new Hierarchical Covering Algorithm (HCA) method to determine the levels of a hierarchical structure based on the Covering Algorithm (CA). The CA constructs neural networks based on samples' own characteristics, and can effectively handle multi-category classification and large-scale data. Further, we abstract characters based on the CA to automatically embody the feature of a deep structure. We apply CA to construct hidden nodes at the lower level, and define a fuzzy equivalence relation (R) over bar on upper spaces to form a hierarchical architecture based on fuzzy quotient space theory. The covering tree naturally becomes from (R) over bar. HCA experiments performed on MNIST dataset show that the covering tree embodies the deep architecture of the problem, and the effects of a deep structure are shown to be better than having a single level.
C1 [Chen, Jie; Zhao, Shu; Zhang, Yanping] Anhui Univ, Dept Comp Sci & Technol, Hefei 230601, Peoples R China.
   [Chen, Jie; Zhao, Shu; Zhang, Yanping] Anhui Univ, Key Lab Intelligent Comp & Signal Proc, Hefei 230601, Peoples R China.
RP Zhang, YP (reprint author), Anhui Univ, Dept Comp Sci & Technol, Hefei 230601, Peoples R China.
EM Zhangyp2@gmail.com
FU National Key Basic Research and Development (973) Program of
   ChinaNational Basic Research Program of China [2007CB311003]; National
   Natural Science Foundation of ChinaNational Natural Science Foundation
   of China [61073117, 61175046]; Young Science Foundation of Anhui
   University [KJQN1118]; Outstanding Young Talents Higher Education
   Institutions of Anhui Province [2011SQRL129ZD]
FX This work was supported by the National Key Basic Research and
   Development (973) Program of China (No. 2007CB311003), the National
   Natural Science Foundation of China (Nos. 61073117 and 61175046), the
   Young Science Foundation of Anhui University (No. KJQN1118), the
   Outstanding Young Talents Higher Education Institutions of Anhui
   Province (No. 2011SQRL129ZD).
CR Bengio Y, 2012, TECHNICAL REPORT
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Bordes A., 2012, 15 INT C ART INT STA
   Boulanger-Lewandowski N., 2012, 29 INT C MACH LEARN
   Boureau Y.-L., 2010, 27 INT C MACH LEARN
   Ciresan D., 2012, TECHNICAL REPORT
   Dahl GE, 2012, IEEE T AUDIO SPEECH, V20, P30, DOI 10.1109/TASL.2011.2134090
   Kivinen J. J., 2012, 27 INT C ART INT STA
   McCulloch Warren S., 1943, BULL MATH BIOPHYS, V5, P115, DOI 10.1007/BF02478259
   Qiu De-hong, 2012, Journal of Chinese Computer Systems, V33, P1568
   Zhang L, 1999, IEEE T NEURAL NETWOR, V10, P925, DOI 10.1109/72.774263
   Zhang L., 2007, THEORY APPL PROBLEM, P81
   Zhang Ling, 1997, Journal of Software, V8, P252
NR 13
TC 3
Z9 4
U1 0
U2 4
PU TSINGHUA UNIV PRESS
PI BEIJING
PA B605D, XUE YAN BUILDING, BEIJING, 100084, PEOPLES R CHINA
SN 1007-0214
EI 1878-7606
J9 TSINGHUA SCI TECHNOL
JI Tsinghua Sci. Technol.
PD FEB
PY 2014
VL 19
IS 1
SI SI
BP 76
EP 81
DI 10.1109/TST.2014.6733210
PG 6
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Engineering, Electrical & Electronic
SC Computer Science; Engineering
GA CV7WA
UT WOS:000364485400008
DA 2020-02-19
ER

PT J
AU Zhang, CX
   Zhang, JS
   Ji, NN
   Guo, G
AF Zhang, Chun-Xia
   Zhang, Jiang-She
   Ji, Nan-Nan
   Guo, Gao
TI Learning ensemble classifiers via restricted Boltzmann machines
SO PATTERN RECOGNITION LETTERS
LA English
DT Article
DE Ensemble classifier; Bagging; Restricted Boltzmann machine; Deep
   learning; Majority voting; Diversity
ID COMBINING CLASSIFIERS; CLASSIFICATION; ALGORITHMS
AB Recently, restricted Boltzmann machines (RBMs) have attracted considerable interest in machine learning field due to their strong ability to extract features. Given some training data, an RBM or a stack of several RBMs can be used to extract informative features. Meanwhile, ensemble learning is an active research area in machine learning owing to their potential to greatly increase the prediction accuracy of a single classifier. However, RBMs have not been studied to work with ensemble learning so far. In this study, we present several methods for integrating RBMs with bagging to generate diverse and accurate individual classifiers. Taking a classification tree as the base learning algorithm, a thoroughly experimental study conducted on 31 real-world data sets yields some promising conclusions. When using the features extracted by RBMs in ensemble learning, the best way is to perform model combination respectively on the original feature set and the one extracted by a single RBM. However, the prediction performance becomes worse when the features detected by a stack of 2 RBMs are also considered. As for the features detected by RBMs, good classification can be obtained only when they are used together with the original features. (C) 2013 Elsevier B.V. All rights reserved.
C1 [Zhang, Chun-Xia; Zhang, Jiang-She; Ji, Nan-Nan] Xi An Jiao Tong Univ, Fac Math & Stat, Inst Stat Decis & Machine Learning, Xian 710049, Shaanxi, Peoples R China.
   [Guo, Gao] Xian Univ Technol, Sch Sci, Dept Appl Math, Xian 710054, Shaanxi, Peoples R China.
RP Zhang, CX (reprint author), Xi An Jiao Tong Univ, Fac Math & Stat, Inst Stat Decis & Machine Learning, Xian 710049, Shaanxi, Peoples R China.
EM cxzhang@mail.xjtu.edu.cn
FU National Basic Research Program of China (973 Program)National Basic
   Research Program of China [2013CB329406]; National Natural Science
   Foundations of ChinaNational Natural Science Foundation of China
   [11201367, 61075006]; Major Research Project of the National Natural
   Science Foundation of ChinaNational Natural Science Foundation of China
   [91230101]; Research Fund for the Doctoral Program of Higher Education
   of ChinaResearch Fund for the Doctoral Program of Higher Education of
   China (RFDP)Specialized Research Fund for the Doctoral Program of Higher
   Education (SRFDP) [20100201120048]; Fundamental Research Funds for the
   Central Universities of ChinaFundamental Research Funds for the Central
   Universities; Foundation of Shaanxi Provincial Department of Education
   in China [09JK615]
FX This research was supported by the National Basic Research Program of
   China (973 Program, No. 2013CB329406), the National Natural Science
   Foundations of China (Nos. 11201367, 61075006), the Major Research
   Project of the National Natural Science Foundation of China (No.
   91230101), the Research Fund for the Doctoral Program of Higher
   Education of China (No. 20100201120048), the Fundamental Research Funds
   for the Central Universities of China and the Foundation of Shaanxi
   Provincial Department of Education in China (No. 09JK615).
CR Bauer E, 1999, MACH LEARN, V36, P105, DOI 10.1023/A:1007515423169
   Baumgartner D, 2012, INTELL DATA ANAL, V16, P233, DOI 10.3233/IDA-2012-0521
   Bengio Yoshua, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P437, DOI 10.1007/978-3-642-35289-8_26
   Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153
   Bengio Y, 2012, COMPUT INTELL-US, V28, P261, DOI 10.1111/j.1467-8640.2012.00419.x
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Breiman L, 1984, CLASSIFICATION REGRE
   Brugge K, 2013, MACH LEARN, V93, P53, DOI 10.1007/s10994-013-5390-3
   Cheng J, 2006, PATTERN RECOGN, V39, P81, DOI 10.1016/j.patcog.2005.06.018
   Crawford M, 2009, LECT NOTES COMPUT SC, V5519, P519, DOI 10.1007/978-3-642-02326-2_52
   De Bock KW, 2011, EXPERT SYST APPL, V38, P12293, DOI 10.1016/j.eswa.2011.04.007
   Demsar J, 2006, J MACH LEARN RES, V7, P1
   Duin R. P. W., 2007, PRTOOLS4 MATLAB TOOL
   Dzeroski S, 2004, MACH LEARN, V54, P255, DOI 10.1023/B.MAC.0000015881.36452.6e
   Fischer Asja, 2012, Progress in Pattern Recognition, Image Analysis, ComputerVision, and Applications. Proceedings 17th Iberoamerican Congress, CIARP 2012, P14, DOI 10.1007/978-3-642-33275-3_2
   Fischer A, 2014, PATTERN RECOGN, V47, P25, DOI 10.1016/j.patcog.2013.05.025
   Frank A, 2010, UCI MACHINE LEARNING
   Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504
   Garcia-Pedrajas N, 2007, J MACH LEARN RES, V8, P1
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hinton GE., 2010, PRACTICAL GUIDE TRAI
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Ho TK, 1998, IEEE T PATTERN ANAL, V20, P832, DOI 10.1109/34.709601
   Hothorn T, 2003, PATTERN RECOGN, V36, P1303, DOI 10.1016/S0031-3203(02)00169-3
   Kuncheva L., 2004, COMBINING PATTERN CL
   Larochelle H, 2012, J MACH LEARN RES, V13, P643
   Loeff N., 2008, P 25 INT C MACH LEAR, P600
   Plumpton CO, 2012, PATTERN RECOGN, V45, P2101, DOI 10.1016/j.patcog.2011.04.023
   Polikar R., 2006, IEEE Circuits and Systems Magazine, V6, P21, DOI 10.1109/MCAS.2006.1688199
   Rodriguez JJ, 2006, IEEE T PATTERN ANAL, V28, P1619, DOI 10.1109/TPAMI.2006.211
   Rodriguez JJ, 2011, LECT NOTES COMPUT SC, V6713, P76, DOI 10.1007/978-3-642-21557-5_10
   Rokach L, 2010, ARTIF INTELL REV, V33, P1, DOI 10.1007/s10462-009-9124-7
   Rokach L, 2009, COMPUT STAT DATA AN, V53, P4046, DOI 10.1016/j.csda.2009.07.017
   Sammut C., 2011, ENCY MACHINE LEARNIN
   Sun ZL, 2012, IEEE SIGNAL PROC LET, V19, P455, DOI 10.1109/LSP.2012.2202317
   Tieleman T., 2008, P 25 INT C MACH LEAR, P1064, DOI DOI 10.1145/1390156.1390290
   Tieleman T., 2009, P 26 ANN INT C MACH, P1033, DOI DOI 10.1145/1553374.1553506
   Ting K.M., 1999, J KNOWL INF SYST, V1
   Tran T, 2011, P 3 AS C MACH LEARN, P213
   Wang GW, 2012, MATH PROBL ENG, DOI 10.1155/2012/346951
   Wang Z, 2013, KNOWL-BASED SYST, V37, P388, DOI 10.1016/j.knosys.2012.08.017
   Webb GI, 2000, MACH LEARN, V40, P159, DOI 10.1023/A:1007659514849
   Welling M., 2005, ADV NEURAL INFORM PR, P1481
   Xianggao Cai, 2012, Proceedings of the 2012 IEEE International Conference on Computer Science and Automation Engineering (CSAE 2012), P80, DOI 10.1109/CSAE.2012.6272913
   Zhang CX, 2011, PATTERN RECOGN LETT, V32, P1756, DOI 10.1016/j.patrec.2011.07.009
   Zhang CX, 2010, PATTERN ANAL APPL, V13, P59, DOI 10.1007/s10044-009-0168-8
   Zhang CX, 2009, LECT NOTES COMPUT SC, V5519, P478, DOI 10.1007/978-3-642-02326-2_48
   Zhang ML, 2013, DATA MIN KNOWL DISC, V26, P98, DOI 10.1007/s10618-011-0243-9
   Zhang Y, 2010, IEEE T PATTERN ANAL, V32, P1758, DOI 10.1109/TPAMI.2009.195
   Zhou ZH, 2012, ENSEMBLE METHODS FDN
   Zhu D, 2010, DECIS SUPPORT SYST, V48, P480, DOI 10.1016/j.dss.2009.06.007
NR 53
TC 20
Z9 22
U1 0
U2 36
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0167-8655
EI 1872-7344
J9 PATTERN RECOGN LETT
JI Pattern Recognit. Lett.
PD JAN 15
PY 2014
VL 36
BP 161
EP 170
DI 10.1016/j.patrec.2013.10.009
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA 282BO
UT WOS:000329145400020
DA 2020-02-19
ER

PT B
AU Chakdar, K
   Potetz, B
AF Chakdar, Kriti
   Potetz, Brian
BE Agah, A
TI Deep Learning for the Semiautomated Analysis of Pap Smears
SO MEDICAL APPLICATIONS OF ARTIFICIAL INTELLIGENCE
LA English
DT Article; Book Chapter
ID CANCER; SEGMENTATION
C1 [Chakdar, Kriti] Argus Hlth Syst Inc, Kansas City, MO 64105 USA.
   [Potetz, Brian] Univ Kansas, Dept Elect Engn & Comp Sci, Lawrence, KS 66045 USA.
   [Potetz, Brian] Google Inc, Los Angeles, CA USA.
RP Chakdar, K (reprint author), Argus Hlth Syst Inc, Kansas City, MO 64105 USA.
CR Beck AH, 2011, SCI TRANSL MED, V3, DOI 10.1126/scitranslmed.3002564
   Bengio Y., 2006, GREEDY LAYERWI UNPUB
   Blekas K., 1998, Journal of Intelligent Systems, V8, P55
   Carpenter AE, 2006, GENOME BIOL, V7, DOI 10.1186/gb-2006-7-10-r100
   Cawley GC, 2004, NEURAL NETWORKS, V17, P1467, DOI 10.1016/j.neunet.2004.07.002
   Demir C., 2009, AUTOMATED CANC DIAGN, P5
   DEMIR C, 2004, TR0414 RENSS POL I
   Doyle S., 2008, IEEE INT S BIOM IM 5
   Doyle S, 2007, I S BIOMED IMAGING, P1284, DOI 10.1109/ISBI.2007.357094
   Esgiar AN, 2002, IEEE T INF TECHNOL B, V6, P54, DOI 10.1109/4233.992163
   Gunduz C, 2004, BIOINFORMATICS, V20, P145, DOI 10.1093/bioinformatics/bth933
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2007, TRENDS COGN SCI, V11, P428, DOI 10.1016/j.tics.2007.09.004
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hoyer PO, 2002, VISION RES, V42, P1593, DOI 10.1016/S0042-6989(02)00017-2
   Joachims T, 1999, ADVANCES IN KERNEL METHODS, P169
   Joachims T., 2008, SUPPORT VECTOR UNPUB
   Karklin Y, 2003, NETWORK-COMP NEURAL, V14, P483, DOI 10.1088/0954-898X/14/3/306
   KOSS LG, 1989, JAMA-J AM MED ASSOC, V261, P737, DOI 10.1001/jama.261.5.737
   Le Roux N, 2008, NEURAL COMPUT, V20, P1631, DOI 10.1162/neco.2008.04-07-510
   Lewicki MS, 1997, ADV NEUR IN, V9, P529
   Malpica N, 1997, CYTOMETRY, V28, P289, DOI 10.1002/(SICI)1097-0320(19970801)28:4<289::AID-CYTO3>3.0.CO;2-7
   Plahl C, 2012, INT CONF ACOUST SPEE, P4165, DOI 10.1109/ICASSP.2012.6288836
   Portilla J, 2003, IEEE T IMAGE PROCESS, V12, P1338, DOI 10.1109/TIP.2003.818640
   Roweis S., 2007, ADV NEURAL INFORM SY, V19
   Salakhutdinov Ruslan, 2007, P SIGIR WORKSH INF R
   Schnorrenberg F, 1996, Technol Health Care, V4, P147
   Schwartz O, 2006, NEURAL COMPUT, V18, P2680, DOI 10.1162/neco.2006.18.11.2680
   Smolle J, 2000, SKIN RES TECHNOL, V6, P58, DOI 10.1034/j.1600-0846.2000.006002058.x
   Spyridonos P, 2001, MED INFORM INTERNET, V26, P179, DOI 10.1080/14639230110065757
   Statnikov A, 2005, INT J MED INFORM, V74, P491, DOI 10.1016/j.ijmedinf.2005.05.002
   Swersky K., 2010, TUTORIAL STOCH UNPUB
   Wahlby C, 2004, J MICROSC-OXFORD, V215, P67, DOI 10.1111/j.0022-2720.2004.01338.x
   Wiltgen M, 2003, INT J MED INFORM, V69, P17, DOI 10.1016/S1386-5056(02)00049-7
   WOLBERG WH, 1995, HUM PATHOL, V26, P792, DOI 10.1016/0046-8177(95)90229-5
   Zhou ZH, 2002, ARTIF INTELL MED, V24, P25, DOI 10.1016/S0933-3657(01)00094-X
NR 36
TC 2
Z9 2
U1 0
U2 0
PU CRC PRESS-TAYLOR & FRANCIS GROUP
PI BOCA RATON
PA 6000 BROKEN SOUND PARKWAY NW, STE 300, BOCA RATON, FL 33487-2742 USA
BN 978-1-4398-8434-8; 978-1-4398-8433-1
PY 2014
BP 193
EP 213
PG 21
WC Computer Science, Artificial Intelligence; Engineering, Biomedical
SC Computer Science; Engineering
GA BC6KX
UT WOS:000354045800014
DA 2020-02-19
ER

PT S
AU Goertzel, B
AF Goertzel, Ben
BA Goertzel, B
   Pennachin, C
   Geisweiller, N
BF Goertzel, B
   Pennachin, C
   Geisweiller, N
TI Integrating CogPrime with a Compositional Spatiotemporal Deep Learning
   Network
SO ENGINEERING GENERAL INTELLIGENCE, PT 2: THE COGPRIME ARCHITECTURE FOR
   INTEGRATIVE, EMBODIED AGI
SE Atlantis Thinking Machines
LA English
DT Article; Book Chapter
NR 0
TC 0
Z9 0
U1 0
U2 0
PU ATLANTIS PRESS
PI PARIS
PA 29 AVENUE LAVMIERE, PARIS, 75019, FRANCE
SN 1877-3273
BN 978-94-6239-030-0; 978-94-6239-029-4
J9 ATLANTIS THINK MACH
PY 2014
VL 6
BP 163
EP 172
DI 10.2991/978-94-6239-030-0_9
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA BB2NY
UT WOS:000341955900010
DA 2020-02-19
ER

PT S
AU Nath, V
   Levinson, SE
AF Nath, Vishnu
   Levinson, Stephen E.
BA Nath, V
   Levinson, SE
BF Nath, V
   Levinson, SE
TI Autonomous Robotics and Deep Learning Introduction
SO AUTONOMOUS ROBOTICS AND DEEP LEARNING
SE SpringerBriefs in Computer Science
LA English
DT Editorial Material; Book Chapter
C1 [Levinson, Stephen E.] Univ Illinois, Urbana, IL USA.
CR Breazeal C., 2007, HRI 07, P153
   ISAAC ASIMOV I, 2008, ROBOT
   Kormushev P., 2010, 2010 IEEE RAS INT C
   Levinson S., 2013, AAAI SPRING S
   Metta G., 2008, 8 WORKSH PERF METR I
   Michalski R. S., 1983, MACHINE LEARNING
   Michie Donald, 1986, MACHINE INTELLIGENCE
   Nath V., 2013, USAGE COMPUTER VISIO
   Nath V., 2014, 28 AAAI C
   Russell SJ, 2010, ARTIFICIAL INTELLIGE
   Sandini G, 2007, LECT NOTES ARTIF INT, V4850, P358
   Spong MW, 2006, ROBOT MODELLING CONT
   Tsagarakis NG, 2007, ADV ROBOTICS, V21, P1151, DOI 10.1163/156855307781389419
   Wells H. G., 2005, WAR WORLDS
NR 14
TC 1
Z9 1
U1 0
U2 2
PU SPRINGER
PI NEW YORK
PA 233 SPRING STREET, NEW YORK, NY 10013, UNITED STATES
SN 2191-5768
BN 978-3-319-05603-6; 978-3-319-05602-9
J9 SPRINGERBRIEF COMPUT
PY 2014
BP 1
EP 3
DI 10.1007/978-3-319-05603-6_1
D2 10.1007/978-3-319-05603-6
PG 3
WC Computer Science, Artificial Intelligence; Robotics
SC Computer Science; Robotics
GA BA5SV
UT WOS:000336989500001
DA 2020-02-19
ER

PT J
AU Zhang, KL
   Chen, XW
AF Zhang, Kunlei
   Chen, Xue-Wen
TI Large-Scale Deep Belief Nets With MapReduce
SO IEEE ACCESS
LA English
DT Article
DE Big data; deep learning; MapReduce; Hadoop; deep belief net (DBN);
   restricted Boltzmann machine (RBM)
AB Deep belief nets (DBNs) with restricted Boltzmann machines (RBMs) as the building block have recently attracted wide attention due to their great performance in various applications. The learning of a DBN starts with pretraining a series of the RBMs followed by fine-tuning the whole net using backpropagation. Generally, the sequential implementation of both RBMs and backpropagation algorithm takes significant amount of computational time to process massive data sets. The emerging big data learning requires distributed computing for the DBNs. In this paper, we present a distributed learning paradigm for the RBMs and the backpropagation algorithm using MapReduce, a popular parallel programming model. Thus, the DBNs can be trained in a distributed way by stacking a series of distributed RBMs for pretraining and a distributed backpropagation for fine-tuning. Through validation on the benchmark data sets of various practical problems, the experimental results demonstrate that the distributed RBMs and DBNs are amenable to large-scale data with a good performance in terms of accuracy and efficiency.
C1 [Zhang, Kunlei; Chen, Xue-Wen] Wayne State Univ, Dept Comp Sci, Detroit, MI 48202 USA.
RP Chen, XW (reprint author), Wayne State Univ, Dept Comp Sci, Detroit, MI 48202 USA.
EM xwen.chen@gmail.com
CR [Anonymous], 2014, APACH HADOOP
   [Anonymous], 2014, MRJOB
   Bengio Y., 2009, LEARNING DEEP ARCHIT, V2
   Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153
   Chu C.-T., 2007, ADV NEURAL INF PROCE, P281, DOI DOI 10.1234/12345678
   Dahl GE, 2012, IEEE T AUDIO SPEECH, V20, P30, DOI 10.1109/TASL.2011.2134090
   Dean J, 2004, USENIX Association Proceedings of the Sixth Symposium on Operating Systems Design and Implementation (OSDE '04), P137
   Dean J., 2012, ADV NEURAL INFORM PR, P1232
   HE YB, 2011, PAR DISTR SYST ICPAD, P473, DOI DOI 10.1109/ICPADS.2011.83
   Hinton G, 2005, AISTATS, V10, P33
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Le Q.V., 2012, P INT C MACH LEARN
   Mohamed AR, 2011, INT CONF ACOUST SPEE, P5060
   Nair V, 2009, ADV NEURAL INF PROCE, V22, P1339
   Panda B, 2009, PROC VLDB ENDOW, V2, P1426, DOI 10.14778/1687553.1687569
   Papadimitriou S, 2008, IEEE DATA MINING, P512, DOI 10.1109/ICDM.2008.142
   Raina R, 2009, P 26 ANN INT C MACH, V382, P873, DOI DOI 10.1145/1553374.1553486
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Salakhutdinov R, 2009, INT J APPROX REASON, V50, P969, DOI 10.1016/j.ijar.2008.11.006
   Scholkopf B., 2006, ADV NEURAL INFORM PR, P1345
   Sun TY, 2009, 2009 INTERNATIONAL CONFERENCE ON PARALLEL AND DISTRIBUTED COMPUTING, APPLICATIONS AND TECHNOLOGIES (PDCAT 2009), P494, DOI 10.1109/PDCAT.2009.46
   Zhai Ke, 2012, P 21 INT C WORLD WID, P879, DOI DOI 10.1145/2187836.2187955
   Zhao WZ, 2009, LECT NOTES COMPUT SC, V5931, P674, DOI 10.1007/978-3-642-10665-1_71
NR 25
TC 29
Z9 32
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 2169-3536
J9 IEEE ACCESS
JI IEEE Access
PY 2014
VL 2
BP 395
EP 403
DI 10.1109/ACCESS.2014.2319813
PG 9
WC Computer Science, Information Systems; Engineering, Electrical &
   Electronic; Telecommunications
SC Computer Science; Engineering; Telecommunications
GA V43AH
UT WOS:000209653800029
OA DOAJ Gold
DA 2020-02-19
ER

PT J
AU Chen, XW
   Lin, XT
AF Chen, Xue-Wen
   Lin, Xiaotong
TI Big Data Deep Learning: Challenges and Perspectives
SO IEEE ACCESS
LA English
DT Article
DE Classifier design and evaluation; feature representation; machine
   learning; neural nets models; parallel processing
ID TIME ADAPTIVE CLASSIFIERS; NEURAL-NETWORKS; PATTERN-CLASSIFICATION;
   BACKPROPAGATION; ARCHITECTURE; STACKING; NETS
AB Deep learning is currently an extremely active research area in machine learning and pattern recognition society. It has gained huge successes in a broad area of applications such as speech recognition, computer vision, and natural language processing. With the sheer size of data available today, big data brings big opportunities and transformative potential for various sectors; on the other hand, it also presents unprecedented challenges to harnessing data and information. As the data keeps getting bigger, deep learning is coming to play a key role in providing big data predictive analytics solutions. In this paper, we provide a brief overview of deep learning, and highlight current research efforts and the challenges to big data, as well as the future trends.
C1 [Chen, Xue-Wen] Wayne State Univ, Dept Comp Sci, Detroit, MI 48404 USA.
   [Lin, Xiaotong] Oakland Univ, Dept Comp Sci & Engn, Rochester, MI 48309 USA.
RP Chen, XW (reprint author), Wayne State Univ, Dept Comp Sci, Detroit, MI 48404 USA.
EM xwen.chen@gmail.com
CR Alippi C, 2008, IEEE T NEURAL NETWOR, V19, P1145, DOI 10.1109/TNN.2008.2000082
   Alippi C, 2008, IEEE T NEURAL NETWOR, V19, P2053, DOI 10.1109/TNN.2008.2003998
   [Anonymous], 2013, CUDA C PROGR GUID PG
   [Anonymous], 2013, FACT SHEET BRAIN IN
   de Oliveira EA, 2007, IEEE T NEURAL NETWOR, V18, P584, DOI 10.1109/TNN.2006.889943
   Bartlett Paul C, 2010, Vet Med Int, V2010, DOI 10.4061/2010/957570
   Bengio Y, 2000, ADV NEUR IN, V12, P400
   Bengio Y., 2012, J MACHINE LEARNING R, P17
   Bengio Y, 2006, ADV NEURAL INFORM PR, P153
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Blum A., 1997, Proceedings of the Tenth Annual Conference on Computational Learning Theory, P45, DOI 10.1145/267460.267475
   Blum A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P92, DOI 10.1145/279943.279962
   Bottou L., 1991, P NEUR
   Bottou L., 1998, ON LINE LEARNING NEU
   Campolucci P, 1999, IEEE T NEURAL NETWOR, V10, P253, DOI 10.1109/72.750549
   CASELLA G, 1992, AM STAT, V46, P167, DOI 10.2307/2685208
   Cesa-Bianchi N., 1994, P C COMP LEARN THEOR, V53, P205
   Chien JT, 2013, IEEE T NEUR NET LEAR, V24, P681, DOI 10.1109/TNNLS.2013.2242090
   Choy MC, 2006, IEEE T NEURAL NETWOR, V17, P1511, DOI 10.1109/TNN.2006.881710
   Chu C. T., 2006, NIPS, P281
   Cires D. C., 2011, P 22 INT JOINT C ART, V22, P1237, DOI DOI 10.5591/978-1-57735-516-8/IJCAI11-210
   Ciresan DC, 2010, NEURAL COMPUT, V22, P3207, DOI 10.1162/NECO_a_00052
   Coates A., 2013, J MACH LEARN RES, P1337
   Collobert R, 2011, J MACH LEARN RES, V12, P2493
   Crego E., 2013, BUSINESS
   Dahl GE, 2012, IEEE T AUDIO SPEECH, V20, P30, DOI 10.1109/TASL.2011.2134090
   Dean J., 2012, ADV NEURAL INFORM PR, P1232
   DEANGULO VR, 1995, IEEE T NEURAL NETWOR, V6, P657, DOI 10.1109/72.377971
   Deng L, 2012, INT CONF ACOUST SPEE, P2133, DOI 10.1109/ICASSP.2012.6288333
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Efrati A., 2013, INFORMATION
   Elwell Ryan, 2009, Proceedings 2009 International Joint Conference on Neural Networks (IJCNN 2009 - Atlanta), P771, DOI 10.1109/IJCNN.2009.5178779
   Elwell R, 2011, IEEE T NEURAL NETWOR, V22, P1517, DOI 10.1109/TNN.2011.2160459
   Farabet Clement, 2011, MACHINE LEARNING VER
   Fergus R., 2009, ADV NEURAL INFORM PR, V22, P522
   Freund Y., 1996, Proceedings of the Ninth Annual Conference on Computational Learning Theory, P325, DOI 10.1145/238061.238163
   Gantz J, 2011, EXTRACTING VALUE CHA
   Gantz J., 2010, DIGITAL UNIVERSE DEC
   GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596
   Glorot X., 2011, P 28 INT C MACH LEAR
   Gutstein S, 2008, INT J ARTIF INTELL T, V17, P555, DOI 10.1142/S0218213008004059
   HESKES TM, 1993, NH MATH LIB, V51, P199
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton G. E., 2010, 2010003 UTML TR
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   HUBEL DH, 1968, J PHYSIOL-LONDON, V195, P215, DOI 10.1113/jphysiol.1968.sp008455
   Hutchinson B, 2013, IEEE T PATTERN ANAL, V35, P1944, DOI 10.1109/TPAMI.2012.268
   Jones N, 2014, NATURE, V505, P146, DOI 10.1038/505146a
   Kavukcuoglu K, 2009, PROC CVPR IEEE, P1605, DOI 10.1109/CVPRW.2009.5206545
   Kirk J., 2013, PCWORLD         0110
   Krizhevsky A, 2009, LEARNING MULTIPLE LA
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Laney D, 2012, IMPORTANCE BIG DATA
   Larochelle H, 2009, J MACH LEARN RES, V10, P1
   Le Callet P, 2006, IEEE T NEURAL NETWOR, V17, P1316, DOI 10.1109/TNN.2006.879766
   Le Q.V., 2012, P INT C MACH LEARN
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y., 1998, NEURAL NETWORKS TRIC
   Lee H., 2006, ADV NEURAL INF PROCE, P801
   Liang NY, 2006, IEEE T NEURAL NETWOR, V17, P1411, DOI 10.1109/TNN.2006.880583
   Lim CP, 2003, IEEE T SYST MAN CY C, V33, P235, DOI 10.1109/TSMCC.2003.813150
   LITTLESTONE N., 1991, P 23 S THEOR COMP, P465
   Martens J., 2010, P 27 INT C MACH LEAR
   Marti R, 2004, COMPUT OPER RES, V31, P1491, DOI 10.1016/S0305-0548(03)00104-7
   McKinsey Global Institute, 2011, BIG DAT NEXT FRONT I
   Mesnil Gregoire, 2013, Proceedings of the 2nd International Conference on Pattern Recognition Applications and Methods. ICPRAM 2013, P345
   Mesnil G., 2011, J MACH LEARN RES, V7, P1
   Nair V, 2009, ADV NEURAL INF PROCE, V22, P1339
   Ngiam J, 2011, P 28 INT C MACH LEAR
   Ni Y, 2012, PROCEEDINGS OF THE 2012 INTERNATIONAL CONFERENCE ON MANAGEMENT INNOVATION AND PUBLIC POLICY (ICMIPP 2012), VOLS 1-6, P1879
   Pan SJ, 2011, IEEE T NEURAL NETWOR, V22, P199, DOI 10.1109/TNN.2010.2091281
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Panda B., 2012, SCALING MACHINE LEAR
   Raina R., 2007, P 24 ICML
   Raina R, 2009, P 26 ANN INT C MACH, V382, P873, DOI DOI 10.1145/1553374.1553486
   Ranzato M. A., 2007, ADV NEURAL INFORM PR, p1185 
   Ranzato M. A., 2008, P 25 INT C MACH LEAR, P792, DOI DOI 10.1145/1390156.1390256
   Rattray M, 1997, J PHYS A-MATH GEN, V30, pL771, DOI 10.1088/0305-4470/30/22/005
   RIEGLER P, 1995, J PHYS A-MATH GEN, V28, pL507, DOI 10.1088/0305-4470/28/20/002
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Rutkowski L, 2004, IEEE T NEURAL NETWOR, V15, P811, DOI 10.1109/TNN.2004.828757
   SAAD D, 1995, PHYS REV LETT, V74, P4337, DOI 10.1103/PhysRevLett.74.4337
   Salakhutdinov R., 2007, P INT C MACH LEARN, V24, P791, DOI DOI 10.1145/1273496.1273596
   Scherer D, 2010, LECT NOTES COMPUT SC, V6354, P92, DOI 10.1007/978-3-642-15825-4_10
   Seide F., 2011, P INTERSPEECH, P437
   Shalev-Shwartz S., 2007, P INT C MACH LEARN
   Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018
   Simard PY, 2003, PROC INT CONF DOC, P958
   Sinha K, 2009, ADV NEURAL INFORM PR, V22, P1687
   Smola A, 2010, PROC VLDB ENDOW, V3, P703, DOI 10.14778/1920841.1920931
   Srivastava N., 2012, P ADV NIPS
   Sugiyama M, 2012, ADAPT COMPUT MACH LE, P1
   Tomov S, 2011, MAGMA USERS GUIDE
   Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128
   Vanhoucke V., 2011, P DEEP LEARN UNS FEA
   Vincent P, 2008, P 25 INT C MACH LEAR, P1096, DOI DOI 10.1145/1390156.1390294
   Wang JH, 2007, J MACH LEARN RES, V8, P1867
   Wang Y., 2011, LANGUAGE UNDERSTANDI
   West AHL, 1997, PHYS REV E, V56, P3426, DOI 10.1103/PhysRevE.56.3426
   Weston J., 2008, P 25 INT C MACH LEAR
   Zeiler MD, 2011, IEEE I CONF COMP VIS, P2018, DOI 10.1109/ICCV.2011.6126474
   Zhang KL, 2014, IEEE ACCESS, V2, P395, DOI 10.1109/ACCESS.2014.2319813
NR 104
TC 276
Z9 290
U1 17
U2 201
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 2169-3536
J9 IEEE ACCESS
JI IEEE Access
PY 2014
VL 2
BP 514
EP 525
DI 10.1109/ACCESS.2014.2325029
PG 12
WC Computer Science, Information Systems; Engineering, Electrical &
   Electronic; Telecommunications
SC Computer Science; Engineering; Telecommunications
GA V43AH
UT WOS:000209653800037
OA DOAJ Gold
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Lopes, N
   Ribeiro, B
AF Lopes, Noel
   Ribeiro, Bernardete
TI Towards adaptive learning with improved convergence of deep belief
   networks on graphics processing units
SO PATTERN RECOGNITION
LA English
DT Article
DE Deep learning; Deep belief networks; Restricted Boltzmann machines;
   Contrastive divergence; Adaptive step size; GPU computing
AB In this paper we focus on two complementary approaches to significantly decrease pre-training time of a deep belief network (DBN). First, we propose an adaptive step size technique to enhance the convergence of the contrastive divergence (CD) algorithm, thereby reducing the number of epochs to train the restricted Boltzmann machine (RBM) that supports the DBN infrastructure. Second, we present a highly scalable graphics processing unit (GPU) parallel implementation of the CD-k algorithm, which boosts notably the training speed. Additionally, extensive experiments are conducted on the MNIST and the HHreco databases. The results suggest that the maximum useful depth of a DBN is related to the number and quality of the training samples. Moreover, it was found that the lower-level layer plays a fundamental role for building successful DBN models. Furthermore, the results contradict the preconceived idea that all the layers should be pre-trained. Finally, it is shown that by incorporating multiple back-propagation (MBP) layers, the DBNs generalization capability is remarkably improved. (C) 2013 Elsevier Ltd. All rights reserved.
C1 [Ribeiro, Bernardete] Univ Coimbra, Dept Informat Engn, P-3000 Coimbra, Portugal.
EM noel@ipg.pt; bribeiro@dei.uc.pt
RI Ribeiro, Bernardete/A-8010-2016
OI Ribeiro, Bernardete/0000-0002-9770-7672; Lopes, Noel/0000-0003-2798-7274
FU FCT (Fundacao para a Ciencia e Tecnologia) [PEst-OE/EGE/UI4056/2011]
FX We would like to express our gratitude to the anonymous reviewers for
   their comments and suggestions. FCT (Fundacao para a Ciencia e
   Tecnologia) is acknowledged for funding project PEst-OE/EGE/UI4056/2011.
CR Almeida L.B., 1997, HDB NEURAL COMPUTATI
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Cataranzo B, 2008, P 25 INT C MACH LEAR, P104, DOI DOI 10.1145/1390156.1390170
   Garland M, 2010, COMMUN ACM, V53, P58, DOI 10.1145/1839676.1839694
   Halfhill T.R., 2009, TECHNICAL REPORT
   Hey T, 2009, 4 PARADIGM DATA INTE
   Hinton G, 2005, AISTATS, V10, P33
   Hinton G., 2010, TECHNICAL REPORT
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Kim S.K., 2010, 18 IEEE ANN INT S FI
   Larochelle H., 2007, P 24 INT C MACH LEAR, P473, DOI DOI 10.1145/1273496.1273556
   Le Ly D, 2010, IEEE T NEURAL NETWOR, V21, P1780, DOI 10.1109/TNN.2010.2073481
   Le Roux N, 2008, NEURAL COMPUT, V20, P1631, DOI 10.1162/neco.2008.04-07-510
   Le Roux N, 2010, NEURAL COMPUT, V22, P2192, DOI 10.1162/neco.2010.08-09-1081
   Lee H., 2009, P 26 ANN INT C MACH, P609, DOI DOI 10.1145/1553374.1553453
   Lopes Noel, 2012, Progress in Pattern Recognition, Image Analysis, ComputerVision, and Applications. Proceedings 17th Iberoamerican Congress, CIARP 2012, P511, DOI 10.1007/978-3-642-33275-3_63
   Lopes Noel, 2011, International Journal of Computer Information Systems and Industrial Management Applications, V3, P355
   Lopes N, 2010, 2010 10th International Conference on Hybrid Intelligent Systems (HIS 2010), P229, DOI 10.1109/HIS.2010.5600028
   Lopes N., 2003, Neural, Parallel & Scientific Computations, V11, P253
   Lopes N, 2012, IEEE IJCNN
   Lopes N, 2011, INT J NEURAL SYST, V21, P31, DOI 10.1142/S0129065711002638
   Ly D.L., 2009, TECHNICAL REPORT
   Markoff J., 2012, INT HERALD TRIBUNE, V24-25, P1
   Owens JD, 2008, P IEEE, V96, P879, DOI 10.1109/JPROC.2008.917757
   Raina R, 2009, P 26 ANN INT C MACH, V382, P873, DOI DOI 10.1145/1553374.1553486
   Ranzato M. A., 2007, ADV NEURAL INFORM PR, p1185 
   Ryoo S, 2008, PPOPP'08: PROCEEDINGS OF THE 2008 ACM SIGPLAN SYMPOSIUM ON PRINCIPLES AND PRACTICE OF PARALLEL PROGRAMMING, P73, DOI 10.1145/1345206.1345220
   Silva F.M., 1990, LECT NOTES COMPUTER, V412
   Srinivas J., 2012, INT J COMPUTER APPL, V48, P45
   Steinkraus D, 2005, PROC INT CONF DOC, P1115, DOI 10.1109/ICDAR.2005.251
   Swersky K., 2010, INF THEOR APPL WORKS, P1, DOI DOI 10.1109/ITA.2010.5454138
   Yu D, 2011, IEEE SIGNAL PROC MAG, V28, P145, DOI 10.1109/MSP.2010.939038
   Zainuddin Z., 2005, INT J COMPUTATIONAL, P172
NR 34
TC 35
Z9 44
U1 7
U2 73
PU ELSEVIER SCI LTD
PI OXFORD
PA THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND
SN 0031-3203
EI 1873-5142
J9 PATTERN RECOGN
JI Pattern Recognit.
PD JAN
PY 2014
VL 47
IS 1
SI SI
BP 114
EP 127
DI 10.1016/j.patcog.2013.06.029
PG 14
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
SC Computer Science; Engineering
GA 251BT
UT WOS:000326903500010
DA 2020-02-19
ER

PT J
AU Humphrey, EJ
   Bello, JP
   Lecun, Y
AF Humphrey, Eric J.
   Bello, Juan P.
   LeCun, Yann
TI Feature learning and deep architectures: new directions for music
   informatics
SO JOURNAL OF INTELLIGENT INFORMATION SYSTEMS
LA English
DT Article
DE Music informatics; Deep learning; Signal processing
ID RETRIEVAL; CHORDS
AB As we look to advance the state of the art in content-based music informatics, there is a general sense that progress is decelerating throughout the field. On closer inspection, performance trajectories across several applications reveal that this is indeed the case, raising some difficult questions for the discipline: why are we slowing down, and what can we do about it? Here, we strive to address both of these concerns. First, we critically review the standard approach to music signal analysis and identify three specific deficiencies to current methods: hand-crafted feature design is sub-optimal and unsustainable, the power of shallow architectures is fundamentally limited, and short-time analysis cannot encode musically meaningful structure. Acknowledging breakthroughs in other perceptual AI domains, we offer that deep learning holds the potential to overcome each of these obstacles. Through conceptual arguments for feature learning and deeper processing architectures, we demonstrate how deep processing models are more powerful extensions of current methods, and why now is the time for this paradigm shift. Finally, we conclude with a discussion of current challenges and the potential impact to further motivate an exploration of this promising research area.
C1 [Humphrey, Eric J.; Bello, Juan P.] NYU, MARL, New York, NY 10003 USA.
   [LeCun, Yann] NYU, Courant Inst, New York, NY 10003 USA.
RP Humphrey, EJ (reprint author), NYU, MARL, 35 West 4th St, New York, NY 10003 USA.
EM ejhumphrey@nyu.edu
CR Anden J., 2011, P 12 INT C MUS INF R
   Bello JP, 2005, IEEE T SPEECH AUDI P, V13, P1035, DOI 10.1109/TSA.2005.851998
   Bengio Y., 2007, LARGE SCALE KERNEL M, V34
   Bengio Y., 2012, ARXIV12065538
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Berenzweig A, 2004, COMPUT MUSIC J, V28, P63, DOI 10.1162/014892604323112257
   Bergstra J., 2010, P PYTH SCI COMP C SC
   Bertin-Mahieux T., 2012, P 13 INT SOC MUS INF, P241
   Bishop CM, 2006, PATTERN RECOGNITION
   Cabral G, 2006, LECT NOTES COMPUT SC, V3902, P185
   Casey MA, 2008, P IEEE, V96, P668, DOI 10.1109/JPROC.2008.916370
   Cho T., 2011, P 12 INT C MUS INF R
   Chordia P, 2011, J NEW MUSIC RES, V40, P105, DOI 10.1080/09298215.2011.576318
   Collobert R., 2011, BIGLEARN NIPS WORKSH
   Dannenberg R. B, 1984, P 1984 INT COMP MUS, V84, P193
   DAVIS SB, 1980, IEEE T ACOUST SPEECH, V28, P357, DOI 10.1109/TASSP.1980.1163420
   Dieleman Sander, 2011, P 12 INT C MUS INF R
   Dixon S, 2007, J NEW MUSIC RES, V36, P39, DOI 10.1080/09298210701653310
   Flexer A., 2012, P 13 INT SOC MUS INF, P175
   FUJISHIMA T, 1999, P INT COMP MUS C
   Goto M., 1995, P INT COMP MUS C, P171
   Grosche P, 2011, IEEE T AUDIO SPEECH, V19, P1688, DOI 10.1109/TASL.2010.2096216
   Hadsell R., 2006, P COMP VIS PATT REC
   Hamel P., 2009, P 10 INT C MUS INF R
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Humphrey E., 2012, P INT C MACH LEARN A
   Humphrey E.J., 2012, P 13 INT C MUS INF R
   Humphrey E.J., 2010, P ICMLA
   Klapuri A, 2006, SIGNAL PROCESSING ME
   Large E. W., 1994, Connection Science, V6, P177, DOI 10.1080/09540099408915723
   Le Q., 2012, P INT C MACH LEARN I
   Le Q.V., 2010, ADV NEURAL INFORM PR, V23
   Le Roux N, 2008, NEURAL COMPUT, V20, P1631, DOI 10.1162/neco.2008.04-07-510
   LeCun Y, 2012, LECT NOTES COMPUT SC, V7583, P496, DOI 10.1007/978-3-642-33863-2_51
   LeCun Yann, 2006, TUTORIAL ENERGY BASE
   Leveau P., 2007, P 8 INT C MUS INF RE
   Levy M, 2007, INT CONF ACOUST SPEE, P1433
   Levy M, 2009, IEEE T MULTIMEDIA, V11, P383, DOI 10.1109/TMM.2009.2012913
   Lyon RF, 2010, NEURAL COMPUT, V22, P2390, DOI 10.1162/NECO_a_00011
   Mandel M., 2005, P 6 INT C MUS INF RE
   Mauch M, 2010, IEEE T AUDIO SPEECH, V18, P1280, DOI 10.1109/TASL.2009.2032947
   McFee B., 2012, P 13 INT C MUS INF R
   Muller M, 2011, IEEE J-STSP, V5, P1088, DOI 10.1109/JSTSP.2011.2112333
   Muller M., 2011, P 12 INT C MUS INF R
   Nam J., 2011, P 12 INT C MUS INF R
   Scheirer ED, 1998, J ACOUST SOC AM, V103, P588, DOI 10.1121/1.421129
   Schmidt E.M., 2011, P NEUR INF PROC SYST
   SHEH A, 2003, P 4 INT C MUS INF RE
   Slaney M, 2011, IEEE MULTIMEDIA, V18, P12, DOI 10.1109/MMUL.2011.34
   Sumi K, 2012, INT CONF ACOUST SPEE, P1997, DOI 10.1109/ICASSP.2012.6288299
   Zils A., 2004, P AES
NR 52
TC 46
Z9 48
U1 1
U2 46
PU SPRINGER
PI DORDRECHT
PA VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN 0925-9902
EI 1573-7675
J9 J INTELL INF SYST
JI J. Intell. Inf. Syst.
PD DEC
PY 2013
VL 41
IS 3
BP 461
EP 481
DI 10.1007/s10844-013-0248-5
PG 21
WC Computer Science, Artificial Intelligence; Computer Science, Information
   Systems
SC Computer Science
GA 251MI
UT WOS:000326932800006
DA 2020-02-19
ER

PT J
AU Zhou, SS
   Chen, QC
   Wang, XL
AF Zhou, Shusen
   Chen, Qingcai
   Wang, Xiaolong
TI Active deep learning method for semi-supervised sentiment classification
SO NEUROCOMPUTING
LA English
DT Article
DE Neural networks; Deep learning; Active learning; Sentiment
   classification
AB In natural language processing community, sentiment classification based on insufficient labeled data is a well-known challenging problem. In this paper, a novel semi-supervised learning algorithm called active deep network (ADN) is proposed to address this problem. First, we propose the semi-supervised learning framework of ADN. ADN is constructed by restricted Boltzmann machines (RBM) with unsupervised learning based on labeled reviews and abundant of unlabeled reviews. Then the constructed structure is fine-tuned by gradient-descent based supervised learning with an exponential loss function. Second, in the semi-supervised learning framework, we apply active learning to identify reviews that should be labeled as training data, then using the selected labeled reviews and all unlabeled reviews to train ADN architecture. Moreover, we combine the information density with ADN, and propose information ADN (IADN) method, which can apply the information density of all unlabeled reviews in choosing the manual labeled reviews. Experiments on five sentiment classification datasets show that ADN and IADN outperform classical semi-supervised learning algorithms, and deep learning techniques applied for sentiment classification. (c) 2013 Elsevier B.V. All rights reserved.
C1 [Zhou, Shusen] Ludong Univ, Sch Informat & Elect Engn, Yantai, Peoples R China.
   [Chen, Qingcai; Wang, Xiaolong] Harbin Inst Technol, Shenzhen Grad Sch, Shenzhen, Peoples R China.
RP Zhou, SS (reprint author), Ludong Univ, Sch Informat & Elect Engn, Yantai, Peoples R China.
EM zhoushusen@hotmail.com; qingcai.chen@hitsz.edu.cn;
   wangxl@insun.hit.edu.cn
FU Scientific Research Fund of Ludong University [LY2013004]; National
   Natural Science Foundation of ChinaNational Natural Science Foundation
   of China [61173075, 60973076]
FX This work is supported in part by the Scientific Research Fund of Ludong
   University (LY2013004) and National Natural Science Foundation of China
   (Nos. 61173075 and 60973076).
CR Aue A., 2005, INT C REC ADV NAT LA
   Bengio Y., 2007, TECHNICAL REPORT
   Blitzer J., 2007, ACL, V45, P440, DOI DOI 10.1109/IRPS.2011.5784441
   Bollegala D., 2011, P 49 ANN M ASS COMP, P132
   Dasgupta S., 2009, P JOINT C 47 ANN M A, P701
   Dave K., 2003, P 12 INT C WORLD WID, P519, DOI DOI 10.1145/775152.775226
   DRUCK G, 2009, [No title captured], P81
   Gamon M., 2004, INT C COMP LING, P841, DOI DOI 10.3115/1220355.1220476
   Goldberg A.B., 2006, P 1 WORKSH GRAPH BAS, P45, DOI DOI 10.3115/1654758.1654769
   He Y., 2011, P 49 ANN M ASS COMP, P123
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Joachims T, 1999, MACHINE LEARNING, PROCEEDINGS, P200
   Kamvar S.D., 2003, P 18 INT JOINT C ART, P561
   Li SS, 2010, ACL 2010: 48TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, P414
   Li Shoushan, 2010, P 23 INT C COMP LING, P635
   Li Shoushan, 2008, P 46 ANN M ASS COMP, P257
   Liu Y, 2011, PATTERN RECOGN, V44, P2287, DOI 10.1016/j.patcog.2010.12.012
   Liu Y, 2010, SIGIR 2010: PROCEEDINGS OF THE 33RD ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH DEVELOPMENT IN INFORMATION RETRIEVAL, P873
   Lu B., 2011, P 49 ANN M ASS COMP, P320
   McDonald R., 2007, ACL 07, V45, P432
   Mullen T., 2004, P 2004 C EMP METH NA, V4, P412, DOI DOI 10.3115/1219044.1219069
   Ng V., 2006, P COLING ACL 2006 MA, P611, DOI DOI 10.3115/1273073.1273152
   Pan SJ, 2010, P 19 INT C WORLD WID, P751, DOI DOI 10.1145/1772690.1772767
   Pang B, 2002, PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING, P79
   Pang B., 2004, ANN M ASS COMP LING, V42, P271, DOI DOI 10.3115/1218955.1218990
   Pang B., 2008, FDN TRENDS INF RETR, V2
   Raina R., 2007, LEARNING, P759, DOI DOI 10.1145/1273496.1273592
   Ranzato M. A., 2008, P 25 INT C MACH LEAR, P792, DOI DOI 10.1145/1390156.1390256
   Read J., 2005, P ACL STUD RES WORKS, P43
   Salakhutdinov R., 2008, P 25 INT C MACH LEAR, P872, DOI [10.1145/1390156.1390266, DOI 10.1145/1390156.1390266]
   Salakhutdinov R., 2007, J MACHINE LEARNING R, P412
   Settles B., 2008, P C EMP METH NAT LAN, P1070, DOI DOI 10.3115/1613715.1613855
   Sindhwani V, 2008, IEEE DATA MINING, P1025, DOI 10.1109/ICDM.2008.113
   Socher R., C EMP METH NAT LANG, P151
   Songbo T, 2007, P 16 ACM C C INF KNO, P979, DOI DOI 10.1145/1321440.1321590
   Tang L, 2009, MINES 2009: FIRST INTERNATIONAL CONFERENCE ON MULTIMEDIA INFORMATION NETWORKING AND SECURITY, VOL 1, PROCEEDINGS, P244, DOI 10.1109/MINES.2009.194
   Tong S, 2002, J MACH LEARN RES, V2, P45
   Wan X., 2009, P JOINT C 47 ANN M A, V1, P235, DOI DOI 10.3115/1687878.1687913
   Wang M, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1899412.1899414
   Wang M, 2009, COMPUT VIS IMAGE UND, V113, P384, DOI 10.1016/j.cviu.2008.08.003
   Wang M, 2009, IEEE T CIRC SYST VID, V19, P733, DOI 10.1109/TCSVT.2009.2017400
   Wei W, 2010, ACL 2010: 48TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, P404
   Xia Y, 2008, ANN M ASS COMP LING, P133
   Zagibalov T., 2008, P 22 INT C COMP LING, P1073, DOI DOI 10.3115/1599081.1599216
   Zha ZJ, 2012, IEEE T MULTIMEDIA, V14, P17, DOI 10.1109/TMM.2011.2174782
   Zha ZJ, 2009, J VIS COMMUN IMAGE R, V20, P97, DOI 10.1016/j.jvcir.2008.11.009
   Zhou S., 2010, P 23 INT C COMP LING, P1515
   Zhu X, 2007, TECHNICAL REPORT
   Zhu X., 2003, ICML 2003 WORKSH CON, P58
NR 50
TC 57
Z9 66
U1 2
U2 128
PU ELSEVIER SCIENCE BV
PI AMSTERDAM
PA PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
SN 0925-2312
EI 1872-8286
J9 NEUROCOMPUTING
JI Neurocomputing
PD NOV 23
PY 2013
VL 120
SI SI
BP 536
EP 546
DI 10.1016/j.neucom.2013.04.017
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA 223XZ
UT WOS:000324847100058
DA 2020-02-19
ER

PT J
AU Carneiro, G
   Nascimento, JC
AF Carneiro, Gustavo
   Nascimento, Jacinto C.
TI Combining Multiple Dynamic Models and Deep Learning Architectures for
   Tracking the Left Ventricle Endocardium in Ultrasound Data
SO IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
LA English
DT Article
DE Left ventricle segmentation; deep belief networks; particle filters;
   dynamical model; discriminative classifiers
ID BOUNDARY DETECTION ALGORITHMS; LEVEL-SET; CARDIAC MR; ECHOCARDIOGRAPHIC
   SEQUENCES; AUTOMATIC SEGMENTATION; MAXIMUM-LIKELIHOOD; TIME
   SEGMENTATION; MEDICAL IMAGES; SHAPE; FRAMEWORK
AB We present a new statistical pattern recognition approach for the problem of left ventricle endocardium tracking in ultrasound data. The problem is formulated as a sequential importance resampling algorithm such that the expected segmentation of the current time step is estimated based on the appearance, shape, and motion models that take into account all previous and current images and previous segmentation contours produced by the method. The new appearance and shape models decouple the affine and nonrigid segmentations of the left ventricle to reduce the running time complexity. The proposed motion model combines the systole and diastole motion patterns and an observation distribution built by a deep neural network. The functionality of our approach is evaluated using a dataset of diseased cases containing 16 sequences and another dataset of normal cases comprised of four sequences, where both sets present long axis views of the left ventricle. Using a training set comprised of diseased and healthy cases, we show that our approach produces more accurate results than current state-of-the-art endocardium tracking methods in two test sequences from healthy subjects. Using three test sequences containing different types of cardiopathies, we show that our method correlates well with interuser statistics produced by four cardiologists.
C1 [Carneiro, Gustavo] Univ Adelaide, Sch Comp Sci, Australian Ctr Visual Technol, Adelaide, SA 5005, Australia.
   [Nascimento, Jacinto C.] Inst Super Tecn, Inst Sistemas & Robot, P-1049001 Lisbon, Portugal.
RP Carneiro, G (reprint author), Univ Adelaide, Sch Comp Sci, Australian Ctr Visual Technol, North Terrace,Inkarni Wardli Bldg, Adelaide, SA 5005, Australia.
EM gustavo.carneiro@adelaide.edu.au; jan@isr.ist.utl.pt
RI Nascimento, Jacinto/B-6128-2009
OI Nascimento, Jacinto/0000-0001-7468-5127; Carneiro,
   Gustavo/0000-0002-5571-6220
FU FCT (ISR/IST) through the PIDDAC Program funds; HEARTRACK
   [PTDC/EEA-CRO/103462/2008]; EU Project IMASEG3D [PIIF-GA-2009-236173]; 
   [PTDC/EEA-CRO/098550/2008]
FX This work was supported by project the FCT (ISR/IST plurianual funding)
   through the PIDDAC Program funds and by project
   PTDC/EEA-CRO/098550/2008. This work was also supported by project
   "HEARTRACK"-PTDC/EEA-CRO/103462/2008. This work was partially funded by
   EU Project IMASEG3D (PIIF-GA-2009-236173).
CR Alberola-Lopez C, 2004, IEEE T MED IMAGING, V23, P658, DOI 10.1109/TMI.2004.826358
   Arulampalam MS, 2002, IEEE T SIGNAL PROCES, V50, P174, DOI 10.1109/78.978374
   Bardinet E, 1996, Med Image Anal, V1, P129, DOI 10.1016/S1361-8415(96)80009-0
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Bernard O., 2007, P IEEE INT C IM PROC
   Bernard O, 2009, IEEE T IMAGE PROCESS, V18, P1179, DOI 10.1109/TIP.2009.2017343
   BLAND JM, 1986, LANCET, V1, P307
   Bosch JG, 2002, IEEE T MED IMAGING, V21, P1374, DOI 10.1109/TMI.2002.806427
   Boyd S., 2004, CONVEX OPTIMIZATION
   Carneiro G., 2010, P IEEE C COMP VIS PA
   Carneiro G, 2008, IEEE T MED IMAGING, V27, P1342, DOI 10.1109/TMI.2008.928917
   Carneiro G, 2012, IEEE T IMAGE PROCESS, V21, P968, DOI 10.1109/TIP.2011.2169273
   Carneiro G, 2010, I S BIOMED IMAGING, P1085, DOI 10.1109/ISBI.2010.5490181
   CARREIRAPERPINA.MA, 2005, P WORKSH ART INT STA
   Chalana V, 1997, IEEE T MED IMAGING, V16, P642, DOI 10.1109/42.640755
   Chalana V, 1996, IEEE T MED IMAGING, V15, P290, DOI 10.1109/42.500138
   Chen T, 2008, IEEE T MED IMAGING, V27, P1084, DOI 10.1109/TMI.2008.918327
   Comaniciu D, 2004, IEEE T MED IMAGING, V23, P849, DOI 10.1109/TMI.2004.827967
   COOTES TF, 1995, COMPUT VIS IMAGE UND, V61, P38, DOI 10.1006/cviu.1995.1004
   Cootes TF, 1999, LECT NOTES COMPUT SC, V1613, P322
   Corsi C, 2002, IEEE T MED IMAGING, V21, P1202, DOI 10.1109/TMI.2002.804418
   Cremers D, 2006, INT J COMPUT VISION, V69, P335, DOI 10.1007/s11263-006-7533-5
   Debreuve E, 2001, IEEE T MED IMAGING, V20, P643, DOI 10.1109/42.932748
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1
   Dias JMB, 1996, IEEE T MED IMAGING, V15, P25, DOI 10.1109/42.481438
   Duan Q, 2010, COMPUT METH PROG BIO, V98, P223, DOI 10.1016/j.cmpb.2009.09.001
   Duda R.O., 2001, PATTERN CLASSIFICATI
   Fisher R. A., 1948, AM STAT, V2, P30, DOI DOI 10.2307/2681650
   Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504
   FRIEDLAND N, 1989, IEEE T MED IMAGING, V8, P344, DOI 10.1109/42.41487
   Fukunaga K., 1990, INTRO STAT PATTERN R
   GEIGER D, 1995, IEEE T PATTERN ANAL, V17, P294, DOI 10.1109/34.368194
   GEORGESCU B, 2005, P IEEE C COMP VIS PA
   HAMMOUDE A, 1988, [No title captured]
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Jacob G, 2002, IEEE T MED IMAGING, V21, P226, DOI 10.1109/42.996341
   Jolly MP, 2009, LECT NOTES COMPUT SC, V5762, P910
   KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570
   Lin N, 2003, MED IMAGE ANAL, V7, P529, DOI 10.1016/S1361-8415(03)00035-5
   Lorenzo-Valdes M, 2004, MED IMAGE ANAL, V8, P255, DOI 10.1016/j.media.2004.06.005
   Lynch M, 2008, IEEE T MED IMAGING, V27, P195, DOI 10.1109/TMI.2007.904681
   MALLADI R, 1995, IEEE T PATTERN ANAL, V17, P158, DOI 10.1109/34.368173
   MCINERNEY T, 1995, COMPUT MED IMAG GRAP, V19, P69, DOI 10.1016/0895-6111(94)00040-9
   Mignotte M, 2001, PATTERN ANAL APPL, V4, P256, DOI 10.1007/PL00010988
   Mikic I, 1998, IEEE T MED IMAGING, V17, P274, DOI 10.1109/42.700739
   Mitchell SC, 2001, IEEE T MED IMAGING, V20, P415, DOI 10.1109/42.925294
   Montagnat J, 2005, MED IMAGE ANAL, V9, P87, DOI 10.1016/j.media.2004.06.025
   Montagnat J, 2003, PATTERN RECOGN LETT, V24, P815, DOI 10.1016/S0167-8655(02)00184-8
   Nascimento JC, 2008, IEEE T IMAGE PROCESS, V17, P392, DOI 10.1109/TIP.2007.915552
   Noble JA, 2006, IEEE T MED IMAGING, V25, P987, DOI 10.1109/TMI.2006.877092
   OKUMA K, 2004, P EUR C COMP VIS
   Paragios N, 2003, IEEE T MED IMAGING, V22, P773, DOI 10.1109/TMI.2003.814785
   Paragios N, 2002, INT J COMPUT VISION, V46, P223, DOI 10.1023/A:1014080923068
   Qian Z, 2006, LECT NOTES COMPUT SC, V4190, P636
   Reiber JHC, 1996, INT J CARDIAC IMAG, V12, P69, DOI 10.1007/BF01880736
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   SALAKHUTDINOV R, 2007, AI STAT
   SANDLER H, 1968, AM HEART J, V75, P325, DOI 10.1016/0002-8703(68)90089-6
   Sarti A, 2005, IEEE T ULTRASON FERR, V52, P947, DOI 10.1109/TUFFC.2005.1504017
   Senegas J, 2004, LECT NOTES COMPUT SC, V3117, P157
   Setarehdan SK, 1999, IEEE T BIO-MED ENG, V46, P1364, DOI 10.1109/10.797997
   Sun W, 2005, LECT NOTES COMPUT SC, V3565, P553
   Sun W, 2008, IEEE T IMAGE PROCESS, V17, P2186, DOI 10.1109/TIP.2008.2004638
   Terzopoulos D., 1993, TRACKING KALMAN SNAK
   Viola P, 2001, PROC CVPR IEEE, P511
   Weng J, 1997, IEEE T MED IMAGING, V16, P378, DOI 10.1109/42.611346
   Yang L, 2008, P IEEE C COMP VIS PA
   Zagrodsky V, 2005, IEEE T MED IMAGING, V24, P1089, DOI 10.1109/TMI.2005.852057
   Zheng YF, 2008, IEEE T MED IMAGING, V27, P1668, DOI 10.1109/TMI.2008.2004421
   Zhou XS, 2005, IEEE T PATTERN ANAL, V27, P115, DOI 10.1109/TPAMI.2005.3
   Zhu X., 2005, 1530 U WISC MAD COMP
   Zhu Y, 2010, IEEE T MED IMAGING, V29, P669, DOI 10.1109/TMI.2009.2031063
NR 73
TC 48
Z9 51
U1 3
U2 109
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 0162-8828
EI 1939-3539
J9 IEEE T PATTERN ANAL
JI IEEE Trans. Pattern Anal. Mach. Intell.
PD NOV
PY 2013
VL 35
IS 11
BP 2592
EP 2607
DI 10.1109/TPAMI.2013.96
PG 16
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
SC Computer Science; Engineering
GA 223SU
UT WOS:000324830900003
PM 24051722
OA Green Published
DA 2020-02-19
ER

PT J
AU Wang, CW
   You, WH
AF Wang, Ching-Wei
   You, Wun-Hong
TI Boosting-SVM: effective learning with reduced data dimension
SO APPLIED INTELLIGENCE
LA English
DT Article
DE Boosting; Support vector machine; Hierarchical learning architecture;
   HOG; German traffic sign
AB Learning problems over high dimensional data are common in real world applications. In this study, a challenging, large and lifelike database, the German traffic sign benchmark data, containing 43 classes and 51840 images, is used to demonstrate the strength of our proposed boosted support vector machine with deep learning architecture. Recognition of traffic signs is difficult, and it involves multiple categories, contains subsets of classes that may appear very similar to each other, and tends to have large variations within class in visual appearances due to illumination changes, partial occlusions, rotations and weather conditions. By combining a low variance error boosting algorithm, a low bias error support vector machine and deep learning architecture, an efficient and effective boosting support vector machine method is presented. It has been shown to greatly reduce data dimension and build classification models with higher prediction accuracy while utilizing fewer features and training instances. In evaluation, the proposed method outperforms Adaboost.M1, cw-Boost, and support vector machine, and it achieves ultra fast processing time (0.0038 per prediction) and high accuracy (93.5 %) on prediction of separate test data utilizes less than 35 % of the training instances. Moreover, the method is applicable to a standard standalone PC without requiring super computers with enormous memory spaces.
C1 [Wang, Ching-Wei; You, Wun-Hong] Natl Taiwan Univ Sci & Technol, Grad Inst Biomed Engn, Taipei 106, Taiwan.
RP Wang, CW (reprint author), Natl Taiwan Univ Sci & Technol, Grad Inst Biomed Engn, Taipei 106, Taiwan.
EM cweiwang@mail.ntust.edu.tw
FU national science councilNational Science Council of Taiwan [NSC-
   101-2628-E-011-006-MY3]
FX The authors thank the anonymous reviewers for their valuable comments.
   The authors thank the anonymous reviewers for their valuable comments,
   and this work is partially supported by the national science council
   (NSC- 101-2628-E-011-006-MY3).
CR Biba M, 2011, APPL INTELL, V34, P279, DOI 10.1007/s10489-009-0195-6
   Breiman L, 1996, 460 UC STAT DEP
   Breiman L, 1996, INT J MACH LEARN, V24, P134
   Chen K, 2011, IEEE T NEURAL NETWOR, V22, P1744, DOI 10.1109/TNN.2011.2167240
   Chung I.-F., 2003, LECT NOTES COMPUTER, P179
   Dalal N, 2005, PROC CVPR IEEE, P886
   Freund Y., 1996, Machine Learning. Proceedings of the Thirteenth International Conference (ICML '96), P148
   Haibo He, 2010, 2010 International Conference on Networking, Sensing and Control (ICNSC 2010), P286, DOI 10.1109/ICNSC.2010.5461483
   Huang CD, 2003, IEEE T NANOBIOSCI, V2, P221, DOI 10.1109/TNB.2003.820284
   Keerthi SS, 2001, NEURAL COMPUT, V13, P637, DOI 10.1162/089976601300014493
   Khor KC, 2012, APPL INTELL, V36, P320, DOI 10.1007/s10489-010-0263-y
   Quinlan JR, 1996, PROCEEDINGS OF THE THIRTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND THE EIGHTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE, VOLS 1 AND 2, P725
   Quinlan JR, 1993, C4 5 PROGRAMS MACHIN, V16
   Stallkamp J., 2012, NEURAL NETWORKS
   Stallkamp J, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P1453, DOI 10.1109/IJCNN.2011.6033395
   Viola P, 2001, PROC CVPR IEEE, P511
   VIOLA P, 2001, 2 INT WORKSH THEOR V
   Wang CW, 2010, APPL INTELL, V33, P357, DOI 10.1007/s10489-009-0172-0
   Witten I. H., 2005, DATA MINING PRACTICA
NR 19
TC 19
Z9 24
U1 1
U2 54
PU SPRINGER
PI DORDRECHT
PA VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN 0924-669X
EI 1573-7497
J9 APPL INTELL
JI Appl. Intell.
PD OCT
PY 2013
VL 39
IS 3
BP 465
EP 474
DI 10.1007/s10489-013-0425-9
PG 10
WC Computer Science, Artificial Intelligence
SC Computer Science
GA 214CF
UT WOS:000324107400002
DA 2020-02-19
ER

PT J
AU Ranzato, M
   Mnih, V
   Susskind, JM
   Hinton, GE
AF Ranzato, Marc'Aurelio
   Mnih, Volodymyr
   Susskind, Joshua M.
   Hinton, Geoffrey E.
TI Modeling Natural Images Using Gated MRFs
SO IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
LA English
DT Article
DE Gated MRF; natural images; deep learning; unsupervised learning; density
   estimation; energy-based model; Boltzmann machine; factored 3-way model;
   generative model; object recognition; denoising; facial expression
   recognition
ID DEEP BELIEF; GAUSSIANS; FEATURES; OBJECT; SCENE; SET
AB This paper describes a Markov Random Field for real-valued image modeling that has two sets of latent variables. One set is used to gate the interactions between all pairs of pixels, while the second set determines the mean intensities of each pixel. This is a powerful model with a conditional distribution over the input that is Gaussian, with both mean and covariance determined by the configuration of latent variables, which is unlike previous models that were restricted to using Gaussians with either a fixed mean or a diagonal covariance matrix. Thanks to the increased flexibility, this gated MRF can generate more realistic samples after training on an unconstrained distribution of high-resolution natural images. Furthermore, the latent variables of the model can be inferred efficiently and can be used as very effective descriptors in recognition tasks. Both generation and discrimination drastically improve as layers of binary latent variables are added to the model, yielding a hierarchical model called a Deep Belief Network.
C1 [Ranzato, Marc'Aurelio; Mnih, Volodymyr; Hinton, Geoffrey E.] Univ Toronto, Dept Comp Sci, Toronto, ON M5S 3G4, Canada.
   [Susskind, Joshua M.] Univ Calif San Diego, Machine Percept Lab, La Jolla, CA 92093 USA.
RP Ranzato, M (reprint author), Univ Toronto, Dept Comp Sci, 6 Kings Coll Rd, Toronto, ON M5S 3G4, Canada.
EM ranzato@cs.toronto.edu
FU NSERCNatural Sciences and Engineering Research Council of Canada;
   CFICanada Foundation for Innovation; CIFARCanadian Institute for
   Advanced Research (CIFAR)
FX The research was funded by Grants from NSERC, CFI, and CIFAR and by
   gifts from Google and Microsoft.
CR Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Black MJ, 1996, INT J COMPUT VISION, V19, P57, DOI 10.1007/BF00131148
   Bosch A., 2007, P 6 ACM INT C IM VID
   Buades A., 2005, P IEEE COMP VIS PATT
   Carreira-Perpignan M. A., 2005, P INT WORKSH ART INT
   Ciresan D., 2011, P 28 INT JOINT C ART
   Dabov K., 2006, P SPIE ELECT IMAGING
   Dailey MN, 2002, J COGNITIVE NEUROSCI, V14, P1158, DOI 10.1162/089892902760807177
   Dalai N., 2005, P IEEE C COMP VIS PA
   Deng J., 2009, P IEEE C COMP VIS PA
   DiCarlo JJ, 2012, NEURON, V73, P415, DOI 10.1016/j.neuron.2012.01.010
   ELAD M, 2006, P IEEE C COMP VIS PA
   Fasel I, 2005, COMPUT VIS IMAGE UND, V98, P182, DOI 10.1016/j.cviu.2004.07.014
   GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596
   Gregor K., 2010, ARXIV10060448
   Hinton G., 2011, P INT C ART NEUR NET
   Hinton G., 2001, P 17 C UNC ART INT
   Hinton G., 1999, P 9 INT C ART NEUR N
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hyvarinen A, 2001, INDEPENDENT COMPONENT ANALYSIS: PRINCIPLES AND PRACTICE, P71
   Jarrett K., 2009, P IEEE INT C COMP VI
   Karklin Y, 2009, NATURE, V457, P83, DOI 10.1038/nature07481
   Koster U., 2007, P 17 INT C ART NEUR
   Krizhevsky A., 2009, THESIS U TORONTO
   Lazebnik S., 2006, P IEEE C COMP VIS PA
   Le Q., 2011, P IEEE C COMP VIS PA
   Le Q., 2010, P ADV NEUR INF PROC
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee H., 2009, P INT C MACH LEARN
   Littlewort G., 2004, P IEEE C COMP VIS PA, P80
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   MacKay D. J. C., 1999, MAXIMUM LIKELIHOOD C
   Mairal J., 2009, P IEEE INT C COMP VI
   Memisevic R, 2010, NEURAL COMPUT, V22, P1473, DOI 10.1162/neco.2010.01-09-953
   Murray I., 2009, P ADV NEUR INF PROC
   Neal Radford M., 1996, BAYESIAN LEARNING NE
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7
   Osindero S., 2008, P ADV NEUR INF PROC
   Portilla J, 2003, IEEE T IMAGE PROCESS, V12, P1338, DOI 10.1109/TIP.2003.818640
   Raina R., 2007, P INT C MACH LEARN
   Ranzato M., 2010, P IEEE C COMP VIS PA
   Ranzato M., 2010, P ADV NEUR INF PROC
   Ranzato M., 2010, P C ART INT STAT
   Ranzato M., 2009, THESIS
   Ranzato M., 2011, P IEEE C COMP VIS PA
   Ranzato M.A., 2007, P IEEE C COMP VIS PA
   ROTH S, 2005, P IEEE C COMP VIS PA
   Schmidt U., 2010, P IEEE C COMP VIS PA
   Sejnowski T., 1986, P AIP C NEUR NETW CO
   Simoncelli EP, 2005, HANDBOOK OF IMAGE AND VIDEO PROCESSING, 2ND EDITION, P431, DOI 10.1016/B978-012119792-6/50089-9
   Susskind J., 2010, TECHNICAL REPORT
   Taylor G., 2007, P ADV NEUR INF PROC
   Teh YW, 2004, J MACH LEARN RES, V4, P1235
   Theis L, 2011, J MACH LEARN RES, V12, P3071
   Tieleman T., 2008, P INT C MACH LEARN
   Tieleman T., 2009, P INT C MACH LEARN
   Tipping ME, 1999, J ROY STAT SOC B, V61, P611, DOI 10.1111/1467-9868.00196
   Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128
   Vincent P., 2008, P INT C MACH LEARN
   Wainwright M., 2000, P ADV NEUR INF PROC
   WEISS Y, 2007, [No title captured]
   Welling M., 2003, P ADV NEUR INF PROC
   Welling M., 2005, P ADV NEUR INF PROC
   Welling M., 2002, P INT C ART NEUR NET
   Williams CKI, 2002, NEURAL COMPUT, V14, P1169, DOI 10.1162/089976602753633439
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Young G, 1940, PSYCHOMETRIKA, V5, P47, DOI 10.1007/BF02288560
   Zhu SC, 1997, IEEE T PATTERN ANAL, V19, P1236, DOI 10.1109/34.632983
   Zontak M., 2011, P IEEE C COMP VIS PA
NR 71
TC 26
Z9 27
U1 2
U2 61
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 0162-8828
EI 1939-3539
J9 IEEE T PATTERN ANAL
JI IEEE Trans. Pattern Anal. Mach. Intell.
PD SEP
PY 2013
VL 35
IS 9
BP 2206
EP 2222
DI 10.1109/TPAMI.2013.29
PG 17
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
SC Computer Science; Engineering
GA 186GB
UT WOS:000322029000012
PM 23868780
DA 2020-02-19
ER

PT J
AU Zhou, SS
   Chen, QC
   Wang, XL
AF Zhou, Shusen
   Chen, Qingcai
   Wang, Xiaolong
TI Convolutional Deep Networks for Visual Data Classification
SO NEURAL PROCESSING LETTERS
LA English
DT Article
DE Semi-supervised learning; Deep learning; Convolutional neural networks;
   Visual data classification
ID ALGORITHM
AB This paper develops a semi-supervised learning algorithm called convolutional deep networks (CDN), to address the image classification problem with deep learning. First, we construct the previous several hidden layers using convolutional restricted Boltzmann machines, which can reduce the dimension and abstract the information of the images effectively. Second, we construct the following hidden layers using restricted Boltzmann machines, which can abstract the information of images quickly. Third, the constructed deep architecture is fine-tuned by gradient-descent based supervised learning with an exponential loss function. CDN can reduce the dimension and abstract the information of the images at the same time efficiently. More importantly, the abstraction and classification procedure of CDN use the same deep architecture to optimize the same parameter in different steps continuously, which can improve the learning ability effectively. We did several experiments on two standard image datasets, and show that CDN are competitive with both representative semi-supervised classifiers and existing deep learning techniques.
C1 [Zhou, Shusen; Chen, Qingcai; Wang, Xiaolong] Harbin Inst Technol, Shenzhen Grad Sch, Key Lab Network Oriented Intelligent Computat, Harbin 150006, Peoples R China.
RP Zhou, SS (reprint author), Harbin Inst Technol, Shenzhen Grad Sch, Key Lab Network Oriented Intelligent Computat, Harbin 150006, Peoples R China.
EM zhoushusen@gmail.com; qingcai.chen@hitsz.edu.cn; wangxl@insun.hit.edu.cn
FU National Natural Science Foundation of ChinaNational Natural Science
   Foundation of China [61272383, 61173075, 60973076]
FX This work was supported in part by the National Natural Science
   Foundation of China (No. 61272383, No. 61173075 and No. 60973076).
CR Chapelle O., 2006, SEMISUPERVISED LEARN
   Collobert R, 2006, J MACH LEARN RES, V7, P1687
   Desjardins G., 2008, EMPIRICAL EVALUATION
   Fei-Fei L, 2004, P C COMP VIS PATT RE, P1
   Feng GY, 2008, NEURAL PROCESS LETT, V27, P247, DOI 10.1007/s11063-008-9073-1
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hinton GE, 2010, PHILOS T R SOC B, V365, P177, DOI 10.1098/rstb.2009.0200
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee H., 2009, P 26 ANN INT C MACH, P609, DOI DOI 10.1145/1553374.1553453
   Liu Y, 2011, PATTERN RECOGN, V44, P2287, DOI 10.1016/j.patcog.2010.12.012
   Machajdik J, 2010, P INT C MULTIMEDIA, P83, DOI DOI 10.1145/1873951.1873965
   Salakhutdinov R., 2007, J MACHINE LEARNING R, P412
   Weston J, 2008, P 25 INT C MACH LEAR, P1168, DOI DOI 10.1145/1390156.1390303
   Zhu X., 2007, SEMISUPERVISED LEARN
NR 17
TC 12
Z9 17
U1 0
U2 72
PU SPRINGER
PI DORDRECHT
PA VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
SN 1370-4621
EI 1573-773X
J9 NEURAL PROCESS LETT
JI Neural Process. Lett.
PD AUG
PY 2013
VL 38
IS 1
BP 17
EP 27
DI 10.1007/s11063-012-9260-y
PG 11
WC Computer Science, Artificial Intelligence
SC Computer Science
GA 184LC
UT WOS:000321890100002
DA 2020-02-19
ER

PT J
AU Memisevic, R
AF Memisevic, Roland
TI Learning to Relate Images
SO IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
LA English
DT Article
DE Learning image relations; spatiotemporal features; mapping units; energy
   models; complex cells
ID STEREO DISPARITY; ENERGY MODELS; COMPUTATION; EMERGENCE; NETWORK
AB A fundamental operation in many vision tasks, including motion understanding, stereopsis, visual odometry, or invariant recognition, is establishing correspondences between images or between images and data from other modalities. Recently, there has been increasing interest in learning to infer correspondences from data using relational, spatiotemporal, and bilinear variants of deep learning methods. These methods use multiplicative interactions between pixels or between features to represent correlation patterns across multiple images. In this paper, we review the recent work on relational feature learning, and we provide an analysis of the role that multiplicative interactions play in learning to encode relations. We also discuss how square-pooling and complex cell models can be viewed as a way to represent multiplicative interactions and thereby as a way to encode relations.
C1 Univ Montreal, Dept Comp Sci & Operat Res, Montreal, PQ H3C 3J7, Canada.
RP Memisevic, R (reprint author), Univ Montreal, Dept Comp Sci & Operat Res, Montreal, PQ H3C 3J7, Canada.
EM memisevr@iro.umontreal.ca
FU German Federal Ministry of Education and Research (BMBF)Federal Ministry
   of Education & Research (BMBF) [01GQ0841]
FX The author thanks Felix Bauer, Yoshua Bengio, Taco Cohen, Georgios
   Exarchakis, Geoffrey Hinton, Kishore Konda, Christoph von der Malsburg,
   Joshua Susskind and Christopher Zach for useful discussions and the
   reviewers for their useful suggestions. This work was supported in part
   by the German Federal Ministry of Education and Research (BMBF) in the
   project 01GQ0841 (BFNT Frankfurt).
CR ADELSON EH, 1985, J OPT SOC AM A, V2, P284, DOI 10.1364/JOSAA.2.000284
   Archie KA, 2000, NAT NEUROSCI, V3, P54, DOI 10.1038/71125
   Bauer F., 2012, THESIS I INFORM
   BECKER S, 1992, NATURE, V355, P161, DOI 10.1038/355161a0
   Bethge M., 2007, P SPIE HUMAN VISION, VXII, P1
   Cadieu CF, 2012, NEURAL COMPUT, V24, P827, DOI 10.1162/NECO_a_00247
   Coates A., 2011, P 14 INT C ART INT S
   Courville A., 2011, P C ART INT STAT
   Denil M, 2012, NEURAL COMPUT, V24, P2151, DOI 10.1162/NECO_a_00312
   Fleet DJ, 1996, VISION RES, V36, P1839, DOI 10.1016/0042-6989(95)00313-4
   FUNAHASHI K, 1989, NEURAL NETWORKS, V2, P183, DOI 10.1016/0893-6080(89)90003-8
   GALLANT JL, 1993, SCIENCE, V259, P100, DOI 10.1126/science.8418487
   GILES CL, 1987, APPL OPTICS, V26, P4972, DOI 10.1364/AO.26.004972
   Gray RM, 2006, FOUND TRENDS COMMUN, V2, DOI 10.1561/0100000006
   Grimes DB, 2005, NEURAL COMPUT, V17, P47, DOI 10.1162/0899766052530893
   Hartley R, 2004, MULTIPLE VIEW GEOMET
   Hinton G., 2010, TECHNICAL REPORT
   Hinton G.F., 1981, P 7 INT JOINT C ART, V2, P683
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hofstadter D.R., 1984, 755 AI MIT
   Horn R. A., 1990, MATRIX ANAL
   Hoyer PO, 2002, VISION RES, V42, P1593, DOI 10.1016/S0042-6989(02)00017-2
   Hyvarinen A, 2000, NEURAL COMPUT, V12, P1705, DOI 10.1162/089976600300015312
   Hyvarinen A, 2000, LECT NOTES COMPUT SC, V1811, P535
   Hyvarinen A, 2009, NATURAL IMAGE STAT P
   Karklin Y., 2006, P ADV NEUR INF PROC, V18
   Kohonen T., 1995, ICANN '95. International Conference on Artificial Neural Networks. Neuronimes '95 Scientific Conference, P3
   Krizhevsky Alex, 2012, P ADV NEUR INF PROC
   Larochelle H., 2010, P ADV NEUR INF PROC, P1243
   Le QV, 2011, PROC CVPR IEEE
   Memisevic R., 2012, P INT C MACH LEARN J
   MEMISEVIC R, 2007, P IEEE C COMP VIS PA
   Memisevic R., 2011, P IEEE INT C COMP VI
   Memisevic R., 2008, THESIS U TORONTO
   Memisevic R., 2010, P ADV NEUR INF PROC
   Memisevic R, 2010, NEURAL COMPUT, V22, P1473, DOI 10.1162/neco.2010.01-09-953
   Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188
   OHZAWA I, 1990, SCIENCE, V249, P1037, DOI 10.1126/science.2396096
   Olshausen B., 1994, THESIS CALIFORNIA I
   Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0
   Olshausen BA, 2007, PROC SPIE, V6492, DOI 10.1117/12.715515
   PLATE TA, 1991, P 12 INT JOINT C ART, P30
   QIAN N, 1994, NEURAL COMPUT, V6, P390, DOI 10.1162/neco.1994.6.3.390
   Ranzato M, 2010, P 13 INT C ART INT S
   Ranzato M, 2010, PROC CVPR IEEE, P2551, DOI 10.1109/CVPR.2010.5539962
   Ross D. A., 2006, P 23 INT C MACH LEAR, P761
   Rumelhart D. E., 1986, PARALLEL DISTRIBUTED, V1, P45
   SANGER TD, 1988, BIOL CYBERN, V59, P405, DOI 10.1007/BF00336114
   SMOLENSKY P, 1990, ARTIF INTELL, V46, P159, DOI 10.1016/0004-3702(90)90007-M
   Smolensky P, 1986, PARALLEL DISTRIBUTED, P194
   Susskind J., 2011, P IEEE C COMP VIS PA
   Sutskever I., 2011, P 28 INT C MACH LEAR, P1017
   Tang Y., 2012, P IEEE C COMP VIS PA
   Taylor G., 2010, P EUR C COMP VIS
   Taylor G. W., 2010, P IEEE C COMP VIS PA
   Taylor GW, 2009, P 26 ANN INT C MACH, P1025, DOI DOI 10.1145/1553374.1553505
   Tenenbaum JB, 2000, NEURAL COMPUT, V12, P1247, DOI 10.1162/089976600300015349
   Troje NF, 1996, VISION RES, V36, P1761, DOI 10.1016/0042-6989(95)00230-8
   Vincent P., 2008, P 25 INT C MACH LEAR
   Von Der Malsburg C, 1994, MODELS NEURAL NETWOR, P95, DOI DOI 10.1007/978-1-4612-4320-5_2
   Wainwright MJ, 2000, ADV NEUR IN, V12, P855
   Welling M., 2002, P ADV NEUR INF PROC
   Zetzsche C, 2005, NETWORK-COMP NEURAL, V16, P191, DOI 10.1080/09548980500463982
   Zou W.Y., 2012, P ADV NEUR INF PROC
NR 65
TC 30
Z9 33
U1 0
U2 61
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 0162-8828
EI 1939-3539
J9 IEEE T PATTERN ANAL
JI IEEE Trans. Pattern Anal. Mach. Intell.
PD AUG
PY 2013
VL 35
IS 8
BP 1829
EP 1846
DI 10.1109/TPAMI.2013.53
PG 18
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
SC Computer Science; Engineering
GA 164AP
UT WOS:000320381400003
PM 23787339
DA 2020-02-19
ER

PT J
AU Farabet, C
   Couprie, C
   Najman, L
   LeCun, Y
AF Farabet, Clement
   Couprie, Camille
   Najman, Laurent
   LeCun, Yann
TI Learning Hierarchical Features for Scene Labeling
SO IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
LA English
DT Article
DE Convolutional networks; deep learning; image segmentation; image
   classification; scene parsing
ID ENERGY MINIMIZATION; OBJECT RECOGNITION
AB Scene labeling consists of labeling each pixel in an image with the category of the object it belongs to. We propose a method that uses a multiscale convolutional network trained from raw pixels to extract dense feature vectors that encode regions of multiple sizes centered on each pixel. The method alleviates the need for engineered features, and produces a powerful representation that captures texture, shape, and contextual information. We report results using multiple postprocessing methods to produce the final labeling. Among those, we propose a technique to automatically retrieve, from a pool of segmentation components, an optimal set of components that best explain the scene; these components are arbitrary, for example, they can be taken from a segmentation tree or from any family of oversegmentations. The system yields record accuracies on the SIFT Flow dataset (33 classes) and the Barcelona dataset (170 classes) and near-record accuracy on Stanford background dataset (eight classes), while being an order of magnitude faster than competing approaches, producing a 320 x 240 image labeling in less than a second, including feature extraction.
C1 [Farabet, Clement; Couprie, Camille; LeCun, Yann] NYU, Courant Inst Math Sci, New York, NY 10003 USA.
   [Farabet, Clement; Najman, Laurent] Univ Paris Est, Equipe A3SI, ESIEE Paris, Lab Informat Gaspard Monge, F-93160 Noisy Le Grand, France.
RP Farabet, C (reprint author), NYU, Courant Inst Math Sci, 12th Fl,715 Broadway, New York, NY 10003 USA.
EM cfarabet@cs.nyu.edu; ccouprie@cs.nyu.edu; l.najman@esiee.fr;
   yann@cs.nyu.edu
RI couprie, camille/H-4092-2014; Najman, Laurent/AAB-4212-2020
OI Najman, Laurent/0000-0002-6190-0235
FU DARPAUnited States Department of DefenseDefense Advanced Research
   Projects Agency (DARPA); ONR MURIMURIOffice of Naval Research; ONROffice
   of Naval Research
FX The authors would like to thank Marco Scoffier for fruitful discussions
   and the 360 degree video collection. They are also grateful to Victor
   Lempitsky who kindly provided them with his results on the Stanford
   database for comparison. This work was funded in part by DARPA contract
   "Integrated deep learning for large scale multimodal data
   representation," ONR MURI "Provably stable vision-based control of
   high-speed flight," ONR grant "Learning Hierarchical Models for
   Information Integration."
CR Arbelaez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114
   Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60
   Boykov YY, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P105, DOI 10.1109/ICCV.2001.937505
   Carreira J, 2012, IEEE T PATTERN ANAL, V34, P1312, DOI 10.1109/TPAMI.2011.231
   Ciresan D, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P1918, DOI 10.1109/IJCNN.2011.6033458
   Dalal N, 2005, PROC CVPR IEEE, P886
   Farabet C., 2010, P INT S CIRC SYST MA
   Farabet C., 2012, P INT C MACH LEARN J
   Farabet C., 2011, P 5 IEEE WORKSH EMB
   Farabet Clement, 2012, CORR
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   Ford Jr L.R., 1955, SIMPLE ALGORITHM FIN
   Fulkerson B, 2009, IEEE I CONF COMP VIS, P670, DOI 10.1109/ICCV.2009.5459175
   Gould S, 2008, INT J COMPUT VISION, V80, P300, DOI 10.1007/s11263-008-0140-x
   Gould S, 2009, IEEE I CONF COMP VIS, P1, DOI 10.1109/ICCV.2009.5459211
   Grangier D., 2009, P INT C MACH LEARN
   Hadsell R., 2006, P IEEE C COMP VIS PA
   He X., 2008, P ADV NEUR INF PROC
   Jain V., 2007, P 11 IEEE INT C COMP
   Jarrett K., 2009, P IEEE INT C COMP VI
   Kavukcuoglu K., 2008, CBLLTR20081201 COUR
   Kavukcuoglu K., 2010, P ADV NEUR INF PROC, V23
   Kavukcuoglu K., 2009, P IEEE C COMP VIS PA
   Kumar MP, 2010, PROC CVPR IEEE, P3217, DOI 10.1109/CVPR.2010.5540072
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y., 1998, NEURAL NETWORKS TRIC
   LeCun Y., 1990, P ADV NEUR INF PROC
   Lee H., 2009, P INT C MACH LEARN
   Lempitsky V., 2011, P ADV NEUR INF PROC
   Liu C., 2009, ARTIFICIAL INTELLIGE
   Munoz D., 2010, P 11 EUR C COMP VIS
   Najman L, 1996, IEEE T PATTERN ANAL, V18, P1163, DOI 10.1109/34.546254
   Ning F, 2005, IEEE T IMAGE PROCESS, V14, P1360, DOI 10.1109/TIP.2005.852470
   Osadchy M, 2007, J MACH LEARN RES, V8, P1197
   Pantofaru C, 2008, LECT NOTES COMPUT SC, V5304, P481, DOI 10.1007/978-3-540-88690-7_36
   Ranzato M.A., 2007, P IEEE C COMP VIS PA
   Russell B., 2007, P NEUR ADV NEUR INF
   Russell C., 2009, P IEEE INT C COMP VI
   Schulz H., 2012, P 11 EUR S ART NEUR
   Shotton J, 2006, LECT NOTES COMPUT SC, V3951, P1
   Simard PY, 2003, PROC INT CONF DOC, P958
   Socher R., 2011, P 26 INT C MACH LEAR
   Tighe J, 2010, LECT NOTES COMPUT SC, V6315, P352, DOI 10.1007/978-3-642-15555-0_26
   Torralba A, 2011, PROC CVPR IEEE, P1521, DOI 10.1109/CVPR.2011.5995347
   Turaga S., 2009, P ADV NEUR INF PROC
   VAILLANT R, 1994, IEE P-VIS IMAGE SIGN, V141, P245, DOI 10.1049/ip-vis:19941301
NR 47
TC 1074
Z9 1151
U1 20
U2 341
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 0162-8828
EI 1939-3539
J9 IEEE T PATTERN ANAL
JI IEEE Trans. Pattern Anal. Mach. Intell.
PD AUG
PY 2013
VL 35
IS 8
BP 1915
EP 1929
DI 10.1109/TPAMI.2012.231
PG 15
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
SC Computer Science; Engineering
GA 164AP
UT WOS:000320381400008
PM 23787344
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Shin, HC
   Orton, MR
   Collins, DJ
   Doran, SJ
   Leach, MO
AF Shin, Hoo-Chang
   Orton, Matthew R.
   Collins, David J.
   Doran, Simon J.
   Leach, Martin O.
TI Stacked Autoencoders for Unsupervised Feature Learning and Multiple
   Organ Detection in a Pilot Study Using 4D Patient Data
SO IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
LA English
DT Article
DE Edge and feature detection; object recognition; pixel classification;
   machine learning; biomedical image processing
ID TUMOR SEGMENTATION; REPRESENTATIONS; STRATEGIES; SPARSE
AB Medical image analysis remains a challenging application area for artificial intelligence. When applying machine learning, obtaining ground-truth labels for supervised learning is more difficult than in many more common applications of machine learning. This is especially so for datasets with abnormalities, as tissue types and the shapes of the organs in these datasets differ widely. However, organ detection in such an abnormal dataset may have many promising potential real-world applications, such as automatic diagnosis, automated radiotherapy planning, and medical image retrieval, where new multimodal medical images provide more information about the imaged tissues for diagnosis. Here, we test the application of deep learning methods to organ identification in magnetic resonance medical images, with visual and temporal hierarchical features learned to categorize object classes from an unlabeled multimodal DCE-MRI dataset so that only a weakly supervised training is required for a classifier. A probabilistic patch-based method was employed for multiple organ detection, with the features learned from the deep learning model. This shows the potential of the deep learning model for application to medical images, despite the difficulty of obtaining libraries of correctly labeled training datasets and despite the intrinsic abnormalities present in patient datasets.
C1 [Shin, Hoo-Chang; Orton, Matthew R.; Collins, David J.; Doran, Simon J.; Leach, Martin O.] Inst Canc Res, Sutton, Surrey, England.
   [Shin, Hoo-Chang; Orton, Matthew R.; Collins, David J.; Doran, Simon J.; Leach, Martin O.] Royal Marsden NHS Fdn Trust, Sutton, Surrey, England.
RP Shin, HC (reprint author), Inst Canc Res, Sutton, Surrey, England.
EM hoo.shin@icr.ac.uk; matthew.orton@icr.ac.uk; david.collins@icr.ac.uk;
   simon.doran@icr.ac.uk; martin.leach@icr.ac.uk
RI leach, martin o/C-2248-2008
OI leach, martin o/0000-0002-0756-5368; Collins, David/0000-0001-8281-1496
FU CRUKCancer Research UK; EPSRC Cancer Imaging CentreEngineering &
   Physical Sciences Research Council (EPSRC); MRCMedical Research Council
   UK (MRC); Department of Health (England) [C1060/A10334]; NHS; Cancer
   Research UKCancer Research UK [16464]; Engineering and Physical Sciences
   Research CouncilEngineering & Physical Sciences Research Council (EPSRC)
   [GR/T20427/01, EP/H046410/1]; National Institute for Health
   ResearchNational Institute for Health Research (NIHR) [NF-SI-0512-10162]
FX The authors would like to acknowledge the support received from the CRUK
   and EPSRC Cancer Imaging Centre in association with the MRC and the
   Department of Health (England) grant C1060/A10334 and also NHS funding
   to the NIHR Biomedical Research Centre and the NIHR Clinical Research
   Facility.
CR [Anonymous], 2009, NEOPLASIA, V11, P102, DOI 10.1593/neo.81328
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1016/j.cviu.2007.09.014
   Bazzani L., 2011, P 28 INT C MACH LEAR, P937
   Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153
   Bergstra J, 2012, J MACH LEARN RES, V13, P281
   Bernstein EJ, 2005, PROC CVPR IEEE, P734
   Clark MC, 1998, IEEE T MED IMAGING, V17, P187, DOI 10.1109/42.700731
   Coates A., 2011, INT C ART INT STAT, V15, P215, DOI 10.1177/1753193410390845
   Collins DJ, 2004, IEEE ENG MED BIOL, V23, P65, DOI 10.1109/MEMB.2004.1360410
   Corso JJ, 2008, IEEE T MED IMAGING, V27, P629, DOI 10.1109/TMI.2007.912817
   Dalal N, 2005, PROC CVPR IEEE, P886
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1
   Farhangfar A., 2009, P 26 ANN INT C MACH, P305
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Fergus R, 2003, PROC CVPR IEEE, P264
   Geremia E, 2010, LECT NOTES COMPUT SC, V6361, P111
   GLOROT Xavier, 2011, P 28 INT C MACH LEAR, V28, P513, DOI DOI 10.1177/1753193411430810
   Goodfellow I., 2009, ADV NEURAL INFORM PR, V22, P646
   Griffin G., 2007, TECHNICAL REPORT
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hoo-Chang Shin, 2011, Proceedings of the 2011 Tenth International Conference on Machine Learning and Applications (ICMLA 2011), P259, DOI 10.1109/ICMLA.2011.38
   HUBEL DH, 1965, J NEUROPHYSIOL, V28, P229
   Iglesias JE, 2011, LECT NOTES COMPUT SC, V6801, P25, DOI 10.1007/978-3-642-22092-0_3
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   KULLBACK S, 1951, ANN MATH STAT, V22, P79, DOI 10.1214/aoms/1177729694
   Larochelle H, 2009, J MACH LEARN RES, V10, P1
   Lazebnik S., 2006, P IEEE INT C COMP VI, V2, P2169, DOI DOI 10.1109/CVPR.2006.68
   Le Q. V., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3361, DOI 10.1109/CVPR.2011.5995496
   Le Q. V., 2011, ADV NEURAL INFORM PR, P1017
   LeCun Yann, 1995, HDB BRAIN THEORY NEU, P255, DOI DOI 10.1109/IJCNN.2004.1381049
   Lee H., 2008, ADV NEURAL INFORM PR, V20, P873
   Lee H, 2011, COMMUN ACM, V54, P95, DOI 10.1145/2001269.2001295
   Li FF, 2006, IEEE T PATTERN ANAL, V28, P594, DOI 10.1109/TPAMI.2006.79
   Li L., 2011, P 28 INT C MACH LEAR, P185
   Linguraru MG, 2008, I S BIOMED IMAGING, P45, DOI 10.1109/ISBI.2008.4540928
   LIU DC, 1989, MATH PROGRAM, V45, P503, DOI 10.1007/BF01589116
   Lowe D., 1999, P INT C COMP VIS, V2, P1150, DOI [10.1109/ICCV.1999.790410, DOI 10.1109/ICCV.1999.790410]
   Marc'Aurelio R., 2007, ADV NEURAL INFORM PR, V20
   Ngiam  Jiquan, 2011, P INT C MACH LEARN
   Niebles JC, 2008, INT J COMPUT VISION, V79, P299, DOI 10.1007/s11263-007-0122-4
   Nowak E, 2006, LECT NOTES COMPUT SC, V3954, P490
   Okada T, 2008, LECT NOTES COMPUT SC, V5241, P502, DOI 10.1007/978-3-540-85988-8_60
   Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7
   Orton MR, 2009, PHYS MED BIOL, V54, P2197, DOI 10.1088/0031-9155/54/7/023
   Pauly O, 2011, LECT NOTES COMPUT SC, V6893, P239, DOI 10.1007/978-3-642-23626-6_30
   Raina R, 2007, P 24 INT C MACH LEAR, P759, DOI DOI 10.1145/1273496.1273592
   Ranzato MA, 2007, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2007.383157
   Rumelhart D.E., 1986, INFORM PROCESSING DY, V1, P194
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Schmah T., 2009, ADV NEURAL INFORM PR, P1409
   Shin H.-C., 2012, P WORKSH CHALL MED I
   Sivic J, 2005, IEEE I CONF COMP VIS, P370
   Snoek J., 2012, ADV NEUR INF PROC SY
   Sohn K, 2011, IEEE I CONF COMP VIS, P2643, DOI 10.1109/ICCV.2011.6126554
   Torralba A, 2007, IEEE T PATTERN ANAL, V29, P854, DOI 10.1109/TPAMI.2007.1055
   Vincent P, 2008, P 25 INT C MACH LEAR, P1096, DOI DOI 10.1145/1390156.1390294
   Weber M, 2000, PROC CVPR IEEE, P101, DOI 10.1109/CVPR.2000.854754
   Yu K., 2008, ADV NEURAL INFORM PR, P1889
   Zeiler MD, 2011, IEEE I CONF COMP VIS, P2018, DOI 10.1109/ICCV.2011.6126474
NR 59
TC 205
Z9 225
U1 10
U2 148
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 0162-8828
EI 1939-3539
J9 IEEE T PATTERN ANAL
JI IEEE Trans. Pattern Anal. Mach. Intell.
PD AUG
PY 2013
VL 35
IS 8
BP 1930
EP 1943
DI 10.1109/TPAMI.2012.277
PG 14
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
SC Computer Science; Engineering
GA 164AP
UT WOS:000320381400009
PM 23787345
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Salakhutdinov, R
   Tenenbaum, JB
   Torralba, A
AF Salakhutdinov, Ruslan
   Tenenbaum, Joshua B.
   Torralba, Antonio
TI Learning with Hierarchical-Deep Models
SO IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
LA English
DT Article
DE Deep networks; deep Boltzmann machines; hierarchical Bayesian models;
   one-shot learning
ID OBJECT
AB We introduce HD (or "Hierarchical-Deep") models, a new compositional learning architecture that integrates deep learning models with structured hierarchical Bayesian (HB) models. Specifically, we show how we can learn a hierarchical Dirichlet process (HDP) prior over the activities of the top-level features in a deep Boltzmann machine (DBM). This compound HDP-DBM model learns to learn novel concepts from very few training example by learning low-level generic features, high-level features that capture correlations among low-level features, and a category hierarchy for sharing priors over the high-level features that are typical of different kinds of concepts. We present efficient learning and inference algorithms for the HDP-DBM model and show that it is able to learn new concepts from very few examples on CIFAR-100 object recognition, handwritten character recognition, and human motion capture datasets.
C1 [Salakhutdinov, Ruslan] Univ Toronto, Dept Stat & Comp Sci, Toronto, ON M5S 3G3, Canada.
   [Tenenbaum, Joshua B.] MIT, Dept Brain & Cognit Sci, Cambridge, MA 02139 USA.
   [Torralba, Antonio] MIT, Comp Sci & Artificial Intelligence Lab, Cambridge, MA 02139 USA.
RP Salakhutdinov, R (reprint author), Univ Toronto, Dept Stat & Comp Sci, Toronto, ON M5S 3G3, Canada.
EM rsalakhu@utstat.toronto.edu; jbt@mit.edu; torralba@mit.edu
FU NSERCNatural Sciences and Engineering Research Council of Canada; ONR
   (MURI)MURIOffice of Naval Research [1015GNA126]; ONROffice of Naval
   Research [N00014-07-1-0937]; ARO [W911NF-08-1-0242]; Qualcomm
FX This research was supported by NSERC, ONR (MURI Grant 1015GNA126), ONR
   N00014-07-1-0937, ARO W911NF-08-1-0242, and Qualcomm.
CR Babenko B., 2009, P IEEE INT C COMP VI
   Bart E, 2005, PROC CVPR IEEE, P672
   Barthe E, 2009, POLICE PRACT RES, V10, P255, DOI 10.1080/15614260802381067
   Blei DM, 2010, J ACM, V57, DOI 10.1145/1667053.1667056
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Canini K.R., 2009, P NIPS WORKSH NONP B
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chen B., 2011, P 28 INT C MACH LEAR, P361
   Coates A., 2011, P 11 INT C DOC AN RE
   Courville A, 2011, P 28 INT C MACH LEAR, P1145
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton G. E., 1983, P IEEE C COMP VIS PA
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   JORDAN M. I., 2010, BAYESIAN NONPARAMETR
   Kemp C., 2006, DEVELOPMENTAL SCI, V10, P307
   Krizhevsky A, 2009, LEARNING MULTIPLE LA
   Lake B., 2011, P 33 ANN C COGN SCI
   Larochelle H, 2009, J MACH LEARN RES, V10, P1
   Lee H., 2009, P 26 ANN INT C MACH, P609, DOI DOI 10.1145/1553374.1553453
   Li FF, 2006, IEEE T PATTERN ANAL, V28, P594, DOI 10.1109/TPAMI.2006.79
   Lin Y., 2011, P ADV NEUR INF PROC, V23
   Mohamed AR, 2012, IEEE T AUDIO SPEECH, V20, P14, DOI 10.1109/TASL.2011.2109382
   Nair V., 2009, P ADV NEUR INF PROC, V21
   Perfors A. Perfors, 2009, P 31 ANN C COGN SCI, P136
   Ranzato M.A., 2008, P ADV NEUR INF PROC
   ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586
   Rodriguez A, 2008, J AM STAT ASSOC, V103, P1131, DOI 10.1198/016214508000000553
   Salakhutdinov R. R., 2010, P ADV NEUR INF PROC, V22
   Salakhutdinov R. R., 2009, P INT C ART INT STAT, V12
   Smith LB, 2002, PSYCHOL SCI, V13, P13, DOI 10.1111/1467-9280.00403
   Socher R, 2011, P 28 INT C MACH LEAR
   Sudderth EB, 2008, INT J COMPUT VISION, V77, P291, DOI 10.1007/s11263-007-0069-5
   Taylor G., 2006, P ADV NEUR INF PROC
   Taylor G.W., 2010, P 11 EUR C COMP VIS
   Teh Y.W., 2001, P ADV NEUR INF PROC, V13
   Teh YW, 2006, J AM STAT ASSOC, V101, P1566, DOI 10.1198/016214506000000302
   Tieleman T., 2008, P 25 INT C MACH LEAR
   Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128
   Torralba A, 2006, LECT NOTES COMPUT SC, V4170, P345
   Vincent P, 2008, P 25 INT C MACH LEAR, P1096, DOI DOI 10.1145/1390156.1390294
   Xu F, 2007, PSYCHOL REV, V114, P245, DOI 10.1037/0033-295X.114.2.245
   YOUNES L, 1989, PROBAB THEORY REL, V82, P625, DOI 10.1007/BF00341287
   Younes L., 2000, CONVERGENCE MARKOVIA
   Yuille A., 2004, P ADV NEUR INF PROC
NR 44
TC 94
Z9 98
U1 0
U2 101
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 0162-8828
EI 1939-3539
J9 IEEE T PATTERN ANAL
JI IEEE Trans. Pattern Anal. Mach. Intell.
PD AUG
PY 2013
VL 35
IS 8
BP 1958
EP 1971
DI 10.1109/TPAMI.2012.269
PG 14
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
SC Computer Science; Engineering
GA 164AP
UT WOS:000320381400011
PM 23787346
DA 2020-02-19
ER

PT J
AU Chen, B
   Polatkan, G
   Sapiro, G
   Blei, D
   Dunson, D
   Carin, L
AF Chen, Bo
   Polatkan, Gungor
   Sapiro, Guillermo
   Blei, David
   Dunson, David
   Carin, Lawrence
TI Deep Learning with Hierarchical Convolutional Factor Analysis
SO IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
LA English
DT Article
DE Bayesian; deep learning; convolutional; dictionary learning; factor
   analysis
AB Unsupervised multilayered ("deep") models are considered for imagery. The model is represented using a hierarchical convolutional factor-analysis construction, with sparse factor loadings and scores. The computation of layer-dependent model parameters is implemented within a Bayesian setting, employing a Gibbs sampler and variational Bayesian (VB) analysis that explicitly exploit the convolutional nature of the expansion. To address large-scale and streaming data, an online version of VB is also developed. The number of dictionary elements at each layer is inferred from the data, based on a beta-Bernoulli implementation of the Indian buffet process. Example results are presented for several image-processing applications, with comparisons to related models in the literature.
C1 [Chen, Bo; Sapiro, Guillermo; Carin, Lawrence] Duke Univ, Dept Elect & Comp Engn, Durham, NC 27708 USA.
   [Polatkan, Gungor; Blei, David] Princeton Univ, Dept Comp Sci, Princeton, NJ 08540 USA.
   [Dunson, David] Duke Univ, Dept Stat, Durham, NC 27708 USA.
RP Chen, B (reprint author), Duke Univ, Dept Elect & Comp Engn, 130 Hudson Hall, Durham, NC 27708 USA.
EM lcarin@ee.duke.edu
FU NIEHS NIH HHSUnited States Department of Health & Human ServicesNational
   Institutes of Health (NIH) - USANIH National Institute of Environmental
   Health Sciences (NIEHS) [R01 ES017436]
CR Adams R. P., 2010, P INT C ART INT STAT
   Bo L., 2011, P ADV NEUR INF PROC
   Boureau Y., 2010, P INT C MACH LEARN
   Boureau Y.-L., 2007, P IEEE C COMP VIS PA
   Boureau Y-L, 2010, P IEEE C COMP VIS PA
   Carvalho CM, 2008, J AM STAT ASSOC, V103, P1438, DOI 10.1198/016214508000000869
   Chen B., 2011, P INT C MACH LEARN
   Cristianini N., 2000, INTRO SUPPORT VECTOR
   Donoho D. L, 2000, P INT WORKSH IND COM, P459
   Griffiths Thomas L., 2005, NEURAL INFORM PROCES, P475
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hoffman Matthew, 2010, P ADV NEUR INF PROC
   Hoyer PO, 2004, J MACH LEARN RES, V5, P1457
   Jarrett K., 2009, P IEEE INT C COMP VI
   Johnson CR, 2008, IEEE SIGNAL PROC MAG, V25, P37, DOI 10.1109/MSP.2008.923513
   Kavukcuoglu K., 2010, P ADV NEUR INF PROC
   Knowles D., 2007, P 7 INT C IND COMP A
   Lazebnik S., 2006, P IEEE C COMP VIS PA
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565
   Lee H., 2009, P NEUR INF PROC SYST
   Lee H., 2008, P NEUR INF PROC SYST
   Lee H., 2009, P INT C MACH LEARN
   Mairal J., 2009, P INT C MACH LEARN
   Marr D., 1982, VISION
   Norouzi M. R. M., 2009, P IEEE C COMP VIS PA
   Paisley J., 2009, P INT C MACH LEARN
   Ranzato M., 2006, P NEUR INF PROC SYST
   thibaux r, 2007, P INT C ART INT STAT
   Tipping ME, 2001, J MACH LEARN RES, V1, P211, DOI 10.1162/15324430152748236
   Vincent P., 2008, P INT C MACH LEARN
   Wang J., 2010, P IEEE C COMP VIS PA
   West M, 2003, BAYESIAN STATISTICS 7, P733
   Weston R.C.J., 2008, P INT C MACH LEARN
   Yu K., 2011, P IEEE C COMP VIS PA
   Zeiler M., 2011, P IEEE INT C COMP VI
   Zeiler Matthew D, 2010, P IEEE C COMP VIS PA
   Zhang H., 2006, P IEEE C COMP VIS PA
   Zhou M., 2009, P NEUR INF PROC SYST
NR 39
TC 52
Z9 59
U1 1
U2 138
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 0162-8828
EI 1939-3539
J9 IEEE T PATTERN ANAL
JI IEEE Trans. Pattern Anal. Mach. Intell.
PD AUG
PY 2013
VL 35
IS 8
BP 1887
EP 1901
DI 10.1109/TPAMI.2013.19
PG 15
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
SC Computer Science; Engineering
GA 164AP
UT WOS:000320381400006
PM 23787342
OA Green Accepted
DA 2020-02-19
ER

PT J
AU Hutchinson, B
   Deng, L
   Yu, D
AF Hutchinson, Brian
   Deng, Li
   Yu, Dong
TI Tensor Deep Stacking Networks
SO IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
LA English
DT Article
DE Deep learning; stacking networks; tensor; bilinear models; handwriting
   image classification; phone classification and recognition; MNIST;
   TIMIT; WSJ
AB A novel deep architecture, the tensor deep stacking network (T-DSN), is presented. The T-DSN consists of multiple, stacked blocks, where each block contains a bilinear mapping from two hidden layers to the output layer, using a weight tensor to incorporate higher order statistics of the hidden binary ([0, 1]) features. A learning algorithm for the T-DSN's weight matrices and tensors is developed and described in which the main parameter estimation burden is shifted to a convex subproblem with a closed-form solution. Using an efficient and scalable parallel implementation for CPU clusters, we train sets of T-DSNs in three popular tasks in increasing order of the data size: handwritten digit recognition using MNIST (60k), isolated state/phone classification and continuous phone recognition using TIMIT (1.1 m), and isolated phone classification using WSJ0 (5.2 m). Experimental results in all three tasks demonstrate the effectiveness of the T-DSN and the associated learning methods in a consistent manner. In particular, a sufficient depth of the T-DSN, a symmetry in the two hidden layers structure in each T-DSN block, our model parameter learning algorithm, and a softmax layer on top of T-DSN are shown to have all contributed to the low error rates observed in the experiments for all three tasks.
C1 [Hutchinson, Brian] Univ Washington, Dept Elect Engn, Seattle, WA 98105 USA.
   [Deng, Li; Yu, Dong] Microsoft Res, Redmond, WA 98052 USA.
RP Hutchinson, B (reprint author), Univ Washington, Dept Elect Engn, Seattle, WA 98105 USA.
EM brianhutchinson@ee.washington.edu; deng@microsoft.com;
   dongyu@microsoft.com
OI Hutchinson, Brian/0000-0002-5537-008X
CR Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Ciresan D.C., 2011, P 22 INT JOINT C ART
   Dahl G., 2010, P ADV NEUR INF PROC
   Dahl GE, 2012, IEEE T AUDIO SPEECH, V20, P30, DOI 10.1109/TASL.2011.2134090
   Deng L., 2012, P IEEE INT C AC SPEE
   Deng L., 2011, P ICML WORKSH LEARN
   Deng L., 2011, P ANN C INT SPEECH C
   Deng L, 2006, IEEE T AUDIO SPEECH, V14, P1492, DOI 10.1109/TASL.2006.878265
   Dunlavy D. M., 2010, SAND20101422 SAND NA
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Hutchinson B., 2012, P IEEE INT C AC SPEE
   Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X
   Le Q.V., 2012, P INT C MACH LEARN
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LEE KF, 1989, IEEE T ACOUST SPEECH, V37, P1641, DOI 10.1109/29.46546
   Mohamed A., 2009, P NIPS WORKSH DEEP L
   Mohamed A., 2010, P ANN C INT SPEECH C
   Mohamed AR, 2012, IEEE T AUDIO SPEECH, V20, P14, DOI 10.1109/TASL.2011.2109382
   Paul D. B., 1992, P INT C SPOK LANG PR
   Petersen K. B., 2008, MATRIX COOKBOOK
   Ranzato M., 2010, P INT C ART INT STAT, V13
   Ranzato M, 2010, PROC CVPR IEEE, P2551, DOI 10.1109/CVPR.2010.5539962
   Tur G., 2012, P IEEE INT C AC SPEE
   Weisstein E., 2012, SYMMETRIC BILINEAR F
   WOLPERT DH, 1992, NEURAL NETWORKS, V5, P241, DOI 10.1016/S0893-6080(05)80023-1
   Yu D., 2012, P ANN C INT SPEECH C
   Yu D., 2012, P IEEE INT C AC SPEE
   Yu D., 2011, P 12 ANN C INT SPEEC
NR 29
TC 65
Z9 69
U1 0
U2 31
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 0162-8828
EI 1939-3539
J9 IEEE T PATTERN ANAL
JI IEEE Trans. Pattern Anal. Mach. Intell.
PD AUG
PY 2013
VL 35
IS 8
BP 1944
EP 1957
DI 10.1109/TPAMI.2012.268
PG 14
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
SC Computer Science; Engineering
GA 164AP
UT WOS:000320381400010
PM 23267198
DA 2020-02-19
ER

PT J
AU Deng, L
   Li, X
AF Deng, Li
   Li, Xiao
TI Machine Learning Paradigms for Speech Recognition: An Overview
SO IEEE TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING
LA English
DT Article
DE Machine learning; speech recognition; supervised; unsupervised;
   discriminative; generative; dynamics; adaptive; Bayesian; deep learning
ID HIDDEN MARKOV-MODELS; DEEP NEURAL-NETWORKS; MINIMUM PHONE ERROR;
   BAYES-RISK; PARAMETER-ESTIMATION; MIXTURE OBSERVATIONS;
   CONVEX-OPTIMIZATION; MAXIMUM-LIKELIHOOD; MULTIPLE TASKS; CLASSIFICATION
AB Automatic Speech Recognition (ASR) has historically been a driving force behind many machine learning (ML) techniques, including the ubiquitously used hidden Markov model, discriminative learning, structured sequence learning, Bayesian learning, and adaptive learning. Moreover, ML can and occasionally does use ASR as a large-scale, realistic application to rigorously test the effectiveness of a given technique, and to inspire new problems arising from the inherently sequential and dynamic nature of speech. On the other hand, even though ASR is available commercially for some applications, it is largely an unsolved problem-for almost all applications, the performance of ASR is not on par with human performance. New insight from modern ML methodology shows great promise to advance the state-of-the-art in ASR technology. This overview article provides readers with an overview of modern ML techniques as utilized in the current and as relevant to future ASR research and systems. The intent is to foster further cross-pollination between the ML and ASR communities than has occurred in the past. The article is organized according to the major ML paradigms that are either popular already or have potential for making significant contributions to ASR technology. The paradigms presented and elaborated in this overview include: generative and discriminative learning; supervised, unsupervised, semi-supervised, and active learning; adaptive and multi-task learning; and Bayesian learning. These learning paradigms are motivated and discussed in the context of ASR technology and applications. We finally present and analyze recent developments of deep learning and learning with sparse representations, focusing on their direct relevance to advancing ASR technology.
C1 [Deng, Li; Li, Xiao] Microsoft Res, Redmond, WA 98052 USA.
RP Deng, L (reprint author), Microsoft Res, Redmond, WA 98052 USA.
EM deng@microsoft.com; mimily@gmail.com
FU mentor-mentee project
FX Our earnest thanks go to MSR for the encouragement and support of the
   "mentor-mentee project" from which this paper grew out of, to Jeff
   Bilmes for contributions during the very early phase of developing this
   paper, to Geoff Hinton, John Platt, Mark Gales, Nelson Morgan, Hynek
   Hermansky, Alex Acero, and Jason Eisner for valuable discussions, to
   Helen Meng as the then Editor-in-Chief for the encouragement and for
   handling the reviews of the white paper leading to this paper during
   2009, and, finally, to the three anonymous reviewers whose desire for
   perfection has made various versions of the revision steadily improve
   the paper's quality as new advances on ML and ASR frequently broke out
   throughout the writing and revision of this paper over the past 3 years.
CR Abdel-Hamid O, 2012, INT CONF ACOUST SPEE, P4277, DOI 10.1109/ICASSP.2012.6288864
   Abrash V., 1995, P EUR
   Acero A., 2000, P ICSLP, P869, DOI DOI 10.1016/S0167-6393(03)00016-5
   Anastasakos T, 1996, ICSLP 96 - FOURTH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE PROCESSING, PROCEEDINGS, VOLS 1-4, P1137, DOI 10.1109/ICSLP.1996.607807
   Ando RK, 2005, J MACH LEARN RES, V6, P1817
   Andrew G, 2012, INT CONF ACOUST SPEE, P4265, DOI 10.1109/ICASSP.2012.6288861
   Argyriou A, 2007, P ADV NEUR INF PROC
   Bacchiani M, 2003, 2003 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL I, PROCEEDINGS, P224
   BAHL LR, 1986, P IEEE INT C AC SPEE, P49
   Baker J., 1976, SPEECH RECOGN
   Baker JM, 2009, IEEE SIGNAL PROC MAG, V26, P78, DOI 10.1109/MSP.2009.932707
   Baker JM, 2009, IEEE SIGNAL PROC MAG, V26, P75, DOI 10.1109/MSP.2009.932166
   Barber D, 2010, IEEE SIGNAL PROC MAG, V27, P18, DOI 10.1109/MSP.2010.938028
   Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907
   BAUM LE, 1966, ANN MATH STAT, V37, P1554, DOI 10.1214/aoms/1177699147
   Baxter J, 1997, MACH LEARN, V28, P7, DOI 10.1023/A:1007327622663
   Baxter J, 2000, J ARTIF INTELL RES, V12, P149, DOI 10.1613/jair.731
   Baxter J., 1995, P WORKSH COMP LEARN
   Belkin M, 2006, J MACH LEARN RES, V7, P2399
   Ben-David S., 2003, P COMP LEARN THEOR
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Bennett KP, 1999, ADV NEUR IN, V11, P368
   Biem A, 2001, IEEE T SPEECH AUDI P, V9, P96, DOI 10.1109/89.902277
   BILMES J, 2003, MATH FDN SPEECH LANG
   BILMES J, 2004, UWEETR20040016 DEP E
   Bilmes J, 1997, TR97021 ICSI
   Bilmes JA, 2006, IEICE T INF SYST, VE89D, P869, DOI 10.1093/ietisy/e89-d.3.869
   Bilmes JA, 2005, IEEE SIGNAL PROC MAG, V22, P89, DOI 10.1109/MSP.2005.1511827
   Bilmes JA, 2003, COMPUT SPEECH LANG, V17, P213, DOI 10.1016/S0885-2308(03)00010-X
   Bilmes J, 2010, IEEE SIGNAL PROC MAG, V27, P29, DOI 10.1109/MSP.2010.938078
   Bishop CM, 2006, PATTERN RECOGNITION
   Blitzer J., 2008, P ADV NEUR INF PROC
   BLUM A, 1998, P WORKSH COMP LEARN
   Bourlard H, 1998, LECT NOTES ARTIF INT, V1387, P389
   BOURLARD H, 1993, IEEE T NEURAL NETWOR, V4, P893, DOI 10.1109/72.286885
   BOURLARD H, 1994, KLUWER INT SERIES EN, V247
   BRIDLE J, 1998, 1998 WORKSH LANG ENG
   Bromberg I., 2007, INTERSPEECH, P1829
   Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734
   Chang TH, 2008, INT CONF ACOUST SPEE, P4053
   Chapelle O., 2006, P INT C MACH LEARN
   CHARLET D, 2001, ACOUST SPEECH SIG PR, P357
   Chawla S., 2001, P INT C MACH LEARN
   Chelba C., 2004, P EMNLP JUL
   Chengalvarayan R, 1998, IEEE T SPEECH AUDI P, V6, P505, DOI 10.1109/89.725317
   Chengalvarayan R, 1997, IEEE T SPEECH AUDI P, V5, P243, DOI 10.1109/89.568731
   Chesta C., 1999, P EUR
   Chien J.-T., 2011, IEEE AUDIO SPEECH LA, V27, P43
   Ciresan DC, 2010, NEURAL COMPUT, V22, P3207, DOI 10.1162/NECO_a_00052
   COLLOBERT R., 2006, J MACH LEARN RES
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411
   Crammer K, 2002, J MACH LEARN RES, V2, P265, DOI 10.1162/15324430260185628
   Dagan I., 1995, P INT C MACH LEARN
   Dahl GE, 2012, IEEE T AUDIO SPEECH, V20, P30, DOI 10.1109/TASL.2011.2134090
   Dahl GE, 2011, INT CONF ACOUST SPEE, P4688
   Dai W., 2008, ADV NEUR INF PROC SY
   Daume H, 2006, J ARTIF INTELL RES, V26, P101, DOI 10.1613/jair.1872
   Daume H., 2008, P EMNLP
   Daume H., 2009, P UNC ART INT
   De Wachter M, 2007, IEEE T AUDIO SPEECH, V15, P1377, DOI 10.1109/TASL.2007.894524
   Dean J, 2012, P ADV NEURAL INFORM
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1
   Deng L, 1997, SPEECH COMMUN, V22, P93, DOI 10.1016/S0167-6393(97)00018-6
   Deng L, 2000, J ACOUST SOC AM, V108, P3036, DOI 10.1121/1.1315288
   Deng L, 2006, IEEE T AUDIO SPEECH, V14, P256, DOI 10.1109/TSA.2005.854107
   DENG L, 1992, SIGNAL PROCESS, V27, P65, DOI 10.1016/0165-1684(92)90112-A
   Deng L, 1998, SPEECH COMMUN, V24, P299, DOI 10.1016/S0167-6393(98)00023-5
   DENG L, 1994, J ACOUST SOC AM, V95, P2702, DOI 10.1121/1.409839
   Deng L, 2004, IEEE T SPEECH AUDI P, V12, P133, DOI 10.1109/TSA.2003.820201
   Deng L, 2003, IEEE T SPEECH AUDI P, V11, P568, DOI 10.1109/TSA.2003.818076
   DENG L, 1991, IEEE T SIGNAL PROCES, V39, P1677, DOI 10.1109/78.134406
   Deng L., 1994, IEEE T SPEECH AUDIO, V2, P101
   DENG L, 2002, ACOUST SPEECH SIG PR, P829
   DENG L, 2006, [No title captured]
   Deng L., 2013, P INT C AC SPEECH SI
   Deng L., 1991, COMPUT SPEECH LANG, V4, P345
   Deng L., 2012, IEEE T AUDIO SPEECH, V20, P2409
   Deng L., 2003, MATH FDN SPEECH LANG, P115
   Deng L., 2010, P INTERSPEECH
   DENG L, 2000, [No title captured], V3, P806
   Deng L., 1999, COMPUTATIONAL MODELS, P199
   Deng L, 2006, IEEE T AUDIO SPEECH, V14, P1492, DOI 10.1109/TASL.2006.878265
   Deng L, 2012, INT CONF ACOUST SPEE, P2133, DOI 10.1109/ICASSP.2012.6288333
   Deng L, 2011, ROBUST SPEECH RECOGNITION OF UNCERTAIN OR MISSING DATA: THEORY AND APPLICATIONS, P67, DOI 10.1007/978-3-642-21317-5_4
   Droppo J., 2004, P IEEE INT C AC SPEE, V1, pI
   Eldar YC, 2010, IEEE SIGNAL PROC MAG, V27, P19, DOI 10.1109/MSP.2010.936016
   EPHRAIM Y, 1990, IEEE T INFORM THEORY, V36, P372, DOI 10.1109/18.52483
   Evgeniou T, 2005, J MACH LEARN RES, V6, P615
   Fox EB, 2010, IEEE SIGNAL PROC MAG, V27, P43, DOI 10.1109/MSP.2010.937999
   Freund Y, 1997, MACH LEARN, V28, P133, DOI 10.1023/A:1007330508534
   Frey B., 2000, P EUR
   Fu Q., 2007, P INTERSPEECH
   Fu Q, 2012, IEEE T AUDIO SPEECH, V20, P780, DOI 10.1109/TASL.2011.2165279
   Gales M, 2012, IEEE SIGNAL PROC MAG, V29, P70, DOI 10.1109/MSP.2012.2207140
   Gales MJF, 1996, IEEE T SPEECH AUDI P, V4, P352, DOI 10.1109/89.536929
   Gales MJF, 2000, IEEE T SPEECH AUDI P, V8, P417, DOI 10.1109/89.848223
   Ganapathiraju A., 2000, P ADV NEUR INF PROC
   Gauvain J.-L., 1991, P DARPA SPEECH NAT L, P272
   Gauvain JL, 1994, IEEE T SPEECH AUDI P, V2, P291, DOI 10.1109/89.279278
   Gemmeke J., 2010, P INTERSPEECH
   Gemmeke JF, 2011, IEEE T AUDIO SPEECH, V19, P2067, DOI 10.1109/TASL.2011.2112350
   Gens R., 2012, P ADV NEUR INF PROC
   Gibson M, 2010, IEEE T AUDIO SPEECH, V18, P1269, DOI 10.1109/TASL.2009.2032607
   Gliozzo A., 2006, P ASS COMP LING
   Goel V, 2004, IEEE T SPEECH AUDI P, V12, P234, DOI 10.1109/TSA.2004.825678
   Goel V, 2000, COMPUT SPEECH LANG, V14, P115, DOI 10.1006/csla.2000.0138
   Golovin D., 2010, P INT C LEARN THEOR
   Gong Y., 1996, P INT C SPOK LANG PR
   Grandvalet Y., 2004, P ADV NEUR INF PROC
   Grandvalet Y., 1996, P ADV NEUR INF PROC
   Guillory A., 2010, P INT C MACH LEARN H
   GUNAWARDANA A, 2005, P INTERSPEECH
   Hakkani-Tur D, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL I, PROCEEDINGS, P429
   HAKKANITUR D, 2002, ACOUST SPEECH SIG PR, P3904
   Ham J., 2005, P INT WORKSH ART INT
   Hamanaka Y, 2010, INT CONF ACOUST SPEE, P4350, DOI 10.1109/ICASSP.2010.5495650
   Hasegawa-Johnson M, 2005, INT CONF ACOUST SPEE, P213
   He X., 2012, P 50 ANN M ASS COMP, P292
   He X., 2008, DISCRIMINATIVE LEARN
   He XD, 2008, IEEE SIGNAL PROC MAG, V25, P14, DOI 10.1109/MSP.2008.926652
   He XD, 2011, IEEE SIGNAL PROC MAG, V28, P126, DOI 10.1109/MSP.2011.941852
   Heigold G, 2010, INT CONF ACOUST SPEE, P5546, DOI 10.1109/ICASSP.2010.5495228
   Heigold G, 2011, IEEE T AUDIO SPEECH, V19, P1138, DOI 10.1109/TASL.2010.2082532
   Heintz I, 2009, IEEE T AUDIO SPEECH, V17, P1533, DOI 10.1109/TASL.2009.2022204
   Heskes T., 2000, P INT C MACH LEARN
   Hifny Y, 2009, IEEE T AUDIO SPEECH, V17, P354, DOI 10.1109/TASL.2008.2010286
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Holmes WJ, 1999, COMPUT SPEECH LANG, V13, P3, DOI 10.1006/csla.1998.0048
   Huang J.-T., 2008, P INTERSPEECH
   Huang X., HDB NATURAL LANGUAGE
   Huang X., 2001, SPOKEN LANGUAGE PROC
   HUTCHINSON B, 2012, 2012 IEEE INT C AC, P4805
   Hutchinson B., 2013, IEEE T PATT IN PRESS
   Jaakkola T., 1999, AITR1668 MIT ART INT
   Jaakkola T., 1998, ADV NEURAL INF PROCE, V11
   JELINEK F, 1976, P IEEE, V64, P532, DOI 10.1109/PROC.1976.10159
   Jiang H, 2006, IEEE T AUDIO SPEECH, V14, P1584, DOI 10.1109/TASL.2006.879805
   Jiang H, 2010, IEEE SIGNAL PROC MAG, V27, P115, DOI 10.1109/MSP.2010.936018
   Jiao F., 2006, P ASS COMP LING
   Joachims T., 2002, CMUCALD02
   Joachims T., 2006, SEMISUPERVISED LEARN
   Joachims T., 1999, P INT C MACH LEARN
   Joachims T., 2003, P INT C MACH LEARN
   Jordan MI, 2010, IEEE SIGNAL PROC MAG, V27, P17, DOI 10.1109/MSP.2010.938115
   JUANG BH, 1986, IEEE T INFORM THEORY, V32, P307
   Juang BH, 1997, IEEE T SPEECH AUDI P, V5, P257, DOI 10.1109/89.568732
   Kalinli O, 2010, IEEE T AUDIO SPEECH, V18, P1889, DOI 10.1109/TASL.2010.2040522
   Kemp T., 1999, P EUR
   King S, 2007, J ACOUST SOC AM, V121, P723, DOI 10.1121/1.2404622
   Kingsbury B., 2012, P INTERSPEECH
   Koller D., 2009, PROBABILISTIC GRAPHI
   Kuhn R., 2000, IEEE T SPEECH AUDIO, V8, P417
   Kumar S., 2004, P HLT NAACL
   Kuo H.-K. J., 2005, P INTERSPEECH
   Kuo HKJ, 2006, IEEE T AUDIO SPEECH, V14, P873, DOI 10.1109/TSA.2005.858064
   Lafferty John, 2001, P 18 INT C MACH LEAR, V1, P282, DOI DOI 10.1038/NPR0T.2006.61
   Lamel L, 2002, COMPUT SPEECH LANG, V16, P115, DOI 10.1006/csla.2001.0186
   Lee C.H., 2004, P INTERSPEECH, P109
   Lee CH, 2000, P IEEE, V88, P1241, DOI 10.1109/5.880082
   Lee L., 2003, P IEEE INT C AC SPEE, V1, pI
   LEGGETTER CJ, 1995, COMPUT SPEECH LANG, V9, P171, DOI 10.1006/csla.1995.0010
   Lewis D., 1994, P INT C MACH LEARN
   Li JY, 2007, 2007 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING, VOLS 1 AND 2, P65, DOI 10.1109/ASRU.2007.4430085
   Li JY, 2009, COMPUT SPEECH LANG, V23, P389, DOI 10.1016/j.csl.2009.02.001
   Li X., 2009, P EMNLP
   Li X., 2007, P INT C ART INT STAT
   Li XW, 2007, IEEE T AUDIO SPEECH, V15, P2383, DOI 10.1109/TASL.2007.905151
   Lin H., 2009, P INTERSPEECH
   Lin H, 2009, INT CONF ACOUST SPEE, P4333, DOI 10.1109/ICASSP.2009.4960588
   Little R. J. A., 1987, STAT ANAL MISSING DA
   Liu C, 2011, IEEE T AUDIO SPEECH, V19, P2474, DOI 10.1109/TASL.2011.2144969
   Ma JZ, 2000, COMPUT SPEECH LANG, V14, P101, DOI 10.1006/csla.2000.0136
   Mak BKW, 2004, IEEE T SPEECH AUDI P, V12, P27, DOI 10.1109/TSA.2003.819951
   MANN GS, 2008, [No title captured]
   Mansour Y., 2009, P WORKSH COMP LEARN
   Mansour Y., 2009, P UNC ART INT
   McAllester D. A., 1998, P WORKSH COMP LEARN
   MCCALLUM A, 2006, [No title captured]
   McDermott E, 2007, IEEE T AUDIO SPEECH, V15, P203, DOI 10.1109/TASL.2006.876778
   Mesot B, 2007, IEEE T AUDIO SPEECH, V15, P1850, DOI 10.1109/TASL.2007.901312
   Miguel A, 2011, IEEE T AUDIO SPEECH, V19, P1476, DOI 10.1109/TASL.2010.2092764
   Mohamed A, 2010, P INTERSPEECH
   Mohamed AR, 2011, INT CONF ACOUST SPEE, P5060
   Mohamed AR, 2012, IEEE T AUDIO SPEECH, V20, P14, DOI 10.1109/TASL.2011.2109382
   Morgan N, 2005, IEEE SIGNAL PROC MAG, V22, P81, DOI 10.1109/MSP.2005.1511826
   Morgan N, 2012, IEEE T AUDIO SPEECH, V20, P7, DOI 10.1109/TASL.2011.2116010
   Morris J, 2006, INTERSPEECH 2006 AND 9TH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE PROCESSING, VOLS 1-5, P597
   Myrvoll T., 2000, P INT C SPOK LANG PR
   Neto J., 1995, P EUR
   Ngiam  Jiquan, 2011, P INT C MACH LEARN
   Nguyen H.T., 2004, P 21 INT C MACH LEAR, P623
   Nigam K, 2000, MACH LEARN, V39, P103, DOI 10.1023/A:1007692713085
   Nigam K., 2000, P INT C INF KNOWL MA
   Ostendorf M, 1996, IEEE T SPEECH AUDI P, V4, P360, DOI 10.1109/89.536930
   OSTENDORF M, 1992, P DARPA WORKSH CSR
   Ozkan E, 2009, IEEE T AUDIO SPEECH, V17, P1518, DOI 10.1109/TASL.2009.2022198
   Padmanabhan M, 1998, IEEE T SPEECH AUDI P, V6, P71, DOI 10.1109/89.650313
   Pernkopf F., 2005, P INT C MACH LEARN B
   PICONE J, 1999, ACOUST SPEECH SIG PR, P109
   Pinto J, 2011, IEEE T AUDIO SPEECH, V19, P225, DOI 10.1109/TASL.2010.2045943
   Povey D, 2002, INT CONF ACOUST SPEE, P105
   Pylkkonen J., 2012, IEEE Transactions on Audio, Speech and Language Processing, V20, P2409, DOI 10.1109/TASL.2012.2203805
   Rabiner L. R., 1993, FUNDAMENTALS SPEECH
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   Renals S, 1994, IEEE T SPEECH AUDI P, V2, P161, DOI 10.1109/89.260359
   Rennie SJ, 2010, IEEE SIGNAL PROC MAG, V27, P66, DOI 10.1109/MSP.2010.938081
   Riccardi G, 2005, IEEE T SPEECH AUDI P, V13, P504, DOI 10.1109/TSA.2005.848882
   Rosti A., 2004, P IEEE INT C AC SPEE, V1, pI
   Ruping S., 2001, P IEEE INT C DAT MIN
   Russell MJ, 2005, COMPUT SPEECH LANG, V19, P205, DOI 10.1016/j.csl.2004.08.001
   Sainath T., 2010, P INTERSPEECH
   Sainath TN, 2012, INT CONF ACOUST SPEE, P4153, DOI 10.1109/ICASSP.2012.6288833
   Sainath TN, 2011, IEEE T AUDIO SPEECH, V19, P2598, DOI 10.1109/TASL.2011.2155060
   Saon G., 2012, P INTERSPEECH
   Saon G, 2012, IEEE T AUDIO SPEECH, V20, P43, DOI 10.1109/TASL.2011.2129911
   Scheffer T., 2001, P INT C ADV INT DAT
   Schluter R, 2011, IEEE T AUDIO SPEECH, V19, P1103, DOI 10.1109/TASL.2010.2091635
   Schluter R, 2001, SPEECH COMMUN, V34, P287, DOI 10.1016/S0167-6393(00)00035-2
   Settles B., 2008, P EMNLP
   Settles Burr, 2010, 1648 U WISC
   Seung H.S., 1992, P ACM WORKSH COMP LE
   SHA F, 2007, [No title captured], V19, P1249
   Sha F, 2006, INT CONF ACOUST SPEE, P265
   Sim KC, 2007, COMPUT SPEECH LANG, V21, P669, DOI 10.1016/j.csl.2007.03.004
   Siniscalchi M., 2013, NEUROCOMPUTING
   Sivaram GSVS, 2012, IEEE T AUDIO SPEECH, V20, P23, DOI 10.1109/TASL.2011.2129510
   Sivaram G.S.V.S., 2010, P INTERSPEECH
   Stadermann J., 2004, P INTERSPEECH
   Stoyanov V., 2011, P AISTAT
   Subramanya A., 2009, P ADV NEUR INF PROC
   Sun JP, 2002, J ACOUST SOC AM, V111, P1086, DOI 10.1121/1.1420380
   Szummer M., 2001, P ADV NEUR INF PROC, V14
   Thrun Sebastian, 1998, LEARNING LEARN
   Tipping ME, 2001, J MACH LEARN RES, V1, P211, DOI 10.1162/15324430152748236
   TONG S, 2000, [No title captured], P999
   Tsochantaridis I., 2004, P INT C MACH LEARN
   Tuske Z., 2012, P INTERSPEECH
   Vapnik V. N., 1998, STAT LEARNING THEORY
   Vinyals O., 2012, P ADV NEUR INF PROC
   Wang C., 2009, P 21 INT JOINT C ART
   Wessel F, 2005, IEEE T SPEECH AUDI P, V13, P23, DOI 10.1109/TSA.2004.838537
   Weston J., 1999, 7th European Symposium on Artificial Neural Networks. ESANN'99. Proceedings, P219
   Woodland P., 1996, COMPUT SPEECH LANG, V10
   Woodland PC, 2002, COMPUT SPEECH LANG, V16, P25, DOI 10.1006/csla.2001.0182
   Wu P., 2004, P INT C MACH LEARN
   Xiao L, 2010, IEEE SIGNAL PROC MAG, V27, P118, DOI 10.1109/MSP.2010.938085
   Xue Y, 2007, J MACH LEARN RES, V8, P35
   Yaman S, 2008, IEEE T AUDIO SPEECH, V16, P1207, DOI 10.1109/TASL.2008.2001106
   Yarowsky D, 1995, P ACL, P189, DOI DOI 10.3115/981658.981684
   YU D, 2009, [No title captured], P676
   Yu D., 2010, P NIPS WORKSH DEEP L
   Yu D., 2012, P INTERSPEECH
   Yu D., 2010, P IEEE INT C AC SPEE
   Yu D, 2012, INT CONF ACOUST SPEE, P4409, DOI 10.1109/ICASSP.2012.6288897
   Yu D, 2012, INT CONF ACOUST SPEE, P4169, DOI 10.1109/ICASSP.2012.6288837
   Yu D, 2013, IEEE T AUDIO SPEECH, V21, P388, DOI 10.1109/TASL.2012.2227738
   Yu D, 2010, COMPUT SPEECH LANG, V24, P433, DOI 10.1016/j.csl.2009.03.004
   Yu D, 2006, INTERSPEECH 2006 AND 9TH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE PROCESSING, VOLS 1-5, P2418
   Yu D, 2009, INT CONF ACOUST SPEE, P4193, DOI 10.1109/ICASSP.2009.4960553
   Yu D, 2008, COMPUT SPEECH LANG, V22, P415, DOI 10.1016/j.csl.2008.03.002
   Yu K., 2005, P INT C MACH LEARN
   Zavaliagkos G., 1995, P EUR
   Zen H., 2004, ISCA SPEECH SYNTH WO, P191
   Zen H, 2012, IEEE T AUDIO SPEECH, V20, P794, DOI 10.1109/TASL.2011.2165280
   Zhang L, 2008, IEEE SIGNAL PROC LET, V15, P245, DOI 10.1109/LSP.2008.917004
   Zhang S., 2010, IEEE SIGNAL PROCESS, V17
   Zhang SX, 2013, IEEE T AUDIO SPEECH, V21, P544, DOI 10.1109/TASL.2012.2227734
   Zhang YD, 2011, INT CONF ACOUST SPEE, P5608
   Zhou D. Y., 2003, P ADV NEURAL INFORM
   Zhu X., 2003, P INT C MACH LEARN
   Zweig G., 2010, P INTERSPEECH
NR 273
TC 82
Z9 85
U1 2
U2 72
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1558-7916
EI 1558-7924
J9 IEEE T AUDIO SPEECH
JI IEEE Trans. Audio Speech Lang. Process.
PD MAY
PY 2013
VL 21
IS 5
BP 1060
EP 1089
DI 10.1109/TASL.2013.2244083
PG 30
WC Acoustics; Engineering, Electrical & Electronic
SC Acoustics; Engineering
GA 164ZD
UT WOS:000320450900001
DA 2020-02-19
ER

PT J
AU Zhang, XL
   Wu, J
AF Zhang, Xiao-Lei
   Wu, Ji
TI Deep Belief Networks Based Voice Activity Detection
SO IEEE TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING
LA English
DT Article
DE Deep learning; information fusion; voice activity detection
ID STATISTICAL-MODEL; MULTIPITCH TRACKING; ALGORITHM; CLASSIFICATION;
   SEGREGATION; NOISY
AB Fusing the advantages of multiple acoustic features is important for the robustness of voice activity detection (VAD). Recently, the machine-learning-based VADs have shown a superiority to traditional VADs on multiple feature fusion tasks. However, existing machine-learning-based VADs only utilize shallow models, which cannot explore the underlying manifold of the features. In this paper, we propose to fuse multiple features via a deep model, called deep belief network (DBN). DBN is a powerful hierarchical generative model for feature extraction. It can describe highly variant functions and discover the manifold of the features. We take the multiple serially-concatenated features as the input layer of DBN, and then extract a new feature by transferring these features through multiple nonlinear hidden layers. Finally, we predict the class of the new feature by a linear classifier. We further analyze that even a single-hidden-layer-based belief network is as powerful as the state-of-the-art models in the machine-learning-based VADs. In our empirical comparison, ten common features are used for performance analysis. Extensive experimental results on the AURORA2 corpus show that the DBN-based VAD not only outperforms eleven referenced VADs, but also can meet the real-time detection demand of VAD. The results also show that the DBN-based VAD can fuse the advantages of multiple features effectively.
C1 [Zhang, Xiao-Lei; Wu, Ji] Tsinghua Univ, Dept Elect Engn, Multimedia Signal & Intelligent Informat Proc Lab, Tsinghua Natl Lab Informat Sci & Technol, Beijing 100084, Peoples R China.
RP Zhang, XL (reprint author), Tsinghua Univ, Dept Elect Engn, Multimedia Signal & Intelligent Informat Proc Lab, Tsinghua Natl Lab Informat Sci & Technol, Beijing 100084, Peoples R China.
EM huoshan6@126.com; wuji_ee@tsinghua.edu.cn
FU National High-Tech. R&D Program of China (863 Program) [2012AA011004];
   National Natural Science Funds of ChinaNational Natural Science
   Foundation of China [61170197]; Planned Science and Technology Project
   of Tsinghua University [20111081023]; China Postdoctoral Science
   FoundationChina Postdoctoral Science Foundation [2012M520278]
FX Manuscript received June 26, 2012; revised October 02, 2012; accepted
   November 19, 2012. Date of publication November 27, 2012; date of
   current version January 11, 2013. This work was supported in part by the
   National High-Tech. R&D Program of China (863 Program) under Grant
   2012AA011004, in part by the National Natural Science Funds of China
   under Grant 61170197, in part by the Planned Science and Technology
   Project of Tsinghua University under Grant 20111081023, and in part by
   the China Postdoctoral Science Foundation funded project under Grant
   2012M520278. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. DeLiang Wang.
CR [Anonymous], 2004, TIAEIAIS127 3GPP2 CS
   [Anonymous], ETSI ES, V202
   Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153
   Bengio Y., 2011, P ICML WORKSH UNS TR, V7, P1
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Benyassine A, 1997, IEEE COMMUN MAG, V35, P64, DOI 10.1109/35.620527
   Bregman AS, 1994, AUDITORY SCENE ANAL
   Carreira-Perpinan M.A., 2005, ARTIF INTELL STAT, P17
   Chang JH, 2006, IEEE T SIGNAL PROCES, V54, P1965, DOI 10.1109/TSP.2006.874403
   Chen M., 2012, P AS C SIGN SYST COM, P1
   Coates A., 2011, ADV NEURAL INFORM PR, P2528
   Collobert R., 2008, P 25 ICML, V25, P160, DOI [10.1145/1390156.1390177, DOI 10.1145/1390156.1390177]
   Cournapeau D, 2010, IEEE J-STSP, V4, P1071, DOI 10.1109/JSTSP.2010.2080821
   Dahl GE, 2012, IEEE T AUDIO SPEECH, V20, P30, DOI 10.1109/TASL.2011.2134090
   Dong EQ, 2002, 2002 6TH INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING PROCEEDINGS, VOLS I AND II, P1124, DOI 10.1109/ICOSP.2002.1179987
   Dong Y, 2012, P 29 INT C MACH LEAR, P1
   Ellis D., 2005, PLP RASTA MFCC INVER
   Erhan D, 2010, J MACH LEARN RES, V11, P625
   Gazor S, 2003, IEEE T SPEECH AUDI P, V11, P498, DOI 10.1109/TSA.2003.815518
   Han K., 2012, IEEE T AUDIO SPEECH, V21, P1
   Hinton G., 2012, NEURAL NETWORKS TRIC, V9, P599, DOI DOI 10.1007/978-3-642-35289-8_32
   Hinton G., 2012, IEEE SIGNAL PROCESS, V11, P229
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hsu C-W, 2003, PRACTICAL GUIDE SUPP
   Hsu CL, 2012, IEEE T AUDIO SPEECH, V20, P1482, DOI 10.1109/TASL.2011.2182510
   Hu GN, 2004, IEEE T NEURAL NETWOR, V15, P1135, DOI 10.1109/TNN.2004.832812
   Hu GN, 2010, IEEE T AUDIO SPEECH, V18, P2067, DOI 10.1109/TASL.2010.2041110
   Hu K, 2013, IEEE T AUDIO SPEECH, V21, P120, DOI 10.1109/TASL.2012.2215591
   Hu K, 2011, IEEE T AUDIO SPEECH, V19, P1600, DOI 10.1109/TASL.2010.2093893
   Jin R., 2010, P 27 INT C MACH LEAR, P1175
   Jin ZZ, 2011, IEEE T AUDIO SPEECH, V19, P2328, DOI 10.1109/TASL.2011.2134086
   Jin ZZ, 2011, IEEE T AUDIO SPEECH, V19, P1091, DOI 10.1109/TASL.2010.2077280
   Jin ZZ, 2009, IEEE T AUDIO SPEECH, V17, P625, DOI 10.1109/TASL.2008.2010633
   Jo QH, 2009, IET SIGNAL PROCESS, V3, P205, DOI 10.1049/iet-spr.2008.0128
   Joachims T, 2009, MACH LEARN, V76, P179, DOI 10.1007/s10994-009-5126-6
   Kang SI, 2008, IEEE SIGNAL PROC LET, V15, P170, DOI 10.1109/LSP.2007.913595
   Kim G, 2009, J ACOUST SOC AM, V126, P1486, DOI 10.1121/1.3184603
   Koller D., 2009, PROBABILISTIC GRAPHI
   Le Q., 2011, P 29 INT C MACH LEAR, P1
   Mohamed A.R., 2010, P INTERSPEECH, P2846
   Mohamed AR, 2012, IEEE T AUDIO SPEECH, V20, P14, DOI 10.1109/TASL.2011.2109382
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Pearce D., 2000, P ICSLP, V4, P29
   Petsatodis T, 2011, IEEE T AUDIO SPEECH, V19, P2314, DOI 10.1109/TASL.2011.2131131
   Ramirez J, 2006, ELECTRON LETT, V42, P426, DOI 10.1049/el:20064068
   Ramirez J, 2005, IEEE SIGNAL PROC LET, V12, P689, DOI 10.1109/LSP.2005.855551
   Ramirez J, 2004, SPEECH COMMUN, V42, P271, DOI 10.1016/j.specom.2003.10.002
   Ramirez J, 2007, IEEE T AUDIO SPEECH, V15, P2177, DOI 10.1109/TASL.2007.903937
   Sha F., 2012, TUTORIAL INTERSPEECH, P1
   Shin JW, 2010, COMPUT SPEECH LANG, V24, P515, DOI 10.1016/j.csl.2009.02.003
   Sohn J, 1999, IEEE SIGNAL PROC LET, V6, P1, DOI 10.1109/97.736233
   Suh Y, 2012, IEEE SIGNAL PROC LET, V19, P507, DOI 10.1109/LSP.2012.2204978
   Sun XJ, 2002, INT CONF ACOUST SPEE, P333
   Tahmasbi R, 2007, IEEE T AUDIO SPEECH, V15, P1129, DOI 10.1109/TASL.2007.894521
   Tchorz J, 2003, IEEE T SPEECH AUDI P, V11, P184, DOI 10.1109/TSA.2003.811542
   Vincent P, 2008, P 25 INT C MACH LEAR, P1096, DOI DOI 10.1145/1390156.1390294
   Wang D.L., 2006, COMPUTATIONAL AUDITO
   Wang DL, 2005, IEEE T NEURAL NETWOR, V16, P1401, DOI 10.1109/TNN.2005.852235
   WANG DL, 1995, IEEE T NEURAL NETWOR, V6, P283, DOI 10.1109/72.363423
   Wang Y, 2012, PROCEEDINGS OF THE ASME INTERNATIONAL MECHANICAL ENGINEERING CONGRESS AND EXPOSITION 2010, VOL 11, P1
   Wang YX, 2013, IEEE T AUDIO SPEECH, V21, P270, DOI 10.1109/TASL.2012.2221459
   Wu J, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 3 AND 4, P3090
   Wu J, 2011, EURASIP J ADV SIG PR, DOI 10.1186/1687-6180-2011-18
   Wu J, 2011, IEEE SIGNAL PROC LET, V18, P466, DOI 10.1109/LSP.2011.2159374
   Wu J, 2011, IEEE SIGNAL PROC LET, V18, P283, DOI 10.1109/LSP.2011.2119482
   Wu MY, 2003, IEEE T SPEECH AUDI P, V11, P229, DOI 10.1109/TSA.2003.811539
   Ying DW, 2011, IEEE T AUDIO SPEECH, V19, P2624, DOI 10.1109/TASL.2011.2125953
   Yu D., 2009, P NIPS WORKSH, P1
   Yu D, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 3 AND 4, P2986
   Yu D, 2011, IEEE SIGNAL PROC MAG, V28, P145, DOI 10.1109/MSP.2010.939038
   Yu T, 2010, IEEE SIGNAL PROC LET, V17, P897, DOI 10.1109/LSP.2010.2066561
   Zhang XL, 2012, IEEE T SYST MAN CY B, V42, P1669, DOI 10.1109/TSMCB.2012.2197824
NR 73
TC 143
Z9 157
U1 2
U2 95
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1558-7916
EI 1558-7924
J9 IEEE T AUDIO SPEECH
JI IEEE Trans. Audio Speech Lang. Process.
PD APR
PY 2013
VL 21
IS 4
BP 697
EP 710
DI 10.1109/TASL.2012.2229986
PG 14
WC Acoustics; Engineering, Electrical & Electronic
SC Acoustics; Engineering
GA 072ZB
UT WOS:000313711600002
DA 2020-02-19
ER

PT J
AU Ji, SW
   Xu, W
   Yang, M
   Yu, K
AF Ji, Shuiwang
   Xu, Wei
   Yang, Ming
   Yu, Kai
TI 3D Convolutional Neural Networks for Human Action Recognition
SO IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE
LA English
DT Article
DE Deep learning; convolutional neural networks; 3D convolution; model
   combination; action recognition
ID FEATURES
AB We consider the automated recognition of human actions in surveillance videos. Most current methods build classifiers based on complex handcrafted features computed from the raw inputs. Convolutional neural networks (CNNs) are a type of deep model that can act directly on the raw inputs. However, such models are currently limited to handling 2D inputs. In this paper, we develop a novel 3D CNN model for action recognition. This model extracts features from both the spatial and the temporal dimensions by performing 3D convolutions, thereby capturing the motion information encoded in multiple adjacent frames. The developed model generates multiple channels of information from the input frames, and the final feature representation combines information from all channels. To further boost the performance, we propose regularizing the outputs with high-level features and combining the predictions of a variety of different models. We apply the developed models to recognize human actions in the real-world environment of airport surveillance videos, and they achieve superior performance in comparison to baseline methods.
C1 [Ji, Shuiwang] Old Dominion Univ, Dept Comp Sci, Norfolk, VA 23529 USA.
   [Xu, Wei] Facebook Inc, Menlo Pk, CA 94304 USA.
   [Yang, Ming] NEC Labs Amer Inc, Cupertino, CA 95014 USA.
   [Yu, Kai] Baidu Inc, Beijing 100085, Peoples R China.
RP Ji, SW (reprint author), Old Dominion Univ, Dept Comp Sci, Suite 3300,4700 Elkhorn Ave, Norfolk, VA 23529 USA.
EM sji@cs.odu.edu; emailweixu@fb.com; myang@nec-labs.com; yukai@baidu.com
RI Yang, Ming-Hsuan/T-9533-2019; Yang, Ming-Hsuan/AAE-7350-2019
OI Yang, Ming-Hsuan/0000-0003-4848-2304; 
CR Ahmed A, 2008, LECT NOTES COMPUT SC, V5304, P69, DOI 10.1007/978-3-540-88690-7_6
   Bengio Y., 2007, LARGE SCALE KERNEL M
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Bromley J., 1994, ADV NEURAL INFORM PR, P737
   Cecotti H, 2011, IEEE T PATTERN ANAL, V33, P433, DOI 10.1109/TPAMI.2010.125
   Collobert R., 2008, P 25 ICML, V25, P160, DOI [10.1145/1390156.1390177, DOI 10.1145/1390156.1390177]
   Delaitre V., 2010, P 21 BRIT MACH VIS C
   Dollar P., 2005, P IEEE INT WORKSH VI, P65
   Duchenne O, 2009, IEEE I CONF COMP VIS, P1491, DOI 10.1109/ICCV.2009.5459279
   Efros AA, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P726
   Fan JL, 2010, IEEE T NEURAL NETWOR, V21, P1610, DOI 10.1109/TNN.2010.2066286
   Freund Y., 1996, Machine Learning. Proceedings of the Thirteenth International Conference (ICML '96), P148
   HANSEN LK, 1990, IEEE T PATTERN ANAL, V12, P993, DOI 10.1109/34.58871
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Jain V., 2009, ADV NEURAL INFORM PR, P769
   Jain V., 2007, P 11 IEEE INT C COMP
   Jarrett K., 2009, P 12 IEEE INT C COMP
   Jhuang H, 2007, IEEE I CONF COMP VIS, P1253
   Ji Shuiwang, 2010, P INT C MACH LEARN, P495
   Junejo IN, 2011, IEEE T PATTERN ANAL, V33, P172, DOI 10.1109/TPAMI.2010.68
   Kim HJ, 2007, LECT NOTES COMPUT SC, V4492, P715
   Kittler J, 1998, IEEE T PATTERN ANAL, V20, P226, DOI 10.1109/34.667881
   Kusy B, 2009, 2009 INTERNATIONAL CONFERENCE ON INFORMATION PROCESSING IN SENSOR NETWORKS (IPSN 2009), P109
   Laptev I, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P432
   Laptev I., 2008, P IEEE C COMP VIS PA
   Laptev I, 2007, IEEE I CONF COMP VIS, P2165
   Lazebnik S., 2006, P IEEE INT C COMP VI, V2, P2169, DOI DOI 10.1109/CVPR.2006.68
   Le Q. V., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3361, DOI 10.1109/CVPR.2011.5995496
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y., 2004, IEEE T IMAGE PROCESS, V14, P1360
   LeCun Y., 1998, NEURAL NETWORKS TRIC
   Lee H., 2009, P 26 ANN INT C MACH, P609, DOI DOI 10.1145/1553374.1553453
   Liu JG, 2009, PROC CVPR IEEE, P1996
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Marszalek M., 2009, P IEEE C COMP VIS PA, P2929, DOI DOI 10.1109/CVPR.2009.5206557
   Mobahi H., 2009, P 26 ANN INT C MACH, P737
   Mutch J, 2008, INT J COMPUT VISION, V80, P45, DOI 10.1007/s11263-007-0118-0
   Niebles JC, 2008, INT J COMPUT VISION, V79, P299, DOI 10.1007/s11263-007-0122-4
   Norouzi M. R. M., 2009, P IEEE C COMP VIS PA
   Ranzato M.A., 2007, P IEEE C COMP VIS PA
   Schindler K, 2008, P IEEE C COMP VIS PA
   Schuldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462
   Serre T, 2007, IEEE T PATTERN ANAL, V29, P411, DOI 10.1109/TPAMI.2007.56
   Taylor GW, 2010, LECT NOTES COMPUT SC, V6316, P140, DOI 10.1007/978-3-642-15567-3_11
   Turaga SC, 2010, NEURAL COMPUT, V22, P511, DOI 10.1162/neco.2009.10-08-881
   Wang H, 2009, P BRIT MACH VIS C, P127
   Wang Y, 2011, IEEE T PATTERN ANAL, V33, P1310, DOI 10.1109/TPAMI.2010.214
   Wang Y, 2009, PROC CVPR IEEE, P872, DOI 10.1109/CVPRW.2009.5206709
   Yang M, 2009, P TREC VID RETR EV W
   Yang M, 2009, IEEE I CONF COMP VIS, P1554, DOI 10.1109/ICCV.2009.5459252
   Yu K., 2009, P ADV NEUR INF PROC, P1889
   Zhu Guan, 2009, P165, DOI 10.1007/978-3-540-74042-1_5
NR 54
TC 1401
Z9 1496
U1 119
U2 857
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 0162-8828
EI 1939-3539
J9 IEEE T PATTERN ANAL
JI IEEE Trans. Pattern Anal. Mach. Intell.
PD JAN
PY 2013
VL 35
IS 1
BP 221
EP 231
DI 10.1109/TPAMI.2012.59
PG 11
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
SC Computer Science; Engineering
GA 037SV
UT WOS:000311127700019
PM 22392705
HC Y
HP N
DA 2020-02-19
ER

PT J
AU Horita, T
   Takanami, I
AF Horita, T.
   Takanami, I.
TI An FPGA-based multiple-weight-and-neuron-fault tolerant digital
   multilayer perceptron
SO NEUROCOMPUTING
LA English
DT Article
DE Multilayer perceptron; Fault tolerance; Weight fault; Neuron fault;
   VHDL; FPGA
ID NETWORKS
AB A digital multilayer perceptron (DMLP) which is tolerant to simultaneous weight and neuron faults is implemented in an FPGA, where the weight faults are assumed to be between the hidden and output layers and the neuron faults are assumed to be in the hidden and output layers. In the implementation, a multilayer perceptron (MLP) trained by the deep learning method 111 is used to cope with the weight faults and the neuron faults in the hidden layer, and an error detecting and correcting code SECDED is used to cope with the neuron faults in the output layer. The implementation process named "FTDMLP-gene" is proposed which consists of three parts; the deep learning method, the VHDL source file generator and the outline of VHDL notation which describes an FTDMLP (fault-tolerant DMLP). The fault-tolerant ability of the FTDMLP implemented is shown. Further, The FTDMLP and the corresponding non-fault tolerant DMLP are compared in terms of hardware size, computing speed and electricity consumption.
   This paper is the extension of [2,3]. (C) 2012 Elsevier B.V. All rights reserved.
C1 [Horita, T.; Takanami, I.] Polytech Univ, Sagamihara, Kanagawa 2520132, Japan.
RP Horita, T (reprint author), Polytech Univ, Hashimoto Dai 4-1-1, Sagamihara, Kanagawa 2520132, Japan.
EM horita@uitec.ac.jp
CR Bettola S, 1998, IEEE T COMPUT, V47, P357, DOI 10.1109/12.660173
   HORITA T, 2005, P ICONIP 2005, P570
   Horita T, 2006, INT SYM DEFEC FAU TO, P554, DOI 10.1109/DFT.2006.8
   Horito T, 2008, IEICE T INF SYST, VE91D, P1168, DOI 10.1093/ietisy/e91-d.4.1168
   Sugawara E, 2004, IEICE T INF SYST, VE87D, P2021
   Takanami I, 2003, IEICE T INF SYST, VE86D, P2536
NR 6
TC 4
Z9 4
U1 0
U2 27
PU ELSEVIER
PI AMSTERDAM
PA RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
SN 0925-2312
EI 1872-8286
J9 NEUROCOMPUTING
JI Neurocomputing
PD JAN 1
PY 2013
VL 99
BP 570
EP 574
DI 10.1016/j.neucom.2012.07.001
PG 5
WC Computer Science, Artificial Intelligence
SC Computer Science
GA 037TL
UT WOS:000311129300056
DA 2020-02-19
ER

EF